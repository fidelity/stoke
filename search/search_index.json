{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Add a little accelerant to your torch About stoke is a lightweight wrapper for PyTorch that provides a simple declarative API for context switching between devices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU with extensions (like optimizer state sharding) by simply changing a few declarative flags. Additionally, stoke exposes configuration settings for every underlying backend for those that want configurability and raw access to the underlying libraries. In short, stoke is the best of PyTorch Lightning Accelerators disconnected from the rest of PyTorch Lightning. Write whatever PyTorch code you want, but leave device and backend context switching to stoke . Supports Devices: CPU, GPU, multi-GPU Distributed: DDP , Horovod , deepspeed (via DDP) Mixed-Precision: AMP , Nvidia Apex , deepspeed (custom APEX like backend) Extensions: fairscale (Optimizer State Sharding, Sharded DDP, Fully Sharded DDP), deepspeed (ZeRO Stage 0-3, etc.) Benefits/Capabilities Declarative style API -- allows you to declare or specify the desired state and let stoke handle the rest Mirrors base PyTorch style model , loss , backward , and step calls Automatic device placement of model(s) and data Universal interface for saving and loading regardless of backend(s) or device Automatic handling of gradient accumulation and clipping Common attrs interface for all backend configuration parameters (with docstrings) Helper methods for printing synced losses, device specific print, number of model parameters Extra(s) - Custom torch.utils.data.distributed.Sampler: BucketedDistributedSampler which buckets data by a sorted idx and then randomly samples from specific bucket(s) to prevent situations like grossly mismatched sequence length leading to wasted computational overhead (ie excess padding)","title":"Home"},{"location":"#about","text":"stoke is a lightweight wrapper for PyTorch that provides a simple declarative API for context switching between devices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU with extensions (like optimizer state sharding) by simply changing a few declarative flags. Additionally, stoke exposes configuration settings for every underlying backend for those that want configurability and raw access to the underlying libraries. In short, stoke is the best of PyTorch Lightning Accelerators disconnected from the rest of PyTorch Lightning. Write whatever PyTorch code you want, but leave device and backend context switching to stoke .","title":"About"},{"location":"#supports","text":"Devices: CPU, GPU, multi-GPU Distributed: DDP , Horovod , deepspeed (via DDP) Mixed-Precision: AMP , Nvidia Apex , deepspeed (custom APEX like backend) Extensions: fairscale (Optimizer State Sharding, Sharded DDP, Fully Sharded DDP), deepspeed (ZeRO Stage 0-3, etc.)","title":"Supports"},{"location":"#benefitscapabilities","text":"Declarative style API -- allows you to declare or specify the desired state and let stoke handle the rest Mirrors base PyTorch style model , loss , backward , and step calls Automatic device placement of model(s) and data Universal interface for saving and loading regardless of backend(s) or device Automatic handling of gradient accumulation and clipping Common attrs interface for all backend configuration parameters (with docstrings) Helper methods for printing synced losses, device specific print, number of model parameters Extra(s) - Custom torch.utils.data.distributed.Sampler: BucketedDistributedSampler which buckets data by a sorted idx and then randomly samples from specific bucket(s) to prevent situations like grossly mismatched sequence length leading to wasted computational overhead (ie excess padding)","title":"Benefits/Capabilities"},{"location":"CONTRIBUTING/","text":"Contributing We welcome all contributions from the community! Any contributions to stoke should come through valid Pull Requests in the public open-source repository. Contribution Guidelines Adhere to PEP-8 standards. Run black and isort linters before creating a PR. Any changes to core functionality must pass all existing unit tests. Additional functionality should have associated unit tests. Provide documentation ( Numpy Docstring format ) whenever possible, even for simple functions or classes.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We welcome all contributions from the community! Any contributions to stoke should come through valid Pull Requests in the public open-source repository.","title":"Contributing"},{"location":"CONTRIBUTING/#contribution-guidelines","text":"Adhere to PEP-8 standards. Run black and isort linters before creating a PR. Any changes to core functionality must pass all existing unit tests. Additional functionality should have associated unit tests. Provide documentation ( Numpy Docstring format ) whenever possible, even for simple functions or classes.","title":"Contribution Guidelines"},{"location":"docs/Examples/","text":"Examples The following examples demonstrate the basic design patterns for using stoke and switching between different configurations: CIFAR10 HuggingFace BERT -- Coming Soon!","title":"Examples"},{"location":"docs/Examples/#examples","text":"The following examples demonstrate the basic design patterns for using stoke and switching between different configurations: CIFAR10 HuggingFace BERT -- Coming Soon!","title":"Examples"},{"location":"docs/Home/","text":"Add a little accelerant to your torch About stoke is a lightweight wrapper for PyTorch that provides a simple declarative API for context switching between devices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU with extensions (like optimizer state sharding) by simply changing a few declarative flags. Additionally, stoke exposes configuration settings for every underlying backend for those that want configurability and raw access to the underlying libraries. In short, stoke is the best of PyTorch Lightning Accelerators disconnected from the rest of PyTorch Lightning. Write whatever PyTorch code you want, but leave device and backend context switching to stoke . Supports Devices: CPU, GPU, multi-GPU Distributed: DDP , Horovod , deepspeed (via DDP) Mixed-Precision: AMP , Nvidia Apex , deepspeed (custom APEX like backend) Extensions: fairscale (Optimizer State Sharding, Sharded DDP, Fully Sharded DDP), deepspeed (ZeRO Stage 0-3, etc.) Benefits/Capabilities Declarative style API -- allows you to declare or specify the desired state and let stoke handle the rest Mirrors base PyTorch style model , loss , backward , and step calls Automatic device placement of model(s) and data Universal interface for saving and loading regardless of backend(s) or device Automatic handling of gradient accumulation and clipping Common attrs interface for all backend configuration parameters (with docstrings) Helper methods for printing synced losses, device specific print, number of model parameters Extra(s) - Custom torch.utils.data.distributed.Sampler: BucketedDistributedSampler which buckets data by a sorted idx and then randomly samples from specific bucket(s) to prevent situations like grossly mismatched sequence length leading to wasted computational overhead (ie excess padding)","title":"Home"},{"location":"docs/Home/#about","text":"stoke is a lightweight wrapper for PyTorch that provides a simple declarative API for context switching between devices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU with extensions (like optimizer state sharding) by simply changing a few declarative flags. Additionally, stoke exposes configuration settings for every underlying backend for those that want configurability and raw access to the underlying libraries. In short, stoke is the best of PyTorch Lightning Accelerators disconnected from the rest of PyTorch Lightning. Write whatever PyTorch code you want, but leave device and backend context switching to stoke .","title":"About"},{"location":"docs/Home/#supports","text":"Devices: CPU, GPU, multi-GPU Distributed: DDP , Horovod , deepspeed (via DDP) Mixed-Precision: AMP , Nvidia Apex , deepspeed (custom APEX like backend) Extensions: fairscale (Optimizer State Sharding, Sharded DDP, Fully Sharded DDP), deepspeed (ZeRO Stage 0-3, etc.)","title":"Supports"},{"location":"docs/Home/#benefitscapabilities","text":"Declarative style API -- allows you to declare or specify the desired state and let stoke handle the rest Mirrors base PyTorch style model , loss , backward , and step calls Automatic device placement of model(s) and data Universal interface for saving and loading regardless of backend(s) or device Automatic handling of gradient accumulation and clipping Common attrs interface for all backend configuration parameters (with docstrings) Helper methods for printing synced losses, device specific print, number of model parameters Extra(s) - Custom torch.utils.data.distributed.Sampler: BucketedDistributedSampler which buckets data by a sorted idx and then randomly samples from specific bucket(s) to prevent situations like grossly mismatched sequence length leading to wasted computational overhead (ie excess padding)","title":"Benefits/Capabilities"},{"location":"docs/Installation/","text":"Installation Requirements Python: 3.6+ Pip Dependencies: attrs, deepspeed, fairscale, horovod, mypy_extensions (python_version < '3.8'), torch Optional Pip Dependencies: mpi4py Build Dependencies: apex (NVIDIA) Tested OS: Unix (Ubuntu 16.04, Ubuntu 18.04), OSX (10.14.6) (Required for FP16 Support) Install NVIDIA Apex If you are planning on using mixed-precision (aka FP16), please install Apex so that stoke supports all FP16 methods. If you are not planning on using mixed precision, this step can actually be skipped (as all imports are in a try/except and are only conditionally imported). Follow the instructions here . (Optional) OpenMPI Support Follow the instructions here or here Also, refer to the Dockerfile here PyPi pip install stoke PyPi From Source pip install git+https://github.com/fidelity/stoke Build From Source git clone https://github.com/fidelity/stoke cd stoke pip install setuptools wheel python setup.py bdist_wheel pip install /dist/stoke-X.X.XxX-py3-none-any.whl","title":"Installation"},{"location":"docs/Installation/#installation","text":"","title":"Installation"},{"location":"docs/Installation/#requirements","text":"Python: 3.6+ Pip Dependencies: attrs, deepspeed, fairscale, horovod, mypy_extensions (python_version < '3.8'), torch Optional Pip Dependencies: mpi4py Build Dependencies: apex (NVIDIA) Tested OS: Unix (Ubuntu 16.04, Ubuntu 18.04), OSX (10.14.6)","title":"Requirements"},{"location":"docs/Installation/#required-for-fp16-support-install-nvidia-apex","text":"If you are planning on using mixed-precision (aka FP16), please install Apex so that stoke supports all FP16 methods. If you are not planning on using mixed precision, this step can actually be skipped (as all imports are in a try/except and are only conditionally imported). Follow the instructions here .","title":"(Required for FP16 Support) Install NVIDIA Apex"},{"location":"docs/Installation/#optional-openmpi-support","text":"Follow the instructions here or here Also, refer to the Dockerfile here","title":"(Optional) OpenMPI Support"},{"location":"docs/Installation/#pypi","text":"pip install stoke","title":"PyPi"},{"location":"docs/Installation/#pypi-from-source","text":"pip install git+https://github.com/fidelity/stoke","title":"PyPi From Source"},{"location":"docs/Installation/#build-from-source","text":"git clone https://github.com/fidelity/stoke cd stoke pip install setuptools wheel python setup.py bdist_wheel pip install /dist/stoke-X.X.XxX-py3-none-any.whl","title":"Build From Source"},{"location":"docs/Launchers/","text":"Launchers Stoke supports the following launchers... PyTorch DDP Prefer the torch.distributed.launch utility described here (Note: the local_rank requirement propagates through to stoke ) python -m torch.distributed.launch, ' --nproc_per_node = NUM_GPUS_YOU_HAVE, --use_env Horovod Refer to the docs here horovodrun -np 4 -H localhost:4 python train.py or horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py Horovod w/ OpenMPI Refer to the docs here . Can also be used with k8s via the MPI Operator mpirun -np 4 \\ --allow-run-as-root -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py or mpirun -np 16 \\ -H server1:4,server2:4,server3:4,server4:4 \\ -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py Deepspeed w/ OpenMPI Prefer the OpenMPI version here over the native launcher. Deepspeed will automatically discover devices, etc. via mpi4py. Can also be used with k8s via the MPI Operator mpirun -np 4 \\ --allow-run-as-root -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py or mpirun -np 16 \\ -H server1:4,server2:4,server3:4,server4:4 \\ -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py PyTorch DDP w/ OpenMPI Leverage Deepspeed functionality to automatically discover devices, etc. via mpi4py. Can also be used with k8s via the MPI Operator mpirun -np 4 \\ --allow-run-as-root -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py or mpirun -np 16 \\ -H server1:4,server2:4,server3:4,server4:4 \\ -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py","title":"Launchers"},{"location":"docs/Launchers/#launchers","text":"Stoke supports the following launchers...","title":"Launchers"},{"location":"docs/Launchers/#pytorch-ddp","text":"Prefer the torch.distributed.launch utility described here (Note: the local_rank requirement propagates through to stoke ) python -m torch.distributed.launch, ' --nproc_per_node = NUM_GPUS_YOU_HAVE, --use_env","title":"PyTorch DDP"},{"location":"docs/Launchers/#horovod","text":"Refer to the docs here horovodrun -np 4 -H localhost:4 python train.py or horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py","title":"Horovod"},{"location":"docs/Launchers/#horovod-w-openmpi","text":"Refer to the docs here . Can also be used with k8s via the MPI Operator mpirun -np 4 \\ --allow-run-as-root -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py or mpirun -np 16 \\ -H server1:4,server2:4,server3:4,server4:4 \\ -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py","title":"Horovod w/ OpenMPI"},{"location":"docs/Launchers/#deepspeed-w-openmpi","text":"Prefer the OpenMPI version here over the native launcher. Deepspeed will automatically discover devices, etc. via mpi4py. Can also be used with k8s via the MPI Operator mpirun -np 4 \\ --allow-run-as-root -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py or mpirun -np 16 \\ -H server1:4,server2:4,server3:4,server4:4 \\ -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py","title":"Deepspeed w/ OpenMPI"},{"location":"docs/Launchers/#pytorch-ddp-w-openmpi","text":"Leverage Deepspeed functionality to automatically discover devices, etc. via mpi4py. Can also be used with k8s via the MPI Operator mpirun -np 4 \\ --allow-run-as-root -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py or mpirun -np 16 \\ -H server1:4,server2:4,server3:4,server4:4 \\ -bind-to none -map-by slot \\ -x NCCL_DEBUG = INFO -x LD_LIBRARY_PATH -x PATH \\ -mca pml ob1 -mca btl ^openib \\ python train.py","title":"PyTorch DDP w/ OpenMPI"},{"location":"docs/Quick-Start/","text":"Quick Start This is a quick and dirty guide to getting up and running with stoke . Read additional documentation for full details and/or refer to the examples here . Basic Definitions Assuming some already existing common PyTorch objects (dataset: torch.utils.data.Dataset , model: torch.nn.Module , loss: torch.nn.(SomeLossFunction) ): import torch # Some existing user defined dataset using torch.utils.data.Dataset class RandomData ( torch . utils . data . Dataset ): pass # An existing model defined with torch.nn.Module class BasicNN ( torch . nn . Module ): pass # Our existing dataset from above dataset = RandomData ( ... ) # Our existing model from above model = BasicNN ( ... ) # A loss function loss = torch . nn . BCEWithLogitsLoss () Optimizer Setup stoke requires a slightly different way to define the optimizer (as it handles instantiation internally) by using StokeOptimizer . Pass in the uninstantiated torch.optim.* class object and any **kwargs that need to be passed to the __init__ call: from stoke import StokeOptimizer from torch.optim import Adam # Some ADAM parameters lr = 0.001 beta1 = 0.9 beta2 = 0.98 epsilon = 1E-09 # Create the StokeOptimizer opt = StokeOptimizer ( optimizer = Adam , optimizer_kwargs = { \"lr\" : lr , \"betas\" : ( beta1 , beta2 ), \"eps\" : epsilon } ) Create Stoke Object Now create the base stoke object. Pass in the model, loss(es), and StokeOptimizer from above as well as any flags/choices to set different backends/functionality/extensions and any necessary configurations. As an example, we set the device type to GPU, use the PyTorch DDP backend for distributed multi-GPU training, toggle native PyTorch AMP mixed precision, add Fairscale optimizer-state-sharding (OSS), and turn on automatic gradient accumulation and clipping (4 steps and clip-by-norm). In addition, let's customize PyTorch DDP, PyTorch AMP and Fairscale OSS with some of our own settings but leave all the others as default configurations. import os from stoke import AMPConfig from stoke import ClipGradNormConfig from stoke import DDPConfig from stoke import DistributedOptions from stoke import FairscaleOSSConfig from stoke import FP16Options from stoke import Stoke # Custom AMP configuration # Change the initial scale factor of the loss scaler amp_config = AMPConfig ( init_scale = 2. ** 14 ) # Custom DDP configuration # Automatically swap out batch_norm layers with sync_batch_norm layers # Notice here we have to deal with the local rank parameter that DDP needs (from env or cmd line) ddp_config = DDPConfig ( local_rank = os . getenv ( 'LOCAL_RANK' ), convert_to_sync_batch_norm = True ) # Custom OSS configuration # activate broadcast_fp16 -- Compress the model shards in fp16 before sharing them in between ranks oss_config = FairscaleOSSConfig ( broadcast_fp16 = True ) # Configure gradient clipping using the configuration object grad_clip = ClipGradNormConfig ( max_norm = 5.0 , norm_type = 2.0 ) # Build the object with the correct options/choices (notice how DistributedOptions and FP16Options are already provided # to make choices simple) and configurations (passed to configs as a list) stoke_obj = Stoke ( model = model , optimizer = opt , loss = loss , batch_size_per_device = 32 , gpu = True , fp16 = FP16Options . amp , distributed = DistributedOptions . ddp , fairscale_oss = True , grad_accum_steps = 4 , grad_clip = grad_clip , configs = [ amp_config , ddp_config , oss_config ] ) Build PyTorch DataLoader Next we need to create a torch.utils.data.DataLoader object. Similar to the optimizer definition this has to be done a little differently with stoke for it to correctly handle each of the different backends. stoke provides a mirrored wrapper to the native torch.utils.data.DataLoader class (as the DataLoader method) that will return a correctly configured torch.utils.data.DataLoader object. Since we are using a distributed backend (DDP) we need to provide a DistributedSampler or similar class to the DataLoader . Note that the Stoke object that we just created has the properties .rank and .world_size which provide common interfaces to this information regardless of the backend! from torch.utils.data.distributed import DistributedSampler # Create our DistributedSampler # Note: dataset is the torch.utils.data.Dataset from the first section sampler = DistributedSampler ( dataset = dataset , num_replicas = stoke_obj . world_size , rank = stoke_obj . rank ) # Call the DataLoader method on the stoke_obj to correctly create a DataLoader instance # The DataLoader object already known the batch size from the Stoke object creation data_loader = stoke_obj . DataLoader ( dataset = dataset , collate_fn = lambda batch : dataset . collate_fn ( batch ), sampler = sampler , num_workers = 4 ) Run a Training Loop At this point, we've successfully configured stoke ! Since stoke handled wrapping/building your torch.nn.Module and torch.utils.data.DataLoader , device placement is handled automatically (in our example the model and data are moved to GPUs). The following simple training loop should look fairly standard, except that the model forward , loss , backward , and step calls are all called on the Stoke object instead of each individual component (as it internally maintains the model, loss, and optimizer and all necessary code for all backends/functionality/extensions). In addition, we use one of many helper functions built into stoke to print the synced and gradient accumulated loss across all devices (an all-reduce across all devices with ReduceOp.SUM and divided by world_size -- that is print only on rank 0 by default) epoch = 0 # Iterate until number epochs while epoch < 100 : # Loop through the dataset for x , y in data_loader : # Use the Stoke wrapped version(s) of model, loss, backward, and step # Forward out = stoke_obj . model ( x ) # Loss loss = stoke_obj . loss ( out , y . to ( dtype = torch . float ) . unsqueeze ( 1 )) # Detach loss and sync across devices -- only after grad accum step has been called stoke_obj . print_mean_accumulated_synced_loss () # Backward stoke_obj . backward ( loss ) # stoke_obj.dump_model_grads() # Step stoke_obj . step () epoch += 1 Save/Load stoke provides a unified interface to save and load model checkpoints regardless of backend/functionality/extensions. Simply call the save or load methods on the Stoke object. # Save the model w/ a dummy extra dict path , tag = stoke_obj . save ( path = '/path/to/save/dir' , name = 'my-checkpoint-name' , extras = { 'foo' : 'bar' } ) # Attempt to load a saved checkpoint -- returns the extras dictionary extras = stoke_obj . load ( path = path , tag = tag )","title":"Quick Start"},{"location":"docs/Quick-Start/#quick-start","text":"This is a quick and dirty guide to getting up and running with stoke . Read additional documentation for full details and/or refer to the examples here .","title":"Quick Start"},{"location":"docs/Quick-Start/#basic-definitions","text":"Assuming some already existing common PyTorch objects (dataset: torch.utils.data.Dataset , model: torch.nn.Module , loss: torch.nn.(SomeLossFunction) ): import torch # Some existing user defined dataset using torch.utils.data.Dataset class RandomData ( torch . utils . data . Dataset ): pass # An existing model defined with torch.nn.Module class BasicNN ( torch . nn . Module ): pass # Our existing dataset from above dataset = RandomData ( ... ) # Our existing model from above model = BasicNN ( ... ) # A loss function loss = torch . nn . BCEWithLogitsLoss ()","title":"Basic Definitions"},{"location":"docs/Quick-Start/#optimizer-setup","text":"stoke requires a slightly different way to define the optimizer (as it handles instantiation internally) by using StokeOptimizer . Pass in the uninstantiated torch.optim.* class object and any **kwargs that need to be passed to the __init__ call: from stoke import StokeOptimizer from torch.optim import Adam # Some ADAM parameters lr = 0.001 beta1 = 0.9 beta2 = 0.98 epsilon = 1E-09 # Create the StokeOptimizer opt = StokeOptimizer ( optimizer = Adam , optimizer_kwargs = { \"lr\" : lr , \"betas\" : ( beta1 , beta2 ), \"eps\" : epsilon } )","title":"Optimizer Setup"},{"location":"docs/Quick-Start/#create-stoke-object","text":"Now create the base stoke object. Pass in the model, loss(es), and StokeOptimizer from above as well as any flags/choices to set different backends/functionality/extensions and any necessary configurations. As an example, we set the device type to GPU, use the PyTorch DDP backend for distributed multi-GPU training, toggle native PyTorch AMP mixed precision, add Fairscale optimizer-state-sharding (OSS), and turn on automatic gradient accumulation and clipping (4 steps and clip-by-norm). In addition, let's customize PyTorch DDP, PyTorch AMP and Fairscale OSS with some of our own settings but leave all the others as default configurations. import os from stoke import AMPConfig from stoke import ClipGradNormConfig from stoke import DDPConfig from stoke import DistributedOptions from stoke import FairscaleOSSConfig from stoke import FP16Options from stoke import Stoke # Custom AMP configuration # Change the initial scale factor of the loss scaler amp_config = AMPConfig ( init_scale = 2. ** 14 ) # Custom DDP configuration # Automatically swap out batch_norm layers with sync_batch_norm layers # Notice here we have to deal with the local rank parameter that DDP needs (from env or cmd line) ddp_config = DDPConfig ( local_rank = os . getenv ( 'LOCAL_RANK' ), convert_to_sync_batch_norm = True ) # Custom OSS configuration # activate broadcast_fp16 -- Compress the model shards in fp16 before sharing them in between ranks oss_config = FairscaleOSSConfig ( broadcast_fp16 = True ) # Configure gradient clipping using the configuration object grad_clip = ClipGradNormConfig ( max_norm = 5.0 , norm_type = 2.0 ) # Build the object with the correct options/choices (notice how DistributedOptions and FP16Options are already provided # to make choices simple) and configurations (passed to configs as a list) stoke_obj = Stoke ( model = model , optimizer = opt , loss = loss , batch_size_per_device = 32 , gpu = True , fp16 = FP16Options . amp , distributed = DistributedOptions . ddp , fairscale_oss = True , grad_accum_steps = 4 , grad_clip = grad_clip , configs = [ amp_config , ddp_config , oss_config ] )","title":"Create Stoke Object"},{"location":"docs/Quick-Start/#build-pytorch-dataloader","text":"Next we need to create a torch.utils.data.DataLoader object. Similar to the optimizer definition this has to be done a little differently with stoke for it to correctly handle each of the different backends. stoke provides a mirrored wrapper to the native torch.utils.data.DataLoader class (as the DataLoader method) that will return a correctly configured torch.utils.data.DataLoader object. Since we are using a distributed backend (DDP) we need to provide a DistributedSampler or similar class to the DataLoader . Note that the Stoke object that we just created has the properties .rank and .world_size which provide common interfaces to this information regardless of the backend! from torch.utils.data.distributed import DistributedSampler # Create our DistributedSampler # Note: dataset is the torch.utils.data.Dataset from the first section sampler = DistributedSampler ( dataset = dataset , num_replicas = stoke_obj . world_size , rank = stoke_obj . rank ) # Call the DataLoader method on the stoke_obj to correctly create a DataLoader instance # The DataLoader object already known the batch size from the Stoke object creation data_loader = stoke_obj . DataLoader ( dataset = dataset , collate_fn = lambda batch : dataset . collate_fn ( batch ), sampler = sampler , num_workers = 4 )","title":"Build PyTorch DataLoader"},{"location":"docs/Quick-Start/#run-a-training-loop","text":"At this point, we've successfully configured stoke ! Since stoke handled wrapping/building your torch.nn.Module and torch.utils.data.DataLoader , device placement is handled automatically (in our example the model and data are moved to GPUs). The following simple training loop should look fairly standard, except that the model forward , loss , backward , and step calls are all called on the Stoke object instead of each individual component (as it internally maintains the model, loss, and optimizer and all necessary code for all backends/functionality/extensions). In addition, we use one of many helper functions built into stoke to print the synced and gradient accumulated loss across all devices (an all-reduce across all devices with ReduceOp.SUM and divided by world_size -- that is print only on rank 0 by default) epoch = 0 # Iterate until number epochs while epoch < 100 : # Loop through the dataset for x , y in data_loader : # Use the Stoke wrapped version(s) of model, loss, backward, and step # Forward out = stoke_obj . model ( x ) # Loss loss = stoke_obj . loss ( out , y . to ( dtype = torch . float ) . unsqueeze ( 1 )) # Detach loss and sync across devices -- only after grad accum step has been called stoke_obj . print_mean_accumulated_synced_loss () # Backward stoke_obj . backward ( loss ) # stoke_obj.dump_model_grads() # Step stoke_obj . step () epoch += 1","title":"Run a Training Loop"},{"location":"docs/Quick-Start/#saveload","text":"stoke provides a unified interface to save and load model checkpoints regardless of backend/functionality/extensions. Simply call the save or load methods on the Stoke object. # Save the model w/ a dummy extra dict path , tag = stoke_obj . save ( path = '/path/to/save/dir' , name = 'my-checkpoint-name' , extras = { 'foo' : 'bar' } ) # Attempt to load a saved checkpoint -- returns the extras dictionary extras = stoke_obj . load ( path = path , tag = tag )","title":"Save/Load"},{"location":"reference/stoke/","text":"Module stoke Stoke is a lightweight wrapper for PyTorch that provides a simple unified interface for context switching Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Stoke is a lightweight wrapper for PyTorch that provides a simple unified interface for context switching Please refer to the documentation provided in the README.md \"\"\" from .configs import * from .data import BucketedDistributedSampler from .status import DistributedOptions, FP16Options from .stoke import Stoke from .utils import ParamNormalize __all__ = [ \"Stoke\", \"ParamNormalize\", \"FP16Options\", \"DistributedOptions\", \"StokeOptimizer\", \"ClipGradNormConfig\", \"ClipGradConfig\", \"FairscaleOSSConfig\", \"FairscaleSDDPConfig\", \"FairscaleFSDPConfig\", \"HorovodConfig\", \"ApexConfig\", \"DeepspeedConfig\", \"DDPConfig\", \"AMPConfig\", \"DeepspeedAIOConfig\", \"DeepspeedActivationCheckpointingConfig\", \"DeepspeedFlopsConfig\", \"DeepspeedFP16Config\", \"DeepspeedPLDConfig\", \"DeepspeedOffloadOptimizerConfig\", \"DeepspeedOffloadParamConfig\", \"DeepspeedTensorboardConfig\", \"DeepspeedZeROConfig\", \"BucketedDistributedSampler\", ] from ._version import get_versions __version__ = get_versions()[\"version\"] del get_versions Sub-modules stoke.configs stoke.data stoke.distributed stoke.extensions stoke.fp16 stoke.io stoke.status stoke.stoke stoke.utils Classes AMPConfig class AMPConfig ( backoff_factor : float = 0.5 , growth_factor : float = 2.0 , growth_interval : int = 2000 , init_scale : float = 65536.0 ) Attributes Name Type Description Default backoff_factor float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration None growth_factor float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. None growth_interval int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor None init_scale float, default: 2.**16 Initial scale factor None ??? example \"View Source\" class AMPConfig: \"\"\"PyTorch AMP configuration class Attributes ---------- backoff_factor : float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration growth_factor : float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. growth_interval : int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor init_scale : float, default: 2.**16 Initial scale factor \"\"\" backoff_factor: float = 0.5 growth_factor: float = 2.0 growth_interval: int = 2000 init_scale: float = 2.0 ** 16 ApexConfig class ApexConfig ( cast_model_outputs : Union [ torch . dtype , NoneType ] = None , convert_to_sync_batch_norm : bool = False , max_loss_scale : float = 16777216.0 , min_loss_scale : Union [ float , NoneType ] = None , scaler_per_loss : bool = False , verbosity : int = 0 ) Attributes Name Type Description Default cast_model_outputs Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm None max_loss_scale float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling None min_loss_scale Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed value scaler_per_loss bool, default: False Option to impose a scaler for each loss instead of a global scaler None verbosity int, default: 0 Set to 0 to suppress Amp-related output None ??? example \"View Source\" class ApexConfig: \"\"\"Nvidia APEX configuration class Attributes ---------- cast_model_outputs: Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm max_loss_scale: float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling min_loss_scale: Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed scaler_per_loss: bool, default: False Option to impose a scaler for each loss instead of a global scaler verbosity: int, default: 0 Set to 0 to suppress Amp-related output \"\"\" cast_model_outputs: Optional[torch.dtype] = None convert_to_sync_batch_norm: bool = False max_loss_scale: float = 2.0 ** 24 min_loss_scale: Optional[float] = None scaler_per_loss: bool = False verbosity: int = 0 BucketedDistributedSampler class BucketedDistributedSampler ( dataset : torch . utils . data . dataset . Dataset , buckets : int , batch_size : int , sorted_idx : List , backend : stoke . status . DistributedOptions , allow_bucket_overlap : bool = False , num_replicas : Union [ int , NoneType ] = None , rank : Union [ int , NoneType ] = None , shuffle : bool = True , seed : int = 0 , drop_last : bool = False , info_rank : int = 0 ) Attributes Name Type Description Default num_replicas int, default: None number of replicas None rank int, default: None current device rank None epoch int current training epoch None drop_last bool, default: False whether to drop last set of samples that don't fit into a batch None shuffle bool, default: True flag to shuffle dataset None seed int, default: 0 seed to use for generators None buckets int number of buckets to break the dataset into None sorted_n_samples list sorted list of samples by the characteristic to bucket by (e.g. seq len) None batch_size int batch size that will be used (needed to make sure slices are correct) None allow_bucket_overlap bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch None slice_size int computed from batch size and number of replicas None num_samples_per_bucket int computed value that represents the number of samples in a single bucket None num_slices_per_bucket int computed value that represents the number of slices available in a bucket None bucket_idx list computed value that make a contiguous list of indices in each bucket None rounded_num_samples_per_bucket int computed value post round for number of samples in a single bucket None rounded_num_samples_per_replica int computed value post round for number of slices available in a bucket None ??? example \"View Source\" class BucketedDistributedSampler(Sampler[T_co]): \"\"\"Sampler that buckets samples by sorted_idx and then randomly samples from a specific bucket to prevent excess padding leading to wasted computation Borrowing heavily from the base DistributedSampler https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler Attributes ---------- num_replicas: int, default: None number of replicas rank: int, default: None current device rank epoch: int current training epoch drop_last: bool, default: False whether to drop last set of samples that don't fit into a batch shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators buckets: int number of buckets to break the dataset into sorted_n_samples: list sorted list of samples by the characteristic to bucket by (e.g. seq len) batch_size: int batch size that will be used (needed to make sure slices are correct) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch slice_size: int computed from batch size and number of replicas num_samples_per_bucket: int computed value that represents the number of samples in a single bucket num_slices_per_bucket: int computed value that represents the number of slices available in a bucket bucket_idx: list computed value that make a contiguous list of indices in each bucket rounded_num_samples_per_bucket: int computed value post round for number of samples in a single bucket rounded_num_samples_per_replica: int computed value post round for number of slices available in a bucket \"\"\" def __init__( self, dataset: Dataset, buckets: int, batch_size: int, sorted_idx: List, backend: DistributedOptions, allow_bucket_overlap: bool = False, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0, drop_last: bool = False, info_rank: int = 0, ) -> None: \"\"\"Init for BucketedDistributedSampler Parameters ---------- dataset: Dataset dataset from which to load the data. buckets: int number of buckets to break the dataset into batch_size: int batch size that will be used (needed to make sure slices are correct) sorted_idx: list sorted list of samples by the characteristic to bucket by (e.g. seq le backend: DistributedOptions which backend is being used (as rank, world size, etc. need to be used) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch num_replicas: int, default: None number of replicas rank: int, default: None current device rank shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators drop_last: bool, default: False whether to drop last set of samples that don't fit into a info_rank: int, default: 0 which device to print information on \"\"\" # If the backend isnt DDP there needs to be an additional import num_replicas, rank = self._conditional_distributed( backend=backend, num_replicas=num_replicas, rank=rank ) self.num_replicas = num_replicas self.rank = rank self.epoch = 0 self.drop_last = drop_last self.shuffle = shuffle self.seed = seed self.buckets = buckets self.sorted_n_samples = sorted_idx # Batch size is needed here so a contiguous iter of buckets can be formed self.batch_size = batch_size # This is a flag to batch up the dropped samples (that would be 'wasted') if drop_last is flagged self.allow_bucket_overlap = allow_bucket_overlap # Calculate the size of each slice that will be indexed across the replicas self.slice_size = self.batch_size * self.num_replicas # Calculate the size of the buckets (rounded or not based on drop last) self.num_samples_per_bucket = self._get_size( len(dataset), self.buckets, self.drop_last ) # Calculate the number of slices per bucket self.num_slices_per_bucket = self._get_size( self.num_samples_per_bucket, self.slice_size, self.drop_last ) if self.num_samples_per_bucket < self.slice_size: raise ValueError( f\"Stoke -- Resulting number of slices (batch * replicas) per bucket \" f\"({self.num_samples_per_bucket}) is less than the batch size \" f\"({self.batch_size})\" ) if self.num_slices_per_bucket < 2: raise ValueError( f\"Stoke -- Number of slices per bucket {self.num_slices_per_bucket} is less than 2 \" f\"which is not recommended\" ) if self.num_samples_per_bucket < 100: raise ValueError( f\"Stoke -- Number of samples per bucket {self.num_samples_per_bucket} is less than 100 \" f\"which is not recommended as this might lead to dropping of excessive data\" ) # Split into buckets and turn into lists self.bucket_idx = [ list(val) for val in np.array_split(self.sorted_n_samples, self.buckets) ] # Calculate the post rounded numbers self.rounded_num_samples_per_bucket = ( self.slice_size * self.num_slices_per_bucket ) self.rounded_num_samples_per_replica = ( self.num_slices_per_bucket * self.batch_size * self.buckets ) # Add the bucket overlap samples if self.allow_bucket_overlap: self.rounded_num_samples_per_replica += ( (len(dataset) - (self.rounded_num_samples_per_bucket * self.buckets)) // self.slice_size ) * self.batch_size if self.rank == info_rank: print( f\"Stoke -- BucketedDistributedSampler -- # Samples Per Bucket: \" f\"{self.rounded_num_samples_per_bucket}, # of Samples Per Replica: \" f\"{self.rounded_num_samples_per_replica}\" ) def _conditional_distributed( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\" Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" return self._check_backend(backend, num_replicas, rank) def _get_backend_functions(self, backend: DistributedOptions): \"\"\"Gets backend functions if needed Parameters ---------- backend: DistributedOptions which backend is being used Returns ------- Tuple[bool, int, int] is_init, num_replicas, rank \"\"\" if backend.value == \"ddp\" or backend.value == \"deepspeed\": return ( torch.distributed.is_initialized, torch.distributed.get_world_size, torch.distributed.get_rank, ) else: return hvd.is_initialized, hvd.size, hvd.rank def _check_backend( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\"Checks the backend for correct device info Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" if num_replicas is None or rank is None: is_avail, get_world_size, get_rank = self._get_backend_functions( backend=backend ) if num_replicas is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) num_replicas = get_world_size() if rank is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) rank = get_rank() return num_replicas, rank @staticmethod def _get_size(data_len: int, split_var: int, drop_last: bool = False): \"\"\"Gets the size of a split Parameters ---------- data_len: int current dataset length split_var: int how many to split into drop_last: bool, default: False drop last hanging samples if not batch_size Returns ------- num_samples: int \"\"\" if drop_last: num_samples = data_len // split_var else: num_samples = ceil(data_len / split_var) return num_samples def __iter__(self) -> Iterator[T_co]: \"\"\"Handles assembling the batches from a bucketed perspective Shuffle bucket order->Pad if necessary->Slice across replicas->Possibly batch up residuals->shuffle bucketed batches->Unroll into list->Make iter Returns ------- Iterator[T_co] \"\"\" # Shuffle the bucketed idx if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute each bucket indices = [ [val[idx] for idx in torch.randperm(len(val), generator=g).tolist()] for val in self.bucket_idx ] else: indices = self.bucket_idx # Iterate over the buckets for idx, val in enumerate(indices): # If this is true we need to handle padding if (self.num_slices_per_bucket * self.slice_size) > len(val): split_val = self._handle_padding(val) indices[idx] = list(itertools.chain(*split_val)) assert len(indices[idx]) == self.rounded_num_samples_per_bucket # Now slice across replicas final_indices = [] for val in indices: for idx in range(self.num_slices_per_bucket): replica_slice = val[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] final_indices.append(replica_slice) # If bucket overlap is allowed then we just batch up the residual indices if self.drop_last and self.allow_bucket_overlap: residual_idx = list( itertools.chain( *[val[self.rounded_num_samples_per_bucket :] for val in indices] ) ) if len(residual_idx) > self.slice_size: # Cut by slices then by replicas residual_idx = [ residual_idx[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] for idx in range(len(residual_idx) // self.slice_size) ] # Append to the final indices final_indices.extend(residual_idx) # Shuffle the bucketed batches if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute the bucket order final_indices = [ final_indices[val] for val in torch.randperm(len(final_indices), generator=g) ] # Unroll into a single list final_indices = list(itertools.chain(*final_indices)) assert len(final_indices) == self.rounded_num_samples_per_replica return iter(final_indices) def _handle_padding(self, idx_list: List): \"\"\"Handles padding out if a batch is short Parameters ---------- idx_list: List list of indices Returns ------- split_val: List list with correctly padded sizes \"\"\" split_val = [] for idx in range(self.num_slices_per_bucket): if idx == (self.num_slices_per_bucket - 1): # Get the short batch short_batch = idx_list[(idx * self.slice_size) :] # Short batch replica slice sizes short_len = [ self.batch_size - len(list(val)) for val in np.array_split(short_batch, self.num_replicas) ] # Pop the necessary values from the entire bucket pad_values = [ idx_list[s_idx : (self.num_replicas * s_len) : self.num_replicas] for s_idx, s_len in enumerate(short_len) ] # If not a consistent list then we need to reorder so that the step size alignment slicing # of the replicas works if len(set(short_len)) != 1: # here we need to find the first larger idx and reorder first_idx = short_len.index(max(set(short_len))) # Reorder pad_values = pad_values[first_idx:] + pad_values[0:first_idx] extended_batch = short_batch + [ pad for pad in list( itertools.chain(*itertools.zip_longest(*pad_values)) ) if pad is not None ] split_val.append(extended_batch) else: split_val.append( idx_list[(idx * self.slice_size) : ((idx + 1) * self.slice_size)] ) return split_val def __len__(self) -> int: return self.rounded_num_samples_per_replica def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch Ancestors (in MRO) torch.utils.data.sampler.Sampler typing.Generic Methods set_epoch def set_epoch ( self , epoch : int ) -> None Sets the epoch for this sampler. When :attr: shuffle=True , this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters: Name Type Description Default epoch int Epoch number None ??? example \"View Source\" def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch ClipGradConfig class ClipGradConfig ( clip_value : float ) Attributes Name Type Description Default clip_value float maximum allowed absolute value of the gradients [-clip_value, clip_value] None ??? example \"View Source\" class ClipGradConfig: \"\"\"Gradient clipping by value configuration class Attributes ---------- clip_value: float maximum allowed absolute value of the gradients [-clip_value, clip_value] \"\"\" clip_value: float ClipGradNormConfig class ClipGradNormConfig ( max_norm : float , norm_type : float ) Attributes Name Type Description Default max_norm float max norm of the gradients None norm_type float type of the used p-norm None ??? example \"View Source\" class ClipGradNormConfig: \"\"\"Gradient clipping by p-norm configuration class Attributes ---------- max_norm: float max norm of the gradients norm_type: float type of the used p-norm \"\"\" max_norm: float norm_type: float DDPConfig class DDPConfig ( local_rank : Union [ int , NoneType ], auto_mpi_discovery : bool = False , convert_to_sync_batch_norm : bool = False , backend : stoke . configs . BackendOptions = 'nccl' , broadcast_buffers : bool = True , bucket_cap_mb : int = 25 , find_unused_parameters : bool = False , gradient_as_bucket_view : bool = False , init_method : str = 'env://' , no_sync : bool = True ) Attributes Name Type Description Default local_rank Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) None auto_mpi_discovery bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html None backend BackendOptions, default: 'nccl' Which communication backend to use None broadcast_buffers bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function None bucket_cap_mb int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) None find_unused_parameters bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach None gradient_as_bucket_view bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. None init_method str, default: 'env://' URL specifying how to initialize the process group None no_sync bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead None ??? example \"View Source\" class DDPConfig: \"\"\"PyTorch DistributedDataParallel configuration class Attributes ---------- local_rank: Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) auto_mpi_discovery: bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html backend: BackendOptions, default: 'nccl' Which communication backend to use broadcast_buffers: bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function bucket_cap_mb: int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) find_unused_parameters: bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach gradient_as_bucket_view: bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. init_method: str, default: 'env://' URL specifying how to initialize the process group no_sync: bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead \"\"\" local_rank: Optional[int] auto_mpi_discovery: bool = False convert_to_sync_batch_norm: bool = False backend: BackendOptions = \"nccl\" broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False gradient_as_bucket_view: bool = False init_method: str = \"env://\" no_sync: bool = True DeepspeedAIOConfig class DeepspeedAIOConfig ( block_size : int = 1048576 , ignore_unused_parameters : bool = True , overlap_events : bool = True , queue_depth : int = 8 , single_submit : bool = False , thread_count : int = 1 ) Attributes Name Type Description Default block_size int, default: 1048576 I/O block size in bytes None ignore_unused_parameters bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. None overlap_events bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. None queue_depth int, default: 8 I/O queue depth None single_submit bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. None thread_count int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. None ??? example \"View Source\" class DeepspeedAIOConfig: \"\"\"Deepspeed asynchronous I/O configuration class Attributes ---------- block_size: int, default: 1048576 I/O block size in bytes ignore_unused_parameters: bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. overlap_events: bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. queue_depth: int, default: 8 I/O queue depth single_submit: bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. thread_count: int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. \"\"\" block_size: int = 1048576 ignore_unused_parameters: bool = True overlap_events: bool = True queue_depth: int = 8 single_submit: bool = False thread_count: int = 1 DeepspeedActivationCheckpointingConfig class DeepspeedActivationCheckpointingConfig ( contiguous_memory_optimization : bool = False , cpu_checkpointing : bool = False , number_checkpoints : Union [ int , NoneType ] = None , partition_activations : bool = False , profile : bool = False , synchronize_checkpoint_boundary : bool = False ) Attributes Name Type Description Default contiguous_memory_optimization bool, default: False Copies partitioned activations so that they are contiguous in memory None cpu_checkpointing bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled None number_checkpoints Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization None partition_activations bool, default: False Enables partition activation when used with model parallelism None profile bool, default: False Logs the forward and backward time for each checkpoint function None synchronize_checkpoint_boundary bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary None ??? example \"View Source\" class DeepspeedActivationCheckpointingConfig: \"\"\"Deepspeed activation checkpointing configuration class Attributes ---------- contiguous_memory_optimization: bool, default: False Copies partitioned activations so that they are contiguous in memory cpu_checkpointing: bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled number_checkpoints: Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization partition_activations: bool, default: False Enables partition activation when used with model parallelism profile: bool, default: False Logs the forward and backward time for each checkpoint function synchronize_checkpoint_boundary: bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary \"\"\" contiguous_memory_optimization: bool = False cpu_checkpointing: bool = False number_checkpoints: Optional[int] = None partition_activations: bool = False profile: bool = False synchronize_checkpoint_boundary: bool = False DeepspeedConfig class DeepspeedConfig ( activation_checkpointing : Union [ stoke . configs . DeepspeedActivationCheckpointingConfig , NoneType ] = DeepspeedActivationCheckpointingConfig ( contiguous_memory_optimization = False , cpu_checkpointing = False , number_checkpoints = None , partition_activations = False , profile = False , synchronize_checkpoint_boundary = False ), aio : Union [ stoke . configs . DeepspeedAIOConfig , NoneType ] = DeepspeedAIOConfig ( block_size = 1048576 , ignore_unused_parameters = True , overlap_events = True , queue_depth = 8 , single_submit = False , thread_count = 1 ), auto_mpi_discovery : bool = True , disable_allgather : bool = False , dist_backend : stoke . configs . BackendOptions = 'nccl' , distributed_port : int = 29500 , dump_state : bool = False , flops_profiler : Union [ stoke . configs . DeepspeedFlopsConfig , NoneType ] = None , fp16 : Union [ stoke . configs . DeepspeedFP16Config , NoneType ] = None , fp32_allreduce : bool = False , gradient_predivide_factor : float = 1.0 , init_method : str = 'env://' , prescale_gradients : bool = False , progressive_layer_drop : Union [ stoke . configs . DeepspeedPLDConfig , NoneType ] = None , sparse_gradients : bool = False , steps_per_print : int = 10 , tensorboard : Union [ stoke . configs . DeepspeedTensorboardConfig , NoneType ] = None , verbose : bool = True , wall_clock_breakdown : bool = False , zero_optimization : Union [ stoke . configs . DeepspeedZeROConfig , NoneType ] = DeepspeedZeROConfig ( allgather_bucket_size = 500000000 , allgather_partitions = True , contiguous_gradients = False , ignore_unused_parameters = True , legacy_stage1 = False , offload_optimizer = None , offload_param = None , overlap_comm = False , reduce_bucket_size = 500000000 , reduce_scatter = True , stage = 0 , stage3_max_live_parameters = 1000000000 , stage3_max_reuse_distance = 1000000000 , stage3_prefetch_bucket_size = 500000000 , stage3_param_persistence_threshold = 1000000 , stage3_gather_fp16_weights_on_model_save = False , sub_group_size = 1000000000000 ) ) Attributes Name Type Description Default activation_checkpointing Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing None aio Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage None auto_mpi_discovery bool, default: True if distributed environment variables are not set, attempt to discover them from MPI None disable_allgather bool, default: False Disables allgather None dist_backend BackendOptions, default: 'nccl' Which communication backend to use None distributed_port int, default: 29500 torch distributed backend port None dump_state bool, default: False Print out state information of DeepSpeed object after initialization None flops_profiler Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown None fp16 Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package None fp32_allreduce bool, default: False During gradient averaging perform allreduce with 32 bit values None gradient_predivide_factor float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs None init_method str, default: 'env://' URL specifying how to initialize the process group None prescale_gradients float, default: 1.0 Scale gradients before doing allreduce None progressive_layer_drop Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping None sparse_gradients bool, default: False Enable sparse compression of torch.nn.Embedding gradients None steps_per_print int, default: 10 Print train loss every N steps None tensorboard Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support None verbose bool, default: True flag to make deepspeed engine verbose with information None wall_clock_breakdown bool, default: False Enable timing of the latency of forward/backward/update training phases None zero_optimization Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations None ??? example \"View Source\" class DeepspeedConfig: \"\"\"Deepspeed configuration class Composed of other configuration classes related to specific functionality Attributes ---------- activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage auto_mpi_discovery: bool, default: True if distributed environment variables are not set, attempt to discover them from MPI disable_allgather: bool, default: False Disables allgather dist_backend: BackendOptions, default: 'nccl' Which communication backend to use distributed_port: int, default: 29500 torch distributed backend port dump_state: bool, default: False Print out state information of DeepSpeed object after initialization flops_profiler: Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown fp16: Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package fp32_allreduce: bool, default: False During gradient averaging perform allreduce with 32 bit values gradient_predivide_factor: float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs init_method: str, default: 'env://' URL specifying how to initialize the process group prescale_gradients: float, default: 1.0 Scale gradients before doing allreduce progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping sparse_gradients: bool, default: False Enable sparse compression of torch.nn.Embedding gradients steps_per_print: int, default: 10 Print train loss every N steps tensorboard: Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support verbose: bool, default: True flag to make deepspeed engine verbose with information wall_clock_breakdown: bool, default: False Enable timing of the latency of forward/backward/update training phases zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations Notes ----- Deepspeed does not use Apex\u2019s AMP mode whihc allows for more flexibility in mixed precision training modes. FP16 here is similar to AMP\u2019s O2 mode \"\"\" activation_checkpointing: Optional[ DeepspeedActivationCheckpointingConfig ] = DeepspeedActivationCheckpointingConfig() aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig() auto_mpi_discovery: bool = True disable_allgather: bool = False dist_backend: BackendOptions = \"nccl\" distributed_port: int = 29500 dump_state: bool = False flops_profiler: Optional[DeepspeedFlopsConfig] = None fp16: Optional[DeepspeedFP16Config] = None fp32_allreduce: bool = False gradient_predivide_factor: float = 1.0 init_method: str = \"env://\" prescale_gradients: bool = False progressive_layer_drop: Optional[DeepspeedPLDConfig] = None sparse_gradients: bool = False steps_per_print: int = 10 tensorboard: Optional[DeepspeedTensorboardConfig] = None verbose: bool = True wall_clock_breakdown: bool = False zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig() DeepspeedFP16Config class DeepspeedFP16Config ( hysteresis : int = 2 , initial_scale_power : int = 32 , loss_scale : float = 0.0 , loss_scale_window : int = 1000 , min_loss_scale : int = 1000 ) Attributes Name Type Description Default hysteresis int, default: 2 represents the delay shift in dynamic loss scaling None initial_scale_power int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power None loss_scale float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) None loss_scale_window int, default: 1000 the window over which to raise/lower the dynamic loss scale value None min_loss_scale int, default: 1000 minimum dynamic loss scale value None ??? example \"View Source\" class DeepspeedFP16Config: \"\"\"Deepspeed FP16 configuration class Attributes ---------- hysteresis: int, default: 2 represents the delay shift in dynamic loss scaling initial_scale_power: int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power loss_scale: float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) loss_scale_window: int, default: 1000 the window over which to raise/lower the dynamic loss scale value min_loss_scale: int, default: 1000 minimum dynamic loss scale value \"\"\" hysteresis: int = 2 initial_scale_power: int = 32 loss_scale: float = 0.0 loss_scale_window: int = 1000 min_loss_scale: int = 1000 DeepspeedFlopsConfig class DeepspeedFlopsConfig ( detailed : bool = True , module_depth : int = - 1 , output_file : Union [ str , NoneType ] = None , profile_step : int = 1 , top_modules : int = 1 ) Attributes Name Type Description Default detailed bool, default: True Whether to print the detailed model profile None module_depth int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). None output_file Optional[str], default: None Path to the output file. If None, the profiler prints to stdout None profile_step int, default: 1 The global training step at which to profile. None top_modules int, default: 1 Limits the aggregated profile output to the number of top modules specified. None ??? example \"View Source\" class DeepspeedFlopsConfig: \"\"\"Deepspeed flops profiler configuration class Attributes ---------- detailed: bool, default: True Whether to print the detailed model profile module_depth: int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). output_file: Optional[str], default: None Path to the output file. If None, the profiler prints to stdout profile_step: int, default: 1 The global training step at which to profile. top_modules: int, default: 1 Limits the aggregated profile output to the number of top modules specified. Notes ----- Warm up steps are needed for accurate time measurement \"\"\" detailed: bool = True module_depth: int = -1 output_file: Optional[str] = None profile_step: int = 1 top_modules: int = 1 DeepspeedOffloadOptimizerConfig class DeepspeedOffloadOptimizerConfig ( buffer_count : int = 4 , device : stoke . configs . OffloadDevice = 'cpu' , fast_init : bool = False , nvme_path : str = '/local_nvme' , pin_memory : bool = False , pipeline : bool = False , pipeline_read : bool = False , pipeline_write : bool = False ) Attributes Name Type Description Default buffer_count int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). None device OffloadDevice, default: 'cpu' Device memory to offload optimizer state None fast_init bool, default: False Enable fast optimizer initialization when offloading to NVMe None nvme_path str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading None pin_memory bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. None pipeline bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set to pipeline_read bool, default: False activate pipeline read (deepspeed has limited docs for what this does) None pipeline_write bool, default: False activate pipeline write(deepspeed has limited docs for what this does) None ??? example \"View Source\" class DeepspeedOffloadOptimizerConfig: \"\"\"Deepspeed optimizer offloading configuration class Attributes ---------- buffer_count: int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). device: OffloadDevice, default: 'cpu' Device memory to offload optimizer state fast_init: bool, default: False Enable fast optimizer initialization when offloading to NVMe nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. pipeline: bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set pipeline_read: bool, default: False activate pipeline read (deepspeed has limited docs for what this does) pipeline_write: bool, default: False activate pipeline write(deepspeed has limited docs for what this does) \"\"\" buffer_count: int = 4 device: OffloadDevice = \"cpu\" fast_init: bool = False nvme_path: str = \"/local_nvme\" pin_memory: bool = False pipeline: bool = False pipeline_read: bool = False pipeline_write: bool = False DeepspeedOffloadParamConfig class DeepspeedOffloadParamConfig ( buffer_count : int = 5 , buffer_size : int = 100000000 , device : stoke . configs . OffloadDevice = 'cpu' , max_in_cpu : int = 1000000000 , nvme_path : str = '/local_nvme' , pin_memory : bool = False ) Attributes Name Type Description Default buffer_count int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe None buffer_size int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe None device OffloadDevice, default: 'cpu' Device memory to offload model parameters None max_in_cpu int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. None nvme_path str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading None pin_memory bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. None ??? example \"View Source\" class DeepspeedOffloadParamConfig: \"\"\"Deepspeed parameter offloading configuration class Attributes ---------- buffer_count: int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe buffer_size: int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe device: OffloadDevice, default: 'cpu' Device memory to offload model parameters max_in_cpu: int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. \"\"\" buffer_count: int = 5 buffer_size: int = int(1e8) device: OffloadDevice = \"cpu\" max_in_cpu: int = int(1e9) nvme_path: str = \"/local_nvme\" pin_memory: bool = False DeepspeedPLDConfig class DeepspeedPLDConfig ( theta : float = 1.0 , gamma : float = 0.001 ) Attributes Name Type Description Default theta float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed None gamma float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases None ??? example \"View Source\" class DeepspeedPLDConfig: \"\"\" Attributes ---------- theta: float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed gamma: float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases \"\"\" theta: float = 1.0 gamma: float = 0.001 DeepspeedTensorboardConfig class DeepspeedTensorboardConfig ( output_path : str = '' , job_name : str = 'DeepSpeedJobName' ) Attributes Name Type Description Default output_path str, default: '' Tensorboard output path None job_name str, default: 'DeepSpeedJobName' Tensorboard job name None ??? example \"View Source\" class DeepspeedTensorboardConfig: \"\"\"Deepspeed Tensorboard configuration class Attributes ---------- output_path: str, default: '' Tensorboard output path job_name: str, default: 'DeepSpeedJobName' Tensorboard job name \"\"\" output_path: str = \"\" job_name: str = \"DeepSpeedJobName\" DeepspeedZeROConfig class DeepspeedZeROConfig ( allgather_bucket_size : int = 500000000 , allgather_partitions : bool = True , contiguous_gradients : bool = False , ignore_unused_parameters : bool = True , legacy_stage1 : bool = False , offload_optimizer : Union [ stoke . configs . DeepspeedOffloadOptimizerConfig , NoneType ] = None , offload_param : Union [ stoke . configs . DeepspeedOffloadParamConfig , NoneType ] = None , overlap_comm : bool = False , reduce_bucket_size : int = 500000000 , reduce_scatter : bool = True , stage : int = 0 , stage3_max_live_parameters : int = 1000000000 , stage3_max_reuse_distance : int = 1000000000 , stage3_prefetch_bucket_size : int = 500000000 , stage3_param_persistence_threshold : int = 1000000 , stage3_gather_fp16_weights_on_model_save : bool = False , sub_group_size : int = 1000000000000 ) Attributes Name Type Description Default allgather_bucket_size int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes None allgather_partitions bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step None contiguous_gradients bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. None ignore_unused_parameters bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 None legacy_stage1 bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons None offload_optimizer Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 None offload_param Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. None overlap_comm bool, default: False Attempts to overlap the reduction of the gradients with backward computation None reduce_bucket_size int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes None reduce_scatter bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients None stage int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively None stage3_max_live_parameters int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. None stage3_max_reuse_distance int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. None stage3_prefetch_bucket_size int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. None stage3_param_persistence_threshold int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). None stage3_gather_fp16_weights_on_model_save bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. None sub_group_size int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. None ??? example \"View Source\" class DeepspeedZeROConfig: \"\"\"Deepspeed ZeRO configuration class Attributes ---------- allgather_bucket_size: int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes allgather_partitions: bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step contiguous_gradients: bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. ignore_unused_parameters: bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 legacy_stage1: bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 offload_param: Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. overlap_comm: bool, default: False Attempts to overlap the reduction of the gradients with backward computation reduce_bucket_size: int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes reduce_scatter: bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients stage: int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively stage3_max_live_parameters: int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. stage3_max_reuse_distance: int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. stage3_prefetch_bucket_size: int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. stage3_param_persistence_threshold: int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). stage3_gather_fp16_weights_on_model_save: bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. sub_group_size: int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. \"\"\" allgather_bucket_size: int = int(5e8) allgather_partitions: bool = True contiguous_gradients: bool = False ignore_unused_parameters: bool = True legacy_stage1: bool = False offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None offload_param: Optional[DeepspeedOffloadParamConfig] = None overlap_comm: bool = False reduce_bucket_size: int = int(5e8) reduce_scatter: bool = True stage: int = 0 stage3_max_live_parameters: int = int(1e9) stage3_max_reuse_distance: int = int(1e9) stage3_prefetch_bucket_size: int = int(5e8) stage3_param_persistence_threshold: int = int(1e6) stage3_gather_fp16_weights_on_model_save: bool = False sub_group_size: int = int(1e12) DistributedOptions class DistributedOptions ( / , * args , ** kwargs ) ??? example \"View Source\" class DistributedOptions(Enum): \"\"\"Enum that defines the options for Distributed backends\"\"\" horovod = \"horovod\" ddp = \"ddp\" deepspeed = \"deepspeed\" Ancestors (in MRO) enum.Enum Class variables ddp deepspeed horovod name value FP16Options class FP16Options ( / , * args , ** kwargs ) ??? example \"View Source\" class FP16Options(Enum): \"\"\"Enum that defines the options for FP16 backends\"\"\" apex_O1 = \"apex_O1\" apex_O2 = \"apex_O2\" amp = \"amp\" deepspeed = \"deepspeed\" Ancestors (in MRO) enum.Enum Class variables amp apex_O1 apex_O2 deepspeed name value FairscaleFSDPConfig class FairscaleFSDPConfig ( bucket_cap_mb : int = 25 , buffer_dtype : Union [ torch . dtype , NoneType ] = None , clear_autocast_cache : bool = False , compute_dtype : Union [ torch . dtype , NoneType ] = None , flatten_parameters : bool = True , force_input_to_fp32 : bool = False , fp32_reduce_scatter : bool = False , gradient_predivide_factor : Union [ float , NoneType ] = None , gradient_postdivide_factor : Union [ float , NoneType ] = None , move_grads_to_cpu : Union [ bool , NoneType ] = None , move_params_to_cpu : bool = False , no_broadcast_optim_state : Union [ bool , NoneType ] = False , reshard_after_forward : bool = True , verbose : bool = False ) Attributes Name Type Description Default bucket_cap_mb int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing None buffer_dtype Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype value clear_autocast_cache bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory None compute_dtype Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. torch.float32 flatten_parameters bool, default: True flatten parameters into a single contiguous tensor, which improves training speed None force_input_to_fp32 bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. None fp32_reduce_scatter bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used None gradient_predivide_factor Optional[float], default: None divide factor before the reduction None gradient_postdivide_factor Optional[float], default: None divide factor after the reduction None move_grads_to_cpu Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used None move_params_to_cpu bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used None no_broadcast_optim_state Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node None reshard_after_forward bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) None verbose bool, default: True turn on verbose output for model\u2019s string representation None ??? example \"View Source\" class FairscaleFSDPConfig: \"\"\"Fairscale Fully Sharded Data Parallel configuration class Attributes ---------- bucket_cap_mb: int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of <dtype, device, process_group> tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing buffer_dtype: Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype clear_autocast_cache: bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory compute_dtype: Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. flatten_parameters: bool, default: True flatten parameters into a single contiguous tensor, which improves training speed force_input_to_fp32: bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. fp32_reduce_scatter: bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used gradient_predivide_factor: Optional[float], default: None divide factor before the reduction gradient_postdivide_factor: Optional[float], default: None divide factor after the reduction move_grads_to_cpu: Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used move_params_to_cpu: bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used no_broadcast_optim_state: Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node reshard_after_forward: bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) verbose: bool, default: True turn on verbose output for model\u2019s string representation Notes ----- mixed_precision: bool This value will automatically be set from the Stoke FP16 selected option (AMP only) state_dict_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup compute_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup \"\"\" bucket_cap_mb: int = 25 buffer_dtype: Optional[torch.dtype] = None clear_autocast_cache: bool = False compute_dtype: Optional[torch.dtype] = None flatten_parameters: bool = True force_input_to_fp32: bool = False fp32_reduce_scatter: bool = False gradient_predivide_factor: Optional[float] = None gradient_postdivide_factor: Optional[float] = None move_grads_to_cpu: Optional[bool] = None move_params_to_cpu: bool = False no_broadcast_optim_state: Optional[bool] = False reshard_after_forward: bool = True verbose: bool = False Descendants stoke.extensions._FairscaleFSDPConfig FairscaleOSSConfig class FairscaleOSSConfig ( broadcast_fp16 : bool = False ) Attributes Name Type Description Default broadcast_fp16 bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. None ??? example \"View Source\" class FairscaleOSSConfig: \"\"\"Fairscale optimizer state sharding configuration class Attributes ---------- broadcast_fp16: bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. \"\"\" broadcast_fp16: bool = False FairscaleSDDPConfig class FairscaleSDDPConfig ( auto_refresh_trainable : bool = True , broadcast_buffers : bool = True , reduce_buffer_size : int = 8388608 , reduce_fp16 : bool = False , sync_models_at_startup : bool = True ) Attributes Name Type Description Default auto_refresh_trainable bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen None broadcast_buffers bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. None reduce_buffer_size int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. None reduce_fp16 bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. None sync_models_at_startup bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state None ??? example \"View Source\" class FairscaleSDDPConfig: \"\"\"Fairscale sharded data parallel (SDDP) configuration class Attributes ---------- auto_refresh_trainable: bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen broadcast_buffers: bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. reduce_buffer_size: int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. reduce_fp16: bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. sync_models_at_startup: bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state \"\"\" auto_refresh_trainable: bool = True broadcast_buffers: bool = True reduce_buffer_size: int = 2 ** 23 reduce_fp16: bool = False sync_models_at_startup: bool = True HorovodConfig class HorovodConfig ( compression : bool = False , convert_to_sync_batch_norm : bool = False , gradient_predivide_factor : float = 1.0 , op : stoke . configs . HorovodOps = 'Average' ) Attributes Name Type Description Default compression bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm None gradient_predivide_factor float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. None op HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. None ??? example \"View Source\" class HorovodConfig: \"\"\"Horovod configuration class Attributes ---------- compression: bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm gradient_predivide_factor: float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. op: HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. \"\"\" compression: bool = False convert_to_sync_batch_norm: bool = False gradient_predivide_factor: float = 1.0 op: HorovodOps = \"Average\" ParamNormalize class ParamNormalize ( / , * args , ** kwargs ) ??? example \"View Source\" class ParamNormalize(Enum): \"\"\"Normalization enum for total number of model parameters used to help with a pretty print\"\"\" THOUSAND = 1e3 MILLION = 1e6 BILLION = 1e9 TRILLION = 1e12 Ancestors (in MRO) enum.Enum Class variables BILLION MILLION THOUSAND TRILLION name value Stoke class Stoke ( model : torch . nn . modules . module . Module , optimizer : stoke . configs . StokeOptimizer , loss : Union [ Callable , List [ Callable ], Tuple [ Callable ]], batch_size_per_device : int , grad_accum_steps : Union [ int , NoneType ] = 1 , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig , NoneType ] = None , gpu : bool = False , fp16 : Union [ stoke . status . FP16Options , NoneType ] = None , distributed : Union [ stoke . status . DistributedOptions , NoneType ] = None , fairscale_oss : bool = False , fairscale_sddp : bool = False , fairscale_fsdp : bool = False , configs : Union [ List [ Union [ stoke . configs . AMPConfig , stoke . configs . ApexConfig , stoke . configs . DDPConfig , stoke . configs . DeepspeedConfig , stoke . configs . FairscaleOSSConfig , stoke . configs . FairscaleSDDPConfig , stoke . configs . FairscaleFSDPConfig , stoke . configs . HorovodConfig ]], NoneType ] = None , info_rank : Union [ int , List [ int ], NoneType ] = 0 , verbose : bool = True , ema_weight : float = 0.1 ) Attributes Name Type Description Default amp_config None None None apex_config None None None batch_size None None None cuda None None None ddp_config None None None deepspeed_config None None None distributed None None None effective_batch_size None None None ema_loss None None None fp16 None None None fsdp_config None None None fully_sharded None None None gpu None None None grad_accum None None None grad_clip None None None horovod_config None None None is_amp None None None is_apex None None None is_ddp None None None is_deepspeed None None None is_horovod None None None loss_access None None None model_access None None None nccl None None None num_model_parameters None None None optimizer None None None oss None None None oss_config None None None rank None None None scaler None None None sddp_config None None None sharded None None None status None None None world_size None None None _agg_loss Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) None _backward_steps int Number of times gradients have been calculated on a batch of samples (calls to backward) None _grad_accum_counter int counter for grad accumulation steps None _loss Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs None _last_step_loss list, tuple, or float last loss step calculation aggregated over device(s) None _model torch.nn.Module instance of torch.nn.Module for Stoke to handle None _optimizer StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs None _optimizer_steps int Number of times step has been called on the optimizer None _runner StokeRunner the dynamically created runtime object that handles all ops None _status StokeStatus StokeStatus object that sets and maintains the current configuration None _verbose bool print verbosity None _rolling_loss_steps int number of steps that have been called for the rolling loss None _rolling_mean_loss list, tuple, or float current ema loss None _ema_weight float weight used for any ema calculation on metrics None ??? example \"View Source\" class Stoke: \"\"\"High level stoke object that manages all necessary configs and provides a unified interface to ops This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model, loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale), mixed-precision (AMP or APEX) and devices (CPU or GPU) Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size ema_loss fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_amp is_apex is_ddp is_deepspeed is_horovod loss_access model_access nccl num_model_parameters optimizer oss oss_config rank scaler sddp_config sharded status world_size _agg_loss: Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) _backward_steps: int Number of times gradients have been calculated on a batch of samples (calls to backward) _grad_accum_counter: int counter for grad accumulation steps _loss: Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs _last_step_loss: list, tuple, or float last loss step calculation aggregated over device(s) _model: torch.nn.Module instance of torch.nn.Module for Stoke to handle _optimizer: StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs _optimizer_steps: int Number of times step has been called on the optimizer _runner: StokeRunner the dynamically created runtime object that handles all ops _status: StokeStatus StokeStatus object that sets and maintains the current configuration _verbose: bool print verbosity _rolling_loss_steps: int number of steps that have been called for the rolling loss _rolling_mean_loss: list, tuple, or float current ema loss _ema_weight: float weight used for any ema calculation on metrics \"\"\" def __init__( self, model: torch.nn.Module, optimizer: StokeOptimizer, loss: Union[Callable, List[Callable], Tuple[Callable]], batch_size_per_device: int, grad_accum_steps: Optional[int] = 1, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None, gpu: bool = False, fp16: Optional[FP16Options] = None, distributed: Optional[DistributedOptions] = None, fairscale_oss: bool = False, fairscale_sddp: bool = False, fairscale_fsdp: bool = False, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ] = None, info_rank: Optional[Union[int, List[int]]] = 0, verbose: bool = True, ema_weight: float = 0.1, ): \"\"\"Init for Stoke class object Parameters ---------- model: torch.nn.Module PyTorch model optimizer: StokeOptimizer Optimizer configuration loss: Union[Callable, List[Callable], Tuple[Callable]] Callable loss function or functions batch_size_per_device: int Batch size at the single device level grad_accum_steps: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None Configuration objects for runtimes info_rank: Optional[Union[int, List[int]]], default = 0 Constrain prints to specific devices verbose: bool, default: True Flag for verbosity ema_weight: float, default: 0.5 weight used for any ema calculation on metrics \"\"\" # Verbosity self._verbose = verbose # Info rank self._info_rank = info_rank # EMA self._ema_weight = ema_weight # Setup the StokeState self._status = StokeStatus( batch_size_per_device=batch_size_per_device, grad_accum=grad_accum_steps, grad_clip=grad_clip, gpu=gpu, fp16=fp16, distributed=distributed, fairscale_oss=fairscale_oss, fairscale_sddp=fairscale_sddp, fairscale_fsdp=fairscale_fsdp, configs=configs, ) # Run some checks self._model = self._check_model(model) self._optimizer = self._check_optimizer(optimizer) self._loss = self._check_loss(loss) # Dynamically construct the StokeRunner from the StokeStatus self._runner, class_info = self._build_runner() # Setup distributed backend self._runner.setup_distributed() # Post here the runner will have the print_device function that is mapped to the self.print here # as it needs rank to be accessible before working if self._verbose: dev_id = ( self.rank if (self.rank == \"cpu\" or self.rank == \"gpu\") else self._info_rank ) self.print(f\"Printing verbose information on rank(s): {dev_id}\") # Print the runner class info from the mixins self.print(class_info) # Possibly place model on GPU depending on StokeStatus -- before wrap calls self._place_model_on_gpu() # Handle the wrap ops in the correct order self._handle_ordered_wrap_ops(optimizer=optimizer) # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 # Set post-init status variables self._status.set_post_init_values(world_size=self.world_size) # Print the final configuration if self._verbose: self.print(msg=self._status) def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of optimizer then the model This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped methods are called Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed through self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer) # Wrap with distributed backend -- in this case the optimizer is passed through self._model, self._optimizer = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer ) def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of model then optimizer Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn't exist yet # don't use the return for the optimizer in this case self._model, _ = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=None ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn't exist yet self._runner.wrap_fp16(model=self._model, optimizer=None) # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed) if (self.sharded and self.oss) or self.is_apex or self.is_horovod: self._wrap_optimizer_then_model(optimizer=optimizer) else: self._wrap_model_then_optimizer(optimizer=optimizer) def _check_accum(self): \"\"\"Checks if the current step is the last accumulation step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0 def _check_pre_accum(self): \"\"\"Checks if we are at the pre-accumulate step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum def _set_loss_to_zero(self): \"\"\"Used to set a loss tracker to zero depending on the type Returns ------- float or list or tuple of reset loss \"\"\" return ( type(self._loss)([0.0] * len(self._loss)) if isinstance(self._loss, (list, tuple)) else 0.0 ) def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\") def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\") def _scale_agg_loss(self): \"\"\"Scales the mean aggregated loss by grad accum Returns ------- scale_vals: list or float of mean aggregated loss \"\"\" if isinstance(self._agg_loss, (list, tuple)): scale_vals = [ val / self.grad_accum for idx, val in enumerate(self._agg_loss) ] else: scale_vals = self._agg_loss / self.grad_accum return scale_vals def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\") def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank) def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line ) @staticmethod def _check_model(model: torch.nn.Module): \"\"\"Verifies the type of the model Parameters ---------- model: torch.nn.Module current torch model Returns ------- None \"\"\" # Check if the model is an nn.Module such that it has a forward method if not isinstance(model, torch.nn.Module): raise TypeError( f\"Stoke -- Model is not of type torch.nn.Module, currently {type(model)}\" ) return model @staticmethod def _check_optimizer(optimizer: StokeOptimizer): \"\"\"Verifies the type of the optimizer Parameters ---------- optimizer: StokeOptimizer Current optimizer configuration TypedDict (aka dict) Returns ------- None \"\"\" if not isinstance(optimizer, dict): raise TypeError( f\"Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}\" ) return optimizer def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]): \"\"\"Checks to make sure the loss function(s) is/are callable Parameters ---------- loss: Union[Callable, List[Callable], Tuple[Callable]] Current callable loss(es) Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): loss = [self._check_loss(val) for val in loss] return loss elif isinstance(loss, Callable): return loss else: raise TypeError( f\"Stoke -- Loss is not of type Callable, currently {type(loss)}\" ) def _place_model_on_gpu(self): \"\"\"Automatically moves the model to GPU device(s) Returns ------- None \"\"\" if self.gpu and not self.is_deepspeed: if self._verbose: self.print(f\"Automatically handling moving model to GPU(s)...\") self._model.cuda() def _build_runner(self): \"\"\"Builds the runtime object from the mixin style classes Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a a single interface. Might prevent some IDE type-hinting as it's dynamic Returns ------- StokeRunner runtime runner object \"\"\" # Get the classes dist_class = self._get_distributed_mixin() fp16_class = self._get_fp16_mixin() optimizer_class = self._get_optimizer_mixin() io_class = self._get_io_mixin() # Python MRO hack to make sure the inits of all the Mixin classes get called def __multiple_mixin_init__(*args, **kwargs): dist_class.__init__(*args, **kwargs) fp16_class.__init__(*args, **kwargs) optimizer_class.__init__(*args, **kwargs) io_class.__init__(*args, **kwargs) # Configs pass through kwargs_dict = { \"amp_config\": self.amp_config, \"apex_config\": self.apex_config, \"ddp_config\": self.ddp_config, \"deepspeed_config\": self.deepspeed_config, \"horovod_config\": self.horovod_config, \"oss_config\": self.oss_config, \"sharded_config\": self.sddp_config, \"fully_sharded_config\": self.fsdp_config, } # Generate the runner class from the mixins based on the StokeStatus runner_class = type( \"StokeRunner\", (dist_class, fp16_class, optimizer_class, io_class), {\"__init__\": __multiple_mixin_init__}, )( verbose=self._verbose, batch_size_per_device=self.batch_size, grad_accum_steps=self.grad_accum, grad_clip=self.grad_clip, info_rank=self._info_rank, loss=self._loss, **kwargs_dict, ) # Make a list of class info for print later class_info = [ f\"Distributed Mixin: {dist_class.__name__}\", f\"Optimizer Mixin: {dist_class.__name__}\", f\"FP16 Mixin: {fp16_class.__name__}\", f\"IO Mixin: {io_class.__name__}\", ] return runner_class, class_info def _get_io_mixin(self): \"\"\"Determines which IO class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated ioclass \"\"\" if self.is_deepspeed: return_class = RunnerIOEnum.deepspeed.value elif self.is_horovod: return_class = RunnerIOEnum.horovod.value elif self.is_ddp: return_class = RunnerIOEnum.ddp.value else: return_class = RunnerIOEnum.base.value return return_class def _get_optimizer_mixin(self): \"\"\"Determines which optimizer class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated optimizer class \"\"\" if self.oss: return_class = RunnerOptimizerEnum.oss.value else: return_class = RunnerOptimizerEnum.base.value return return_class def _get_distributed_mixin(self): \"\"\"Determines which distributed class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated distributed class \"\"\" # if not gpu then fall to cpu single if not self.gpu: return_class = RunnerDistEnum.cpu.value # if gpu but no distributed then fall to single gpu elif self.gpu and (self.distributed is None): return_class = RunnerDistEnum.gpu.value elif self.gpu and (self.distributed is not None): return_class = RunnerDistEnum[self.distributed].value else: raise ValueError(\"Stoke -- Cannot map to a valid distributed class\") return return_class def _get_fp16_mixin(self): \"\"\"Determines which fp16 class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated fp16 class \"\"\" if self.fp16 is not None: return_class = RunnerFP16Enum[self.fp16].value else: return_class = RunnerFP16Enum.full.value return return_class def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) def model(self, *args, **kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs) def loss(self, *args, **kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]): \"\"\"Handles calculating the ema loss Parameters ---------- loss: Union[float, List[float], Tuple[float]] current calculated loss list, tuple or float Returns ------- None \"\"\" self._rolling_loss_steps += 1 if isinstance(loss, (list, tuple)): self._rolling_mean_loss = type(self._rolling_mean_loss)( self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx]) for idx, val in enumerate(loss) ) else: self._rolling_mean_loss = self._ema_loss( value=loss, current_mean=self._rolling_mean_loss ) def _ema_loss(self, value: float, current_mean: float): \"\"\"Calculate the ema of the loss Parameters ---------- value: float current loss value current_mean: float current mean value Returns ------- current ema value: float \"\"\" if self._rolling_loss_steps == 1: return value else: return (self._ema_weight * value) + ( (1.0 - self._ema_weight) * current_mean ) def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1 def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) def _reset(self): \"\"\"Resets the state post optimizer step call Returns ------- None \"\"\" if self._verbose: self.print(\"Resetting all grad/variables for next optimizer step\") # Zero the grads if not deepspeed if not self.is_deepspeed: self.zero_grads() # Reset counter self._grad_accum_counter = 0 # Reset agg loss -- single or mutiple losses self._agg_loss = self._set_loss_to_zero() def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" ) def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device) def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod ) def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset() def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" ) def _load_fp16_state_dict_fn(self): \"\"\"Returns the function to load the sacler state dict Returns ------- mp_state_dict_fn: Callable, default: None callable function to load the scaler state dict \"\"\" mp_state_dict_fn = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict_fn = amp.load_state_dict except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) else: mp_state_dict_fn = self.scaler.load_state_dict return mp_state_dict_fn def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier() @property def step_loss(self): \"\"\"Gets the last step loss synced across device(s) (unscaled)\"\"\" return self._last_step_loss @property def model_access(self): \"\"\"Interface for model access due to the different types between the DP, DDP, and SDDP implementations\"\"\" if isinstance(self._model, (DDP, DP, SDDP, FSDP)): return self._model.module else: return self._model @property def loss_access(self): \"\"\"Gets loss tensor(s)\"\"\" return self._loss @property def optimizer(self): \"\"\"Gets the optimizer\"\"\" return self._optimizer @property def scaler(self): \"\"\"Gets the current scaler object\"\"\" return self._runner.scaler @property def fp16_state_dict(self): \"\"\"Gets the fp16 state dict from various methods\"\"\" mp_state_dict = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict = amp.state_dict() except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) elif self.is_amp: mp_state_dict = self.scaler.state_dict() return mp_state_dict @property def status(self): \"\"\"Gets the StokeStatus object\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.batch_size @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self._status.effective_batch_size @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.grad_clip @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.grad_accum @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.gpu @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.cuda @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.nccl @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.fp16 @property def is_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self._status.is_fp16_apex @property def is_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self._status.is_fp16_amp @property def distributed(self): \"\"\"Shortcut to distributed status\"\"\" return self._status.distributed @property def is_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self._status.is_distributed_ddp @property def is_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self._status.is_distributed_horovod @property def is_deepspeed(self): \"\"\"Returns if Deepspeed is acticated\"\"\" return self._status.is_distributed_deepspeed @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.oss @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.sharded @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.fully_sharded @property def world_size(self): \"\"\"Shortcut to get world size\"\"\" return self._runner.world_size @property def rank(self): \"\"\"Shortcut to get rank\"\"\" return self._runner.rank @property def amp_config(self): \"\"\"Returns amp config or None based on amp state\"\"\" return self._status.amp_config if self.is_amp else None @property def apex_config(self): \"\"\"Returns apex config or None based on apex state\"\"\" return self._status.apex_config if self.is_apex else None @property def ddp_config(self): \"\"\"Returns ddp config or None based on ddp state\"\"\" return self._status.ddp_config if self.is_ddp else None @property def deepspeed_config(self): \"\"\"Returns deepspeed config or None based on deepspeed state\"\"\" return self._status.deepspeed_config if self.is_deepspeed else None @property def oss_config(self): \"\"\"Returns oss config or None based on ossstate\"\"\" return self._status.oss_config if self.oss else None @property def sddp_config(self): \"\"\"Returns sddp config or None based on sddp state\"\"\" return self._status.sddp_config if self.sharded else None @property def fsdp_config(self): \"\"\"Returns fsdp config or None based on fsdp state\"\"\" return self._status.fsdp_config if self.fully_sharded else None @property def horovod_config(self): \"\"\"Returns horovod config or None based on horovod state\"\"\" return self._status.horovod_config if self.is_horovod else None @property def num_model_parameters(self): \"\"\"Returns number of parameters that require gradients\"\"\" return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad) @property def ema_loss(self): \"\"\"Returns the current rolling mean loss\"\"\" return self._rolling_mean_loss Instance variables amp_config Returns amp config or None based on amp state apex_config Returns apex config or None based on apex state batch_size Shortcut to batch size cuda Shortcut to get cuda status ddp_config Returns ddp config or None based on ddp state deepspeed_config Returns deepspeed config or None based on deepspeed state distributed Shortcut to distributed status effective_batch_size Shortcut to effective batch size ema_loss Returns the current rolling mean loss fp16 Shortcut to get FP16 status fp16_state_dict Gets the fp16 state dict from various methods fsdp_config Returns fsdp config or None based on fsdp state fully_sharded Returns if Fairscale fully sharded DDP status gpu Shortcut to get GPU status grad_accum Shortcut to get grad accumulation grad_clip Shortcut to get grad clip horovod_config Returns horovod config or None based on horovod state is_amp Returns if AMP is activated is_apex Returns if APEX is activated is_ddp Returns if DDP is activated is_deepspeed Returns if Deepspeed is acticated is_horovod Returns if Horovod is activated loss_access Gets loss tensor(s) model_access Interface for model access due to the different types between the DP, DDP, and SDDP implementations nccl Shortcut to get nccl status num_model_parameters Returns number of parameters that require gradients optimizer Gets the optimizer oss Returns if Fairscale optimizer state sharding status oss_config Returns oss config or None based on ossstate rank Shortcut to get rank scaler Gets the current scaler object sddp_config Returns sddp config or None based on sddp state sharded Returns if Fairscale sharded DDP status status Gets the StokeStatus object step_loss Gets the last step loss synced across device(s) (unscaled) world_size Shortcut to get world size Methods DataLoader def DataLoader ( self , dataset : torch . utils . data . dataset . Dataset [ + T_co ], shuffle : bool = False , sampler : Union [ torch . utils . data . sampler . Sampler [ int ], NoneType ] = None , batch_sampler : Union [ torch . utils . data . sampler . Sampler [ Sequence [ int ]], NoneType ] = None , num_workers : int = 0 , collate_fn : Callable [[ List [ ~ T ]], Any ] = None , pin_memory : bool = False , drop_last : bool = False , timeout : float = 0 , worker_init_fn : Callable [[ int ], NoneType ] = None , multiprocessing_context = None , generator = None , * , prefetch_factor : int = 2 , persistent_workers : bool = False ) Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters: Name Type Description Default dataset Dataset dataset from which to load the data. None shuffle bool, default: False set to True to have the data reshuffled at every epoch. None sampler Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, :attr: shuffle must not be specified. None batch_sampler Sampler or Iterable, default: None: like :attr: sampler , but returns a batch of indices at a time. Mutually exclusive with :attr: batch_size , :attr: shuffle , :attr: sampler , and :attr: drop_last . None num_workers int, default: 0 how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. None collate_fn callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. None pin_memory bool, default: False: If True , the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr: collate_fn returns a batch that is a custom type, see the example below. None drop_last bool, default: False set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. None timeout numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. None worker_init_fn callable, default: None If not None , this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1] ) as input, after seeding and before data loading. None prefetch_factor int, default: 2 Number of samples loaded in advance by each worker. 2 means there will be a total of 2 * num_workers samples prefetched across all workers. None persistent_workers bool, default: False If True , the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. None Returns: Type Description StokeDataLoader wrapped torch.utils.data.DataLoader object ??? example \"View Source\" def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) backward def backward ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]] ) Wrapped backwards call Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) None Returns: Type Description None None ??? example \"View Source\" def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1 barrier def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier() detach_and_sync_loss def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) None device default: None device to sync across None Returns: Type Description loss that is synced across devices and all_reduced w/ SUM None ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device) dump_model_parameter_info def dump_model_parameter_info ( self ) Dumps all parameter information for named parameters (shape, device, dtype) Returns: Type Description None None ??? example \"View Source\" def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" ) load def load ( self , path : str , tag : str , strict : bool = True ) Loads a model checkpoint using the correct backend interface Parameters: Name Type Description Default path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None strict bool ignore non-matching keys None Returns: Type Description dict, default: None a dictionary of any custom fields the user passed to the save function ??? example \"View Source\" def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras loss def loss ( self , * args , ** kwargs ) Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters: Name Type Description Default *args list or tuple Additional arguments should be passed as keyword arguments None **kwargs dict Extra arguments passed to the loss function call(s) None Returns: Type Description outputs of callable loss function(s) None ??? example \"View Source\" def loss(self, args, *kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss model def model ( self , * args , ** kwargs ) Wrapped model forward call Parameters: Name Type Description Default *args list or tuple Additional arguments should be passed as keyword arguments None **kwargs dict Extra arguments passed to the model forward call None Returns: Type Description model forward output None ??? example \"View Source\" def model(self, args, *kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs) print def print ( self , msg : Union [ str , List [ str ]], single_line : bool = False ) Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters: Name Type Description Default msg str message to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line ) print_ema_loss def print_ema_loss ( self , prepend_msg : str = 'Current EMA Loss' , single_line : bool = False ) Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default prepend_msg str, default: \"Current EMA Loss\" message prepend to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\") print_mean_accumulated_synced_loss def print_mean_accumulated_synced_loss ( self , prepend_msg : str = 'Mean Accumulated & Synced Loss' , pre_backwards : bool = True , single_line : bool = False ) Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default prepend_msg str, default: \"Mean Accumulated & Synced Loss\" message prepend to print None pre_backwards bool, default: True if being called pre backward step None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\") print_num_model_parameters def print_num_model_parameters ( self , normalize : stoke . utils . ParamNormalize = < ParamNormalize . MILLION : 1000000.0 > ) Parameters: Name Type Description Default normalize ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing None Returns: Type Description None None ??? example \"View Source\" def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" ) print_on_devices def print_on_devices ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 ) Wraps runner print interface for shorter semantics Parameters: Name Type Description Default msg str message to print None rank Union[int, List[int]], default: 0 which ranks to print on None Returns: Type Description None None ??? example \"View Source\" def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank) print_synced_loss def print_synced_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], prepend_msg : str = 'Step Synced Loss' , device = None , single_line : bool = False ) Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None prepend_msg str, default: \"Step Synced Loss\" message prepend to print None device default: None specify the device to place the synced loss on (defaults to same device) same single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\") reset def reset ( self ) Public method for resetting the underlying stoke state Returns: Type Description None None ??? example \"View Source\" def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset() reset_ema def reset_ema ( self ) Used to reset the current state of the rolling mean loss Returns: Type Description None None ??? example \"View Source\" def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 reset_tracking def reset_tracking ( self ) Public method for resetting all underlying stoke tracked variables Returns: Type Description None None ??? example \"View Source\" def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 save def save ( self , path : str , name : str = UUID ( '1bec68f4-7df7-48d2-a526-14685e92f54f' ), extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Saves a model checkpoint using the correct backend interface Parameters: Name Type Description Default path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None name str, default: uuid4() name used to save checkpoint file None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag step def step ( self ) Wrapped step call Handles grad clipping internally Returns: Type Description None None ??? example \"View Source\" def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) zero_grads def zero_grads ( self ) Zeros the optimizer grads depending on the optimizer type Returns: Type Description None None ??? example \"View Source\" def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod ) StokeOptimizer class StokeOptimizer ( / , * args , ** kwargs ) Attributes Name Type Description Default optimizer Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class None optimizer_kwargs Dict any keyword args to be unrolled into the optimizer at instantiation time None ??? example \"View Source\" class StokeOptimizer(TypedDict): \"\"\"Stoke optimizer wrapper class Given all the different backends and extensions the optimizer might need to be instantiated in a different way thus this typed dict holds the configuration without instantiation Attributes ---------- optimizer: Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class optimizer_kwargs: Dict any keyword args to be unrolled into the optimizer at instantiation time \"\"\" optimizer: Type[torch.optim.Optimizer] optimizer_kwargs: Dict Ancestors (in MRO) builtins.dict Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values","title":"Index"},{"location":"reference/stoke/#module-stoke","text":"Stoke is a lightweight wrapper for PyTorch that provides a simple unified interface for context switching Please refer to the documentation provided in the README.md ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Stoke is a lightweight wrapper for PyTorch that provides a simple unified interface for context switching Please refer to the documentation provided in the README.md \"\"\" from .configs import * from .data import BucketedDistributedSampler from .status import DistributedOptions, FP16Options from .stoke import Stoke from .utils import ParamNormalize __all__ = [ \"Stoke\", \"ParamNormalize\", \"FP16Options\", \"DistributedOptions\", \"StokeOptimizer\", \"ClipGradNormConfig\", \"ClipGradConfig\", \"FairscaleOSSConfig\", \"FairscaleSDDPConfig\", \"FairscaleFSDPConfig\", \"HorovodConfig\", \"ApexConfig\", \"DeepspeedConfig\", \"DDPConfig\", \"AMPConfig\", \"DeepspeedAIOConfig\", \"DeepspeedActivationCheckpointingConfig\", \"DeepspeedFlopsConfig\", \"DeepspeedFP16Config\", \"DeepspeedPLDConfig\", \"DeepspeedOffloadOptimizerConfig\", \"DeepspeedOffloadParamConfig\", \"DeepspeedTensorboardConfig\", \"DeepspeedZeROConfig\", \"BucketedDistributedSampler\", ] from ._version import get_versions __version__ = get_versions()[\"version\"] del get_versions","title":"Module stoke"},{"location":"reference/stoke/#sub-modules","text":"stoke.configs stoke.data stoke.distributed stoke.extensions stoke.fp16 stoke.io stoke.status stoke.stoke stoke.utils","title":"Sub-modules"},{"location":"reference/stoke/#classes","text":"","title":"Classes"},{"location":"reference/stoke/#ampconfig","text":"class AMPConfig ( backoff_factor : float = 0.5 , growth_factor : float = 2.0 , growth_interval : int = 2000 , init_scale : float = 65536.0 )","title":"AMPConfig"},{"location":"reference/stoke/#attributes","text":"Name Type Description Default backoff_factor float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration None growth_factor float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. None growth_interval int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor None init_scale float, default: 2.**16 Initial scale factor None ??? example \"View Source\" class AMPConfig: \"\"\"PyTorch AMP configuration class Attributes ---------- backoff_factor : float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration growth_factor : float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. growth_interval : int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor init_scale : float, default: 2.**16 Initial scale factor \"\"\" backoff_factor: float = 0.5 growth_factor: float = 2.0 growth_interval: int = 2000 init_scale: float = 2.0 ** 16","title":"Attributes"},{"location":"reference/stoke/#apexconfig","text":"class ApexConfig ( cast_model_outputs : Union [ torch . dtype , NoneType ] = None , convert_to_sync_batch_norm : bool = False , max_loss_scale : float = 16777216.0 , min_loss_scale : Union [ float , NoneType ] = None , scaler_per_loss : bool = False , verbosity : int = 0 )","title":"ApexConfig"},{"location":"reference/stoke/#attributes_1","text":"Name Type Description Default cast_model_outputs Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm None max_loss_scale float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling None min_loss_scale Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed value scaler_per_loss bool, default: False Option to impose a scaler for each loss instead of a global scaler None verbosity int, default: 0 Set to 0 to suppress Amp-related output None ??? example \"View Source\" class ApexConfig: \"\"\"Nvidia APEX configuration class Attributes ---------- cast_model_outputs: Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm max_loss_scale: float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling min_loss_scale: Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed scaler_per_loss: bool, default: False Option to impose a scaler for each loss instead of a global scaler verbosity: int, default: 0 Set to 0 to suppress Amp-related output \"\"\" cast_model_outputs: Optional[torch.dtype] = None convert_to_sync_batch_norm: bool = False max_loss_scale: float = 2.0 ** 24 min_loss_scale: Optional[float] = None scaler_per_loss: bool = False verbosity: int = 0","title":"Attributes"},{"location":"reference/stoke/#bucketeddistributedsampler","text":"class BucketedDistributedSampler ( dataset : torch . utils . data . dataset . Dataset , buckets : int , batch_size : int , sorted_idx : List , backend : stoke . status . DistributedOptions , allow_bucket_overlap : bool = False , num_replicas : Union [ int , NoneType ] = None , rank : Union [ int , NoneType ] = None , shuffle : bool = True , seed : int = 0 , drop_last : bool = False , info_rank : int = 0 )","title":"BucketedDistributedSampler"},{"location":"reference/stoke/#attributes_2","text":"Name Type Description Default num_replicas int, default: None number of replicas None rank int, default: None current device rank None epoch int current training epoch None drop_last bool, default: False whether to drop last set of samples that don't fit into a batch None shuffle bool, default: True flag to shuffle dataset None seed int, default: 0 seed to use for generators None buckets int number of buckets to break the dataset into None sorted_n_samples list sorted list of samples by the characteristic to bucket by (e.g. seq len) None batch_size int batch size that will be used (needed to make sure slices are correct) None allow_bucket_overlap bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch None slice_size int computed from batch size and number of replicas None num_samples_per_bucket int computed value that represents the number of samples in a single bucket None num_slices_per_bucket int computed value that represents the number of slices available in a bucket None bucket_idx list computed value that make a contiguous list of indices in each bucket None rounded_num_samples_per_bucket int computed value post round for number of samples in a single bucket None rounded_num_samples_per_replica int computed value post round for number of slices available in a bucket None ??? example \"View Source\" class BucketedDistributedSampler(Sampler[T_co]): \"\"\"Sampler that buckets samples by sorted_idx and then randomly samples from a specific bucket to prevent excess padding leading to wasted computation Borrowing heavily from the base DistributedSampler https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler Attributes ---------- num_replicas: int, default: None number of replicas rank: int, default: None current device rank epoch: int current training epoch drop_last: bool, default: False whether to drop last set of samples that don't fit into a batch shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators buckets: int number of buckets to break the dataset into sorted_n_samples: list sorted list of samples by the characteristic to bucket by (e.g. seq len) batch_size: int batch size that will be used (needed to make sure slices are correct) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch slice_size: int computed from batch size and number of replicas num_samples_per_bucket: int computed value that represents the number of samples in a single bucket num_slices_per_bucket: int computed value that represents the number of slices available in a bucket bucket_idx: list computed value that make a contiguous list of indices in each bucket rounded_num_samples_per_bucket: int computed value post round for number of samples in a single bucket rounded_num_samples_per_replica: int computed value post round for number of slices available in a bucket \"\"\" def __init__( self, dataset: Dataset, buckets: int, batch_size: int, sorted_idx: List, backend: DistributedOptions, allow_bucket_overlap: bool = False, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0, drop_last: bool = False, info_rank: int = 0, ) -> None: \"\"\"Init for BucketedDistributedSampler Parameters ---------- dataset: Dataset dataset from which to load the data. buckets: int number of buckets to break the dataset into batch_size: int batch size that will be used (needed to make sure slices are correct) sorted_idx: list sorted list of samples by the characteristic to bucket by (e.g. seq le backend: DistributedOptions which backend is being used (as rank, world size, etc. need to be used) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch num_replicas: int, default: None number of replicas rank: int, default: None current device rank shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators drop_last: bool, default: False whether to drop last set of samples that don't fit into a info_rank: int, default: 0 which device to print information on \"\"\" # If the backend isnt DDP there needs to be an additional import num_replicas, rank = self._conditional_distributed( backend=backend, num_replicas=num_replicas, rank=rank ) self.num_replicas = num_replicas self.rank = rank self.epoch = 0 self.drop_last = drop_last self.shuffle = shuffle self.seed = seed self.buckets = buckets self.sorted_n_samples = sorted_idx # Batch size is needed here so a contiguous iter of buckets can be formed self.batch_size = batch_size # This is a flag to batch up the dropped samples (that would be 'wasted') if drop_last is flagged self.allow_bucket_overlap = allow_bucket_overlap # Calculate the size of each slice that will be indexed across the replicas self.slice_size = self.batch_size * self.num_replicas # Calculate the size of the buckets (rounded or not based on drop last) self.num_samples_per_bucket = self._get_size( len(dataset), self.buckets, self.drop_last ) # Calculate the number of slices per bucket self.num_slices_per_bucket = self._get_size( self.num_samples_per_bucket, self.slice_size, self.drop_last ) if self.num_samples_per_bucket < self.slice_size: raise ValueError( f\"Stoke -- Resulting number of slices (batch * replicas) per bucket \" f\"({self.num_samples_per_bucket}) is less than the batch size \" f\"({self.batch_size})\" ) if self.num_slices_per_bucket < 2: raise ValueError( f\"Stoke -- Number of slices per bucket {self.num_slices_per_bucket} is less than 2 \" f\"which is not recommended\" ) if self.num_samples_per_bucket < 100: raise ValueError( f\"Stoke -- Number of samples per bucket {self.num_samples_per_bucket} is less than 100 \" f\"which is not recommended as this might lead to dropping of excessive data\" ) # Split into buckets and turn into lists self.bucket_idx = [ list(val) for val in np.array_split(self.sorted_n_samples, self.buckets) ] # Calculate the post rounded numbers self.rounded_num_samples_per_bucket = ( self.slice_size * self.num_slices_per_bucket ) self.rounded_num_samples_per_replica = ( self.num_slices_per_bucket * self.batch_size * self.buckets ) # Add the bucket overlap samples if self.allow_bucket_overlap: self.rounded_num_samples_per_replica += ( (len(dataset) - (self.rounded_num_samples_per_bucket * self.buckets)) // self.slice_size ) * self.batch_size if self.rank == info_rank: print( f\"Stoke -- BucketedDistributedSampler -- # Samples Per Bucket: \" f\"{self.rounded_num_samples_per_bucket}, # of Samples Per Replica: \" f\"{self.rounded_num_samples_per_replica}\" ) def _conditional_distributed( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\" Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" return self._check_backend(backend, num_replicas, rank) def _get_backend_functions(self, backend: DistributedOptions): \"\"\"Gets backend functions if needed Parameters ---------- backend: DistributedOptions which backend is being used Returns ------- Tuple[bool, int, int] is_init, num_replicas, rank \"\"\" if backend.value == \"ddp\" or backend.value == \"deepspeed\": return ( torch.distributed.is_initialized, torch.distributed.get_world_size, torch.distributed.get_rank, ) else: return hvd.is_initialized, hvd.size, hvd.rank def _check_backend( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\"Checks the backend for correct device info Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" if num_replicas is None or rank is None: is_avail, get_world_size, get_rank = self._get_backend_functions( backend=backend ) if num_replicas is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) num_replicas = get_world_size() if rank is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) rank = get_rank() return num_replicas, rank @staticmethod def _get_size(data_len: int, split_var: int, drop_last: bool = False): \"\"\"Gets the size of a split Parameters ---------- data_len: int current dataset length split_var: int how many to split into drop_last: bool, default: False drop last hanging samples if not batch_size Returns ------- num_samples: int \"\"\" if drop_last: num_samples = data_len // split_var else: num_samples = ceil(data_len / split_var) return num_samples def __iter__(self) -> Iterator[T_co]: \"\"\"Handles assembling the batches from a bucketed perspective Shuffle bucket order->Pad if necessary->Slice across replicas->Possibly batch up residuals->shuffle bucketed batches->Unroll into list->Make iter Returns ------- Iterator[T_co] \"\"\" # Shuffle the bucketed idx if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute each bucket indices = [ [val[idx] for idx in torch.randperm(len(val), generator=g).tolist()] for val in self.bucket_idx ] else: indices = self.bucket_idx # Iterate over the buckets for idx, val in enumerate(indices): # If this is true we need to handle padding if (self.num_slices_per_bucket * self.slice_size) > len(val): split_val = self._handle_padding(val) indices[idx] = list(itertools.chain(*split_val)) assert len(indices[idx]) == self.rounded_num_samples_per_bucket # Now slice across replicas final_indices = [] for val in indices: for idx in range(self.num_slices_per_bucket): replica_slice = val[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] final_indices.append(replica_slice) # If bucket overlap is allowed then we just batch up the residual indices if self.drop_last and self.allow_bucket_overlap: residual_idx = list( itertools.chain( *[val[self.rounded_num_samples_per_bucket :] for val in indices] ) ) if len(residual_idx) > self.slice_size: # Cut by slices then by replicas residual_idx = [ residual_idx[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] for idx in range(len(residual_idx) // self.slice_size) ] # Append to the final indices final_indices.extend(residual_idx) # Shuffle the bucketed batches if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute the bucket order final_indices = [ final_indices[val] for val in torch.randperm(len(final_indices), generator=g) ] # Unroll into a single list final_indices = list(itertools.chain(*final_indices)) assert len(final_indices) == self.rounded_num_samples_per_replica return iter(final_indices) def _handle_padding(self, idx_list: List): \"\"\"Handles padding out if a batch is short Parameters ---------- idx_list: List list of indices Returns ------- split_val: List list with correctly padded sizes \"\"\" split_val = [] for idx in range(self.num_slices_per_bucket): if idx == (self.num_slices_per_bucket - 1): # Get the short batch short_batch = idx_list[(idx * self.slice_size) :] # Short batch replica slice sizes short_len = [ self.batch_size - len(list(val)) for val in np.array_split(short_batch, self.num_replicas) ] # Pop the necessary values from the entire bucket pad_values = [ idx_list[s_idx : (self.num_replicas * s_len) : self.num_replicas] for s_idx, s_len in enumerate(short_len) ] # If not a consistent list then we need to reorder so that the step size alignment slicing # of the replicas works if len(set(short_len)) != 1: # here we need to find the first larger idx and reorder first_idx = short_len.index(max(set(short_len))) # Reorder pad_values = pad_values[first_idx:] + pad_values[0:first_idx] extended_batch = short_batch + [ pad for pad in list( itertools.chain(*itertools.zip_longest(*pad_values)) ) if pad is not None ] split_val.append(extended_batch) else: split_val.append( idx_list[(idx * self.slice_size) : ((idx + 1) * self.slice_size)] ) return split_val def __len__(self) -> int: return self.rounded_num_samples_per_replica def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch","title":"Attributes"},{"location":"reference/stoke/#ancestors-in-mro","text":"torch.utils.data.sampler.Sampler typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/stoke/#methods","text":"","title":"Methods"},{"location":"reference/stoke/#set_epoch","text":"def set_epoch ( self , epoch : int ) -> None Sets the epoch for this sampler. When :attr: shuffle=True , this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters: Name Type Description Default epoch int Epoch number None ??? example \"View Source\" def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch","title":"set_epoch"},{"location":"reference/stoke/#clipgradconfig","text":"class ClipGradConfig ( clip_value : float )","title":"ClipGradConfig"},{"location":"reference/stoke/#attributes_3","text":"Name Type Description Default clip_value float maximum allowed absolute value of the gradients [-clip_value, clip_value] None ??? example \"View Source\" class ClipGradConfig: \"\"\"Gradient clipping by value configuration class Attributes ---------- clip_value: float maximum allowed absolute value of the gradients [-clip_value, clip_value] \"\"\" clip_value: float","title":"Attributes"},{"location":"reference/stoke/#clipgradnormconfig","text":"class ClipGradNormConfig ( max_norm : float , norm_type : float )","title":"ClipGradNormConfig"},{"location":"reference/stoke/#attributes_4","text":"Name Type Description Default max_norm float max norm of the gradients None norm_type float type of the used p-norm None ??? example \"View Source\" class ClipGradNormConfig: \"\"\"Gradient clipping by p-norm configuration class Attributes ---------- max_norm: float max norm of the gradients norm_type: float type of the used p-norm \"\"\" max_norm: float norm_type: float","title":"Attributes"},{"location":"reference/stoke/#ddpconfig","text":"class DDPConfig ( local_rank : Union [ int , NoneType ], auto_mpi_discovery : bool = False , convert_to_sync_batch_norm : bool = False , backend : stoke . configs . BackendOptions = 'nccl' , broadcast_buffers : bool = True , bucket_cap_mb : int = 25 , find_unused_parameters : bool = False , gradient_as_bucket_view : bool = False , init_method : str = 'env://' , no_sync : bool = True )","title":"DDPConfig"},{"location":"reference/stoke/#attributes_5","text":"Name Type Description Default local_rank Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) None auto_mpi_discovery bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html None backend BackendOptions, default: 'nccl' Which communication backend to use None broadcast_buffers bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function None bucket_cap_mb int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) None find_unused_parameters bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach None gradient_as_bucket_view bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. None init_method str, default: 'env://' URL specifying how to initialize the process group None no_sync bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead None ??? example \"View Source\" class DDPConfig: \"\"\"PyTorch DistributedDataParallel configuration class Attributes ---------- local_rank: Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) auto_mpi_discovery: bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html backend: BackendOptions, default: 'nccl' Which communication backend to use broadcast_buffers: bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function bucket_cap_mb: int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) find_unused_parameters: bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach gradient_as_bucket_view: bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. init_method: str, default: 'env://' URL specifying how to initialize the process group no_sync: bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead \"\"\" local_rank: Optional[int] auto_mpi_discovery: bool = False convert_to_sync_batch_norm: bool = False backend: BackendOptions = \"nccl\" broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False gradient_as_bucket_view: bool = False init_method: str = \"env://\" no_sync: bool = True","title":"Attributes"},{"location":"reference/stoke/#deepspeedaioconfig","text":"class DeepspeedAIOConfig ( block_size : int = 1048576 , ignore_unused_parameters : bool = True , overlap_events : bool = True , queue_depth : int = 8 , single_submit : bool = False , thread_count : int = 1 )","title":"DeepspeedAIOConfig"},{"location":"reference/stoke/#attributes_6","text":"Name Type Description Default block_size int, default: 1048576 I/O block size in bytes None ignore_unused_parameters bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. None overlap_events bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. None queue_depth int, default: 8 I/O queue depth None single_submit bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. None thread_count int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. None ??? example \"View Source\" class DeepspeedAIOConfig: \"\"\"Deepspeed asynchronous I/O configuration class Attributes ---------- block_size: int, default: 1048576 I/O block size in bytes ignore_unused_parameters: bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. overlap_events: bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. queue_depth: int, default: 8 I/O queue depth single_submit: bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. thread_count: int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. \"\"\" block_size: int = 1048576 ignore_unused_parameters: bool = True overlap_events: bool = True queue_depth: int = 8 single_submit: bool = False thread_count: int = 1","title":"Attributes"},{"location":"reference/stoke/#deepspeedactivationcheckpointingconfig","text":"class DeepspeedActivationCheckpointingConfig ( contiguous_memory_optimization : bool = False , cpu_checkpointing : bool = False , number_checkpoints : Union [ int , NoneType ] = None , partition_activations : bool = False , profile : bool = False , synchronize_checkpoint_boundary : bool = False )","title":"DeepspeedActivationCheckpointingConfig"},{"location":"reference/stoke/#attributes_7","text":"Name Type Description Default contiguous_memory_optimization bool, default: False Copies partitioned activations so that they are contiguous in memory None cpu_checkpointing bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled None number_checkpoints Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization None partition_activations bool, default: False Enables partition activation when used with model parallelism None profile bool, default: False Logs the forward and backward time for each checkpoint function None synchronize_checkpoint_boundary bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary None ??? example \"View Source\" class DeepspeedActivationCheckpointingConfig: \"\"\"Deepspeed activation checkpointing configuration class Attributes ---------- contiguous_memory_optimization: bool, default: False Copies partitioned activations so that they are contiguous in memory cpu_checkpointing: bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled number_checkpoints: Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization partition_activations: bool, default: False Enables partition activation when used with model parallelism profile: bool, default: False Logs the forward and backward time for each checkpoint function synchronize_checkpoint_boundary: bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary \"\"\" contiguous_memory_optimization: bool = False cpu_checkpointing: bool = False number_checkpoints: Optional[int] = None partition_activations: bool = False profile: bool = False synchronize_checkpoint_boundary: bool = False","title":"Attributes"},{"location":"reference/stoke/#deepspeedconfig","text":"class DeepspeedConfig ( activation_checkpointing : Union [ stoke . configs . DeepspeedActivationCheckpointingConfig , NoneType ] = DeepspeedActivationCheckpointingConfig ( contiguous_memory_optimization = False , cpu_checkpointing = False , number_checkpoints = None , partition_activations = False , profile = False , synchronize_checkpoint_boundary = False ), aio : Union [ stoke . configs . DeepspeedAIOConfig , NoneType ] = DeepspeedAIOConfig ( block_size = 1048576 , ignore_unused_parameters = True , overlap_events = True , queue_depth = 8 , single_submit = False , thread_count = 1 ), auto_mpi_discovery : bool = True , disable_allgather : bool = False , dist_backend : stoke . configs . BackendOptions = 'nccl' , distributed_port : int = 29500 , dump_state : bool = False , flops_profiler : Union [ stoke . configs . DeepspeedFlopsConfig , NoneType ] = None , fp16 : Union [ stoke . configs . DeepspeedFP16Config , NoneType ] = None , fp32_allreduce : bool = False , gradient_predivide_factor : float = 1.0 , init_method : str = 'env://' , prescale_gradients : bool = False , progressive_layer_drop : Union [ stoke . configs . DeepspeedPLDConfig , NoneType ] = None , sparse_gradients : bool = False , steps_per_print : int = 10 , tensorboard : Union [ stoke . configs . DeepspeedTensorboardConfig , NoneType ] = None , verbose : bool = True , wall_clock_breakdown : bool = False , zero_optimization : Union [ stoke . configs . DeepspeedZeROConfig , NoneType ] = DeepspeedZeROConfig ( allgather_bucket_size = 500000000 , allgather_partitions = True , contiguous_gradients = False , ignore_unused_parameters = True , legacy_stage1 = False , offload_optimizer = None , offload_param = None , overlap_comm = False , reduce_bucket_size = 500000000 , reduce_scatter = True , stage = 0 , stage3_max_live_parameters = 1000000000 , stage3_max_reuse_distance = 1000000000 , stage3_prefetch_bucket_size = 500000000 , stage3_param_persistence_threshold = 1000000 , stage3_gather_fp16_weights_on_model_save = False , sub_group_size = 1000000000000 ) )","title":"DeepspeedConfig"},{"location":"reference/stoke/#attributes_8","text":"Name Type Description Default activation_checkpointing Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing None aio Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage None auto_mpi_discovery bool, default: True if distributed environment variables are not set, attempt to discover them from MPI None disable_allgather bool, default: False Disables allgather None dist_backend BackendOptions, default: 'nccl' Which communication backend to use None distributed_port int, default: 29500 torch distributed backend port None dump_state bool, default: False Print out state information of DeepSpeed object after initialization None flops_profiler Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown None fp16 Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package None fp32_allreduce bool, default: False During gradient averaging perform allreduce with 32 bit values None gradient_predivide_factor float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs None init_method str, default: 'env://' URL specifying how to initialize the process group None prescale_gradients float, default: 1.0 Scale gradients before doing allreduce None progressive_layer_drop Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping None sparse_gradients bool, default: False Enable sparse compression of torch.nn.Embedding gradients None steps_per_print int, default: 10 Print train loss every N steps None tensorboard Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support None verbose bool, default: True flag to make deepspeed engine verbose with information None wall_clock_breakdown bool, default: False Enable timing of the latency of forward/backward/update training phases None zero_optimization Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations None ??? example \"View Source\" class DeepspeedConfig: \"\"\"Deepspeed configuration class Composed of other configuration classes related to specific functionality Attributes ---------- activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage auto_mpi_discovery: bool, default: True if distributed environment variables are not set, attempt to discover them from MPI disable_allgather: bool, default: False Disables allgather dist_backend: BackendOptions, default: 'nccl' Which communication backend to use distributed_port: int, default: 29500 torch distributed backend port dump_state: bool, default: False Print out state information of DeepSpeed object after initialization flops_profiler: Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown fp16: Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package fp32_allreduce: bool, default: False During gradient averaging perform allreduce with 32 bit values gradient_predivide_factor: float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs init_method: str, default: 'env://' URL specifying how to initialize the process group prescale_gradients: float, default: 1.0 Scale gradients before doing allreduce progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping sparse_gradients: bool, default: False Enable sparse compression of torch.nn.Embedding gradients steps_per_print: int, default: 10 Print train loss every N steps tensorboard: Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support verbose: bool, default: True flag to make deepspeed engine verbose with information wall_clock_breakdown: bool, default: False Enable timing of the latency of forward/backward/update training phases zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations Notes ----- Deepspeed does not use Apex\u2019s AMP mode whihc allows for more flexibility in mixed precision training modes. FP16 here is similar to AMP\u2019s O2 mode \"\"\" activation_checkpointing: Optional[ DeepspeedActivationCheckpointingConfig ] = DeepspeedActivationCheckpointingConfig() aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig() auto_mpi_discovery: bool = True disable_allgather: bool = False dist_backend: BackendOptions = \"nccl\" distributed_port: int = 29500 dump_state: bool = False flops_profiler: Optional[DeepspeedFlopsConfig] = None fp16: Optional[DeepspeedFP16Config] = None fp32_allreduce: bool = False gradient_predivide_factor: float = 1.0 init_method: str = \"env://\" prescale_gradients: bool = False progressive_layer_drop: Optional[DeepspeedPLDConfig] = None sparse_gradients: bool = False steps_per_print: int = 10 tensorboard: Optional[DeepspeedTensorboardConfig] = None verbose: bool = True wall_clock_breakdown: bool = False zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig()","title":"Attributes"},{"location":"reference/stoke/#deepspeedfp16config","text":"class DeepspeedFP16Config ( hysteresis : int = 2 , initial_scale_power : int = 32 , loss_scale : float = 0.0 , loss_scale_window : int = 1000 , min_loss_scale : int = 1000 )","title":"DeepspeedFP16Config"},{"location":"reference/stoke/#attributes_9","text":"Name Type Description Default hysteresis int, default: 2 represents the delay shift in dynamic loss scaling None initial_scale_power int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power None loss_scale float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) None loss_scale_window int, default: 1000 the window over which to raise/lower the dynamic loss scale value None min_loss_scale int, default: 1000 minimum dynamic loss scale value None ??? example \"View Source\" class DeepspeedFP16Config: \"\"\"Deepspeed FP16 configuration class Attributes ---------- hysteresis: int, default: 2 represents the delay shift in dynamic loss scaling initial_scale_power: int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power loss_scale: float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) loss_scale_window: int, default: 1000 the window over which to raise/lower the dynamic loss scale value min_loss_scale: int, default: 1000 minimum dynamic loss scale value \"\"\" hysteresis: int = 2 initial_scale_power: int = 32 loss_scale: float = 0.0 loss_scale_window: int = 1000 min_loss_scale: int = 1000","title":"Attributes"},{"location":"reference/stoke/#deepspeedflopsconfig","text":"class DeepspeedFlopsConfig ( detailed : bool = True , module_depth : int = - 1 , output_file : Union [ str , NoneType ] = None , profile_step : int = 1 , top_modules : int = 1 )","title":"DeepspeedFlopsConfig"},{"location":"reference/stoke/#attributes_10","text":"Name Type Description Default detailed bool, default: True Whether to print the detailed model profile None module_depth int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). None output_file Optional[str], default: None Path to the output file. If None, the profiler prints to stdout None profile_step int, default: 1 The global training step at which to profile. None top_modules int, default: 1 Limits the aggregated profile output to the number of top modules specified. None ??? example \"View Source\" class DeepspeedFlopsConfig: \"\"\"Deepspeed flops profiler configuration class Attributes ---------- detailed: bool, default: True Whether to print the detailed model profile module_depth: int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). output_file: Optional[str], default: None Path to the output file. If None, the profiler prints to stdout profile_step: int, default: 1 The global training step at which to profile. top_modules: int, default: 1 Limits the aggregated profile output to the number of top modules specified. Notes ----- Warm up steps are needed for accurate time measurement \"\"\" detailed: bool = True module_depth: int = -1 output_file: Optional[str] = None profile_step: int = 1 top_modules: int = 1","title":"Attributes"},{"location":"reference/stoke/#deepspeedoffloadoptimizerconfig","text":"class DeepspeedOffloadOptimizerConfig ( buffer_count : int = 4 , device : stoke . configs . OffloadDevice = 'cpu' , fast_init : bool = False , nvme_path : str = '/local_nvme' , pin_memory : bool = False , pipeline : bool = False , pipeline_read : bool = False , pipeline_write : bool = False )","title":"DeepspeedOffloadOptimizerConfig"},{"location":"reference/stoke/#attributes_11","text":"Name Type Description Default buffer_count int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). None device OffloadDevice, default: 'cpu' Device memory to offload optimizer state None fast_init bool, default: False Enable fast optimizer initialization when offloading to NVMe None nvme_path str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading None pin_memory bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. None pipeline bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set to pipeline_read bool, default: False activate pipeline read (deepspeed has limited docs for what this does) None pipeline_write bool, default: False activate pipeline write(deepspeed has limited docs for what this does) None ??? example \"View Source\" class DeepspeedOffloadOptimizerConfig: \"\"\"Deepspeed optimizer offloading configuration class Attributes ---------- buffer_count: int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). device: OffloadDevice, default: 'cpu' Device memory to offload optimizer state fast_init: bool, default: False Enable fast optimizer initialization when offloading to NVMe nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. pipeline: bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set pipeline_read: bool, default: False activate pipeline read (deepspeed has limited docs for what this does) pipeline_write: bool, default: False activate pipeline write(deepspeed has limited docs for what this does) \"\"\" buffer_count: int = 4 device: OffloadDevice = \"cpu\" fast_init: bool = False nvme_path: str = \"/local_nvme\" pin_memory: bool = False pipeline: bool = False pipeline_read: bool = False pipeline_write: bool = False","title":"Attributes"},{"location":"reference/stoke/#deepspeedoffloadparamconfig","text":"class DeepspeedOffloadParamConfig ( buffer_count : int = 5 , buffer_size : int = 100000000 , device : stoke . configs . OffloadDevice = 'cpu' , max_in_cpu : int = 1000000000 , nvme_path : str = '/local_nvme' , pin_memory : bool = False )","title":"DeepspeedOffloadParamConfig"},{"location":"reference/stoke/#attributes_12","text":"Name Type Description Default buffer_count int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe None buffer_size int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe None device OffloadDevice, default: 'cpu' Device memory to offload model parameters None max_in_cpu int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. None nvme_path str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading None pin_memory bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. None ??? example \"View Source\" class DeepspeedOffloadParamConfig: \"\"\"Deepspeed parameter offloading configuration class Attributes ---------- buffer_count: int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe buffer_size: int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe device: OffloadDevice, default: 'cpu' Device memory to offload model parameters max_in_cpu: int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. \"\"\" buffer_count: int = 5 buffer_size: int = int(1e8) device: OffloadDevice = \"cpu\" max_in_cpu: int = int(1e9) nvme_path: str = \"/local_nvme\" pin_memory: bool = False","title":"Attributes"},{"location":"reference/stoke/#deepspeedpldconfig","text":"class DeepspeedPLDConfig ( theta : float = 1.0 , gamma : float = 0.001 )","title":"DeepspeedPLDConfig"},{"location":"reference/stoke/#attributes_13","text":"Name Type Description Default theta float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed None gamma float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases None ??? example \"View Source\" class DeepspeedPLDConfig: \"\"\" Attributes ---------- theta: float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed gamma: float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases \"\"\" theta: float = 1.0 gamma: float = 0.001","title":"Attributes"},{"location":"reference/stoke/#deepspeedtensorboardconfig","text":"class DeepspeedTensorboardConfig ( output_path : str = '' , job_name : str = 'DeepSpeedJobName' )","title":"DeepspeedTensorboardConfig"},{"location":"reference/stoke/#attributes_14","text":"Name Type Description Default output_path str, default: '' Tensorboard output path None job_name str, default: 'DeepSpeedJobName' Tensorboard job name None ??? example \"View Source\" class DeepspeedTensorboardConfig: \"\"\"Deepspeed Tensorboard configuration class Attributes ---------- output_path: str, default: '' Tensorboard output path job_name: str, default: 'DeepSpeedJobName' Tensorboard job name \"\"\" output_path: str = \"\" job_name: str = \"DeepSpeedJobName\"","title":"Attributes"},{"location":"reference/stoke/#deepspeedzeroconfig","text":"class DeepspeedZeROConfig ( allgather_bucket_size : int = 500000000 , allgather_partitions : bool = True , contiguous_gradients : bool = False , ignore_unused_parameters : bool = True , legacy_stage1 : bool = False , offload_optimizer : Union [ stoke . configs . DeepspeedOffloadOptimizerConfig , NoneType ] = None , offload_param : Union [ stoke . configs . DeepspeedOffloadParamConfig , NoneType ] = None , overlap_comm : bool = False , reduce_bucket_size : int = 500000000 , reduce_scatter : bool = True , stage : int = 0 , stage3_max_live_parameters : int = 1000000000 , stage3_max_reuse_distance : int = 1000000000 , stage3_prefetch_bucket_size : int = 500000000 , stage3_param_persistence_threshold : int = 1000000 , stage3_gather_fp16_weights_on_model_save : bool = False , sub_group_size : int = 1000000000000 )","title":"DeepspeedZeROConfig"},{"location":"reference/stoke/#attributes_15","text":"Name Type Description Default allgather_bucket_size int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes None allgather_partitions bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step None contiguous_gradients bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. None ignore_unused_parameters bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 None legacy_stage1 bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons None offload_optimizer Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 None offload_param Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. None overlap_comm bool, default: False Attempts to overlap the reduction of the gradients with backward computation None reduce_bucket_size int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes None reduce_scatter bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients None stage int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively None stage3_max_live_parameters int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. None stage3_max_reuse_distance int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. None stage3_prefetch_bucket_size int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. None stage3_param_persistence_threshold int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). None stage3_gather_fp16_weights_on_model_save bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. None sub_group_size int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. None ??? example \"View Source\" class DeepspeedZeROConfig: \"\"\"Deepspeed ZeRO configuration class Attributes ---------- allgather_bucket_size: int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes allgather_partitions: bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step contiguous_gradients: bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. ignore_unused_parameters: bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 legacy_stage1: bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 offload_param: Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. overlap_comm: bool, default: False Attempts to overlap the reduction of the gradients with backward computation reduce_bucket_size: int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes reduce_scatter: bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients stage: int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively stage3_max_live_parameters: int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. stage3_max_reuse_distance: int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. stage3_prefetch_bucket_size: int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. stage3_param_persistence_threshold: int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). stage3_gather_fp16_weights_on_model_save: bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. sub_group_size: int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. \"\"\" allgather_bucket_size: int = int(5e8) allgather_partitions: bool = True contiguous_gradients: bool = False ignore_unused_parameters: bool = True legacy_stage1: bool = False offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None offload_param: Optional[DeepspeedOffloadParamConfig] = None overlap_comm: bool = False reduce_bucket_size: int = int(5e8) reduce_scatter: bool = True stage: int = 0 stage3_max_live_parameters: int = int(1e9) stage3_max_reuse_distance: int = int(1e9) stage3_prefetch_bucket_size: int = int(5e8) stage3_param_persistence_threshold: int = int(1e6) stage3_gather_fp16_weights_on_model_save: bool = False sub_group_size: int = int(1e12)","title":"Attributes"},{"location":"reference/stoke/#distributedoptions","text":"class DistributedOptions ( / , * args , ** kwargs ) ??? example \"View Source\" class DistributedOptions(Enum): \"\"\"Enum that defines the options for Distributed backends\"\"\" horovod = \"horovod\" ddp = \"ddp\" deepspeed = \"deepspeed\"","title":"DistributedOptions"},{"location":"reference/stoke/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/#class-variables","text":"ddp deepspeed horovod name value","title":"Class variables"},{"location":"reference/stoke/#fp16options","text":"class FP16Options ( / , * args , ** kwargs ) ??? example \"View Source\" class FP16Options(Enum): \"\"\"Enum that defines the options for FP16 backends\"\"\" apex_O1 = \"apex_O1\" apex_O2 = \"apex_O2\" amp = \"amp\" deepspeed = \"deepspeed\"","title":"FP16Options"},{"location":"reference/stoke/#ancestors-in-mro_2","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/#class-variables_1","text":"amp apex_O1 apex_O2 deepspeed name value","title":"Class variables"},{"location":"reference/stoke/#fairscalefsdpconfig","text":"class FairscaleFSDPConfig ( bucket_cap_mb : int = 25 , buffer_dtype : Union [ torch . dtype , NoneType ] = None , clear_autocast_cache : bool = False , compute_dtype : Union [ torch . dtype , NoneType ] = None , flatten_parameters : bool = True , force_input_to_fp32 : bool = False , fp32_reduce_scatter : bool = False , gradient_predivide_factor : Union [ float , NoneType ] = None , gradient_postdivide_factor : Union [ float , NoneType ] = None , move_grads_to_cpu : Union [ bool , NoneType ] = None , move_params_to_cpu : bool = False , no_broadcast_optim_state : Union [ bool , NoneType ] = False , reshard_after_forward : bool = True , verbose : bool = False )","title":"FairscaleFSDPConfig"},{"location":"reference/stoke/#attributes_16","text":"Name Type Description Default bucket_cap_mb int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing None buffer_dtype Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype value clear_autocast_cache bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory None compute_dtype Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. torch.float32 flatten_parameters bool, default: True flatten parameters into a single contiguous tensor, which improves training speed None force_input_to_fp32 bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. None fp32_reduce_scatter bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used None gradient_predivide_factor Optional[float], default: None divide factor before the reduction None gradient_postdivide_factor Optional[float], default: None divide factor after the reduction None move_grads_to_cpu Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used None move_params_to_cpu bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used None no_broadcast_optim_state Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node None reshard_after_forward bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) None verbose bool, default: True turn on verbose output for model\u2019s string representation None ??? example \"View Source\" class FairscaleFSDPConfig: \"\"\"Fairscale Fully Sharded Data Parallel configuration class Attributes ---------- bucket_cap_mb: int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of <dtype, device, process_group> tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing buffer_dtype: Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype clear_autocast_cache: bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory compute_dtype: Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. flatten_parameters: bool, default: True flatten parameters into a single contiguous tensor, which improves training speed force_input_to_fp32: bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. fp32_reduce_scatter: bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used gradient_predivide_factor: Optional[float], default: None divide factor before the reduction gradient_postdivide_factor: Optional[float], default: None divide factor after the reduction move_grads_to_cpu: Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used move_params_to_cpu: bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used no_broadcast_optim_state: Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node reshard_after_forward: bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) verbose: bool, default: True turn on verbose output for model\u2019s string representation Notes ----- mixed_precision: bool This value will automatically be set from the Stoke FP16 selected option (AMP only) state_dict_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup compute_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup \"\"\" bucket_cap_mb: int = 25 buffer_dtype: Optional[torch.dtype] = None clear_autocast_cache: bool = False compute_dtype: Optional[torch.dtype] = None flatten_parameters: bool = True force_input_to_fp32: bool = False fp32_reduce_scatter: bool = False gradient_predivide_factor: Optional[float] = None gradient_postdivide_factor: Optional[float] = None move_grads_to_cpu: Optional[bool] = None move_params_to_cpu: bool = False no_broadcast_optim_state: Optional[bool] = False reshard_after_forward: bool = True verbose: bool = False","title":"Attributes"},{"location":"reference/stoke/#descendants","text":"stoke.extensions._FairscaleFSDPConfig","title":"Descendants"},{"location":"reference/stoke/#fairscaleossconfig","text":"class FairscaleOSSConfig ( broadcast_fp16 : bool = False )","title":"FairscaleOSSConfig"},{"location":"reference/stoke/#attributes_17","text":"Name Type Description Default broadcast_fp16 bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. None ??? example \"View Source\" class FairscaleOSSConfig: \"\"\"Fairscale optimizer state sharding configuration class Attributes ---------- broadcast_fp16: bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. \"\"\" broadcast_fp16: bool = False","title":"Attributes"},{"location":"reference/stoke/#fairscalesddpconfig","text":"class FairscaleSDDPConfig ( auto_refresh_trainable : bool = True , broadcast_buffers : bool = True , reduce_buffer_size : int = 8388608 , reduce_fp16 : bool = False , sync_models_at_startup : bool = True )","title":"FairscaleSDDPConfig"},{"location":"reference/stoke/#attributes_18","text":"Name Type Description Default auto_refresh_trainable bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen None broadcast_buffers bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. None reduce_buffer_size int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. None reduce_fp16 bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. None sync_models_at_startup bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state None ??? example \"View Source\" class FairscaleSDDPConfig: \"\"\"Fairscale sharded data parallel (SDDP) configuration class Attributes ---------- auto_refresh_trainable: bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen broadcast_buffers: bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. reduce_buffer_size: int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. reduce_fp16: bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. sync_models_at_startup: bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state \"\"\" auto_refresh_trainable: bool = True broadcast_buffers: bool = True reduce_buffer_size: int = 2 ** 23 reduce_fp16: bool = False sync_models_at_startup: bool = True","title":"Attributes"},{"location":"reference/stoke/#horovodconfig","text":"class HorovodConfig ( compression : bool = False , convert_to_sync_batch_norm : bool = False , gradient_predivide_factor : float = 1.0 , op : stoke . configs . HorovodOps = 'Average' )","title":"HorovodConfig"},{"location":"reference/stoke/#attributes_19","text":"Name Type Description Default compression bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm None gradient_predivide_factor float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. None op HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. None ??? example \"View Source\" class HorovodConfig: \"\"\"Horovod configuration class Attributes ---------- compression: bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm gradient_predivide_factor: float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. op: HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. \"\"\" compression: bool = False convert_to_sync_batch_norm: bool = False gradient_predivide_factor: float = 1.0 op: HorovodOps = \"Average\"","title":"Attributes"},{"location":"reference/stoke/#paramnormalize","text":"class ParamNormalize ( / , * args , ** kwargs ) ??? example \"View Source\" class ParamNormalize(Enum): \"\"\"Normalization enum for total number of model parameters used to help with a pretty print\"\"\" THOUSAND = 1e3 MILLION = 1e6 BILLION = 1e9 TRILLION = 1e12","title":"ParamNormalize"},{"location":"reference/stoke/#ancestors-in-mro_3","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/#class-variables_2","text":"BILLION MILLION THOUSAND TRILLION name value","title":"Class variables"},{"location":"reference/stoke/#stoke","text":"class Stoke ( model : torch . nn . modules . module . Module , optimizer : stoke . configs . StokeOptimizer , loss : Union [ Callable , List [ Callable ], Tuple [ Callable ]], batch_size_per_device : int , grad_accum_steps : Union [ int , NoneType ] = 1 , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig , NoneType ] = None , gpu : bool = False , fp16 : Union [ stoke . status . FP16Options , NoneType ] = None , distributed : Union [ stoke . status . DistributedOptions , NoneType ] = None , fairscale_oss : bool = False , fairscale_sddp : bool = False , fairscale_fsdp : bool = False , configs : Union [ List [ Union [ stoke . configs . AMPConfig , stoke . configs . ApexConfig , stoke . configs . DDPConfig , stoke . configs . DeepspeedConfig , stoke . configs . FairscaleOSSConfig , stoke . configs . FairscaleSDDPConfig , stoke . configs . FairscaleFSDPConfig , stoke . configs . HorovodConfig ]], NoneType ] = None , info_rank : Union [ int , List [ int ], NoneType ] = 0 , verbose : bool = True , ema_weight : float = 0.1 )","title":"Stoke"},{"location":"reference/stoke/#attributes_20","text":"Name Type Description Default amp_config None None None apex_config None None None batch_size None None None cuda None None None ddp_config None None None deepspeed_config None None None distributed None None None effective_batch_size None None None ema_loss None None None fp16 None None None fsdp_config None None None fully_sharded None None None gpu None None None grad_accum None None None grad_clip None None None horovod_config None None None is_amp None None None is_apex None None None is_ddp None None None is_deepspeed None None None is_horovod None None None loss_access None None None model_access None None None nccl None None None num_model_parameters None None None optimizer None None None oss None None None oss_config None None None rank None None None scaler None None None sddp_config None None None sharded None None None status None None None world_size None None None _agg_loss Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) None _backward_steps int Number of times gradients have been calculated on a batch of samples (calls to backward) None _grad_accum_counter int counter for grad accumulation steps None _loss Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs None _last_step_loss list, tuple, or float last loss step calculation aggregated over device(s) None _model torch.nn.Module instance of torch.nn.Module for Stoke to handle None _optimizer StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs None _optimizer_steps int Number of times step has been called on the optimizer None _runner StokeRunner the dynamically created runtime object that handles all ops None _status StokeStatus StokeStatus object that sets and maintains the current configuration None _verbose bool print verbosity None _rolling_loss_steps int number of steps that have been called for the rolling loss None _rolling_mean_loss list, tuple, or float current ema loss None _ema_weight float weight used for any ema calculation on metrics None ??? example \"View Source\" class Stoke: \"\"\"High level stoke object that manages all necessary configs and provides a unified interface to ops This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model, loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale), mixed-precision (AMP or APEX) and devices (CPU or GPU) Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size ema_loss fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_amp is_apex is_ddp is_deepspeed is_horovod loss_access model_access nccl num_model_parameters optimizer oss oss_config rank scaler sddp_config sharded status world_size _agg_loss: Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) _backward_steps: int Number of times gradients have been calculated on a batch of samples (calls to backward) _grad_accum_counter: int counter for grad accumulation steps _loss: Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs _last_step_loss: list, tuple, or float last loss step calculation aggregated over device(s) _model: torch.nn.Module instance of torch.nn.Module for Stoke to handle _optimizer: StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs _optimizer_steps: int Number of times step has been called on the optimizer _runner: StokeRunner the dynamically created runtime object that handles all ops _status: StokeStatus StokeStatus object that sets and maintains the current configuration _verbose: bool print verbosity _rolling_loss_steps: int number of steps that have been called for the rolling loss _rolling_mean_loss: list, tuple, or float current ema loss _ema_weight: float weight used for any ema calculation on metrics \"\"\" def __init__( self, model: torch.nn.Module, optimizer: StokeOptimizer, loss: Union[Callable, List[Callable], Tuple[Callable]], batch_size_per_device: int, grad_accum_steps: Optional[int] = 1, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None, gpu: bool = False, fp16: Optional[FP16Options] = None, distributed: Optional[DistributedOptions] = None, fairscale_oss: bool = False, fairscale_sddp: bool = False, fairscale_fsdp: bool = False, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ] = None, info_rank: Optional[Union[int, List[int]]] = 0, verbose: bool = True, ema_weight: float = 0.1, ): \"\"\"Init for Stoke class object Parameters ---------- model: torch.nn.Module PyTorch model optimizer: StokeOptimizer Optimizer configuration loss: Union[Callable, List[Callable], Tuple[Callable]] Callable loss function or functions batch_size_per_device: int Batch size at the single device level grad_accum_steps: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None Configuration objects for runtimes info_rank: Optional[Union[int, List[int]]], default = 0 Constrain prints to specific devices verbose: bool, default: True Flag for verbosity ema_weight: float, default: 0.5 weight used for any ema calculation on metrics \"\"\" # Verbosity self._verbose = verbose # Info rank self._info_rank = info_rank # EMA self._ema_weight = ema_weight # Setup the StokeState self._status = StokeStatus( batch_size_per_device=batch_size_per_device, grad_accum=grad_accum_steps, grad_clip=grad_clip, gpu=gpu, fp16=fp16, distributed=distributed, fairscale_oss=fairscale_oss, fairscale_sddp=fairscale_sddp, fairscale_fsdp=fairscale_fsdp, configs=configs, ) # Run some checks self._model = self._check_model(model) self._optimizer = self._check_optimizer(optimizer) self._loss = self._check_loss(loss) # Dynamically construct the StokeRunner from the StokeStatus self._runner, class_info = self._build_runner() # Setup distributed backend self._runner.setup_distributed() # Post here the runner will have the print_device function that is mapped to the self.print here # as it needs rank to be accessible before working if self._verbose: dev_id = ( self.rank if (self.rank == \"cpu\" or self.rank == \"gpu\") else self._info_rank ) self.print(f\"Printing verbose information on rank(s): {dev_id}\") # Print the runner class info from the mixins self.print(class_info) # Possibly place model on GPU depending on StokeStatus -- before wrap calls self._place_model_on_gpu() # Handle the wrap ops in the correct order self._handle_ordered_wrap_ops(optimizer=optimizer) # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 # Set post-init status variables self._status.set_post_init_values(world_size=self.world_size) # Print the final configuration if self._verbose: self.print(msg=self._status) def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of optimizer then the model This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped methods are called Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed through self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer) # Wrap with distributed backend -- in this case the optimizer is passed through self._model, self._optimizer = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer ) def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of model then optimizer Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn't exist yet # don't use the return for the optimizer in this case self._model, _ = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=None ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn't exist yet self._runner.wrap_fp16(model=self._model, optimizer=None) # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed) if (self.sharded and self.oss) or self.is_apex or self.is_horovod: self._wrap_optimizer_then_model(optimizer=optimizer) else: self._wrap_model_then_optimizer(optimizer=optimizer) def _check_accum(self): \"\"\"Checks if the current step is the last accumulation step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0 def _check_pre_accum(self): \"\"\"Checks if we are at the pre-accumulate step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum def _set_loss_to_zero(self): \"\"\"Used to set a loss tracker to zero depending on the type Returns ------- float or list or tuple of reset loss \"\"\" return ( type(self._loss)([0.0] * len(self._loss)) if isinstance(self._loss, (list, tuple)) else 0.0 ) def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\") def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\") def _scale_agg_loss(self): \"\"\"Scales the mean aggregated loss by grad accum Returns ------- scale_vals: list or float of mean aggregated loss \"\"\" if isinstance(self._agg_loss, (list, tuple)): scale_vals = [ val / self.grad_accum for idx, val in enumerate(self._agg_loss) ] else: scale_vals = self._agg_loss / self.grad_accum return scale_vals def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\") def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank) def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line ) @staticmethod def _check_model(model: torch.nn.Module): \"\"\"Verifies the type of the model Parameters ---------- model: torch.nn.Module current torch model Returns ------- None \"\"\" # Check if the model is an nn.Module such that it has a forward method if not isinstance(model, torch.nn.Module): raise TypeError( f\"Stoke -- Model is not of type torch.nn.Module, currently {type(model)}\" ) return model @staticmethod def _check_optimizer(optimizer: StokeOptimizer): \"\"\"Verifies the type of the optimizer Parameters ---------- optimizer: StokeOptimizer Current optimizer configuration TypedDict (aka dict) Returns ------- None \"\"\" if not isinstance(optimizer, dict): raise TypeError( f\"Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}\" ) return optimizer def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]): \"\"\"Checks to make sure the loss function(s) is/are callable Parameters ---------- loss: Union[Callable, List[Callable], Tuple[Callable]] Current callable loss(es) Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): loss = [self._check_loss(val) for val in loss] return loss elif isinstance(loss, Callable): return loss else: raise TypeError( f\"Stoke -- Loss is not of type Callable, currently {type(loss)}\" ) def _place_model_on_gpu(self): \"\"\"Automatically moves the model to GPU device(s) Returns ------- None \"\"\" if self.gpu and not self.is_deepspeed: if self._verbose: self.print(f\"Automatically handling moving model to GPU(s)...\") self._model.cuda() def _build_runner(self): \"\"\"Builds the runtime object from the mixin style classes Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a a single interface. Might prevent some IDE type-hinting as it's dynamic Returns ------- StokeRunner runtime runner object \"\"\" # Get the classes dist_class = self._get_distributed_mixin() fp16_class = self._get_fp16_mixin() optimizer_class = self._get_optimizer_mixin() io_class = self._get_io_mixin() # Python MRO hack to make sure the inits of all the Mixin classes get called def __multiple_mixin_init__(*args, **kwargs): dist_class.__init__(*args, **kwargs) fp16_class.__init__(*args, **kwargs) optimizer_class.__init__(*args, **kwargs) io_class.__init__(*args, **kwargs) # Configs pass through kwargs_dict = { \"amp_config\": self.amp_config, \"apex_config\": self.apex_config, \"ddp_config\": self.ddp_config, \"deepspeed_config\": self.deepspeed_config, \"horovod_config\": self.horovod_config, \"oss_config\": self.oss_config, \"sharded_config\": self.sddp_config, \"fully_sharded_config\": self.fsdp_config, } # Generate the runner class from the mixins based on the StokeStatus runner_class = type( \"StokeRunner\", (dist_class, fp16_class, optimizer_class, io_class), {\"__init__\": __multiple_mixin_init__}, )( verbose=self._verbose, batch_size_per_device=self.batch_size, grad_accum_steps=self.grad_accum, grad_clip=self.grad_clip, info_rank=self._info_rank, loss=self._loss, **kwargs_dict, ) # Make a list of class info for print later class_info = [ f\"Distributed Mixin: {dist_class.__name__}\", f\"Optimizer Mixin: {dist_class.__name__}\", f\"FP16 Mixin: {fp16_class.__name__}\", f\"IO Mixin: {io_class.__name__}\", ] return runner_class, class_info def _get_io_mixin(self): \"\"\"Determines which IO class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated ioclass \"\"\" if self.is_deepspeed: return_class = RunnerIOEnum.deepspeed.value elif self.is_horovod: return_class = RunnerIOEnum.horovod.value elif self.is_ddp: return_class = RunnerIOEnum.ddp.value else: return_class = RunnerIOEnum.base.value return return_class def _get_optimizer_mixin(self): \"\"\"Determines which optimizer class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated optimizer class \"\"\" if self.oss: return_class = RunnerOptimizerEnum.oss.value else: return_class = RunnerOptimizerEnum.base.value return return_class def _get_distributed_mixin(self): \"\"\"Determines which distributed class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated distributed class \"\"\" # if not gpu then fall to cpu single if not self.gpu: return_class = RunnerDistEnum.cpu.value # if gpu but no distributed then fall to single gpu elif self.gpu and (self.distributed is None): return_class = RunnerDistEnum.gpu.value elif self.gpu and (self.distributed is not None): return_class = RunnerDistEnum[self.distributed].value else: raise ValueError(\"Stoke -- Cannot map to a valid distributed class\") return return_class def _get_fp16_mixin(self): \"\"\"Determines which fp16 class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated fp16 class \"\"\" if self.fp16 is not None: return_class = RunnerFP16Enum[self.fp16].value else: return_class = RunnerFP16Enum.full.value return return_class def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) def model(self, *args, **kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs) def loss(self, *args, **kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]): \"\"\"Handles calculating the ema loss Parameters ---------- loss: Union[float, List[float], Tuple[float]] current calculated loss list, tuple or float Returns ------- None \"\"\" self._rolling_loss_steps += 1 if isinstance(loss, (list, tuple)): self._rolling_mean_loss = type(self._rolling_mean_loss)( self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx]) for idx, val in enumerate(loss) ) else: self._rolling_mean_loss = self._ema_loss( value=loss, current_mean=self._rolling_mean_loss ) def _ema_loss(self, value: float, current_mean: float): \"\"\"Calculate the ema of the loss Parameters ---------- value: float current loss value current_mean: float current mean value Returns ------- current ema value: float \"\"\" if self._rolling_loss_steps == 1: return value else: return (self._ema_weight * value) + ( (1.0 - self._ema_weight) * current_mean ) def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1 def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) def _reset(self): \"\"\"Resets the state post optimizer step call Returns ------- None \"\"\" if self._verbose: self.print(\"Resetting all grad/variables for next optimizer step\") # Zero the grads if not deepspeed if not self.is_deepspeed: self.zero_grads() # Reset counter self._grad_accum_counter = 0 # Reset agg loss -- single or mutiple losses self._agg_loss = self._set_loss_to_zero() def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" ) def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device) def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod ) def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset() def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" ) def _load_fp16_state_dict_fn(self): \"\"\"Returns the function to load the sacler state dict Returns ------- mp_state_dict_fn: Callable, default: None callable function to load the scaler state dict \"\"\" mp_state_dict_fn = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict_fn = amp.load_state_dict except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) else: mp_state_dict_fn = self.scaler.load_state_dict return mp_state_dict_fn def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier() @property def step_loss(self): \"\"\"Gets the last step loss synced across device(s) (unscaled)\"\"\" return self._last_step_loss @property def model_access(self): \"\"\"Interface for model access due to the different types between the DP, DDP, and SDDP implementations\"\"\" if isinstance(self._model, (DDP, DP, SDDP, FSDP)): return self._model.module else: return self._model @property def loss_access(self): \"\"\"Gets loss tensor(s)\"\"\" return self._loss @property def optimizer(self): \"\"\"Gets the optimizer\"\"\" return self._optimizer @property def scaler(self): \"\"\"Gets the current scaler object\"\"\" return self._runner.scaler @property def fp16_state_dict(self): \"\"\"Gets the fp16 state dict from various methods\"\"\" mp_state_dict = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict = amp.state_dict() except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) elif self.is_amp: mp_state_dict = self.scaler.state_dict() return mp_state_dict @property def status(self): \"\"\"Gets the StokeStatus object\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.batch_size @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self._status.effective_batch_size @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.grad_clip @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.grad_accum @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.gpu @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.cuda @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.nccl @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.fp16 @property def is_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self._status.is_fp16_apex @property def is_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self._status.is_fp16_amp @property def distributed(self): \"\"\"Shortcut to distributed status\"\"\" return self._status.distributed @property def is_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self._status.is_distributed_ddp @property def is_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self._status.is_distributed_horovod @property def is_deepspeed(self): \"\"\"Returns if Deepspeed is acticated\"\"\" return self._status.is_distributed_deepspeed @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.oss @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.sharded @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.fully_sharded @property def world_size(self): \"\"\"Shortcut to get world size\"\"\" return self._runner.world_size @property def rank(self): \"\"\"Shortcut to get rank\"\"\" return self._runner.rank @property def amp_config(self): \"\"\"Returns amp config or None based on amp state\"\"\" return self._status.amp_config if self.is_amp else None @property def apex_config(self): \"\"\"Returns apex config or None based on apex state\"\"\" return self._status.apex_config if self.is_apex else None @property def ddp_config(self): \"\"\"Returns ddp config or None based on ddp state\"\"\" return self._status.ddp_config if self.is_ddp else None @property def deepspeed_config(self): \"\"\"Returns deepspeed config or None based on deepspeed state\"\"\" return self._status.deepspeed_config if self.is_deepspeed else None @property def oss_config(self): \"\"\"Returns oss config or None based on ossstate\"\"\" return self._status.oss_config if self.oss else None @property def sddp_config(self): \"\"\"Returns sddp config or None based on sddp state\"\"\" return self._status.sddp_config if self.sharded else None @property def fsdp_config(self): \"\"\"Returns fsdp config or None based on fsdp state\"\"\" return self._status.fsdp_config if self.fully_sharded else None @property def horovod_config(self): \"\"\"Returns horovod config or None based on horovod state\"\"\" return self._status.horovod_config if self.is_horovod else None @property def num_model_parameters(self): \"\"\"Returns number of parameters that require gradients\"\"\" return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad) @property def ema_loss(self): \"\"\"Returns the current rolling mean loss\"\"\" return self._rolling_mean_loss","title":"Attributes"},{"location":"reference/stoke/#instance-variables","text":"amp_config Returns amp config or None based on amp state apex_config Returns apex config or None based on apex state batch_size Shortcut to batch size cuda Shortcut to get cuda status ddp_config Returns ddp config or None based on ddp state deepspeed_config Returns deepspeed config or None based on deepspeed state distributed Shortcut to distributed status effective_batch_size Shortcut to effective batch size ema_loss Returns the current rolling mean loss fp16 Shortcut to get FP16 status fp16_state_dict Gets the fp16 state dict from various methods fsdp_config Returns fsdp config or None based on fsdp state fully_sharded Returns if Fairscale fully sharded DDP status gpu Shortcut to get GPU status grad_accum Shortcut to get grad accumulation grad_clip Shortcut to get grad clip horovod_config Returns horovod config or None based on horovod state is_amp Returns if AMP is activated is_apex Returns if APEX is activated is_ddp Returns if DDP is activated is_deepspeed Returns if Deepspeed is acticated is_horovod Returns if Horovod is activated loss_access Gets loss tensor(s) model_access Interface for model access due to the different types between the DP, DDP, and SDDP implementations nccl Shortcut to get nccl status num_model_parameters Returns number of parameters that require gradients optimizer Gets the optimizer oss Returns if Fairscale optimizer state sharding status oss_config Returns oss config or None based on ossstate rank Shortcut to get rank scaler Gets the current scaler object sddp_config Returns sddp config or None based on sddp state sharded Returns if Fairscale sharded DDP status status Gets the StokeStatus object step_loss Gets the last step loss synced across device(s) (unscaled) world_size Shortcut to get world size","title":"Instance variables"},{"location":"reference/stoke/#methods_1","text":"","title":"Methods"},{"location":"reference/stoke/#dataloader","text":"def DataLoader ( self , dataset : torch . utils . data . dataset . Dataset [ + T_co ], shuffle : bool = False , sampler : Union [ torch . utils . data . sampler . Sampler [ int ], NoneType ] = None , batch_sampler : Union [ torch . utils . data . sampler . Sampler [ Sequence [ int ]], NoneType ] = None , num_workers : int = 0 , collate_fn : Callable [[ List [ ~ T ]], Any ] = None , pin_memory : bool = False , drop_last : bool = False , timeout : float = 0 , worker_init_fn : Callable [[ int ], NoneType ] = None , multiprocessing_context = None , generator = None , * , prefetch_factor : int = 2 , persistent_workers : bool = False ) Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters: Name Type Description Default dataset Dataset dataset from which to load the data. None shuffle bool, default: False set to True to have the data reshuffled at every epoch. None sampler Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, :attr: shuffle must not be specified. None batch_sampler Sampler or Iterable, default: None: like :attr: sampler , but returns a batch of indices at a time. Mutually exclusive with :attr: batch_size , :attr: shuffle , :attr: sampler , and :attr: drop_last . None num_workers int, default: 0 how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. None collate_fn callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. None pin_memory bool, default: False: If True , the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr: collate_fn returns a batch that is a custom type, see the example below. None drop_last bool, default: False set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. None timeout numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. None worker_init_fn callable, default: None If not None , this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1] ) as input, after seeding and before data loading. None prefetch_factor int, default: 2 Number of samples loaded in advance by each worker. 2 means there will be a total of 2 * num_workers samples prefetched across all workers. None persistent_workers bool, default: False If True , the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. None Returns: Type Description StokeDataLoader wrapped torch.utils.data.DataLoader object ??? example \"View Source\" def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, )","title":"DataLoader"},{"location":"reference/stoke/#backward","text":"def backward ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]] ) Wrapped backwards call Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) None Returns: Type Description None None ??? example \"View Source\" def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1","title":"backward"},{"location":"reference/stoke/#barrier","text":"def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier()","title":"barrier"},{"location":"reference/stoke/#detach_and_sync_loss","text":"def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) None device default: None device to sync across None Returns: Type Description loss that is synced across devices and all_reduced w/ SUM None ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device)","title":"detach_and_sync_loss"},{"location":"reference/stoke/#dump_model_parameter_info","text":"def dump_model_parameter_info ( self ) Dumps all parameter information for named parameters (shape, device, dtype) Returns: Type Description None None ??? example \"View Source\" def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" )","title":"dump_model_parameter_info"},{"location":"reference/stoke/#load","text":"def load ( self , path : str , tag : str , strict : bool = True ) Loads a model checkpoint using the correct backend interface Parameters: Name Type Description Default path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None strict bool ignore non-matching keys None Returns: Type Description dict, default: None a dictionary of any custom fields the user passed to the save function ??? example \"View Source\" def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras","title":"load"},{"location":"reference/stoke/#loss","text":"def loss ( self , * args , ** kwargs ) Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters: Name Type Description Default *args list or tuple Additional arguments should be passed as keyword arguments None **kwargs dict Extra arguments passed to the loss function call(s) None Returns: Type Description outputs of callable loss function(s) None ??? example \"View Source\" def loss(self, args, *kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss","title":"loss"},{"location":"reference/stoke/#model","text":"def model ( self , * args , ** kwargs ) Wrapped model forward call Parameters: Name Type Description Default *args list or tuple Additional arguments should be passed as keyword arguments None **kwargs dict Extra arguments passed to the model forward call None Returns: Type Description model forward output None ??? example \"View Source\" def model(self, args, *kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs)","title":"model"},{"location":"reference/stoke/#print","text":"def print ( self , msg : Union [ str , List [ str ]], single_line : bool = False ) Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters: Name Type Description Default msg str message to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line )","title":"print"},{"location":"reference/stoke/#print_ema_loss","text":"def print_ema_loss ( self , prepend_msg : str = 'Current EMA Loss' , single_line : bool = False ) Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default prepend_msg str, default: \"Current EMA Loss\" message prepend to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\")","title":"print_ema_loss"},{"location":"reference/stoke/#print_mean_accumulated_synced_loss","text":"def print_mean_accumulated_synced_loss ( self , prepend_msg : str = 'Mean Accumulated & Synced Loss' , pre_backwards : bool = True , single_line : bool = False ) Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default prepend_msg str, default: \"Mean Accumulated & Synced Loss\" message prepend to print None pre_backwards bool, default: True if being called pre backward step None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\")","title":"print_mean_accumulated_synced_loss"},{"location":"reference/stoke/#print_num_model_parameters","text":"def print_num_model_parameters ( self , normalize : stoke . utils . ParamNormalize = < ParamNormalize . MILLION : 1000000.0 > ) Parameters: Name Type Description Default normalize ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing None Returns: Type Description None None ??? example \"View Source\" def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" )","title":"print_num_model_parameters"},{"location":"reference/stoke/#print_on_devices","text":"def print_on_devices ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 ) Wraps runner print interface for shorter semantics Parameters: Name Type Description Default msg str message to print None rank Union[int, List[int]], default: 0 which ranks to print on None Returns: Type Description None None ??? example \"View Source\" def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank)","title":"print_on_devices"},{"location":"reference/stoke/#print_synced_loss","text":"def print_synced_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], prepend_msg : str = 'Step Synced Loss' , device = None , single_line : bool = False ) Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None prepend_msg str, default: \"Step Synced Loss\" message prepend to print None device default: None specify the device to place the synced loss on (defaults to same device) same single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\")","title":"print_synced_loss"},{"location":"reference/stoke/#reset","text":"def reset ( self ) Public method for resetting the underlying stoke state Returns: Type Description None None ??? example \"View Source\" def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset()","title":"reset"},{"location":"reference/stoke/#reset_ema","text":"def reset_ema ( self ) Used to reset the current state of the rolling mean loss Returns: Type Description None None ??? example \"View Source\" def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0","title":"reset_ema"},{"location":"reference/stoke/#reset_tracking","text":"def reset_tracking ( self ) Public method for resetting all underlying stoke tracked variables Returns: Type Description None None ??? example \"View Source\" def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0","title":"reset_tracking"},{"location":"reference/stoke/#save","text":"def save ( self , path : str , name : str = UUID ( '1bec68f4-7df7-48d2-a526-14685e92f54f' ), extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Saves a model checkpoint using the correct backend interface Parameters: Name Type Description Default path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None name str, default: uuid4() name used to save checkpoint file None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag","title":"save"},{"location":"reference/stoke/#step","text":"def step ( self ) Wrapped step call Handles grad clipping internally Returns: Type Description None None ??? example \"View Source\" def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer )","title":"step"},{"location":"reference/stoke/#zero_grads","text":"def zero_grads ( self ) Zeros the optimizer grads depending on the optimizer type Returns: Type Description None None ??? example \"View Source\" def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod )","title":"zero_grads"},{"location":"reference/stoke/#stokeoptimizer","text":"class StokeOptimizer ( / , * args , ** kwargs )","title":"StokeOptimizer"},{"location":"reference/stoke/#attributes_21","text":"Name Type Description Default optimizer Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class None optimizer_kwargs Dict any keyword args to be unrolled into the optimizer at instantiation time None ??? example \"View Source\" class StokeOptimizer(TypedDict): \"\"\"Stoke optimizer wrapper class Given all the different backends and extensions the optimizer might need to be instantiated in a different way thus this typed dict holds the configuration without instantiation Attributes ---------- optimizer: Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class optimizer_kwargs: Dict any keyword args to be unrolled into the optimizer at instantiation time \"\"\" optimizer: Type[torch.optim.Optimizer] optimizer_kwargs: Dict","title":"Attributes"},{"location":"reference/stoke/#ancestors-in-mro_4","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/stoke/#methods_2","text":"","title":"Methods"},{"location":"reference/stoke/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/stoke/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/stoke/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/stoke/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/stoke/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/stoke/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/stoke/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/stoke/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/stoke/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/stoke/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/stoke/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/stoke/configs/","text":"Module stoke.configs Handles all config objects None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles all config objects\"\"\" from enum import Enum from typing import Dict, Optional, Type import attr import torch try: from typing import TypedDict except ImportError: from mypy_extensions import TypedDict class HorovodOps(Enum): \"\"\"Horovod ops options\"\"\" Average = \"Average\" Sum = \"Sum\" Adasum = \"Adasum\" class OffloadDevice(Enum): \"\"\"Offload device options\"\"\" none = \"none\" cpu = \"cpu\" nvme = \"nvme\" class BackendOptions(Enum): \"\"\"Communication backend options\"\"\" nccl = \"nccl\" mpi = \" mpi\" gloo = \"gloo\" @attr.s(auto_attribs=True) class AMPConfig: \"\"\"PyTorch AMP configuration class Attributes ---------- backoff_factor : float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration growth_factor : float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. growth_interval : int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor init_scale : float, default: 2.**16 Initial scale factor \"\"\" backoff_factor: float = 0.5 growth_factor: float = 2.0 growth_interval: int = 2000 init_scale: float = 2.0 ** 16 @attr.s(auto_attribs=True) class ApexConfig: \"\"\"Nvidia APEX configuration class Attributes ---------- cast_model_outputs: Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm max_loss_scale: float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling min_loss_scale: Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed scaler_per_loss: bool, default: False Option to impose a scaler for each loss instead of a global scaler verbosity: int, default: 0 Set to 0 to suppress Amp-related output \"\"\" cast_model_outputs: Optional[torch.dtype] = None convert_to_sync_batch_norm: bool = False max_loss_scale: float = 2.0 ** 24 min_loss_scale: Optional[float] = None scaler_per_loss: bool = False verbosity: int = 0 @attr.s(auto_attribs=True) class ClipGradConfig: \"\"\"Gradient clipping by value configuration class Attributes ---------- clip_value: float maximum allowed absolute value of the gradients [-clip_value, clip_value] \"\"\" clip_value: float @attr.s(auto_attribs=True) class ClipGradNormConfig: \"\"\"Gradient clipping by p-norm configuration class Attributes ---------- max_norm: float max norm of the gradients norm_type: float type of the used p-norm \"\"\" max_norm: float norm_type: float @attr.s(auto_attribs=True) class DDPConfig: \"\"\"PyTorch DistributedDataParallel configuration class Attributes ---------- local_rank: Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) auto_mpi_discovery: bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html backend: BackendOptions, default: 'nccl' Which communication backend to use broadcast_buffers: bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function bucket_cap_mb: int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) find_unused_parameters: bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach gradient_as_bucket_view: bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. init_method: str, default: 'env://' URL specifying how to initialize the process group no_sync: bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead \"\"\" local_rank: Optional[int] auto_mpi_discovery: bool = False convert_to_sync_batch_norm: bool = False backend: BackendOptions = \"nccl\" broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False gradient_as_bucket_view: bool = False init_method: str = \"env://\" no_sync: bool = True @attr.s(auto_attribs=True) class DeepspeedAIOConfig: \"\"\"Deepspeed asynchronous I/O configuration class Attributes ---------- block_size: int, default: 1048576 I/O block size in bytes ignore_unused_parameters: bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. overlap_events: bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. queue_depth: int, default: 8 I/O queue depth single_submit: bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. thread_count: int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. \"\"\" block_size: int = 1048576 ignore_unused_parameters: bool = True overlap_events: bool = True queue_depth: int = 8 single_submit: bool = False thread_count: int = 1 @attr.s(auto_attribs=True) class DeepspeedActivationCheckpointingConfig: \"\"\"Deepspeed activation checkpointing configuration class Attributes ---------- contiguous_memory_optimization: bool, default: False Copies partitioned activations so that they are contiguous in memory cpu_checkpointing: bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled number_checkpoints: Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization partition_activations: bool, default: False Enables partition activation when used with model parallelism profile: bool, default: False Logs the forward and backward time for each checkpoint function synchronize_checkpoint_boundary: bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary \"\"\" contiguous_memory_optimization: bool = False cpu_checkpointing: bool = False number_checkpoints: Optional[int] = None partition_activations: bool = False profile: bool = False synchronize_checkpoint_boundary: bool = False @attr.s(auto_attribs=True) class DeepspeedFlopsConfig: \"\"\"Deepspeed flops profiler configuration class Attributes ---------- detailed: bool, default: True Whether to print the detailed model profile module_depth: int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). output_file: Optional[str], default: None Path to the output file. If None, the profiler prints to stdout profile_step: int, default: 1 The global training step at which to profile. top_modules: int, default: 1 Limits the aggregated profile output to the number of top modules specified. Notes ----- Warm up steps are needed for accurate time measurement \"\"\" detailed: bool = True module_depth: int = -1 output_file: Optional[str] = None profile_step: int = 1 top_modules: int = 1 @attr.s(auto_attribs=True) class DeepspeedFP16Config: \"\"\"Deepspeed FP16 configuration class Attributes ---------- hysteresis: int, default: 2 represents the delay shift in dynamic loss scaling initial_scale_power: int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power loss_scale: float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) loss_scale_window: int, default: 1000 the window over which to raise/lower the dynamic loss scale value min_loss_scale: int, default: 1000 minimum dynamic loss scale value \"\"\" hysteresis: int = 2 initial_scale_power: int = 32 loss_scale: float = 0.0 loss_scale_window: int = 1000 min_loss_scale: int = 1000 @attr.s(auto_attribs=True) class DeepspeedOffloadOptimizerConfig: \"\"\"Deepspeed optimizer offloading configuration class Attributes ---------- buffer_count: int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). device: OffloadDevice, default: 'cpu' Device memory to offload optimizer state fast_init: bool, default: False Enable fast optimizer initialization when offloading to NVMe nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. pipeline: bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set pipeline_read: bool, default: False activate pipeline read (deepspeed has limited docs for what this does) pipeline_write: bool, default: False activate pipeline write(deepspeed has limited docs for what this does) \"\"\" buffer_count: int = 4 device: OffloadDevice = \"cpu\" fast_init: bool = False nvme_path: str = \"/local_nvme\" pin_memory: bool = False pipeline: bool = False pipeline_read: bool = False pipeline_write: bool = False @attr.s(auto_attribs=True) class DeepspeedOffloadParamConfig: \"\"\"Deepspeed parameter offloading configuration class Attributes ---------- buffer_count: int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe buffer_size: int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe device: OffloadDevice, default: 'cpu' Device memory to offload model parameters max_in_cpu: int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. \"\"\" buffer_count: int = 5 buffer_size: int = int(1e8) device: OffloadDevice = \"cpu\" max_in_cpu: int = int(1e9) nvme_path: str = \"/local_nvme\" pin_memory: bool = False @attr.s(auto_attribs=True) class DeepspeedPLDConfig: \"\"\" Attributes ---------- theta: float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed gamma: float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases \"\"\" theta: float = 1.0 gamma: float = 0.001 @attr.s(auto_attribs=True) class DeepspeedTensorboardConfig: \"\"\"Deepspeed Tensorboard configuration class Attributes ---------- output_path: str, default: '' Tensorboard output path job_name: str, default: 'DeepSpeedJobName' Tensorboard job name \"\"\" output_path: str = \"\" job_name: str = \"DeepSpeedJobName\" @attr.s(auto_attribs=True) class DeepspeedZeROConfig: \"\"\"Deepspeed ZeRO configuration class Attributes ---------- allgather_bucket_size: int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes allgather_partitions: bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step contiguous_gradients: bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. ignore_unused_parameters: bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 legacy_stage1: bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 offload_param: Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. overlap_comm: bool, default: False Attempts to overlap the reduction of the gradients with backward computation reduce_bucket_size: int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes reduce_scatter: bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients stage: int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively stage3_max_live_parameters: int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. stage3_max_reuse_distance: int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. stage3_prefetch_bucket_size: int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. stage3_param_persistence_threshold: int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). stage3_gather_fp16_weights_on_model_save: bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. sub_group_size: int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. \"\"\" allgather_bucket_size: int = int(5e8) allgather_partitions: bool = True contiguous_gradients: bool = False ignore_unused_parameters: bool = True legacy_stage1: bool = False offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None offload_param: Optional[DeepspeedOffloadParamConfig] = None overlap_comm: bool = False reduce_bucket_size: int = int(5e8) reduce_scatter: bool = True stage: int = 0 stage3_max_live_parameters: int = int(1e9) stage3_max_reuse_distance: int = int(1e9) stage3_prefetch_bucket_size: int = int(5e8) stage3_param_persistence_threshold: int = int(1e6) stage3_gather_fp16_weights_on_model_save: bool = False sub_group_size: int = int(1e12) @attr.s(auto_attribs=True) class DeepspeedConfig: \"\"\"Deepspeed configuration class Composed of other configuration classes related to specific functionality Attributes ---------- activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage auto_mpi_discovery: bool, default: True if distributed environment variables are not set, attempt to discover them from MPI disable_allgather: bool, default: False Disables allgather dist_backend: BackendOptions, default: 'nccl' Which communication backend to use distributed_port: int, default: 29500 torch distributed backend port dump_state: bool, default: False Print out state information of DeepSpeed object after initialization flops_profiler: Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown fp16: Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package fp32_allreduce: bool, default: False During gradient averaging perform allreduce with 32 bit values gradient_predivide_factor: float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs init_method: str, default: 'env://' URL specifying how to initialize the process group prescale_gradients: float, default: 1.0 Scale gradients before doing allreduce progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping sparse_gradients: bool, default: False Enable sparse compression of torch.nn.Embedding gradients steps_per_print: int, default: 10 Print train loss every N steps tensorboard: Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support verbose: bool, default: True flag to make deepspeed engine verbose with information wall_clock_breakdown: bool, default: False Enable timing of the latency of forward/backward/update training phases zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations Notes ----- Deepspeed does not use Apex\u2019s AMP mode whihc allows for more flexibility in mixed precision training modes. FP16 here is similar to AMP\u2019s O2 mode \"\"\" activation_checkpointing: Optional[ DeepspeedActivationCheckpointingConfig ] = DeepspeedActivationCheckpointingConfig() aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig() auto_mpi_discovery: bool = True disable_allgather: bool = False dist_backend: BackendOptions = \"nccl\" distributed_port: int = 29500 dump_state: bool = False flops_profiler: Optional[DeepspeedFlopsConfig] = None fp16: Optional[DeepspeedFP16Config] = None fp32_allreduce: bool = False gradient_predivide_factor: float = 1.0 init_method: str = \"env://\" prescale_gradients: bool = False progressive_layer_drop: Optional[DeepspeedPLDConfig] = None sparse_gradients: bool = False steps_per_print: int = 10 tensorboard: Optional[DeepspeedTensorboardConfig] = None verbose: bool = True wall_clock_breakdown: bool = False zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig() @attr.s(auto_attribs=True) class FairscaleOSSConfig: \"\"\"Fairscale optimizer state sharding configuration class Attributes ---------- broadcast_fp16: bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. \"\"\" broadcast_fp16: bool = False @attr.s(auto_attribs=True) class FairscaleSDDPConfig: \"\"\"Fairscale sharded data parallel (SDDP) configuration class Attributes ---------- auto_refresh_trainable: bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen broadcast_buffers: bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. reduce_buffer_size: int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. reduce_fp16: bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. sync_models_at_startup: bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state \"\"\" auto_refresh_trainable: bool = True broadcast_buffers: bool = True reduce_buffer_size: int = 2 ** 23 reduce_fp16: bool = False sync_models_at_startup: bool = True @attr.s(auto_attribs=True) class FairscaleFSDPConfig: \"\"\"Fairscale Fully Sharded Data Parallel configuration class Attributes ---------- bucket_cap_mb: int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of <dtype, device, process_group> tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing buffer_dtype: Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype clear_autocast_cache: bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory compute_dtype: Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. flatten_parameters: bool, default: True flatten parameters into a single contiguous tensor, which improves training speed force_input_to_fp32: bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. fp32_reduce_scatter: bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used gradient_predivide_factor: Optional[float], default: None divide factor before the reduction gradient_postdivide_factor: Optional[float], default: None divide factor after the reduction move_grads_to_cpu: Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used move_params_to_cpu: bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used no_broadcast_optim_state: Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node reshard_after_forward: bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) verbose: bool, default: True turn on verbose output for model\u2019s string representation Notes ----- mixed_precision: bool This value will automatically be set from the Stoke FP16 selected option (AMP only) state_dict_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup compute_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup \"\"\" bucket_cap_mb: int = 25 buffer_dtype: Optional[torch.dtype] = None clear_autocast_cache: bool = False compute_dtype: Optional[torch.dtype] = None flatten_parameters: bool = True force_input_to_fp32: bool = False fp32_reduce_scatter: bool = False gradient_predivide_factor: Optional[float] = None gradient_postdivide_factor: Optional[float] = None move_grads_to_cpu: Optional[bool] = None move_params_to_cpu: bool = False no_broadcast_optim_state: Optional[bool] = False reshard_after_forward: bool = True verbose: bool = False @attr.s(auto_attribs=True) class HorovodConfig: \"\"\"Horovod configuration class Attributes ---------- compression: bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm gradient_predivide_factor: float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. op: HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. \"\"\" compression: bool = False convert_to_sync_batch_norm: bool = False gradient_predivide_factor: float = 1.0 op: HorovodOps = \"Average\" class StokeOptimizer(TypedDict): \"\"\"Stoke optimizer wrapper class Given all the different backends and extensions the optimizer might need to be instantiated in a different way thus this typed dict holds the configuration without instantiation Attributes ---------- optimizer: Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class optimizer_kwargs: Dict any keyword args to be unrolled into the optimizer at instantiation time \"\"\" optimizer: Type[torch.optim.Optimizer] optimizer_kwargs: Dict Classes AMPConfig class AMPConfig ( backoff_factor : float = 0.5 , growth_factor : float = 2.0 , growth_interval : int = 2000 , init_scale : float = 65536.0 ) Attributes Name Type Description Default backoff_factor float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration None growth_factor float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. None growth_interval int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor None init_scale float, default: 2.**16 Initial scale factor None ??? example \"View Source\" class AMPConfig: \"\"\"PyTorch AMP configuration class Attributes ---------- backoff_factor : float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration growth_factor : float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. growth_interval : int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor init_scale : float, default: 2.**16 Initial scale factor \"\"\" backoff_factor: float = 0.5 growth_factor: float = 2.0 growth_interval: int = 2000 init_scale: float = 2.0 ** 16 ApexConfig class ApexConfig ( cast_model_outputs : Union [ torch . dtype , NoneType ] = None , convert_to_sync_batch_norm : bool = False , max_loss_scale : float = 16777216.0 , min_loss_scale : Union [ float , NoneType ] = None , scaler_per_loss : bool = False , verbosity : int = 0 ) Attributes Name Type Description Default cast_model_outputs Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm None max_loss_scale float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling None min_loss_scale Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed value scaler_per_loss bool, default: False Option to impose a scaler for each loss instead of a global scaler None verbosity int, default: 0 Set to 0 to suppress Amp-related output None ??? example \"View Source\" class ApexConfig: \"\"\"Nvidia APEX configuration class Attributes ---------- cast_model_outputs: Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm max_loss_scale: float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling min_loss_scale: Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed scaler_per_loss: bool, default: False Option to impose a scaler for each loss instead of a global scaler verbosity: int, default: 0 Set to 0 to suppress Amp-related output \"\"\" cast_model_outputs: Optional[torch.dtype] = None convert_to_sync_batch_norm: bool = False max_loss_scale: float = 2.0 ** 24 min_loss_scale: Optional[float] = None scaler_per_loss: bool = False verbosity: int = 0 BackendOptions class BackendOptions ( / , * args , ** kwargs ) ??? example \"View Source\" class BackendOptions(Enum): \"\"\"Communication backend options\"\"\" nccl = \"nccl\" mpi = \" mpi\" gloo = \"gloo\" Ancestors (in MRO) enum.Enum Class variables gloo mpi name nccl value ClipGradConfig class ClipGradConfig ( clip_value : float ) Attributes Name Type Description Default clip_value float maximum allowed absolute value of the gradients [-clip_value, clip_value] None ??? example \"View Source\" class ClipGradConfig: \"\"\"Gradient clipping by value configuration class Attributes ---------- clip_value: float maximum allowed absolute value of the gradients [-clip_value, clip_value] \"\"\" clip_value: float ClipGradNormConfig class ClipGradNormConfig ( max_norm : float , norm_type : float ) Attributes Name Type Description Default max_norm float max norm of the gradients None norm_type float type of the used p-norm None ??? example \"View Source\" class ClipGradNormConfig: \"\"\"Gradient clipping by p-norm configuration class Attributes ---------- max_norm: float max norm of the gradients norm_type: float type of the used p-norm \"\"\" max_norm: float norm_type: float DDPConfig class DDPConfig ( local_rank : Union [ int , NoneType ], auto_mpi_discovery : bool = False , convert_to_sync_batch_norm : bool = False , backend : stoke . configs . BackendOptions = 'nccl' , broadcast_buffers : bool = True , bucket_cap_mb : int = 25 , find_unused_parameters : bool = False , gradient_as_bucket_view : bool = False , init_method : str = 'env://' , no_sync : bool = True ) Attributes Name Type Description Default local_rank Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) None auto_mpi_discovery bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html None backend BackendOptions, default: 'nccl' Which communication backend to use None broadcast_buffers bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function None bucket_cap_mb int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) None find_unused_parameters bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach None gradient_as_bucket_view bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. None init_method str, default: 'env://' URL specifying how to initialize the process group None no_sync bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead None ??? example \"View Source\" class DDPConfig: \"\"\"PyTorch DistributedDataParallel configuration class Attributes ---------- local_rank: Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) auto_mpi_discovery: bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html backend: BackendOptions, default: 'nccl' Which communication backend to use broadcast_buffers: bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function bucket_cap_mb: int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) find_unused_parameters: bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach gradient_as_bucket_view: bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. init_method: str, default: 'env://' URL specifying how to initialize the process group no_sync: bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead \"\"\" local_rank: Optional[int] auto_mpi_discovery: bool = False convert_to_sync_batch_norm: bool = False backend: BackendOptions = \"nccl\" broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False gradient_as_bucket_view: bool = False init_method: str = \"env://\" no_sync: bool = True DeepspeedAIOConfig class DeepspeedAIOConfig ( block_size : int = 1048576 , ignore_unused_parameters : bool = True , overlap_events : bool = True , queue_depth : int = 8 , single_submit : bool = False , thread_count : int = 1 ) Attributes Name Type Description Default block_size int, default: 1048576 I/O block size in bytes None ignore_unused_parameters bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. None overlap_events bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. None queue_depth int, default: 8 I/O queue depth None single_submit bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. None thread_count int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. None ??? example \"View Source\" class DeepspeedAIOConfig: \"\"\"Deepspeed asynchronous I/O configuration class Attributes ---------- block_size: int, default: 1048576 I/O block size in bytes ignore_unused_parameters: bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. overlap_events: bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. queue_depth: int, default: 8 I/O queue depth single_submit: bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. thread_count: int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. \"\"\" block_size: int = 1048576 ignore_unused_parameters: bool = True overlap_events: bool = True queue_depth: int = 8 single_submit: bool = False thread_count: int = 1 DeepspeedActivationCheckpointingConfig class DeepspeedActivationCheckpointingConfig ( contiguous_memory_optimization : bool = False , cpu_checkpointing : bool = False , number_checkpoints : Union [ int , NoneType ] = None , partition_activations : bool = False , profile : bool = False , synchronize_checkpoint_boundary : bool = False ) Attributes Name Type Description Default contiguous_memory_optimization bool, default: False Copies partitioned activations so that they are contiguous in memory None cpu_checkpointing bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled None number_checkpoints Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization None partition_activations bool, default: False Enables partition activation when used with model parallelism None profile bool, default: False Logs the forward and backward time for each checkpoint function None synchronize_checkpoint_boundary bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary None ??? example \"View Source\" class DeepspeedActivationCheckpointingConfig: \"\"\"Deepspeed activation checkpointing configuration class Attributes ---------- contiguous_memory_optimization: bool, default: False Copies partitioned activations so that they are contiguous in memory cpu_checkpointing: bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled number_checkpoints: Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization partition_activations: bool, default: False Enables partition activation when used with model parallelism profile: bool, default: False Logs the forward and backward time for each checkpoint function synchronize_checkpoint_boundary: bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary \"\"\" contiguous_memory_optimization: bool = False cpu_checkpointing: bool = False number_checkpoints: Optional[int] = None partition_activations: bool = False profile: bool = False synchronize_checkpoint_boundary: bool = False DeepspeedConfig class DeepspeedConfig ( activation_checkpointing : Union [ stoke . configs . DeepspeedActivationCheckpointingConfig , NoneType ] = DeepspeedActivationCheckpointingConfig ( contiguous_memory_optimization = False , cpu_checkpointing = False , number_checkpoints = None , partition_activations = False , profile = False , synchronize_checkpoint_boundary = False ), aio : Union [ stoke . configs . DeepspeedAIOConfig , NoneType ] = DeepspeedAIOConfig ( block_size = 1048576 , ignore_unused_parameters = True , overlap_events = True , queue_depth = 8 , single_submit = False , thread_count = 1 ), auto_mpi_discovery : bool = True , disable_allgather : bool = False , dist_backend : stoke . configs . BackendOptions = 'nccl' , distributed_port : int = 29500 , dump_state : bool = False , flops_profiler : Union [ stoke . configs . DeepspeedFlopsConfig , NoneType ] = None , fp16 : Union [ stoke . configs . DeepspeedFP16Config , NoneType ] = None , fp32_allreduce : bool = False , gradient_predivide_factor : float = 1.0 , init_method : str = 'env://' , prescale_gradients : bool = False , progressive_layer_drop : Union [ stoke . configs . DeepspeedPLDConfig , NoneType ] = None , sparse_gradients : bool = False , steps_per_print : int = 10 , tensorboard : Union [ stoke . configs . DeepspeedTensorboardConfig , NoneType ] = None , verbose : bool = True , wall_clock_breakdown : bool = False , zero_optimization : Union [ stoke . configs . DeepspeedZeROConfig , NoneType ] = DeepspeedZeROConfig ( allgather_bucket_size = 500000000 , allgather_partitions = True , contiguous_gradients = False , ignore_unused_parameters = True , legacy_stage1 = False , offload_optimizer = None , offload_param = None , overlap_comm = False , reduce_bucket_size = 500000000 , reduce_scatter = True , stage = 0 , stage3_max_live_parameters = 1000000000 , stage3_max_reuse_distance = 1000000000 , stage3_prefetch_bucket_size = 500000000 , stage3_param_persistence_threshold = 1000000 , stage3_gather_fp16_weights_on_model_save = False , sub_group_size = 1000000000000 ) ) Attributes Name Type Description Default activation_checkpointing Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing None aio Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage None auto_mpi_discovery bool, default: True if distributed environment variables are not set, attempt to discover them from MPI None disable_allgather bool, default: False Disables allgather None dist_backend BackendOptions, default: 'nccl' Which communication backend to use None distributed_port int, default: 29500 torch distributed backend port None dump_state bool, default: False Print out state information of DeepSpeed object after initialization None flops_profiler Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown None fp16 Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package None fp32_allreduce bool, default: False During gradient averaging perform allreduce with 32 bit values None gradient_predivide_factor float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs None init_method str, default: 'env://' URL specifying how to initialize the process group None prescale_gradients float, default: 1.0 Scale gradients before doing allreduce None progressive_layer_drop Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping None sparse_gradients bool, default: False Enable sparse compression of torch.nn.Embedding gradients None steps_per_print int, default: 10 Print train loss every N steps None tensorboard Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support None verbose bool, default: True flag to make deepspeed engine verbose with information None wall_clock_breakdown bool, default: False Enable timing of the latency of forward/backward/update training phases None zero_optimization Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations None ??? example \"View Source\" class DeepspeedConfig: \"\"\"Deepspeed configuration class Composed of other configuration classes related to specific functionality Attributes ---------- activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage auto_mpi_discovery: bool, default: True if distributed environment variables are not set, attempt to discover them from MPI disable_allgather: bool, default: False Disables allgather dist_backend: BackendOptions, default: 'nccl' Which communication backend to use distributed_port: int, default: 29500 torch distributed backend port dump_state: bool, default: False Print out state information of DeepSpeed object after initialization flops_profiler: Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown fp16: Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package fp32_allreduce: bool, default: False During gradient averaging perform allreduce with 32 bit values gradient_predivide_factor: float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs init_method: str, default: 'env://' URL specifying how to initialize the process group prescale_gradients: float, default: 1.0 Scale gradients before doing allreduce progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping sparse_gradients: bool, default: False Enable sparse compression of torch.nn.Embedding gradients steps_per_print: int, default: 10 Print train loss every N steps tensorboard: Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support verbose: bool, default: True flag to make deepspeed engine verbose with information wall_clock_breakdown: bool, default: False Enable timing of the latency of forward/backward/update training phases zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations Notes ----- Deepspeed does not use Apex\u2019s AMP mode whihc allows for more flexibility in mixed precision training modes. FP16 here is similar to AMP\u2019s O2 mode \"\"\" activation_checkpointing: Optional[ DeepspeedActivationCheckpointingConfig ] = DeepspeedActivationCheckpointingConfig() aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig() auto_mpi_discovery: bool = True disable_allgather: bool = False dist_backend: BackendOptions = \"nccl\" distributed_port: int = 29500 dump_state: bool = False flops_profiler: Optional[DeepspeedFlopsConfig] = None fp16: Optional[DeepspeedFP16Config] = None fp32_allreduce: bool = False gradient_predivide_factor: float = 1.0 init_method: str = \"env://\" prescale_gradients: bool = False progressive_layer_drop: Optional[DeepspeedPLDConfig] = None sparse_gradients: bool = False steps_per_print: int = 10 tensorboard: Optional[DeepspeedTensorboardConfig] = None verbose: bool = True wall_clock_breakdown: bool = False zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig() DeepspeedFP16Config class DeepspeedFP16Config ( hysteresis : int = 2 , initial_scale_power : int = 32 , loss_scale : float = 0.0 , loss_scale_window : int = 1000 , min_loss_scale : int = 1000 ) Attributes Name Type Description Default hysteresis int, default: 2 represents the delay shift in dynamic loss scaling None initial_scale_power int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power None loss_scale float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) None loss_scale_window int, default: 1000 the window over which to raise/lower the dynamic loss scale value None min_loss_scale int, default: 1000 minimum dynamic loss scale value None ??? example \"View Source\" class DeepspeedFP16Config: \"\"\"Deepspeed FP16 configuration class Attributes ---------- hysteresis: int, default: 2 represents the delay shift in dynamic loss scaling initial_scale_power: int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power loss_scale: float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) loss_scale_window: int, default: 1000 the window over which to raise/lower the dynamic loss scale value min_loss_scale: int, default: 1000 minimum dynamic loss scale value \"\"\" hysteresis: int = 2 initial_scale_power: int = 32 loss_scale: float = 0.0 loss_scale_window: int = 1000 min_loss_scale: int = 1000 DeepspeedFlopsConfig class DeepspeedFlopsConfig ( detailed : bool = True , module_depth : int = - 1 , output_file : Union [ str , NoneType ] = None , profile_step : int = 1 , top_modules : int = 1 ) Attributes Name Type Description Default detailed bool, default: True Whether to print the detailed model profile None module_depth int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). None output_file Optional[str], default: None Path to the output file. If None, the profiler prints to stdout None profile_step int, default: 1 The global training step at which to profile. None top_modules int, default: 1 Limits the aggregated profile output to the number of top modules specified. None ??? example \"View Source\" class DeepspeedFlopsConfig: \"\"\"Deepspeed flops profiler configuration class Attributes ---------- detailed: bool, default: True Whether to print the detailed model profile module_depth: int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). output_file: Optional[str], default: None Path to the output file. If None, the profiler prints to stdout profile_step: int, default: 1 The global training step at which to profile. top_modules: int, default: 1 Limits the aggregated profile output to the number of top modules specified. Notes ----- Warm up steps are needed for accurate time measurement \"\"\" detailed: bool = True module_depth: int = -1 output_file: Optional[str] = None profile_step: int = 1 top_modules: int = 1 DeepspeedOffloadOptimizerConfig class DeepspeedOffloadOptimizerConfig ( buffer_count : int = 4 , device : stoke . configs . OffloadDevice = 'cpu' , fast_init : bool = False , nvme_path : str = '/local_nvme' , pin_memory : bool = False , pipeline : bool = False , pipeline_read : bool = False , pipeline_write : bool = False ) Attributes Name Type Description Default buffer_count int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). None device OffloadDevice, default: 'cpu' Device memory to offload optimizer state None fast_init bool, default: False Enable fast optimizer initialization when offloading to NVMe None nvme_path str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading None pin_memory bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. None pipeline bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set to pipeline_read bool, default: False activate pipeline read (deepspeed has limited docs for what this does) None pipeline_write bool, default: False activate pipeline write(deepspeed has limited docs for what this does) None ??? example \"View Source\" class DeepspeedOffloadOptimizerConfig: \"\"\"Deepspeed optimizer offloading configuration class Attributes ---------- buffer_count: int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). device: OffloadDevice, default: 'cpu' Device memory to offload optimizer state fast_init: bool, default: False Enable fast optimizer initialization when offloading to NVMe nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. pipeline: bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set pipeline_read: bool, default: False activate pipeline read (deepspeed has limited docs for what this does) pipeline_write: bool, default: False activate pipeline write(deepspeed has limited docs for what this does) \"\"\" buffer_count: int = 4 device: OffloadDevice = \"cpu\" fast_init: bool = False nvme_path: str = \"/local_nvme\" pin_memory: bool = False pipeline: bool = False pipeline_read: bool = False pipeline_write: bool = False DeepspeedOffloadParamConfig class DeepspeedOffloadParamConfig ( buffer_count : int = 5 , buffer_size : int = 100000000 , device : stoke . configs . OffloadDevice = 'cpu' , max_in_cpu : int = 1000000000 , nvme_path : str = '/local_nvme' , pin_memory : bool = False ) Attributes Name Type Description Default buffer_count int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe None buffer_size int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe None device OffloadDevice, default: 'cpu' Device memory to offload model parameters None max_in_cpu int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. None nvme_path str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading None pin_memory bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. None ??? example \"View Source\" class DeepspeedOffloadParamConfig: \"\"\"Deepspeed parameter offloading configuration class Attributes ---------- buffer_count: int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe buffer_size: int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe device: OffloadDevice, default: 'cpu' Device memory to offload model parameters max_in_cpu: int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. \"\"\" buffer_count: int = 5 buffer_size: int = int(1e8) device: OffloadDevice = \"cpu\" max_in_cpu: int = int(1e9) nvme_path: str = \"/local_nvme\" pin_memory: bool = False DeepspeedPLDConfig class DeepspeedPLDConfig ( theta : float = 1.0 , gamma : float = 0.001 ) Attributes Name Type Description Default theta float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed None gamma float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases None ??? example \"View Source\" class DeepspeedPLDConfig: \"\"\" Attributes ---------- theta: float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed gamma: float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases \"\"\" theta: float = 1.0 gamma: float = 0.001 DeepspeedTensorboardConfig class DeepspeedTensorboardConfig ( output_path : str = '' , job_name : str = 'DeepSpeedJobName' ) Attributes Name Type Description Default output_path str, default: '' Tensorboard output path None job_name str, default: 'DeepSpeedJobName' Tensorboard job name None ??? example \"View Source\" class DeepspeedTensorboardConfig: \"\"\"Deepspeed Tensorboard configuration class Attributes ---------- output_path: str, default: '' Tensorboard output path job_name: str, default: 'DeepSpeedJobName' Tensorboard job name \"\"\" output_path: str = \"\" job_name: str = \"DeepSpeedJobName\" DeepspeedZeROConfig class DeepspeedZeROConfig ( allgather_bucket_size : int = 500000000 , allgather_partitions : bool = True , contiguous_gradients : bool = False , ignore_unused_parameters : bool = True , legacy_stage1 : bool = False , offload_optimizer : Union [ stoke . configs . DeepspeedOffloadOptimizerConfig , NoneType ] = None , offload_param : Union [ stoke . configs . DeepspeedOffloadParamConfig , NoneType ] = None , overlap_comm : bool = False , reduce_bucket_size : int = 500000000 , reduce_scatter : bool = True , stage : int = 0 , stage3_max_live_parameters : int = 1000000000 , stage3_max_reuse_distance : int = 1000000000 , stage3_prefetch_bucket_size : int = 500000000 , stage3_param_persistence_threshold : int = 1000000 , stage3_gather_fp16_weights_on_model_save : bool = False , sub_group_size : int = 1000000000000 ) Attributes Name Type Description Default allgather_bucket_size int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes None allgather_partitions bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step None contiguous_gradients bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. None ignore_unused_parameters bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 None legacy_stage1 bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons None offload_optimizer Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 None offload_param Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. None overlap_comm bool, default: False Attempts to overlap the reduction of the gradients with backward computation None reduce_bucket_size int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes None reduce_scatter bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients None stage int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively None stage3_max_live_parameters int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. None stage3_max_reuse_distance int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. None stage3_prefetch_bucket_size int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. None stage3_param_persistence_threshold int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). None stage3_gather_fp16_weights_on_model_save bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. None sub_group_size int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. None ??? example \"View Source\" class DeepspeedZeROConfig: \"\"\"Deepspeed ZeRO configuration class Attributes ---------- allgather_bucket_size: int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes allgather_partitions: bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step contiguous_gradients: bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. ignore_unused_parameters: bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 legacy_stage1: bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 offload_param: Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. overlap_comm: bool, default: False Attempts to overlap the reduction of the gradients with backward computation reduce_bucket_size: int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes reduce_scatter: bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients stage: int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively stage3_max_live_parameters: int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. stage3_max_reuse_distance: int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. stage3_prefetch_bucket_size: int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. stage3_param_persistence_threshold: int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). stage3_gather_fp16_weights_on_model_save: bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. sub_group_size: int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. \"\"\" allgather_bucket_size: int = int(5e8) allgather_partitions: bool = True contiguous_gradients: bool = False ignore_unused_parameters: bool = True legacy_stage1: bool = False offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None offload_param: Optional[DeepspeedOffloadParamConfig] = None overlap_comm: bool = False reduce_bucket_size: int = int(5e8) reduce_scatter: bool = True stage: int = 0 stage3_max_live_parameters: int = int(1e9) stage3_max_reuse_distance: int = int(1e9) stage3_prefetch_bucket_size: int = int(5e8) stage3_param_persistence_threshold: int = int(1e6) stage3_gather_fp16_weights_on_model_save: bool = False sub_group_size: int = int(1e12) FairscaleFSDPConfig class FairscaleFSDPConfig ( bucket_cap_mb : int = 25 , buffer_dtype : Union [ torch . dtype , NoneType ] = None , clear_autocast_cache : bool = False , compute_dtype : Union [ torch . dtype , NoneType ] = None , flatten_parameters : bool = True , force_input_to_fp32 : bool = False , fp32_reduce_scatter : bool = False , gradient_predivide_factor : Union [ float , NoneType ] = None , gradient_postdivide_factor : Union [ float , NoneType ] = None , move_grads_to_cpu : Union [ bool , NoneType ] = None , move_params_to_cpu : bool = False , no_broadcast_optim_state : Union [ bool , NoneType ] = False , reshard_after_forward : bool = True , verbose : bool = False ) Attributes Name Type Description Default bucket_cap_mb int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing None buffer_dtype Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype value clear_autocast_cache bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory None compute_dtype Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. torch.float32 flatten_parameters bool, default: True flatten parameters into a single contiguous tensor, which improves training speed None force_input_to_fp32 bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. None fp32_reduce_scatter bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used None gradient_predivide_factor Optional[float], default: None divide factor before the reduction None gradient_postdivide_factor Optional[float], default: None divide factor after the reduction None move_grads_to_cpu Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used None move_params_to_cpu bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used None no_broadcast_optim_state Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node None reshard_after_forward bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) None verbose bool, default: True turn on verbose output for model\u2019s string representation None ??? example \"View Source\" class FairscaleFSDPConfig: \"\"\"Fairscale Fully Sharded Data Parallel configuration class Attributes ---------- bucket_cap_mb: int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of <dtype, device, process_group> tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing buffer_dtype: Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype clear_autocast_cache: bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory compute_dtype: Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. flatten_parameters: bool, default: True flatten parameters into a single contiguous tensor, which improves training speed force_input_to_fp32: bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. fp32_reduce_scatter: bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used gradient_predivide_factor: Optional[float], default: None divide factor before the reduction gradient_postdivide_factor: Optional[float], default: None divide factor after the reduction move_grads_to_cpu: Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used move_params_to_cpu: bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used no_broadcast_optim_state: Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node reshard_after_forward: bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) verbose: bool, default: True turn on verbose output for model\u2019s string representation Notes ----- mixed_precision: bool This value will automatically be set from the Stoke FP16 selected option (AMP only) state_dict_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup compute_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup \"\"\" bucket_cap_mb: int = 25 buffer_dtype: Optional[torch.dtype] = None clear_autocast_cache: bool = False compute_dtype: Optional[torch.dtype] = None flatten_parameters: bool = True force_input_to_fp32: bool = False fp32_reduce_scatter: bool = False gradient_predivide_factor: Optional[float] = None gradient_postdivide_factor: Optional[float] = None move_grads_to_cpu: Optional[bool] = None move_params_to_cpu: bool = False no_broadcast_optim_state: Optional[bool] = False reshard_after_forward: bool = True verbose: bool = False Descendants stoke.extensions._FairscaleFSDPConfig FairscaleOSSConfig class FairscaleOSSConfig ( broadcast_fp16 : bool = False ) Attributes Name Type Description Default broadcast_fp16 bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. None ??? example \"View Source\" class FairscaleOSSConfig: \"\"\"Fairscale optimizer state sharding configuration class Attributes ---------- broadcast_fp16: bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. \"\"\" broadcast_fp16: bool = False FairscaleSDDPConfig class FairscaleSDDPConfig ( auto_refresh_trainable : bool = True , broadcast_buffers : bool = True , reduce_buffer_size : int = 8388608 , reduce_fp16 : bool = False , sync_models_at_startup : bool = True ) Attributes Name Type Description Default auto_refresh_trainable bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen None broadcast_buffers bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. None reduce_buffer_size int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. None reduce_fp16 bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. None sync_models_at_startup bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state None ??? example \"View Source\" class FairscaleSDDPConfig: \"\"\"Fairscale sharded data parallel (SDDP) configuration class Attributes ---------- auto_refresh_trainable: bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen broadcast_buffers: bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. reduce_buffer_size: int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. reduce_fp16: bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. sync_models_at_startup: bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state \"\"\" auto_refresh_trainable: bool = True broadcast_buffers: bool = True reduce_buffer_size: int = 2 ** 23 reduce_fp16: bool = False sync_models_at_startup: bool = True HorovodConfig class HorovodConfig ( compression : bool = False , convert_to_sync_batch_norm : bool = False , gradient_predivide_factor : float = 1.0 , op : stoke . configs . HorovodOps = 'Average' ) Attributes Name Type Description Default compression bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm None gradient_predivide_factor float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. None op HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. None ??? example \"View Source\" class HorovodConfig: \"\"\"Horovod configuration class Attributes ---------- compression: bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm gradient_predivide_factor: float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. op: HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. \"\"\" compression: bool = False convert_to_sync_batch_norm: bool = False gradient_predivide_factor: float = 1.0 op: HorovodOps = \"Average\" HorovodOps class HorovodOps ( / , * args , ** kwargs ) ??? example \"View Source\" class HorovodOps(Enum): \"\"\"Horovod ops options\"\"\" Average = \"Average\" Sum = \"Sum\" Adasum = \"Adasum\" Ancestors (in MRO) enum.Enum Class variables Adasum Average Sum name value OffloadDevice class OffloadDevice ( / , * args , ** kwargs ) ??? example \"View Source\" class OffloadDevice(Enum): \"\"\"Offload device options\"\"\" none = \"none\" cpu = \"cpu\" nvme = \"nvme\" Ancestors (in MRO) enum.Enum Class variables cpu name none nvme value StokeOptimizer class StokeOptimizer ( / , * args , ** kwargs ) Attributes Name Type Description Default optimizer Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class None optimizer_kwargs Dict any keyword args to be unrolled into the optimizer at instantiation time None ??? example \"View Source\" class StokeOptimizer(TypedDict): \"\"\"Stoke optimizer wrapper class Given all the different backends and extensions the optimizer might need to be instantiated in a different way thus this typed dict holds the configuration without instantiation Attributes ---------- optimizer: Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class optimizer_kwargs: Dict any keyword args to be unrolled into the optimizer at instantiation time \"\"\" optimizer: Type[torch.optim.Optimizer] optimizer_kwargs: Dict Ancestors (in MRO) builtins.dict Methods clear def clear ( ... ) D.clear() -> None. Remove all items from D. copy def copy ( ... ) D.copy() -> a shallow copy of D fromkeys def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value. get def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default. items def items ( ... ) D.items() -> a set-like object providing a view on D's items keys def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys pop def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised popitem def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty. setdefault def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default. update def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k] values def values ( ... ) D.values() -> an object providing a view on D's values","title":"Configs"},{"location":"reference/stoke/configs/#module-stokeconfigs","text":"Handles all config objects None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles all config objects\"\"\" from enum import Enum from typing import Dict, Optional, Type import attr import torch try: from typing import TypedDict except ImportError: from mypy_extensions import TypedDict class HorovodOps(Enum): \"\"\"Horovod ops options\"\"\" Average = \"Average\" Sum = \"Sum\" Adasum = \"Adasum\" class OffloadDevice(Enum): \"\"\"Offload device options\"\"\" none = \"none\" cpu = \"cpu\" nvme = \"nvme\" class BackendOptions(Enum): \"\"\"Communication backend options\"\"\" nccl = \"nccl\" mpi = \" mpi\" gloo = \"gloo\" @attr.s(auto_attribs=True) class AMPConfig: \"\"\"PyTorch AMP configuration class Attributes ---------- backoff_factor : float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration growth_factor : float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. growth_interval : int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor init_scale : float, default: 2.**16 Initial scale factor \"\"\" backoff_factor: float = 0.5 growth_factor: float = 2.0 growth_interval: int = 2000 init_scale: float = 2.0 ** 16 @attr.s(auto_attribs=True) class ApexConfig: \"\"\"Nvidia APEX configuration class Attributes ---------- cast_model_outputs: Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm max_loss_scale: float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling min_loss_scale: Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed scaler_per_loss: bool, default: False Option to impose a scaler for each loss instead of a global scaler verbosity: int, default: 0 Set to 0 to suppress Amp-related output \"\"\" cast_model_outputs: Optional[torch.dtype] = None convert_to_sync_batch_norm: bool = False max_loss_scale: float = 2.0 ** 24 min_loss_scale: Optional[float] = None scaler_per_loss: bool = False verbosity: int = 0 @attr.s(auto_attribs=True) class ClipGradConfig: \"\"\"Gradient clipping by value configuration class Attributes ---------- clip_value: float maximum allowed absolute value of the gradients [-clip_value, clip_value] \"\"\" clip_value: float @attr.s(auto_attribs=True) class ClipGradNormConfig: \"\"\"Gradient clipping by p-norm configuration class Attributes ---------- max_norm: float max norm of the gradients norm_type: float type of the used p-norm \"\"\" max_norm: float norm_type: float @attr.s(auto_attribs=True) class DDPConfig: \"\"\"PyTorch DistributedDataParallel configuration class Attributes ---------- local_rank: Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) auto_mpi_discovery: bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html backend: BackendOptions, default: 'nccl' Which communication backend to use broadcast_buffers: bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function bucket_cap_mb: int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) find_unused_parameters: bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach gradient_as_bucket_view: bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. init_method: str, default: 'env://' URL specifying how to initialize the process group no_sync: bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead \"\"\" local_rank: Optional[int] auto_mpi_discovery: bool = False convert_to_sync_batch_norm: bool = False backend: BackendOptions = \"nccl\" broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False gradient_as_bucket_view: bool = False init_method: str = \"env://\" no_sync: bool = True @attr.s(auto_attribs=True) class DeepspeedAIOConfig: \"\"\"Deepspeed asynchronous I/O configuration class Attributes ---------- block_size: int, default: 1048576 I/O block size in bytes ignore_unused_parameters: bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. overlap_events: bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. queue_depth: int, default: 8 I/O queue depth single_submit: bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. thread_count: int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. \"\"\" block_size: int = 1048576 ignore_unused_parameters: bool = True overlap_events: bool = True queue_depth: int = 8 single_submit: bool = False thread_count: int = 1 @attr.s(auto_attribs=True) class DeepspeedActivationCheckpointingConfig: \"\"\"Deepspeed activation checkpointing configuration class Attributes ---------- contiguous_memory_optimization: bool, default: False Copies partitioned activations so that they are contiguous in memory cpu_checkpointing: bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled number_checkpoints: Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization partition_activations: bool, default: False Enables partition activation when used with model parallelism profile: bool, default: False Logs the forward and backward time for each checkpoint function synchronize_checkpoint_boundary: bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary \"\"\" contiguous_memory_optimization: bool = False cpu_checkpointing: bool = False number_checkpoints: Optional[int] = None partition_activations: bool = False profile: bool = False synchronize_checkpoint_boundary: bool = False @attr.s(auto_attribs=True) class DeepspeedFlopsConfig: \"\"\"Deepspeed flops profiler configuration class Attributes ---------- detailed: bool, default: True Whether to print the detailed model profile module_depth: int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). output_file: Optional[str], default: None Path to the output file. If None, the profiler prints to stdout profile_step: int, default: 1 The global training step at which to profile. top_modules: int, default: 1 Limits the aggregated profile output to the number of top modules specified. Notes ----- Warm up steps are needed for accurate time measurement \"\"\" detailed: bool = True module_depth: int = -1 output_file: Optional[str] = None profile_step: int = 1 top_modules: int = 1 @attr.s(auto_attribs=True) class DeepspeedFP16Config: \"\"\"Deepspeed FP16 configuration class Attributes ---------- hysteresis: int, default: 2 represents the delay shift in dynamic loss scaling initial_scale_power: int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power loss_scale: float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) loss_scale_window: int, default: 1000 the window over which to raise/lower the dynamic loss scale value min_loss_scale: int, default: 1000 minimum dynamic loss scale value \"\"\" hysteresis: int = 2 initial_scale_power: int = 32 loss_scale: float = 0.0 loss_scale_window: int = 1000 min_loss_scale: int = 1000 @attr.s(auto_attribs=True) class DeepspeedOffloadOptimizerConfig: \"\"\"Deepspeed optimizer offloading configuration class Attributes ---------- buffer_count: int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). device: OffloadDevice, default: 'cpu' Device memory to offload optimizer state fast_init: bool, default: False Enable fast optimizer initialization when offloading to NVMe nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. pipeline: bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set pipeline_read: bool, default: False activate pipeline read (deepspeed has limited docs for what this does) pipeline_write: bool, default: False activate pipeline write(deepspeed has limited docs for what this does) \"\"\" buffer_count: int = 4 device: OffloadDevice = \"cpu\" fast_init: bool = False nvme_path: str = \"/local_nvme\" pin_memory: bool = False pipeline: bool = False pipeline_read: bool = False pipeline_write: bool = False @attr.s(auto_attribs=True) class DeepspeedOffloadParamConfig: \"\"\"Deepspeed parameter offloading configuration class Attributes ---------- buffer_count: int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe buffer_size: int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe device: OffloadDevice, default: 'cpu' Device memory to offload model parameters max_in_cpu: int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. \"\"\" buffer_count: int = 5 buffer_size: int = int(1e8) device: OffloadDevice = \"cpu\" max_in_cpu: int = int(1e9) nvme_path: str = \"/local_nvme\" pin_memory: bool = False @attr.s(auto_attribs=True) class DeepspeedPLDConfig: \"\"\" Attributes ---------- theta: float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed gamma: float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases \"\"\" theta: float = 1.0 gamma: float = 0.001 @attr.s(auto_attribs=True) class DeepspeedTensorboardConfig: \"\"\"Deepspeed Tensorboard configuration class Attributes ---------- output_path: str, default: '' Tensorboard output path job_name: str, default: 'DeepSpeedJobName' Tensorboard job name \"\"\" output_path: str = \"\" job_name: str = \"DeepSpeedJobName\" @attr.s(auto_attribs=True) class DeepspeedZeROConfig: \"\"\"Deepspeed ZeRO configuration class Attributes ---------- allgather_bucket_size: int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes allgather_partitions: bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step contiguous_gradients: bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. ignore_unused_parameters: bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 legacy_stage1: bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 offload_param: Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. overlap_comm: bool, default: False Attempts to overlap the reduction of the gradients with backward computation reduce_bucket_size: int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes reduce_scatter: bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients stage: int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively stage3_max_live_parameters: int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. stage3_max_reuse_distance: int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. stage3_prefetch_bucket_size: int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. stage3_param_persistence_threshold: int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). stage3_gather_fp16_weights_on_model_save: bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. sub_group_size: int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. \"\"\" allgather_bucket_size: int = int(5e8) allgather_partitions: bool = True contiguous_gradients: bool = False ignore_unused_parameters: bool = True legacy_stage1: bool = False offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None offload_param: Optional[DeepspeedOffloadParamConfig] = None overlap_comm: bool = False reduce_bucket_size: int = int(5e8) reduce_scatter: bool = True stage: int = 0 stage3_max_live_parameters: int = int(1e9) stage3_max_reuse_distance: int = int(1e9) stage3_prefetch_bucket_size: int = int(5e8) stage3_param_persistence_threshold: int = int(1e6) stage3_gather_fp16_weights_on_model_save: bool = False sub_group_size: int = int(1e12) @attr.s(auto_attribs=True) class DeepspeedConfig: \"\"\"Deepspeed configuration class Composed of other configuration classes related to specific functionality Attributes ---------- activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage auto_mpi_discovery: bool, default: True if distributed environment variables are not set, attempt to discover them from MPI disable_allgather: bool, default: False Disables allgather dist_backend: BackendOptions, default: 'nccl' Which communication backend to use distributed_port: int, default: 29500 torch distributed backend port dump_state: bool, default: False Print out state information of DeepSpeed object after initialization flops_profiler: Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown fp16: Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package fp32_allreduce: bool, default: False During gradient averaging perform allreduce with 32 bit values gradient_predivide_factor: float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs init_method: str, default: 'env://' URL specifying how to initialize the process group prescale_gradients: float, default: 1.0 Scale gradients before doing allreduce progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping sparse_gradients: bool, default: False Enable sparse compression of torch.nn.Embedding gradients steps_per_print: int, default: 10 Print train loss every N steps tensorboard: Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support verbose: bool, default: True flag to make deepspeed engine verbose with information wall_clock_breakdown: bool, default: False Enable timing of the latency of forward/backward/update training phases zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations Notes ----- Deepspeed does not use Apex\u2019s AMP mode whihc allows for more flexibility in mixed precision training modes. FP16 here is similar to AMP\u2019s O2 mode \"\"\" activation_checkpointing: Optional[ DeepspeedActivationCheckpointingConfig ] = DeepspeedActivationCheckpointingConfig() aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig() auto_mpi_discovery: bool = True disable_allgather: bool = False dist_backend: BackendOptions = \"nccl\" distributed_port: int = 29500 dump_state: bool = False flops_profiler: Optional[DeepspeedFlopsConfig] = None fp16: Optional[DeepspeedFP16Config] = None fp32_allreduce: bool = False gradient_predivide_factor: float = 1.0 init_method: str = \"env://\" prescale_gradients: bool = False progressive_layer_drop: Optional[DeepspeedPLDConfig] = None sparse_gradients: bool = False steps_per_print: int = 10 tensorboard: Optional[DeepspeedTensorboardConfig] = None verbose: bool = True wall_clock_breakdown: bool = False zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig() @attr.s(auto_attribs=True) class FairscaleOSSConfig: \"\"\"Fairscale optimizer state sharding configuration class Attributes ---------- broadcast_fp16: bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. \"\"\" broadcast_fp16: bool = False @attr.s(auto_attribs=True) class FairscaleSDDPConfig: \"\"\"Fairscale sharded data parallel (SDDP) configuration class Attributes ---------- auto_refresh_trainable: bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen broadcast_buffers: bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. reduce_buffer_size: int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. reduce_fp16: bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. sync_models_at_startup: bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state \"\"\" auto_refresh_trainable: bool = True broadcast_buffers: bool = True reduce_buffer_size: int = 2 ** 23 reduce_fp16: bool = False sync_models_at_startup: bool = True @attr.s(auto_attribs=True) class FairscaleFSDPConfig: \"\"\"Fairscale Fully Sharded Data Parallel configuration class Attributes ---------- bucket_cap_mb: int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of <dtype, device, process_group> tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing buffer_dtype: Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype clear_autocast_cache: bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory compute_dtype: Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. flatten_parameters: bool, default: True flatten parameters into a single contiguous tensor, which improves training speed force_input_to_fp32: bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. fp32_reduce_scatter: bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used gradient_predivide_factor: Optional[float], default: None divide factor before the reduction gradient_postdivide_factor: Optional[float], default: None divide factor after the reduction move_grads_to_cpu: Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used move_params_to_cpu: bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used no_broadcast_optim_state: Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node reshard_after_forward: bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) verbose: bool, default: True turn on verbose output for model\u2019s string representation Notes ----- mixed_precision: bool This value will automatically be set from the Stoke FP16 selected option (AMP only) state_dict_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup compute_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup \"\"\" bucket_cap_mb: int = 25 buffer_dtype: Optional[torch.dtype] = None clear_autocast_cache: bool = False compute_dtype: Optional[torch.dtype] = None flatten_parameters: bool = True force_input_to_fp32: bool = False fp32_reduce_scatter: bool = False gradient_predivide_factor: Optional[float] = None gradient_postdivide_factor: Optional[float] = None move_grads_to_cpu: Optional[bool] = None move_params_to_cpu: bool = False no_broadcast_optim_state: Optional[bool] = False reshard_after_forward: bool = True verbose: bool = False @attr.s(auto_attribs=True) class HorovodConfig: \"\"\"Horovod configuration class Attributes ---------- compression: bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm gradient_predivide_factor: float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. op: HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. \"\"\" compression: bool = False convert_to_sync_batch_norm: bool = False gradient_predivide_factor: float = 1.0 op: HorovodOps = \"Average\" class StokeOptimizer(TypedDict): \"\"\"Stoke optimizer wrapper class Given all the different backends and extensions the optimizer might need to be instantiated in a different way thus this typed dict holds the configuration without instantiation Attributes ---------- optimizer: Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class optimizer_kwargs: Dict any keyword args to be unrolled into the optimizer at instantiation time \"\"\" optimizer: Type[torch.optim.Optimizer] optimizer_kwargs: Dict","title":"Module stoke.configs"},{"location":"reference/stoke/configs/#classes","text":"","title":"Classes"},{"location":"reference/stoke/configs/#ampconfig","text":"class AMPConfig ( backoff_factor : float = 0.5 , growth_factor : float = 2.0 , growth_interval : int = 2000 , init_scale : float = 65536.0 )","title":"AMPConfig"},{"location":"reference/stoke/configs/#attributes","text":"Name Type Description Default backoff_factor float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration None growth_factor float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. None growth_interval int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor None init_scale float, default: 2.**16 Initial scale factor None ??? example \"View Source\" class AMPConfig: \"\"\"PyTorch AMP configuration class Attributes ---------- backoff_factor : float, default: 0.5 Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration growth_factor : float, default: 2.0 Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations. growth_interval : int, default: 2000 Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by growth_factor init_scale : float, default: 2.**16 Initial scale factor \"\"\" backoff_factor: float = 0.5 growth_factor: float = 2.0 growth_interval: int = 2000 init_scale: float = 2.0 ** 16","title":"Attributes"},{"location":"reference/stoke/configs/#apexconfig","text":"class ApexConfig ( cast_model_outputs : Union [ torch . dtype , NoneType ] = None , convert_to_sync_batch_norm : bool = False , max_loss_scale : float = 16777216.0 , min_loss_scale : Union [ float , NoneType ] = None , scaler_per_loss : bool = False , verbosity : int = 0 )","title":"ApexConfig"},{"location":"reference/stoke/configs/#attributes_1","text":"Name Type Description Default cast_model_outputs Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm None max_loss_scale float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling None min_loss_scale Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed value scaler_per_loss bool, default: False Option to impose a scaler for each loss instead of a global scaler None verbosity int, default: 0 Set to 0 to suppress Amp-related output None ??? example \"View Source\" class ApexConfig: \"\"\"Nvidia APEX configuration class Attributes ---------- cast_model_outputs: Optional[torch.dtype], default: None Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm max_loss_scale: float, default: 2.**24 Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling min_loss_scale: Optional[float], default: None Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None means that no floor is imposed scaler_per_loss: bool, default: False Option to impose a scaler for each loss instead of a global scaler verbosity: int, default: 0 Set to 0 to suppress Amp-related output \"\"\" cast_model_outputs: Optional[torch.dtype] = None convert_to_sync_batch_norm: bool = False max_loss_scale: float = 2.0 ** 24 min_loss_scale: Optional[float] = None scaler_per_loss: bool = False verbosity: int = 0","title":"Attributes"},{"location":"reference/stoke/configs/#backendoptions","text":"class BackendOptions ( / , * args , ** kwargs ) ??? example \"View Source\" class BackendOptions(Enum): \"\"\"Communication backend options\"\"\" nccl = \"nccl\" mpi = \" mpi\" gloo = \"gloo\"","title":"BackendOptions"},{"location":"reference/stoke/configs/#ancestors-in-mro","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/configs/#class-variables","text":"gloo mpi name nccl value","title":"Class variables"},{"location":"reference/stoke/configs/#clipgradconfig","text":"class ClipGradConfig ( clip_value : float )","title":"ClipGradConfig"},{"location":"reference/stoke/configs/#attributes_2","text":"Name Type Description Default clip_value float maximum allowed absolute value of the gradients [-clip_value, clip_value] None ??? example \"View Source\" class ClipGradConfig: \"\"\"Gradient clipping by value configuration class Attributes ---------- clip_value: float maximum allowed absolute value of the gradients [-clip_value, clip_value] \"\"\" clip_value: float","title":"Attributes"},{"location":"reference/stoke/configs/#clipgradnormconfig","text":"class ClipGradNormConfig ( max_norm : float , norm_type : float )","title":"ClipGradNormConfig"},{"location":"reference/stoke/configs/#attributes_3","text":"Name Type Description Default max_norm float max norm of the gradients None norm_type float type of the used p-norm None ??? example \"View Source\" class ClipGradNormConfig: \"\"\"Gradient clipping by p-norm configuration class Attributes ---------- max_norm: float max norm of the gradients norm_type: float type of the used p-norm \"\"\" max_norm: float norm_type: float","title":"Attributes"},{"location":"reference/stoke/configs/#ddpconfig","text":"class DDPConfig ( local_rank : Union [ int , NoneType ], auto_mpi_discovery : bool = False , convert_to_sync_batch_norm : bool = False , backend : stoke . configs . BackendOptions = 'nccl' , broadcast_buffers : bool = True , bucket_cap_mb : int = 25 , find_unused_parameters : bool = False , gradient_as_bucket_view : bool = False , init_method : str = 'env://' , no_sync : bool = True )","title":"DDPConfig"},{"location":"reference/stoke/configs/#attributes_4","text":"Name Type Description Default local_rank Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) None auto_mpi_discovery bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html None backend BackendOptions, default: 'nccl' Which communication backend to use None broadcast_buffers bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function None bucket_cap_mb int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) None find_unused_parameters bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach None gradient_as_bucket_view bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. None init_method str, default: 'env://' URL specifying how to initialize the process group None no_sync bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead None ??? example \"View Source\" class DDPConfig: \"\"\"PyTorch DistributedDataParallel configuration class Attributes ---------- local_rank: Optional[int] Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg) auto_mpi_discovery: bool, default: False if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed function call) convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html backend: BackendOptions, default: 'nccl' Which communication backend to use broadcast_buffers: bool, default: True Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function bucket_cap_mb: int, default: 25 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) find_unused_parameters: bool, default: False Traverse the autograd graph from all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach gradient_as_bucket_view: bool, default: False When set to True, gradients will be views pointing to different offsets of allreduce communication buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by referring to the zero_grad() function in torch/optim/optimizer.py as a solution. init_method: str, default: 'env://' URL specifying how to initialize the process group no_sync: bool, default: True for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass after exiting the context. no sync might lead to higher memory usage but lower communication overhead \"\"\" local_rank: Optional[int] auto_mpi_discovery: bool = False convert_to_sync_batch_norm: bool = False backend: BackendOptions = \"nccl\" broadcast_buffers: bool = True bucket_cap_mb: int = 25 find_unused_parameters: bool = False gradient_as_bucket_view: bool = False init_method: str = \"env://\" no_sync: bool = True","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedaioconfig","text":"class DeepspeedAIOConfig ( block_size : int = 1048576 , ignore_unused_parameters : bool = True , overlap_events : bool = True , queue_depth : int = 8 , single_submit : bool = False , thread_count : int = 1 )","title":"DeepspeedAIOConfig"},{"location":"reference/stoke/configs/#attributes_5","text":"Name Type Description Default block_size int, default: 1048576 I/O block size in bytes None ignore_unused_parameters bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. None overlap_events bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. None queue_depth int, default: 8 I/O queue depth None single_submit bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. None thread_count int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. None ??? example \"View Source\" class DeepspeedAIOConfig: \"\"\"Deepspeed asynchronous I/O configuration class Attributes ---------- block_size: int, default: 1048576 I/O block size in bytes ignore_unused_parameters: bool, default: True Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks. This controls whether or not training should terminate with an error message when unused parameters are detected. overlap_events: bool, default: True Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests. queue_depth: int, default: 8 I/O queue depth single_submit: bool, default: False Submit requests to storage device as multiple individual requests as opposed to one block of requests. thread_count: int, default: 1 Intra-request parallelism for each read/write submitted by a user thread. \"\"\" block_size: int = 1048576 ignore_unused_parameters: bool = True overlap_events: bool = True queue_depth: int = 8 single_submit: bool = False thread_count: int = 1","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedactivationcheckpointingconfig","text":"class DeepspeedActivationCheckpointingConfig ( contiguous_memory_optimization : bool = False , cpu_checkpointing : bool = False , number_checkpoints : Union [ int , NoneType ] = None , partition_activations : bool = False , profile : bool = False , synchronize_checkpoint_boundary : bool = False )","title":"DeepspeedActivationCheckpointingConfig"},{"location":"reference/stoke/configs/#attributes_6","text":"Name Type Description Default contiguous_memory_optimization bool, default: False Copies partitioned activations so that they are contiguous in memory None cpu_checkpointing bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled None number_checkpoints Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization None partition_activations bool, default: False Enables partition activation when used with model parallelism None profile bool, default: False Logs the forward and backward time for each checkpoint function None synchronize_checkpoint_boundary bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary None ??? example \"View Source\" class DeepspeedActivationCheckpointingConfig: \"\"\"Deepspeed activation checkpointing configuration class Attributes ---------- contiguous_memory_optimization: bool, default: False Copies partitioned activations so that they are contiguous in memory cpu_checkpointing: bool, default: False Offloads partitioned activations to CPU if partition_activations is enabled number_checkpoints: Optional[int], default: None Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization partition_activations: bool, default: False Enables partition activation when used with model parallelism profile: bool, default: False Logs the forward and backward time for each checkpoint function synchronize_checkpoint_boundary: bool, default: False Inserts torch.cuda.synchronize() at each checkpoint boundary \"\"\" contiguous_memory_optimization: bool = False cpu_checkpointing: bool = False number_checkpoints: Optional[int] = None partition_activations: bool = False profile: bool = False synchronize_checkpoint_boundary: bool = False","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedconfig","text":"class DeepspeedConfig ( activation_checkpointing : Union [ stoke . configs . DeepspeedActivationCheckpointingConfig , NoneType ] = DeepspeedActivationCheckpointingConfig ( contiguous_memory_optimization = False , cpu_checkpointing = False , number_checkpoints = None , partition_activations = False , profile = False , synchronize_checkpoint_boundary = False ), aio : Union [ stoke . configs . DeepspeedAIOConfig , NoneType ] = DeepspeedAIOConfig ( block_size = 1048576 , ignore_unused_parameters = True , overlap_events = True , queue_depth = 8 , single_submit = False , thread_count = 1 ), auto_mpi_discovery : bool = True , disable_allgather : bool = False , dist_backend : stoke . configs . BackendOptions = 'nccl' , distributed_port : int = 29500 , dump_state : bool = False , flops_profiler : Union [ stoke . configs . DeepspeedFlopsConfig , NoneType ] = None , fp16 : Union [ stoke . configs . DeepspeedFP16Config , NoneType ] = None , fp32_allreduce : bool = False , gradient_predivide_factor : float = 1.0 , init_method : str = 'env://' , prescale_gradients : bool = False , progressive_layer_drop : Union [ stoke . configs . DeepspeedPLDConfig , NoneType ] = None , sparse_gradients : bool = False , steps_per_print : int = 10 , tensorboard : Union [ stoke . configs . DeepspeedTensorboardConfig , NoneType ] = None , verbose : bool = True , wall_clock_breakdown : bool = False , zero_optimization : Union [ stoke . configs . DeepspeedZeROConfig , NoneType ] = DeepspeedZeROConfig ( allgather_bucket_size = 500000000 , allgather_partitions = True , contiguous_gradients = False , ignore_unused_parameters = True , legacy_stage1 = False , offload_optimizer = None , offload_param = None , overlap_comm = False , reduce_bucket_size = 500000000 , reduce_scatter = True , stage = 0 , stage3_max_live_parameters = 1000000000 , stage3_max_reuse_distance = 1000000000 , stage3_prefetch_bucket_size = 500000000 , stage3_param_persistence_threshold = 1000000 , stage3_gather_fp16_weights_on_model_save = False , sub_group_size = 1000000000000 ) )","title":"DeepspeedConfig"},{"location":"reference/stoke/configs/#attributes_7","text":"Name Type Description Default activation_checkpointing Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing None aio Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage None auto_mpi_discovery bool, default: True if distributed environment variables are not set, attempt to discover them from MPI None disable_allgather bool, default: False Disables allgather None dist_backend BackendOptions, default: 'nccl' Which communication backend to use None distributed_port int, default: 29500 torch distributed backend port None dump_state bool, default: False Print out state information of DeepSpeed object after initialization None flops_profiler Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown None fp16 Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package None fp32_allreduce bool, default: False During gradient averaging perform allreduce with 32 bit values None gradient_predivide_factor float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs None init_method str, default: 'env://' URL specifying how to initialize the process group None prescale_gradients float, default: 1.0 Scale gradients before doing allreduce None progressive_layer_drop Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping None sparse_gradients bool, default: False Enable sparse compression of torch.nn.Embedding gradients None steps_per_print int, default: 10 Print train loss every N steps None tensorboard Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support None verbose bool, default: True flag to make deepspeed engine verbose with information None wall_clock_breakdown bool, default: False Enable timing of the latency of forward/backward/update training phases None zero_optimization Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations None ??? example \"View Source\" class DeepspeedConfig: \"\"\"Deepspeed configuration class Composed of other configuration classes related to specific functionality Attributes ---------- activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig() Enables and configures activation checkpointing aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig() Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent (NVMe) storage auto_mpi_discovery: bool, default: True if distributed environment variables are not set, attempt to discover them from MPI disable_allgather: bool, default: False Disables allgather dist_backend: BackendOptions, default: 'nccl' Which communication backend to use distributed_port: int, default: 29500 torch distributed backend port dump_state: bool, default: False Print out state information of DeepSpeed object after initialization flops_profiler: Optional[DeepspeedFlopsConfig], default: None Enables and configures the flops profiler. This would also enable wall_clock_breakdown fp16: Optional[DeepspeedFP16Config], default: None Enables and configures mixed precision/FP16 training that leverages NVIDIA\u2019s Apex package fp32_allreduce: bool, default: False During gradient averaging perform allreduce with 32 bit values gradient_predivide_factor: float, default: 1.0 Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability when scaling to large numbers of GPUs init_method: str, default: 'env://' URL specifying how to initialize the process group prescale_gradients: float, default: 1.0 Scale gradients before doing allreduce progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None Enables and configures progressive layer dropping sparse_gradients: bool, default: False Enable sparse compression of torch.nn.Embedding gradients steps_per_print: int, default: 10 Print train loss every N steps tensorboard: Optional[DeepspeedTensorboardConfig], default: None Enables and configures tensorboard support verbose: bool, default: True flag to make deepspeed engine verbose with information wall_clock_breakdown: bool, default: False Enable timing of the latency of forward/backward/update training phases zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig() Enables and configures ZeRO memory optimizations Notes ----- Deepspeed does not use Apex\u2019s AMP mode whihc allows for more flexibility in mixed precision training modes. FP16 here is similar to AMP\u2019s O2 mode \"\"\" activation_checkpointing: Optional[ DeepspeedActivationCheckpointingConfig ] = DeepspeedActivationCheckpointingConfig() aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig() auto_mpi_discovery: bool = True disable_allgather: bool = False dist_backend: BackendOptions = \"nccl\" distributed_port: int = 29500 dump_state: bool = False flops_profiler: Optional[DeepspeedFlopsConfig] = None fp16: Optional[DeepspeedFP16Config] = None fp32_allreduce: bool = False gradient_predivide_factor: float = 1.0 init_method: str = \"env://\" prescale_gradients: bool = False progressive_layer_drop: Optional[DeepspeedPLDConfig] = None sparse_gradients: bool = False steps_per_print: int = 10 tensorboard: Optional[DeepspeedTensorboardConfig] = None verbose: bool = True wall_clock_breakdown: bool = False zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig()","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedfp16config","text":"class DeepspeedFP16Config ( hysteresis : int = 2 , initial_scale_power : int = 32 , loss_scale : float = 0.0 , loss_scale_window : int = 1000 , min_loss_scale : int = 1000 )","title":"DeepspeedFP16Config"},{"location":"reference/stoke/configs/#attributes_8","text":"Name Type Description Default hysteresis int, default: 2 represents the delay shift in dynamic loss scaling None initial_scale_power int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power None loss_scale float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) None loss_scale_window int, default: 1000 the window over which to raise/lower the dynamic loss scale value None min_loss_scale int, default: 1000 minimum dynamic loss scale value None ??? example \"View Source\" class DeepspeedFP16Config: \"\"\"Deepspeed FP16 configuration class Attributes ---------- hysteresis: int, default: 2 represents the delay shift in dynamic loss scaling initial_scale_power: int, default: 32 power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power loss_scale: float, default: 0.0 loss scaling value for FP16 training (0.0 --> dynamic scaling) loss_scale_window: int, default: 1000 the window over which to raise/lower the dynamic loss scale value min_loss_scale: int, default: 1000 minimum dynamic loss scale value \"\"\" hysteresis: int = 2 initial_scale_power: int = 32 loss_scale: float = 0.0 loss_scale_window: int = 1000 min_loss_scale: int = 1000","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedflopsconfig","text":"class DeepspeedFlopsConfig ( detailed : bool = True , module_depth : int = - 1 , output_file : Union [ str , NoneType ] = None , profile_step : int = 1 , top_modules : int = 1 )","title":"DeepspeedFlopsConfig"},{"location":"reference/stoke/configs/#attributes_9","text":"Name Type Description Default detailed bool, default: True Whether to print the detailed model profile None module_depth int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). None output_file Optional[str], default: None Path to the output file. If None, the profiler prints to stdout None profile_step int, default: 1 The global training step at which to profile. None top_modules int, default: 1 Limits the aggregated profile output to the number of top modules specified. None ??? example \"View Source\" class DeepspeedFlopsConfig: \"\"\"Deepspeed flops profiler configuration class Attributes ---------- detailed: bool, default: True Whether to print the detailed model profile module_depth: int, default: -1 The depth of the model at which to print the aggregated module information. When set to -1, it prints information from the top module to the innermost modules (the maximum depth). output_file: Optional[str], default: None Path to the output file. If None, the profiler prints to stdout profile_step: int, default: 1 The global training step at which to profile. top_modules: int, default: 1 Limits the aggregated profile output to the number of top modules specified. Notes ----- Warm up steps are needed for accurate time measurement \"\"\" detailed: bool = True module_depth: int = -1 output_file: Optional[str] = None profile_step: int = 1 top_modules: int = 1","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedoffloadoptimizerconfig","text":"class DeepspeedOffloadOptimizerConfig ( buffer_count : int = 4 , device : stoke . configs . OffloadDevice = 'cpu' , fast_init : bool = False , nvme_path : str = '/local_nvme' , pin_memory : bool = False , pipeline : bool = False , pipeline_read : bool = False , pipeline_write : bool = False )","title":"DeepspeedOffloadOptimizerConfig"},{"location":"reference/stoke/configs/#attributes_10","text":"Name Type Description Default buffer_count int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). None device OffloadDevice, default: 'cpu' Device memory to offload optimizer state None fast_init bool, default: False Enable fast optimizer initialization when offloading to NVMe None nvme_path str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading None pin_memory bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. None pipeline bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set to pipeline_read bool, default: False activate pipeline read (deepspeed has limited docs for what this does) None pipeline_write bool, default: False activate pipeline write(deepspeed has limited docs for what this does) None ??? example \"View Source\" class DeepspeedOffloadOptimizerConfig: \"\"\"Deepspeed optimizer offloading configuration class Attributes ---------- buffer_count: int, default: 4 Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter, gradient, momentum, and variance). device: OffloadDevice, default: 'cpu' Device memory to offload optimizer state fast_init: bool, default: False Enable fast optimizer initialization when offloading to NVMe nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for optimizer state offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. pipeline: bool, default: False pipeline activated (will default to True if either pipeline_read or pipeline_write is set pipeline_read: bool, default: False activate pipeline read (deepspeed has limited docs for what this does) pipeline_write: bool, default: False activate pipeline write(deepspeed has limited docs for what this does) \"\"\" buffer_count: int = 4 device: OffloadDevice = \"cpu\" fast_init: bool = False nvme_path: str = \"/local_nvme\" pin_memory: bool = False pipeline: bool = False pipeline_read: bool = False pipeline_write: bool = False","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedoffloadparamconfig","text":"class DeepspeedOffloadParamConfig ( buffer_count : int = 5 , buffer_size : int = 100000000 , device : stoke . configs . OffloadDevice = 'cpu' , max_in_cpu : int = 1000000000 , nvme_path : str = '/local_nvme' , pin_memory : bool = False )","title":"DeepspeedOffloadParamConfig"},{"location":"reference/stoke/configs/#attributes_11","text":"Name Type Description Default buffer_count int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe None buffer_size int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe None device OffloadDevice, default: 'cpu' Device memory to offload model parameters None max_in_cpu int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. None nvme_path str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading None pin_memory bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. None ??? example \"View Source\" class DeepspeedOffloadParamConfig: \"\"\"Deepspeed parameter offloading configuration class Attributes ---------- buffer_count: int, default: 5 Number of buffers in buffer pool for parameter offloading to NVMe buffer_size: int, default: int(1E8) Size of buffers in buffer pool for parameter offloading to NVMe device: OffloadDevice, default: 'cpu' Device memory to offload model parameters max_in_cpu: int, default: int(1E9) Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled. nvme_path: str, default: '/local_nvme' Filesystem path for NVMe device for parameter offloading pin_memory: bool, default: False Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead. \"\"\" buffer_count: int = 5 buffer_size: int = int(1e8) device: OffloadDevice = \"cpu\" max_in_cpu: int = int(1e9) nvme_path: str = \"/local_nvme\" pin_memory: bool = False","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedpldconfig","text":"class DeepspeedPLDConfig ( theta : float = 1.0 , gamma : float = 0.001 )","title":"DeepspeedPLDConfig"},{"location":"reference/stoke/configs/#attributes_12","text":"Name Type Description Default theta float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed None gamma float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases None ??? example \"View Source\" class DeepspeedPLDConfig: \"\"\" Attributes ---------- theta: float, default: 1.0 Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value, the faster the training speed gamma: float, default: 0.001 Hyper-parameter that controls how fast the drop ratio increases \"\"\" theta: float = 1.0 gamma: float = 0.001","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedtensorboardconfig","text":"class DeepspeedTensorboardConfig ( output_path : str = '' , job_name : str = 'DeepSpeedJobName' )","title":"DeepspeedTensorboardConfig"},{"location":"reference/stoke/configs/#attributes_13","text":"Name Type Description Default output_path str, default: '' Tensorboard output path None job_name str, default: 'DeepSpeedJobName' Tensorboard job name None ??? example \"View Source\" class DeepspeedTensorboardConfig: \"\"\"Deepspeed Tensorboard configuration class Attributes ---------- output_path: str, default: '' Tensorboard output path job_name: str, default: 'DeepSpeedJobName' Tensorboard job name \"\"\" output_path: str = \"\" job_name: str = \"DeepSpeedJobName\"","title":"Attributes"},{"location":"reference/stoke/configs/#deepspeedzeroconfig","text":"class DeepspeedZeROConfig ( allgather_bucket_size : int = 500000000 , allgather_partitions : bool = True , contiguous_gradients : bool = False , ignore_unused_parameters : bool = True , legacy_stage1 : bool = False , offload_optimizer : Union [ stoke . configs . DeepspeedOffloadOptimizerConfig , NoneType ] = None , offload_param : Union [ stoke . configs . DeepspeedOffloadParamConfig , NoneType ] = None , overlap_comm : bool = False , reduce_bucket_size : int = 500000000 , reduce_scatter : bool = True , stage : int = 0 , stage3_max_live_parameters : int = 1000000000 , stage3_max_reuse_distance : int = 1000000000 , stage3_prefetch_bucket_size : int = 500000000 , stage3_param_persistence_threshold : int = 1000000 , stage3_gather_fp16_weights_on_model_save : bool = False , sub_group_size : int = 1000000000000 )","title":"DeepspeedZeROConfig"},{"location":"reference/stoke/configs/#attributes_14","text":"Name Type Description Default allgather_bucket_size int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes None allgather_partitions bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step None contiguous_gradients bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. None ignore_unused_parameters bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 None legacy_stage1 bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons None offload_optimizer Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 None offload_param Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. None overlap_comm bool, default: False Attempts to overlap the reduction of the gradients with backward computation None reduce_bucket_size int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes None reduce_scatter bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients None stage int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively None stage3_max_live_parameters int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. None stage3_max_reuse_distance int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. None stage3_prefetch_bucket_size int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. None stage3_param_persistence_threshold int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). None stage3_gather_fp16_weights_on_model_save bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. None sub_group_size int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. None ??? example \"View Source\" class DeepspeedZeROConfig: \"\"\"Deepspeed ZeRO configuration class Attributes ---------- allgather_bucket_size: int, default: int(5E8) Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes allgather_partitions: bool, default: True Chooses between allgather collective or a series of broadcast collectives to gather updated parameters from all the GPUs at the end of each step contiguous_gradients: bool, default: False Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward pass. Only useful when running very large models. ignore_unused_parameters: bool, default: True Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707 legacy_stage1: bool, default: False Use deepspeed < v0.3.17 zero stage 1, kept for backwards compatability reasons offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3 offload_param: Optional[DeepspeedOffloadParamConfig], default: None Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch sizes. Valid only with stage 3. overlap_comm: bool, default: False Attempts to overlap the reduction of the gradients with backward computation reduce_bucket_size: int, default: int(5E8) Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large model sizes reduce_scatter: bool, default: True Uses reduce or reduce scatter instead of allreduce to average gradients stage: int, default: 0 Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively stage3_max_live_parameters: int, default: int(1E9) The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but perform more communication. stage3_max_reuse_distance: int, default: int(1E9) Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less memory, but perform more communication. stage3_prefetch_bucket_size: int, default: int(5E8) The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase stalls due to communication. stage3_param_persistence_threshold: int, default: int(1E6) Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly increase communication (especially latency-bound messages). stage3_gather_fp16_weights_on_model_save: bool, default: False Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned across GPUs, they aren\u2019t part of state_dict, so this function automatically gather the weights when this option is enabled and then saves the fp16 model weights. sub_group_size: int, default: int(1E12) sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are grouped into buckets of sub_group_size and each buckets is updated one at a time. \"\"\" allgather_bucket_size: int = int(5e8) allgather_partitions: bool = True contiguous_gradients: bool = False ignore_unused_parameters: bool = True legacy_stage1: bool = False offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None offload_param: Optional[DeepspeedOffloadParamConfig] = None overlap_comm: bool = False reduce_bucket_size: int = int(5e8) reduce_scatter: bool = True stage: int = 0 stage3_max_live_parameters: int = int(1e9) stage3_max_reuse_distance: int = int(1e9) stage3_prefetch_bucket_size: int = int(5e8) stage3_param_persistence_threshold: int = int(1e6) stage3_gather_fp16_weights_on_model_save: bool = False sub_group_size: int = int(1e12)","title":"Attributes"},{"location":"reference/stoke/configs/#fairscalefsdpconfig","text":"class FairscaleFSDPConfig ( bucket_cap_mb : int = 25 , buffer_dtype : Union [ torch . dtype , NoneType ] = None , clear_autocast_cache : bool = False , compute_dtype : Union [ torch . dtype , NoneType ] = None , flatten_parameters : bool = True , force_input_to_fp32 : bool = False , fp32_reduce_scatter : bool = False , gradient_predivide_factor : Union [ float , NoneType ] = None , gradient_postdivide_factor : Union [ float , NoneType ] = None , move_grads_to_cpu : Union [ bool , NoneType ] = None , move_params_to_cpu : bool = False , no_broadcast_optim_state : Union [ bool , NoneType ] = False , reshard_after_forward : bool = True , verbose : bool = False )","title":"FairscaleFSDPConfig"},{"location":"reference/stoke/configs/#attributes_15","text":"Name Type Description Default bucket_cap_mb int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing None buffer_dtype Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype value clear_autocast_cache bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory None compute_dtype Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. torch.float32 flatten_parameters bool, default: True flatten parameters into a single contiguous tensor, which improves training speed None force_input_to_fp32 bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. None fp32_reduce_scatter bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used None gradient_predivide_factor Optional[float], default: None divide factor before the reduction None gradient_postdivide_factor Optional[float], default: None divide factor after the reduction None move_grads_to_cpu Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used None move_params_to_cpu bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used None no_broadcast_optim_state Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node None reshard_after_forward bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) None verbose bool, default: True turn on verbose output for model\u2019s string representation None ??? example \"View Source\" class FairscaleFSDPConfig: \"\"\"Fairscale Fully Sharded Data Parallel configuration class Attributes ---------- bucket_cap_mb: int, default: 25 FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters. bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during the backward pass and freed at the end of the backward pass to save more memory for other phases of the training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the backward. In FSDP, the buffer size does not change with model size (it changes based on number of <dtype, device, process_group> tuples) and gradient ready order matters little since FSDP has a final flush call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with compute is done differently too. Values <= 0 disable bucketing buffer_dtype: Optional[torch.dtype], default: None dtype for buffers for computation. defaults to value of compute_dtype clear_autocast_cache: bool, default: False When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save GPU memory compute_dtype: Optional[torch.dtype], default: None dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set, in which case it defaults to torch.float16. flatten_parameters: bool, default: True flatten parameters into a single contiguous tensor, which improves training speed force_input_to_fp32: bool, default: False: force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper. fp32_reduce_scatter: bool, default: False reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used gradient_predivide_factor: Optional[float], default: None divide factor before the reduction gradient_postdivide_factor: Optional[float], default: None divide factor after the reduction move_grads_to_cpu: Optional[bool], default: None move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used move_params_to_cpu: bool, default: False offload FP32 params to CPU. This is only relevant when FP16 AMP is used no_broadcast_optim_state: Optional[bool], default: False do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few parameters can fit on one node reshard_after_forward: bool, default: True reshard parameters after the forward pass. This saves memory but slows training. This is only relevant when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html) verbose: bool, default: True turn on verbose output for model\u2019s string representation Notes ----- mixed_precision: bool This value will automatically be set from the Stoke FP16 selected option (AMP only) state_dict_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup compute_device: torch.device this is not exposed as it should be managed internally from the DDP backend setup \"\"\" bucket_cap_mb: int = 25 buffer_dtype: Optional[torch.dtype] = None clear_autocast_cache: bool = False compute_dtype: Optional[torch.dtype] = None flatten_parameters: bool = True force_input_to_fp32: bool = False fp32_reduce_scatter: bool = False gradient_predivide_factor: Optional[float] = None gradient_postdivide_factor: Optional[float] = None move_grads_to_cpu: Optional[bool] = None move_params_to_cpu: bool = False no_broadcast_optim_state: Optional[bool] = False reshard_after_forward: bool = True verbose: bool = False","title":"Attributes"},{"location":"reference/stoke/configs/#descendants","text":"stoke.extensions._FairscaleFSDPConfig","title":"Descendants"},{"location":"reference/stoke/configs/#fairscaleossconfig","text":"class FairscaleOSSConfig ( broadcast_fp16 : bool = False )","title":"FairscaleOSSConfig"},{"location":"reference/stoke/configs/#attributes_16","text":"Name Type Description Default broadcast_fp16 bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. None ??? example \"View Source\" class FairscaleOSSConfig: \"\"\"Fairscale optimizer state sharding configuration class Attributes ---------- broadcast_fp16: bool, default: False Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy. \"\"\" broadcast_fp16: bool = False","title":"Attributes"},{"location":"reference/stoke/configs/#fairscalesddpconfig","text":"class FairscaleSDDPConfig ( auto_refresh_trainable : bool = True , broadcast_buffers : bool = True , reduce_buffer_size : int = 8388608 , reduce_fp16 : bool = False , sync_models_at_startup : bool = True )","title":"FairscaleSDDPConfig"},{"location":"reference/stoke/configs/#attributes_17","text":"Name Type Description Default auto_refresh_trainable bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen None broadcast_buffers bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. None reduce_buffer_size int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. None reduce_fp16 bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. None sync_models_at_startup bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state None ??? example \"View Source\" class FairscaleSDDPConfig: \"\"\"Fairscale sharded data parallel (SDDP) configuration class Attributes ---------- auto_refresh_trainable: bool, default: True Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a parameter is frozen or unfrozen broadcast_buffers: bool, default: True Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters. reduce_buffer_size: int, default: 2 ** 23 he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact the long term memory consumption, because these buckets correspond to parameters which will not be sharded. Set to 0 to remove all bucketing, 1M to 8M is usually reasonable. reduce_fp16: bool, default: False cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve performance for multi node jobs using PyTorch AMP. The effect is similar to DDP\u2019s fp16_compress_hook and will also save some memory. sync_models_at_startup: bool, default: True Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or the training restarts from a saved state \"\"\" auto_refresh_trainable: bool = True broadcast_buffers: bool = True reduce_buffer_size: int = 2 ** 23 reduce_fp16: bool = False sync_models_at_startup: bool = True","title":"Attributes"},{"location":"reference/stoke/configs/#horovodconfig","text":"class HorovodConfig ( compression : bool = False , convert_to_sync_batch_norm : bool = False , gradient_predivide_factor : float = 1.0 , op : stoke . configs . HorovodOps = 'Average' )","title":"HorovodConfig"},{"location":"reference/stoke/configs/#attributes_18","text":"Name Type Description Default compression bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. None convert_to_sync_batch_norm bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm None gradient_predivide_factor float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. None op HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. None ??? example \"View Source\" class HorovodConfig: \"\"\"Horovod configuration class Attributes ---------- compression: bool, default: False Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter update step. convert_to_sync_batch_norm: bool, default: False Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm gradient_predivide_factor: float, default: 1.0 If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum. op: HorovodOps, default: 'Average' The reduction operation to use when combining gradients across different ranks. \"\"\" compression: bool = False convert_to_sync_batch_norm: bool = False gradient_predivide_factor: float = 1.0 op: HorovodOps = \"Average\"","title":"Attributes"},{"location":"reference/stoke/configs/#horovodops","text":"class HorovodOps ( / , * args , ** kwargs ) ??? example \"View Source\" class HorovodOps(Enum): \"\"\"Horovod ops options\"\"\" Average = \"Average\" Sum = \"Sum\" Adasum = \"Adasum\"","title":"HorovodOps"},{"location":"reference/stoke/configs/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/configs/#class-variables_1","text":"Adasum Average Sum name value","title":"Class variables"},{"location":"reference/stoke/configs/#offloaddevice","text":"class OffloadDevice ( / , * args , ** kwargs ) ??? example \"View Source\" class OffloadDevice(Enum): \"\"\"Offload device options\"\"\" none = \"none\" cpu = \"cpu\" nvme = \"nvme\"","title":"OffloadDevice"},{"location":"reference/stoke/configs/#ancestors-in-mro_2","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/configs/#class-variables_2","text":"cpu name none nvme value","title":"Class variables"},{"location":"reference/stoke/configs/#stokeoptimizer","text":"class StokeOptimizer ( / , * args , ** kwargs )","title":"StokeOptimizer"},{"location":"reference/stoke/configs/#attributes_19","text":"Name Type Description Default optimizer Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class None optimizer_kwargs Dict any keyword args to be unrolled into the optimizer at instantiation time None ??? example \"View Source\" class StokeOptimizer(TypedDict): \"\"\"Stoke optimizer wrapper class Given all the different backends and extensions the optimizer might need to be instantiated in a different way thus this typed dict holds the configuration without instantiation Attributes ---------- optimizer: Type[torch.optim.Optimizer] un-instantiated torch.optim.Optimizer class optimizer_kwargs: Dict any keyword args to be unrolled into the optimizer at instantiation time \"\"\" optimizer: Type[torch.optim.Optimizer] optimizer_kwargs: Dict","title":"Attributes"},{"location":"reference/stoke/configs/#ancestors-in-mro_3","text":"builtins.dict","title":"Ancestors (in MRO)"},{"location":"reference/stoke/configs/#methods","text":"","title":"Methods"},{"location":"reference/stoke/configs/#clear","text":"def clear ( ... ) D.clear() -> None. Remove all items from D.","title":"clear"},{"location":"reference/stoke/configs/#copy","text":"def copy ( ... ) D.copy() -> a shallow copy of D","title":"copy"},{"location":"reference/stoke/configs/#fromkeys","text":"def fromkeys ( iterable , value = None , / ) Create a new dictionary with keys from iterable and values set to value.","title":"fromkeys"},{"location":"reference/stoke/configs/#get","text":"def get ( self , key , default = None , / ) Return the value for key if key is in the dictionary, else default.","title":"get"},{"location":"reference/stoke/configs/#items","text":"def items ( ... ) D.items() -> a set-like object providing a view on D's items","title":"items"},{"location":"reference/stoke/configs/#keys","text":"def keys ( ... ) D.keys() -> a set-like object providing a view on D's keys","title":"keys"},{"location":"reference/stoke/configs/#pop","text":"def pop ( ... ) D.pop(k[,d]) -> v, remove specified key and return the corresponding value. If key is not found, d is returned if given, otherwise KeyError is raised","title":"pop"},{"location":"reference/stoke/configs/#popitem","text":"def popitem ( self , / ) Remove and return a (key, value) pair as a 2-tuple. Pairs are returned in LIFO (last-in, first-out) order. Raises KeyError if the dict is empty.","title":"popitem"},{"location":"reference/stoke/configs/#setdefault","text":"def setdefault ( self , key , default = None , / ) Insert key with a value of default if key is not in the dictionary. Return the value for key if key is in the dictionary, else default.","title":"setdefault"},{"location":"reference/stoke/configs/#update","text":"def update ( ... ) D.update([E, ]**F) -> None. Update D from dict/iterable E and F. If E is present and has a .keys() method, then does: for k in E: D[k] = E[k] If E is present and lacks a .keys() method, then does: for k, v in E: D[k] = v In either case, this is followed by: for k in F: D[k] = F[k]","title":"update"},{"location":"reference/stoke/configs/#values","text":"def values ( ... ) D.values() -> an object providing a view on D's values","title":"values"},{"location":"reference/stoke/data/","text":"Module stoke.data Handles any data (e.g. loader, sampler, etc.) related classes None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles any data (e.g. loader, sampler, etc.) related classes\"\"\" import itertools from math import ceil from typing import Dict, Iterator, List, Optional, Sequence, Tuple, Union import horovod.torch as hvd import numpy as np import torch import torch.distributed as dist from torch.utils.data import DataLoader as DL from torch.utils.data import Dataset from torch.utils.data.distributed import Sampler from stoke.status import DistributedOptions, FP16Options from stoke.utils import T_co, _collate_fn_t, _worker_init_fn_t class StokeDataLoader(DL): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs Attributes ---------- _gpu: bool _fp16: Optional[FP16Options] See Also -------- torch.utils.data.DataLoader: base DataLoader class that this inherits from (check for all attributes) \"\"\" def __init__( self, gpu: bool, fp16: Optional[FP16Options], dataset: Dataset[T_co], batch_size: Optional[int] = 1, shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Maps to torch.utils.data.DataLoader __init__ Shim is necessary to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. batch_size: int, default: 1 how many samples per batch to load . shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Call super init for the actual torch DataLoader super(StokeDataLoader, self).__init__( dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) self._gpu = gpu self._fp16 = fp16 def __iter__(self): \"\"\"Underlying iter of the DataLoader that yields samples Wrap the base __iter__ with a call to place on the device if flagged Yields ------ Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] data placed on the correct device \"\"\" # Iterate using the base class iter but override the yield by pushing to device prior if gpu flag is true for val in super().__iter__(): yield val if not self._gpu else self._place_data_on_gpu(val) def _place_data_on_gpu( self, data: Union[ torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor], ], ): \"\"\"Determine data structure and then place on the correct device (cast in the context of deepspeed FP16 as it wants half dtype as input) Parameters ---------- data: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] current data coming from the underlying __iter__ Returns ------- data: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] data moved to the correct device \"\"\" if isinstance(data, torch.Tensor): # TODO: Check if one of the APEX version needs a cast too? # Move to the correct cuda device w/ the correct type -- deepspeed FP16 requires a cast to half if fp16 if self._fp16 == \"deepspeed\": return data.to(device=\"cuda\", dtype=torch.half) else: return data.to(device=\"cuda\", dtype=data.dtype) elif isinstance(data, (list, tuple)): return type(data)(self._place_data_on_gpu(data=val) for val in data) elif isinstance(data, dict): return {k: self._place_data_on_gpu(v) for k, v in data.items()} elif ~(hasattr(data, \"to\")): return data else: raise TypeError( f\"Stoke -- Unsupported data type passed to _place_data_on_gpu \" f\"(torch.Tensor, tuple, list, dict), currently {type(data)}\" ) class BucketedDistributedSampler(Sampler[T_co]): \"\"\"Sampler that buckets samples by sorted_idx and then randomly samples from a specific bucket to prevent excess padding leading to wasted computation Borrowing heavily from the base DistributedSampler https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler Attributes ---------- num_replicas: int, default: None number of replicas rank: int, default: None current device rank epoch: int current training epoch drop_last: bool, default: False whether to drop last set of samples that don't fit into a batch shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators buckets: int number of buckets to break the dataset into sorted_n_samples: list sorted list of samples by the characteristic to bucket by (e.g. seq len) batch_size: int batch size that will be used (needed to make sure slices are correct) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch slice_size: int computed from batch size and number of replicas num_samples_per_bucket: int computed value that represents the number of samples in a single bucket num_slices_per_bucket: int computed value that represents the number of slices available in a bucket bucket_idx: list computed value that make a contiguous list of indices in each bucket rounded_num_samples_per_bucket: int computed value post round for number of samples in a single bucket rounded_num_samples_per_replica: int computed value post round for number of slices available in a bucket \"\"\" def __init__( self, dataset: Dataset, buckets: int, batch_size: int, sorted_idx: List, backend: DistributedOptions, allow_bucket_overlap: bool = False, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0, drop_last: bool = False, info_rank: int = 0, ) -> None: \"\"\"Init for BucketedDistributedSampler Parameters ---------- dataset: Dataset dataset from which to load the data. buckets: int number of buckets to break the dataset into batch_size: int batch size that will be used (needed to make sure slices are correct) sorted_idx: list sorted list of samples by the characteristic to bucket by (e.g. seq le backend: DistributedOptions which backend is being used (as rank, world size, etc. need to be used) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch num_replicas: int, default: None number of replicas rank: int, default: None current device rank shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators drop_last: bool, default: False whether to drop last set of samples that don't fit into a info_rank: int, default: 0 which device to print information on \"\"\" # If the backend isnt DDP there needs to be an additional import num_replicas, rank = self._conditional_distributed( backend=backend, num_replicas=num_replicas, rank=rank ) self.num_replicas = num_replicas self.rank = rank self.epoch = 0 self.drop_last = drop_last self.shuffle = shuffle self.seed = seed self.buckets = buckets self.sorted_n_samples = sorted_idx # Batch size is needed here so a contiguous iter of buckets can be formed self.batch_size = batch_size # This is a flag to batch up the dropped samples (that would be 'wasted') if drop_last is flagged self.allow_bucket_overlap = allow_bucket_overlap # Calculate the size of each slice that will be indexed across the replicas self.slice_size = self.batch_size * self.num_replicas # Calculate the size of the buckets (rounded or not based on drop last) self.num_samples_per_bucket = self._get_size( len(dataset), self.buckets, self.drop_last ) # Calculate the number of slices per bucket self.num_slices_per_bucket = self._get_size( self.num_samples_per_bucket, self.slice_size, self.drop_last ) if self.num_samples_per_bucket < self.slice_size: raise ValueError( f\"Stoke -- Resulting number of slices (batch * replicas) per bucket \" f\"({self.num_samples_per_bucket}) is less than the batch size \" f\"({self.batch_size})\" ) if self.num_slices_per_bucket < 2: raise ValueError( f\"Stoke -- Number of slices per bucket {self.num_slices_per_bucket} is less than 2 \" f\"which is not recommended\" ) if self.num_samples_per_bucket < 100: raise ValueError( f\"Stoke -- Number of samples per bucket {self.num_samples_per_bucket} is less than 100 \" f\"which is not recommended as this might lead to dropping of excessive data\" ) # Split into buckets and turn into lists self.bucket_idx = [ list(val) for val in np.array_split(self.sorted_n_samples, self.buckets) ] # Calculate the post rounded numbers self.rounded_num_samples_per_bucket = ( self.slice_size * self.num_slices_per_bucket ) self.rounded_num_samples_per_replica = ( self.num_slices_per_bucket * self.batch_size * self.buckets ) # Add the bucket overlap samples if self.allow_bucket_overlap: self.rounded_num_samples_per_replica += ( (len(dataset) - (self.rounded_num_samples_per_bucket * self.buckets)) // self.slice_size ) * self.batch_size if self.rank == info_rank: print( f\"Stoke -- BucketedDistributedSampler -- # Samples Per Bucket: \" f\"{self.rounded_num_samples_per_bucket}, # of Samples Per Replica: \" f\"{self.rounded_num_samples_per_replica}\" ) def _conditional_distributed( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\" Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" return self._check_backend(backend, num_replicas, rank) def _get_backend_functions(self, backend: DistributedOptions): \"\"\"Gets backend functions if needed Parameters ---------- backend: DistributedOptions which backend is being used Returns ------- Tuple[bool, int, int] is_init, num_replicas, rank \"\"\" if backend.value == \"ddp\" or backend.value == \"deepspeed\": return ( torch.distributed.is_initialized, torch.distributed.get_world_size, torch.distributed.get_rank, ) else: return hvd.is_initialized, hvd.size, hvd.rank def _check_backend( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\"Checks the backend for correct device info Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" if num_replicas is None or rank is None: is_avail, get_world_size, get_rank = self._get_backend_functions( backend=backend ) if num_replicas is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) num_replicas = get_world_size() if rank is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) rank = get_rank() return num_replicas, rank @staticmethod def _get_size(data_len: int, split_var: int, drop_last: bool = False): \"\"\"Gets the size of a split Parameters ---------- data_len: int current dataset length split_var: int how many to split into drop_last: bool, default: False drop last hanging samples if not batch_size Returns ------- num_samples: int \"\"\" if drop_last: num_samples = data_len // split_var else: num_samples = ceil(data_len / split_var) return num_samples def __iter__(self) -> Iterator[T_co]: \"\"\"Handles assembling the batches from a bucketed perspective Shuffle bucket order->Pad if necessary->Slice across replicas->Possibly batch up residuals->shuffle bucketed batches->Unroll into list->Make iter Returns ------- Iterator[T_co] \"\"\" # Shuffle the bucketed idx if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute each bucket indices = [ [val[idx] for idx in torch.randperm(len(val), generator=g).tolist()] for val in self.bucket_idx ] else: indices = self.bucket_idx # Iterate over the buckets for idx, val in enumerate(indices): # If this is true we need to handle padding if (self.num_slices_per_bucket * self.slice_size) > len(val): split_val = self._handle_padding(val) indices[idx] = list(itertools.chain(*split_val)) assert len(indices[idx]) == self.rounded_num_samples_per_bucket # Now slice across replicas final_indices = [] for val in indices: for idx in range(self.num_slices_per_bucket): replica_slice = val[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] final_indices.append(replica_slice) # If bucket overlap is allowed then we just batch up the residual indices if self.drop_last and self.allow_bucket_overlap: residual_idx = list( itertools.chain( *[val[self.rounded_num_samples_per_bucket :] for val in indices] ) ) if len(residual_idx) > self.slice_size: # Cut by slices then by replicas residual_idx = [ residual_idx[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] for idx in range(len(residual_idx) // self.slice_size) ] # Append to the final indices final_indices.extend(residual_idx) # Shuffle the bucketed batches if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute the bucket order final_indices = [ final_indices[val] for val in torch.randperm(len(final_indices), generator=g) ] # Unroll into a single list final_indices = list(itertools.chain(*final_indices)) assert len(final_indices) == self.rounded_num_samples_per_replica return iter(final_indices) def _handle_padding(self, idx_list: List): \"\"\"Handles padding out if a batch is short Parameters ---------- idx_list: List list of indices Returns ------- split_val: List list with correctly padded sizes \"\"\" split_val = [] for idx in range(self.num_slices_per_bucket): if idx == (self.num_slices_per_bucket - 1): # Get the short batch short_batch = idx_list[(idx * self.slice_size) :] # Short batch replica slice sizes short_len = [ self.batch_size - len(list(val)) for val in np.array_split(short_batch, self.num_replicas) ] # Pop the necessary values from the entire bucket pad_values = [ idx_list[s_idx : (self.num_replicas * s_len) : self.num_replicas] for s_idx, s_len in enumerate(short_len) ] # If not a consistent list then we need to reorder so that the step size alignment slicing # of the replicas works if len(set(short_len)) != 1: # here we need to find the first larger idx and reorder first_idx = short_len.index(max(set(short_len))) # Reorder pad_values = pad_values[first_idx:] + pad_values[0:first_idx] extended_batch = short_batch + [ pad for pad in list( itertools.chain(*itertools.zip_longest(*pad_values)) ) if pad is not None ] split_val.append(extended_batch) else: split_val.append( idx_list[(idx * self.slice_size) : ((idx + 1) * self.slice_size)] ) return split_val def __len__(self) -> int: return self.rounded_num_samples_per_replica def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch Classes BucketedDistributedSampler class BucketedDistributedSampler ( dataset : torch . utils . data . dataset . Dataset , buckets : int , batch_size : int , sorted_idx : List , backend : stoke . status . DistributedOptions , allow_bucket_overlap : bool = False , num_replicas : Union [ int , NoneType ] = None , rank : Union [ int , NoneType ] = None , shuffle : bool = True , seed : int = 0 , drop_last : bool = False , info_rank : int = 0 ) Attributes Name Type Description Default num_replicas int, default: None number of replicas None rank int, default: None current device rank None epoch int current training epoch None drop_last bool, default: False whether to drop last set of samples that don't fit into a batch None shuffle bool, default: True flag to shuffle dataset None seed int, default: 0 seed to use for generators None buckets int number of buckets to break the dataset into None sorted_n_samples list sorted list of samples by the characteristic to bucket by (e.g. seq len) None batch_size int batch size that will be used (needed to make sure slices are correct) None allow_bucket_overlap bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch None slice_size int computed from batch size and number of replicas None num_samples_per_bucket int computed value that represents the number of samples in a single bucket None num_slices_per_bucket int computed value that represents the number of slices available in a bucket None bucket_idx list computed value that make a contiguous list of indices in each bucket None rounded_num_samples_per_bucket int computed value post round for number of samples in a single bucket None rounded_num_samples_per_replica int computed value post round for number of slices available in a bucket None ??? example \"View Source\" class BucketedDistributedSampler(Sampler[T_co]): \"\"\"Sampler that buckets samples by sorted_idx and then randomly samples from a specific bucket to prevent excess padding leading to wasted computation Borrowing heavily from the base DistributedSampler https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler Attributes ---------- num_replicas: int, default: None number of replicas rank: int, default: None current device rank epoch: int current training epoch drop_last: bool, default: False whether to drop last set of samples that don't fit into a batch shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators buckets: int number of buckets to break the dataset into sorted_n_samples: list sorted list of samples by the characteristic to bucket by (e.g. seq len) batch_size: int batch size that will be used (needed to make sure slices are correct) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch slice_size: int computed from batch size and number of replicas num_samples_per_bucket: int computed value that represents the number of samples in a single bucket num_slices_per_bucket: int computed value that represents the number of slices available in a bucket bucket_idx: list computed value that make a contiguous list of indices in each bucket rounded_num_samples_per_bucket: int computed value post round for number of samples in a single bucket rounded_num_samples_per_replica: int computed value post round for number of slices available in a bucket \"\"\" def __init__( self, dataset: Dataset, buckets: int, batch_size: int, sorted_idx: List, backend: DistributedOptions, allow_bucket_overlap: bool = False, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0, drop_last: bool = False, info_rank: int = 0, ) -> None: \"\"\"Init for BucketedDistributedSampler Parameters ---------- dataset: Dataset dataset from which to load the data. buckets: int number of buckets to break the dataset into batch_size: int batch size that will be used (needed to make sure slices are correct) sorted_idx: list sorted list of samples by the characteristic to bucket by (e.g. seq le backend: DistributedOptions which backend is being used (as rank, world size, etc. need to be used) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch num_replicas: int, default: None number of replicas rank: int, default: None current device rank shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators drop_last: bool, default: False whether to drop last set of samples that don't fit into a info_rank: int, default: 0 which device to print information on \"\"\" # If the backend isnt DDP there needs to be an additional import num_replicas, rank = self._conditional_distributed( backend=backend, num_replicas=num_replicas, rank=rank ) self.num_replicas = num_replicas self.rank = rank self.epoch = 0 self.drop_last = drop_last self.shuffle = shuffle self.seed = seed self.buckets = buckets self.sorted_n_samples = sorted_idx # Batch size is needed here so a contiguous iter of buckets can be formed self.batch_size = batch_size # This is a flag to batch up the dropped samples (that would be 'wasted') if drop_last is flagged self.allow_bucket_overlap = allow_bucket_overlap # Calculate the size of each slice that will be indexed across the replicas self.slice_size = self.batch_size * self.num_replicas # Calculate the size of the buckets (rounded or not based on drop last) self.num_samples_per_bucket = self._get_size( len(dataset), self.buckets, self.drop_last ) # Calculate the number of slices per bucket self.num_slices_per_bucket = self._get_size( self.num_samples_per_bucket, self.slice_size, self.drop_last ) if self.num_samples_per_bucket < self.slice_size: raise ValueError( f\"Stoke -- Resulting number of slices (batch * replicas) per bucket \" f\"({self.num_samples_per_bucket}) is less than the batch size \" f\"({self.batch_size})\" ) if self.num_slices_per_bucket < 2: raise ValueError( f\"Stoke -- Number of slices per bucket {self.num_slices_per_bucket} is less than 2 \" f\"which is not recommended\" ) if self.num_samples_per_bucket < 100: raise ValueError( f\"Stoke -- Number of samples per bucket {self.num_samples_per_bucket} is less than 100 \" f\"which is not recommended as this might lead to dropping of excessive data\" ) # Split into buckets and turn into lists self.bucket_idx = [ list(val) for val in np.array_split(self.sorted_n_samples, self.buckets) ] # Calculate the post rounded numbers self.rounded_num_samples_per_bucket = ( self.slice_size * self.num_slices_per_bucket ) self.rounded_num_samples_per_replica = ( self.num_slices_per_bucket * self.batch_size * self.buckets ) # Add the bucket overlap samples if self.allow_bucket_overlap: self.rounded_num_samples_per_replica += ( (len(dataset) - (self.rounded_num_samples_per_bucket * self.buckets)) // self.slice_size ) * self.batch_size if self.rank == info_rank: print( f\"Stoke -- BucketedDistributedSampler -- # Samples Per Bucket: \" f\"{self.rounded_num_samples_per_bucket}, # of Samples Per Replica: \" f\"{self.rounded_num_samples_per_replica}\" ) def _conditional_distributed( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\" Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" return self._check_backend(backend, num_replicas, rank) def _get_backend_functions(self, backend: DistributedOptions): \"\"\"Gets backend functions if needed Parameters ---------- backend: DistributedOptions which backend is being used Returns ------- Tuple[bool, int, int] is_init, num_replicas, rank \"\"\" if backend.value == \"ddp\" or backend.value == \"deepspeed\": return ( torch.distributed.is_initialized, torch.distributed.get_world_size, torch.distributed.get_rank, ) else: return hvd.is_initialized, hvd.size, hvd.rank def _check_backend( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\"Checks the backend for correct device info Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" if num_replicas is None or rank is None: is_avail, get_world_size, get_rank = self._get_backend_functions( backend=backend ) if num_replicas is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) num_replicas = get_world_size() if rank is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) rank = get_rank() return num_replicas, rank @staticmethod def _get_size(data_len: int, split_var: int, drop_last: bool = False): \"\"\"Gets the size of a split Parameters ---------- data_len: int current dataset length split_var: int how many to split into drop_last: bool, default: False drop last hanging samples if not batch_size Returns ------- num_samples: int \"\"\" if drop_last: num_samples = data_len // split_var else: num_samples = ceil(data_len / split_var) return num_samples def __iter__(self) -> Iterator[T_co]: \"\"\"Handles assembling the batches from a bucketed perspective Shuffle bucket order->Pad if necessary->Slice across replicas->Possibly batch up residuals->shuffle bucketed batches->Unroll into list->Make iter Returns ------- Iterator[T_co] \"\"\" # Shuffle the bucketed idx if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute each bucket indices = [ [val[idx] for idx in torch.randperm(len(val), generator=g).tolist()] for val in self.bucket_idx ] else: indices = self.bucket_idx # Iterate over the buckets for idx, val in enumerate(indices): # If this is true we need to handle padding if (self.num_slices_per_bucket * self.slice_size) > len(val): split_val = self._handle_padding(val) indices[idx] = list(itertools.chain(*split_val)) assert len(indices[idx]) == self.rounded_num_samples_per_bucket # Now slice across replicas final_indices = [] for val in indices: for idx in range(self.num_slices_per_bucket): replica_slice = val[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] final_indices.append(replica_slice) # If bucket overlap is allowed then we just batch up the residual indices if self.drop_last and self.allow_bucket_overlap: residual_idx = list( itertools.chain( *[val[self.rounded_num_samples_per_bucket :] for val in indices] ) ) if len(residual_idx) > self.slice_size: # Cut by slices then by replicas residual_idx = [ residual_idx[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] for idx in range(len(residual_idx) // self.slice_size) ] # Append to the final indices final_indices.extend(residual_idx) # Shuffle the bucketed batches if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute the bucket order final_indices = [ final_indices[val] for val in torch.randperm(len(final_indices), generator=g) ] # Unroll into a single list final_indices = list(itertools.chain(*final_indices)) assert len(final_indices) == self.rounded_num_samples_per_replica return iter(final_indices) def _handle_padding(self, idx_list: List): \"\"\"Handles padding out if a batch is short Parameters ---------- idx_list: List list of indices Returns ------- split_val: List list with correctly padded sizes \"\"\" split_val = [] for idx in range(self.num_slices_per_bucket): if idx == (self.num_slices_per_bucket - 1): # Get the short batch short_batch = idx_list[(idx * self.slice_size) :] # Short batch replica slice sizes short_len = [ self.batch_size - len(list(val)) for val in np.array_split(short_batch, self.num_replicas) ] # Pop the necessary values from the entire bucket pad_values = [ idx_list[s_idx : (self.num_replicas * s_len) : self.num_replicas] for s_idx, s_len in enumerate(short_len) ] # If not a consistent list then we need to reorder so that the step size alignment slicing # of the replicas works if len(set(short_len)) != 1: # here we need to find the first larger idx and reorder first_idx = short_len.index(max(set(short_len))) # Reorder pad_values = pad_values[first_idx:] + pad_values[0:first_idx] extended_batch = short_batch + [ pad for pad in list( itertools.chain(*itertools.zip_longest(*pad_values)) ) if pad is not None ] split_val.append(extended_batch) else: split_val.append( idx_list[(idx * self.slice_size) : ((idx + 1) * self.slice_size)] ) return split_val def __len__(self) -> int: return self.rounded_num_samples_per_replica def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch Ancestors (in MRO) torch.utils.data.sampler.Sampler typing.Generic Methods set_epoch def set_epoch ( self , epoch : int ) -> None Sets the epoch for this sampler. When :attr: shuffle=True , this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters: Name Type Description Default epoch int Epoch number None ??? example \"View Source\" def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch StokeDataLoader class StokeDataLoader ( gpu : bool , fp16 : Union [ stoke . status . FP16Options , NoneType ], dataset : torch . utils . data . dataset . Dataset [ + T_co ], batch_size : Union [ int , NoneType ] = 1 , shuffle : bool = False , sampler : Union [ torch . utils . data . sampler . Sampler [ int ], NoneType ] = None , batch_sampler : Union [ torch . utils . data . sampler . Sampler [ Sequence [ int ]], NoneType ] = None , num_workers : int = 0 , collate_fn : Callable [[ List [ ~ T ]], Any ] = None , pin_memory : bool = False , drop_last : bool = False , timeout : float = 0 , worker_init_fn : Callable [[ int ], NoneType ] = None , multiprocessing_context = None , generator = None , * , prefetch_factor : int = 2 , persistent_workers : bool = False ) Attributes Name Type Description Default _gpu bool None None _fp16 Optional[FP16Options] None None ??? example \"View Source\" class StokeDataLoader(DL): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs Attributes ---------- _gpu: bool _fp16: Optional[FP16Options] See Also -------- torch.utils.data.DataLoader: base DataLoader class that this inherits from (check for all attributes) \"\"\" def __init__( self, gpu: bool, fp16: Optional[FP16Options], dataset: Dataset[T_co], batch_size: Optional[int] = 1, shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Maps to torch.utils.data.DataLoader __init__ Shim is necessary to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. batch_size: int, default: 1 how many samples per batch to load . shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Call super init for the actual torch DataLoader super(StokeDataLoader, self).__init__( dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) self._gpu = gpu self._fp16 = fp16 def __iter__(self): \"\"\"Underlying iter of the DataLoader that yields samples Wrap the base __iter__ with a call to place on the device if flagged Yields ------ Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] data placed on the correct device \"\"\" # Iterate using the base class iter but override the yield by pushing to device prior if gpu flag is true for val in super().__iter__(): yield val if not self._gpu else self._place_data_on_gpu(val) def _place_data_on_gpu( self, data: Union[ torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor], ], ): \"\"\"Determine data structure and then place on the correct device (cast in the context of deepspeed FP16 as it wants half dtype as input) Parameters ---------- data: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] current data coming from the underlying __iter__ Returns ------- data: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] data moved to the correct device \"\"\" if isinstance(data, torch.Tensor): # TODO: Check if one of the APEX version needs a cast too? # Move to the correct cuda device w/ the correct type -- deepspeed FP16 requires a cast to half if fp16 if self._fp16 == \"deepspeed\": return data.to(device=\"cuda\", dtype=torch.half) else: return data.to(device=\"cuda\", dtype=data.dtype) elif isinstance(data, (list, tuple)): return type(data)(self._place_data_on_gpu(data=val) for val in data) elif isinstance(data, dict): return {k: self._place_data_on_gpu(v) for k, v in data.items()} elif ~(hasattr(data, \"to\")): return data else: raise TypeError( f\"Stoke -- Unsupported data type passed to _place_data_on_gpu \" f\"(torch.Tensor, tuple, list, dict), currently {type(data)}\" ) Ancestors (in MRO) torch.utils.data.dataloader.DataLoader typing.Generic Instance variables multiprocessing_context Methods check_worker_number_rationality def check_worker_number_rationality ( self ) ??? example \"View Source\" def check_worker_number_rationality(self): # This function check whether the dataloader's worker number is rational based on # current system's resource. Current rule is that if the number of workers this # Dataloader will create is bigger than the number of logical cpus that is allowed to # use, than we will pop up a warning to let user pay attention. # # eg. If current system has 2 physical CPUs with 16 cores each. And each core support 2 # threads, then the total logical cpus here is 2 * 16 * 2 = 64. Let's say current # DataLoader process can use half of them which is 32, then the rational max number of # worker that initiated from this process is 32. # Now, let's say the created DataLoader has num_works = 40, which is bigger than 32. # So the warning message is triggered to notify the user to lower the worker number if # necessary. # # # [Note] Please note that this function repects `cpuset` only when os.sched_getaffinity is # available (available in most of Linux system, but not OSX and Windows). # When os.sched_getaffinity is not available, os.cpu_count() is called instead, but # it doesn't repect cpuset. # We don't take threading into account since each worker process is single threaded # at this time. # # We don't set any threading flags (eg. OMP_NUM_THREADS, MKL_NUM_THREADS, etc) # other than `torch.set_num_threads` to 1 in the worker process, if the passing # in functions use 3rd party modules that rely on those threading flags to determine # how many thread to create (eg. numpy, etc), then it is caller's responsibility to # set those flags correctly. def _create_warning_msg(num_worker_suggest, num_worker_created, cpuset_checked): suggested_max_worker_msg = (( \"Our suggested max number of worker in current system is {}{}, which is smaller \" \"than what this DataLoader is going to create.\").format( num_worker_suggest, (\"\" if cpuset_checked else \" (`cpuset` is not taken into account)\")) ) if num_worker_suggest is not None else ( \"DataLoader is not able to compute a suggested max number of worker in current system.\") warn_msg = ( \"This DataLoader will create {} worker processes in total. {} \" \"Please be aware that excessive worker creation might get DataLoader running slow or even freeze, \" \"lower the worker number to avoid potential slowness/freeze if necessary.\").format( num_worker_created, suggested_max_worker_msg) return warn_msg if not self.num_workers or self.num_workers == 0: return # try to compute a suggested max number of worker based on system's resource max_num_worker_suggest = None cpuset_checked = False if hasattr(os, 'sched_getaffinity'): try: max_num_worker_suggest = len(os.sched_getaffinity(0)) cpuset_checked = True except Exception: pass if max_num_worker_suggest is None: # os.cpu_count() could return Optional[int] # get cpu count first and check None in order to satify mypy check cpu_count = os.cpu_count() if cpu_count is not None: max_num_worker_suggest = cpu_count if max_num_worker_suggest is None: warnings.warn(_create_warning_msg( max_num_worker_suggest, self.num_workers, cpuset_checked)) return if self.num_workers > max_num_worker_suggest: warnings.warn(_create_warning_msg( max_num_worker_suggest, self.num_workers, cpuset_checked))","title":"Data"},{"location":"reference/stoke/data/#module-stokedata","text":"Handles any data (e.g. loader, sampler, etc.) related classes None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles any data (e.g. loader, sampler, etc.) related classes\"\"\" import itertools from math import ceil from typing import Dict, Iterator, List, Optional, Sequence, Tuple, Union import horovod.torch as hvd import numpy as np import torch import torch.distributed as dist from torch.utils.data import DataLoader as DL from torch.utils.data import Dataset from torch.utils.data.distributed import Sampler from stoke.status import DistributedOptions, FP16Options from stoke.utils import T_co, _collate_fn_t, _worker_init_fn_t class StokeDataLoader(DL): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs Attributes ---------- _gpu: bool _fp16: Optional[FP16Options] See Also -------- torch.utils.data.DataLoader: base DataLoader class that this inherits from (check for all attributes) \"\"\" def __init__( self, gpu: bool, fp16: Optional[FP16Options], dataset: Dataset[T_co], batch_size: Optional[int] = 1, shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Maps to torch.utils.data.DataLoader __init__ Shim is necessary to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. batch_size: int, default: 1 how many samples per batch to load . shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Call super init for the actual torch DataLoader super(StokeDataLoader, self).__init__( dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) self._gpu = gpu self._fp16 = fp16 def __iter__(self): \"\"\"Underlying iter of the DataLoader that yields samples Wrap the base __iter__ with a call to place on the device if flagged Yields ------ Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] data placed on the correct device \"\"\" # Iterate using the base class iter but override the yield by pushing to device prior if gpu flag is true for val in super().__iter__(): yield val if not self._gpu else self._place_data_on_gpu(val) def _place_data_on_gpu( self, data: Union[ torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor], ], ): \"\"\"Determine data structure and then place on the correct device (cast in the context of deepspeed FP16 as it wants half dtype as input) Parameters ---------- data: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] current data coming from the underlying __iter__ Returns ------- data: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] data moved to the correct device \"\"\" if isinstance(data, torch.Tensor): # TODO: Check if one of the APEX version needs a cast too? # Move to the correct cuda device w/ the correct type -- deepspeed FP16 requires a cast to half if fp16 if self._fp16 == \"deepspeed\": return data.to(device=\"cuda\", dtype=torch.half) else: return data.to(device=\"cuda\", dtype=data.dtype) elif isinstance(data, (list, tuple)): return type(data)(self._place_data_on_gpu(data=val) for val in data) elif isinstance(data, dict): return {k: self._place_data_on_gpu(v) for k, v in data.items()} elif ~(hasattr(data, \"to\")): return data else: raise TypeError( f\"Stoke -- Unsupported data type passed to _place_data_on_gpu \" f\"(torch.Tensor, tuple, list, dict), currently {type(data)}\" ) class BucketedDistributedSampler(Sampler[T_co]): \"\"\"Sampler that buckets samples by sorted_idx and then randomly samples from a specific bucket to prevent excess padding leading to wasted computation Borrowing heavily from the base DistributedSampler https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler Attributes ---------- num_replicas: int, default: None number of replicas rank: int, default: None current device rank epoch: int current training epoch drop_last: bool, default: False whether to drop last set of samples that don't fit into a batch shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators buckets: int number of buckets to break the dataset into sorted_n_samples: list sorted list of samples by the characteristic to bucket by (e.g. seq len) batch_size: int batch size that will be used (needed to make sure slices are correct) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch slice_size: int computed from batch size and number of replicas num_samples_per_bucket: int computed value that represents the number of samples in a single bucket num_slices_per_bucket: int computed value that represents the number of slices available in a bucket bucket_idx: list computed value that make a contiguous list of indices in each bucket rounded_num_samples_per_bucket: int computed value post round for number of samples in a single bucket rounded_num_samples_per_replica: int computed value post round for number of slices available in a bucket \"\"\" def __init__( self, dataset: Dataset, buckets: int, batch_size: int, sorted_idx: List, backend: DistributedOptions, allow_bucket_overlap: bool = False, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0, drop_last: bool = False, info_rank: int = 0, ) -> None: \"\"\"Init for BucketedDistributedSampler Parameters ---------- dataset: Dataset dataset from which to load the data. buckets: int number of buckets to break the dataset into batch_size: int batch size that will be used (needed to make sure slices are correct) sorted_idx: list sorted list of samples by the characteristic to bucket by (e.g. seq le backend: DistributedOptions which backend is being used (as rank, world size, etc. need to be used) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch num_replicas: int, default: None number of replicas rank: int, default: None current device rank shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators drop_last: bool, default: False whether to drop last set of samples that don't fit into a info_rank: int, default: 0 which device to print information on \"\"\" # If the backend isnt DDP there needs to be an additional import num_replicas, rank = self._conditional_distributed( backend=backend, num_replicas=num_replicas, rank=rank ) self.num_replicas = num_replicas self.rank = rank self.epoch = 0 self.drop_last = drop_last self.shuffle = shuffle self.seed = seed self.buckets = buckets self.sorted_n_samples = sorted_idx # Batch size is needed here so a contiguous iter of buckets can be formed self.batch_size = batch_size # This is a flag to batch up the dropped samples (that would be 'wasted') if drop_last is flagged self.allow_bucket_overlap = allow_bucket_overlap # Calculate the size of each slice that will be indexed across the replicas self.slice_size = self.batch_size * self.num_replicas # Calculate the size of the buckets (rounded or not based on drop last) self.num_samples_per_bucket = self._get_size( len(dataset), self.buckets, self.drop_last ) # Calculate the number of slices per bucket self.num_slices_per_bucket = self._get_size( self.num_samples_per_bucket, self.slice_size, self.drop_last ) if self.num_samples_per_bucket < self.slice_size: raise ValueError( f\"Stoke -- Resulting number of slices (batch * replicas) per bucket \" f\"({self.num_samples_per_bucket}) is less than the batch size \" f\"({self.batch_size})\" ) if self.num_slices_per_bucket < 2: raise ValueError( f\"Stoke -- Number of slices per bucket {self.num_slices_per_bucket} is less than 2 \" f\"which is not recommended\" ) if self.num_samples_per_bucket < 100: raise ValueError( f\"Stoke -- Number of samples per bucket {self.num_samples_per_bucket} is less than 100 \" f\"which is not recommended as this might lead to dropping of excessive data\" ) # Split into buckets and turn into lists self.bucket_idx = [ list(val) for val in np.array_split(self.sorted_n_samples, self.buckets) ] # Calculate the post rounded numbers self.rounded_num_samples_per_bucket = ( self.slice_size * self.num_slices_per_bucket ) self.rounded_num_samples_per_replica = ( self.num_slices_per_bucket * self.batch_size * self.buckets ) # Add the bucket overlap samples if self.allow_bucket_overlap: self.rounded_num_samples_per_replica += ( (len(dataset) - (self.rounded_num_samples_per_bucket * self.buckets)) // self.slice_size ) * self.batch_size if self.rank == info_rank: print( f\"Stoke -- BucketedDistributedSampler -- # Samples Per Bucket: \" f\"{self.rounded_num_samples_per_bucket}, # of Samples Per Replica: \" f\"{self.rounded_num_samples_per_replica}\" ) def _conditional_distributed( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\" Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" return self._check_backend(backend, num_replicas, rank) def _get_backend_functions(self, backend: DistributedOptions): \"\"\"Gets backend functions if needed Parameters ---------- backend: DistributedOptions which backend is being used Returns ------- Tuple[bool, int, int] is_init, num_replicas, rank \"\"\" if backend.value == \"ddp\" or backend.value == \"deepspeed\": return ( torch.distributed.is_initialized, torch.distributed.get_world_size, torch.distributed.get_rank, ) else: return hvd.is_initialized, hvd.size, hvd.rank def _check_backend( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\"Checks the backend for correct device info Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" if num_replicas is None or rank is None: is_avail, get_world_size, get_rank = self._get_backend_functions( backend=backend ) if num_replicas is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) num_replicas = get_world_size() if rank is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) rank = get_rank() return num_replicas, rank @staticmethod def _get_size(data_len: int, split_var: int, drop_last: bool = False): \"\"\"Gets the size of a split Parameters ---------- data_len: int current dataset length split_var: int how many to split into drop_last: bool, default: False drop last hanging samples if not batch_size Returns ------- num_samples: int \"\"\" if drop_last: num_samples = data_len // split_var else: num_samples = ceil(data_len / split_var) return num_samples def __iter__(self) -> Iterator[T_co]: \"\"\"Handles assembling the batches from a bucketed perspective Shuffle bucket order->Pad if necessary->Slice across replicas->Possibly batch up residuals->shuffle bucketed batches->Unroll into list->Make iter Returns ------- Iterator[T_co] \"\"\" # Shuffle the bucketed idx if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute each bucket indices = [ [val[idx] for idx in torch.randperm(len(val), generator=g).tolist()] for val in self.bucket_idx ] else: indices = self.bucket_idx # Iterate over the buckets for idx, val in enumerate(indices): # If this is true we need to handle padding if (self.num_slices_per_bucket * self.slice_size) > len(val): split_val = self._handle_padding(val) indices[idx] = list(itertools.chain(*split_val)) assert len(indices[idx]) == self.rounded_num_samples_per_bucket # Now slice across replicas final_indices = [] for val in indices: for idx in range(self.num_slices_per_bucket): replica_slice = val[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] final_indices.append(replica_slice) # If bucket overlap is allowed then we just batch up the residual indices if self.drop_last and self.allow_bucket_overlap: residual_idx = list( itertools.chain( *[val[self.rounded_num_samples_per_bucket :] for val in indices] ) ) if len(residual_idx) > self.slice_size: # Cut by slices then by replicas residual_idx = [ residual_idx[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] for idx in range(len(residual_idx) // self.slice_size) ] # Append to the final indices final_indices.extend(residual_idx) # Shuffle the bucketed batches if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute the bucket order final_indices = [ final_indices[val] for val in torch.randperm(len(final_indices), generator=g) ] # Unroll into a single list final_indices = list(itertools.chain(*final_indices)) assert len(final_indices) == self.rounded_num_samples_per_replica return iter(final_indices) def _handle_padding(self, idx_list: List): \"\"\"Handles padding out if a batch is short Parameters ---------- idx_list: List list of indices Returns ------- split_val: List list with correctly padded sizes \"\"\" split_val = [] for idx in range(self.num_slices_per_bucket): if idx == (self.num_slices_per_bucket - 1): # Get the short batch short_batch = idx_list[(idx * self.slice_size) :] # Short batch replica slice sizes short_len = [ self.batch_size - len(list(val)) for val in np.array_split(short_batch, self.num_replicas) ] # Pop the necessary values from the entire bucket pad_values = [ idx_list[s_idx : (self.num_replicas * s_len) : self.num_replicas] for s_idx, s_len in enumerate(short_len) ] # If not a consistent list then we need to reorder so that the step size alignment slicing # of the replicas works if len(set(short_len)) != 1: # here we need to find the first larger idx and reorder first_idx = short_len.index(max(set(short_len))) # Reorder pad_values = pad_values[first_idx:] + pad_values[0:first_idx] extended_batch = short_batch + [ pad for pad in list( itertools.chain(*itertools.zip_longest(*pad_values)) ) if pad is not None ] split_val.append(extended_batch) else: split_val.append( idx_list[(idx * self.slice_size) : ((idx + 1) * self.slice_size)] ) return split_val def __len__(self) -> int: return self.rounded_num_samples_per_replica def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch","title":"Module stoke.data"},{"location":"reference/stoke/data/#classes","text":"","title":"Classes"},{"location":"reference/stoke/data/#bucketeddistributedsampler","text":"class BucketedDistributedSampler ( dataset : torch . utils . data . dataset . Dataset , buckets : int , batch_size : int , sorted_idx : List , backend : stoke . status . DistributedOptions , allow_bucket_overlap : bool = False , num_replicas : Union [ int , NoneType ] = None , rank : Union [ int , NoneType ] = None , shuffle : bool = True , seed : int = 0 , drop_last : bool = False , info_rank : int = 0 )","title":"BucketedDistributedSampler"},{"location":"reference/stoke/data/#attributes","text":"Name Type Description Default num_replicas int, default: None number of replicas None rank int, default: None current device rank None epoch int current training epoch None drop_last bool, default: False whether to drop last set of samples that don't fit into a batch None shuffle bool, default: True flag to shuffle dataset None seed int, default: 0 seed to use for generators None buckets int number of buckets to break the dataset into None sorted_n_samples list sorted list of samples by the characteristic to bucket by (e.g. seq len) None batch_size int batch size that will be used (needed to make sure slices are correct) None allow_bucket_overlap bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch None slice_size int computed from batch size and number of replicas None num_samples_per_bucket int computed value that represents the number of samples in a single bucket None num_slices_per_bucket int computed value that represents the number of slices available in a bucket None bucket_idx list computed value that make a contiguous list of indices in each bucket None rounded_num_samples_per_bucket int computed value post round for number of samples in a single bucket None rounded_num_samples_per_replica int computed value post round for number of slices available in a bucket None ??? example \"View Source\" class BucketedDistributedSampler(Sampler[T_co]): \"\"\"Sampler that buckets samples by sorted_idx and then randomly samples from a specific bucket to prevent excess padding leading to wasted computation Borrowing heavily from the base DistributedSampler https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler Attributes ---------- num_replicas: int, default: None number of replicas rank: int, default: None current device rank epoch: int current training epoch drop_last: bool, default: False whether to drop last set of samples that don't fit into a batch shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators buckets: int number of buckets to break the dataset into sorted_n_samples: list sorted list of samples by the characteristic to bucket by (e.g. seq len) batch_size: int batch size that will be used (needed to make sure slices are correct) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch slice_size: int computed from batch size and number of replicas num_samples_per_bucket: int computed value that represents the number of samples in a single bucket num_slices_per_bucket: int computed value that represents the number of slices available in a bucket bucket_idx: list computed value that make a contiguous list of indices in each bucket rounded_num_samples_per_bucket: int computed value post round for number of samples in a single bucket rounded_num_samples_per_replica: int computed value post round for number of slices available in a bucket \"\"\" def __init__( self, dataset: Dataset, buckets: int, batch_size: int, sorted_idx: List, backend: DistributedOptions, allow_bucket_overlap: bool = False, num_replicas: Optional[int] = None, rank: Optional[int] = None, shuffle: bool = True, seed: int = 0, drop_last: bool = False, info_rank: int = 0, ) -> None: \"\"\"Init for BucketedDistributedSampler Parameters ---------- dataset: Dataset dataset from which to load the data. buckets: int number of buckets to break the dataset into batch_size: int batch size that will be used (needed to make sure slices are correct) sorted_idx: list sorted list of samples by the characteristic to bucket by (e.g. seq le backend: DistributedOptions which backend is being used (as rank, world size, etc. need to be used) allow_bucket_overlap: bool, default: False allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into an un-bucketed batch num_replicas: int, default: None number of replicas rank: int, default: None current device rank shuffle: bool, default: True flag to shuffle dataset seed: int, default: 0 seed to use for generators drop_last: bool, default: False whether to drop last set of samples that don't fit into a info_rank: int, default: 0 which device to print information on \"\"\" # If the backend isnt DDP there needs to be an additional import num_replicas, rank = self._conditional_distributed( backend=backend, num_replicas=num_replicas, rank=rank ) self.num_replicas = num_replicas self.rank = rank self.epoch = 0 self.drop_last = drop_last self.shuffle = shuffle self.seed = seed self.buckets = buckets self.sorted_n_samples = sorted_idx # Batch size is needed here so a contiguous iter of buckets can be formed self.batch_size = batch_size # This is a flag to batch up the dropped samples (that would be 'wasted') if drop_last is flagged self.allow_bucket_overlap = allow_bucket_overlap # Calculate the size of each slice that will be indexed across the replicas self.slice_size = self.batch_size * self.num_replicas # Calculate the size of the buckets (rounded or not based on drop last) self.num_samples_per_bucket = self._get_size( len(dataset), self.buckets, self.drop_last ) # Calculate the number of slices per bucket self.num_slices_per_bucket = self._get_size( self.num_samples_per_bucket, self.slice_size, self.drop_last ) if self.num_samples_per_bucket < self.slice_size: raise ValueError( f\"Stoke -- Resulting number of slices (batch * replicas) per bucket \" f\"({self.num_samples_per_bucket}) is less than the batch size \" f\"({self.batch_size})\" ) if self.num_slices_per_bucket < 2: raise ValueError( f\"Stoke -- Number of slices per bucket {self.num_slices_per_bucket} is less than 2 \" f\"which is not recommended\" ) if self.num_samples_per_bucket < 100: raise ValueError( f\"Stoke -- Number of samples per bucket {self.num_samples_per_bucket} is less than 100 \" f\"which is not recommended as this might lead to dropping of excessive data\" ) # Split into buckets and turn into lists self.bucket_idx = [ list(val) for val in np.array_split(self.sorted_n_samples, self.buckets) ] # Calculate the post rounded numbers self.rounded_num_samples_per_bucket = ( self.slice_size * self.num_slices_per_bucket ) self.rounded_num_samples_per_replica = ( self.num_slices_per_bucket * self.batch_size * self.buckets ) # Add the bucket overlap samples if self.allow_bucket_overlap: self.rounded_num_samples_per_replica += ( (len(dataset) - (self.rounded_num_samples_per_bucket * self.buckets)) // self.slice_size ) * self.batch_size if self.rank == info_rank: print( f\"Stoke -- BucketedDistributedSampler -- # Samples Per Bucket: \" f\"{self.rounded_num_samples_per_bucket}, # of Samples Per Replica: \" f\"{self.rounded_num_samples_per_replica}\" ) def _conditional_distributed( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\" Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" return self._check_backend(backend, num_replicas, rank) def _get_backend_functions(self, backend: DistributedOptions): \"\"\"Gets backend functions if needed Parameters ---------- backend: DistributedOptions which backend is being used Returns ------- Tuple[bool, int, int] is_init, num_replicas, rank \"\"\" if backend.value == \"ddp\" or backend.value == \"deepspeed\": return ( torch.distributed.is_initialized, torch.distributed.get_world_size, torch.distributed.get_rank, ) else: return hvd.is_initialized, hvd.size, hvd.rank def _check_backend( self, backend: DistributedOptions, num_replicas: Optional[int], rank: Optional[int], ): \"\"\"Checks the backend for correct device info Parameters ---------- backend: DistributedOptions which backend is being used num_replicas: int, default: None total number of replicas rank: int, default: None current device rank Returns ------- Tuple[int, int] num_replicas, rank \"\"\" if num_replicas is None or rank is None: is_avail, get_world_size, get_rank = self._get_backend_functions( backend=backend ) if num_replicas is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) num_replicas = get_world_size() if rank is None: if not is_avail(): raise RuntimeError( \"Requires distributed package (torch.dist or hvd) to be available\" ) rank = get_rank() return num_replicas, rank @staticmethod def _get_size(data_len: int, split_var: int, drop_last: bool = False): \"\"\"Gets the size of a split Parameters ---------- data_len: int current dataset length split_var: int how many to split into drop_last: bool, default: False drop last hanging samples if not batch_size Returns ------- num_samples: int \"\"\" if drop_last: num_samples = data_len // split_var else: num_samples = ceil(data_len / split_var) return num_samples def __iter__(self) -> Iterator[T_co]: \"\"\"Handles assembling the batches from a bucketed perspective Shuffle bucket order->Pad if necessary->Slice across replicas->Possibly batch up residuals->shuffle bucketed batches->Unroll into list->Make iter Returns ------- Iterator[T_co] \"\"\" # Shuffle the bucketed idx if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute each bucket indices = [ [val[idx] for idx in torch.randperm(len(val), generator=g).tolist()] for val in self.bucket_idx ] else: indices = self.bucket_idx # Iterate over the buckets for idx, val in enumerate(indices): # If this is true we need to handle padding if (self.num_slices_per_bucket * self.slice_size) > len(val): split_val = self._handle_padding(val) indices[idx] = list(itertools.chain(*split_val)) assert len(indices[idx]) == self.rounded_num_samples_per_bucket # Now slice across replicas final_indices = [] for val in indices: for idx in range(self.num_slices_per_bucket): replica_slice = val[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] final_indices.append(replica_slice) # If bucket overlap is allowed then we just batch up the residual indices if self.drop_last and self.allow_bucket_overlap: residual_idx = list( itertools.chain( *[val[self.rounded_num_samples_per_bucket :] for val in indices] ) ) if len(residual_idx) > self.slice_size: # Cut by slices then by replicas residual_idx = [ residual_idx[ (idx * self.slice_size) : ((idx + 1) * self.slice_size) ][self.rank : self.slice_size : self.num_replicas] for idx in range(len(residual_idx) // self.slice_size) ] # Append to the final indices final_indices.extend(residual_idx) # Shuffle the bucketed batches if self.shuffle: # deterministically shuffle based on epoch and seed g = torch.Generator() g.manual_seed(self.seed + self.epoch) # Permute the bucket order final_indices = [ final_indices[val] for val in torch.randperm(len(final_indices), generator=g) ] # Unroll into a single list final_indices = list(itertools.chain(*final_indices)) assert len(final_indices) == self.rounded_num_samples_per_replica return iter(final_indices) def _handle_padding(self, idx_list: List): \"\"\"Handles padding out if a batch is short Parameters ---------- idx_list: List list of indices Returns ------- split_val: List list with correctly padded sizes \"\"\" split_val = [] for idx in range(self.num_slices_per_bucket): if idx == (self.num_slices_per_bucket - 1): # Get the short batch short_batch = idx_list[(idx * self.slice_size) :] # Short batch replica slice sizes short_len = [ self.batch_size - len(list(val)) for val in np.array_split(short_batch, self.num_replicas) ] # Pop the necessary values from the entire bucket pad_values = [ idx_list[s_idx : (self.num_replicas * s_len) : self.num_replicas] for s_idx, s_len in enumerate(short_len) ] # If not a consistent list then we need to reorder so that the step size alignment slicing # of the replicas works if len(set(short_len)) != 1: # here we need to find the first larger idx and reorder first_idx = short_len.index(max(set(short_len))) # Reorder pad_values = pad_values[first_idx:] + pad_values[0:first_idx] extended_batch = short_batch + [ pad for pad in list( itertools.chain(*itertools.zip_longest(*pad_values)) ) if pad is not None ] split_val.append(extended_batch) else: split_val.append( idx_list[(idx * self.slice_size) : ((idx + 1) * self.slice_size)] ) return split_val def __len__(self) -> int: return self.rounded_num_samples_per_replica def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch","title":"Attributes"},{"location":"reference/stoke/data/#ancestors-in-mro","text":"torch.utils.data.sampler.Sampler typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/stoke/data/#methods","text":"","title":"Methods"},{"location":"reference/stoke/data/#set_epoch","text":"def set_epoch ( self , epoch : int ) -> None Sets the epoch for this sampler. When :attr: shuffle=True , this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters: Name Type Description Default epoch int Epoch number None ??? example \"View Source\" def set_epoch(self, epoch: int) -> None: \"\"\"Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas use a different random ordering for each epoch. Otherwise, the next iteration of this sampler will yield the same ordering. Parameters ---------- epoch: int Epoch number \"\"\" self.epoch = epoch","title":"set_epoch"},{"location":"reference/stoke/data/#stokedataloader","text":"class StokeDataLoader ( gpu : bool , fp16 : Union [ stoke . status . FP16Options , NoneType ], dataset : torch . utils . data . dataset . Dataset [ + T_co ], batch_size : Union [ int , NoneType ] = 1 , shuffle : bool = False , sampler : Union [ torch . utils . data . sampler . Sampler [ int ], NoneType ] = None , batch_sampler : Union [ torch . utils . data . sampler . Sampler [ Sequence [ int ]], NoneType ] = None , num_workers : int = 0 , collate_fn : Callable [[ List [ ~ T ]], Any ] = None , pin_memory : bool = False , drop_last : bool = False , timeout : float = 0 , worker_init_fn : Callable [[ int ], NoneType ] = None , multiprocessing_context = None , generator = None , * , prefetch_factor : int = 2 , persistent_workers : bool = False )","title":"StokeDataLoader"},{"location":"reference/stoke/data/#attributes_1","text":"Name Type Description Default _gpu bool None None _fp16 Optional[FP16Options] None None ??? example \"View Source\" class StokeDataLoader(DL): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs Attributes ---------- _gpu: bool _fp16: Optional[FP16Options] See Also -------- torch.utils.data.DataLoader: base DataLoader class that this inherits from (check for all attributes) \"\"\" def __init__( self, gpu: bool, fp16: Optional[FP16Options], dataset: Dataset[T_co], batch_size: Optional[int] = 1, shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Maps to torch.utils.data.DataLoader __init__ Shim is necessary to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. batch_size: int, default: 1 how many samples per batch to load . shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Call super init for the actual torch DataLoader super(StokeDataLoader, self).__init__( dataset=dataset, batch_size=batch_size, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) self._gpu = gpu self._fp16 = fp16 def __iter__(self): \"\"\"Underlying iter of the DataLoader that yields samples Wrap the base __iter__ with a call to place on the device if flagged Yields ------ Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] data placed on the correct device \"\"\" # Iterate using the base class iter but override the yield by pushing to device prior if gpu flag is true for val in super().__iter__(): yield val if not self._gpu else self._place_data_on_gpu(val) def _place_data_on_gpu( self, data: Union[ torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor], ], ): \"\"\"Determine data structure and then place on the correct device (cast in the context of deepspeed FP16 as it wants half dtype as input) Parameters ---------- data: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] current data coming from the underlying __iter__ Returns ------- data: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor], Dict[str, torch.Tensor]] data moved to the correct device \"\"\" if isinstance(data, torch.Tensor): # TODO: Check if one of the APEX version needs a cast too? # Move to the correct cuda device w/ the correct type -- deepspeed FP16 requires a cast to half if fp16 if self._fp16 == \"deepspeed\": return data.to(device=\"cuda\", dtype=torch.half) else: return data.to(device=\"cuda\", dtype=data.dtype) elif isinstance(data, (list, tuple)): return type(data)(self._place_data_on_gpu(data=val) for val in data) elif isinstance(data, dict): return {k: self._place_data_on_gpu(v) for k, v in data.items()} elif ~(hasattr(data, \"to\")): return data else: raise TypeError( f\"Stoke -- Unsupported data type passed to _place_data_on_gpu \" f\"(torch.Tensor, tuple, list, dict), currently {type(data)}\" )","title":"Attributes"},{"location":"reference/stoke/data/#ancestors-in-mro_1","text":"torch.utils.data.dataloader.DataLoader typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/stoke/data/#instance-variables","text":"multiprocessing_context","title":"Instance variables"},{"location":"reference/stoke/data/#methods_1","text":"","title":"Methods"},{"location":"reference/stoke/data/#check_worker_number_rationality","text":"def check_worker_number_rationality ( self ) ??? example \"View Source\" def check_worker_number_rationality(self): # This function check whether the dataloader's worker number is rational based on # current system's resource. Current rule is that if the number of workers this # Dataloader will create is bigger than the number of logical cpus that is allowed to # use, than we will pop up a warning to let user pay attention. # # eg. If current system has 2 physical CPUs with 16 cores each. And each core support 2 # threads, then the total logical cpus here is 2 * 16 * 2 = 64. Let's say current # DataLoader process can use half of them which is 32, then the rational max number of # worker that initiated from this process is 32. # Now, let's say the created DataLoader has num_works = 40, which is bigger than 32. # So the warning message is triggered to notify the user to lower the worker number if # necessary. # # # [Note] Please note that this function repects `cpuset` only when os.sched_getaffinity is # available (available in most of Linux system, but not OSX and Windows). # When os.sched_getaffinity is not available, os.cpu_count() is called instead, but # it doesn't repect cpuset. # We don't take threading into account since each worker process is single threaded # at this time. # # We don't set any threading flags (eg. OMP_NUM_THREADS, MKL_NUM_THREADS, etc) # other than `torch.set_num_threads` to 1 in the worker process, if the passing # in functions use 3rd party modules that rely on those threading flags to determine # how many thread to create (eg. numpy, etc), then it is caller's responsibility to # set those flags correctly. def _create_warning_msg(num_worker_suggest, num_worker_created, cpuset_checked): suggested_max_worker_msg = (( \"Our suggested max number of worker in current system is {}{}, which is smaller \" \"than what this DataLoader is going to create.\").format( num_worker_suggest, (\"\" if cpuset_checked else \" (`cpuset` is not taken into account)\")) ) if num_worker_suggest is not None else ( \"DataLoader is not able to compute a suggested max number of worker in current system.\") warn_msg = ( \"This DataLoader will create {} worker processes in total. {} \" \"Please be aware that excessive worker creation might get DataLoader running slow or even freeze, \" \"lower the worker number to avoid potential slowness/freeze if necessary.\").format( num_worker_created, suggested_max_worker_msg) return warn_msg if not self.num_workers or self.num_workers == 0: return # try to compute a suggested max number of worker based on system's resource max_num_worker_suggest = None cpuset_checked = False if hasattr(os, 'sched_getaffinity'): try: max_num_worker_suggest = len(os.sched_getaffinity(0)) cpuset_checked = True except Exception: pass if max_num_worker_suggest is None: # os.cpu_count() could return Optional[int] # get cpu count first and check None in order to satify mypy check cpu_count = os.cpu_count() if cpu_count is not None: max_num_worker_suggest = cpu_count if max_num_worker_suggest is None: warnings.warn(_create_warning_msg( max_num_worker_suggest, self.num_workers, cpuset_checked)) return if self.num_workers > max_num_worker_suggest: warnings.warn(_create_warning_msg( max_num_worker_suggest, self.num_workers, cpuset_checked))","title":"check_worker_number_rationality"},{"location":"reference/stoke/distributed/","text":"Module stoke.distributed Handles distributed related classes -- mixin style None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles distributed related classes -- mixin style\"\"\" import os from abc import ABC, abstractmethod from contextlib import nullcontext from enum import Enum from typing import List, Optional, Tuple, Union import deepspeed as ds import horovod.torch as hvd import torch from deepspeed.utils.distributed import mpi_discovery from fairscale.optim.oss import OSS from stoke.configs import ClipGradConfig, ClipGradNormConfig from stoke.extensions import ( DistributedHandlerEnum, FairscaleFSDPExtension, FairscaleSDDPExtension, ) from stoke.utils import unrolled_print class BaseDistributed(ABC): \"\"\"Base class for distributed backends This class handles common functionality for all of the different distributed backends including setup, loss sync, gradient accumulation context, step context and various properties/attributes related to distributed frameworks Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, device_id: Optional[Union[int, str]], batch_size_per_device: int, info_rank: Union[int, List[int]], name: str, verbose: bool = True, ): \"\"\"Init for BaseDistributed class Parameters ---------- device_id: int, default: None Current device id batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information name: str name of current backend verbose: bool, default: True flag for Stoke print verbosity \"\"\" self._batch_size_per_device = batch_size_per_device self._device_id = device_id self._info_rank = info_rank self._name = name self._verbose = verbose def _print_info(self): \"\"\"Basic print of backend initialization status Returns ------- None \"\"\" self._print_device(f\"{self._name} Initialized: {self.initialized}\") def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item() def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass def _call_init(self): \"\"\"Base init call Nothing to do here... Returns ------- None \"\"\" pass def _print_device(self, msg: Union[str, List[str]]): \"\"\"Prints a str of list of strs on the currently set _info_rank Internal version of public print_device that always points to the set _info_rank Parameters ---------- msg: Union[str, List[str]] message(s) to print Returns ------- None \"\"\" self.print_device(msg=msg, rank=self._info_rank) def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass @property def device_id(self): \"\"\"Returns the current device id\"\"\" return self._device_id @property @abstractmethod def rank(self): pass @property @abstractmethod def world_size(self): pass @property @abstractmethod def initialized(self): pass class DistributedNullCPU(BaseDistributed): def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init for DistributedNullCPU Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Device ID set to None as it is not needed for non distributed CPU \"\"\" super(DistributedNullCPU, self).__init__( device_id=\"cpu\", batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch CPU\", verbose=verbose, ) @property def rank(self): \"\"\"Returns current distributed rank No rank so return string of cpu \"\"\" return \"cpu\" @property def world_size(self): \"\"\"Returns current world size\"\"\" return 1 @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return True class DistributedNullGPU(BaseDistributed): def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init for DistributedNullCPU Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Device ID set to the current CUDA device as there is only a single GPU being used \"\"\" super(DistributedNullGPU, self).__init__( device_id=torch.cuda.current_device(), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch GPU\", verbose=verbose, ) @property def rank(self): \"\"\"Returns current distributed rank No rank so return string of gpu \"\"\" return \"gpu\" @property def world_size(self): \"\"\"Returns current world size\"\"\" return 1 @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return True class DistributedDDP(BaseDistributed): \"\"\"Class for using DDP as the distributed backend This class handles common functionality for the DDP backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _ddp_config: DDPConfig Configuration object for DDP backend _ddp_handler wrapper method that will modify the DDP instance to use SDDP if flagged _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init call for DistributedDDP Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here ddp_config, sharded_config, or fully_sharded_config might be passed in \"\"\" self._ddp_config = kwargs[\"ddp_config\"] super(DistributedDDP, self).__init__( device_id=self._ddp_config.local_rank, batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch DDP\", verbose=verbose, ) # This creates the wrapper method depending on DDP or SDDP self._ddp_handler = self._create_ddp_handler(kwargs)( verbose=self._verbose, sddp_config=kwargs[\"sharded_config\"], fsdp_config=kwargs[\"fully_sharded_config\"], ddp_config=self._ddp_config, ) @staticmethod def _create_ddp_handler(kwargs: dict): \"\"\"Determines which DDP related class to use based on the kwarg config passed through Parameters ---------- kwargs: dict Extra arguments from the __init__ call Returns ------- FairscaleSDDPExtension or BaseDDP \"\"\" if kwargs[\"sharded_config\"] is not None: return DistributedHandlerEnum.sddp.value elif kwargs[\"fully_sharded_config\"] is not None: return DistributedHandlerEnum.fsdp.value else: return DistributedHandlerEnum.base.value def _call_init(self): \"\"\"Does any backend initialization work related to DDP setup Borrows code from DeepSpeed to setup DDP via openMPI https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/utils/distributed.py Returns ------- None \"\"\" # Borrowing a bit of code from deepspeed required_env = [ \"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\", ] if self._ddp_config.auto_mpi_discovery and not all( map(lambda v: v in os.environ, required_env) ): try: from mpi4py import MPI mpi_discovery(verbose=True) except ImportError as e: print( e, \": mpi4py cannot be imported -- please install Stoke with the MPI option (pip install stoke[mpi])\", ) # Initialize call for DDP torch.distributed.init_process_group( backend=self._ddp_config.backend, init_method=self._ddp_config.init_method ) def setup_distributed(self): \"\"\"Handles any underlying DDP setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) # Call the init fnc here after device id is set self._call_init() def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" self._print_device(f\"{self._name} Class: {type(self._ddp_handler).__name__}\") # Print info if verbose if self._verbose: self._print_info() self._print_device( [ f\"{self._name} -- Device ID: {torch.cuda.current_device()}\", f\"{self._name} -- Rank: {self.rank}\", ] ) if self._ddp_config.convert_to_sync_batch_norm: self.print_device( f\"Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...\" ) torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model) if self._verbose and isinstance( self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension) ): self._print_device( f\"Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}\" ) # Pass through to the handler for DDP wrappers model, optimizer = self._ddp_handler.handle_ddp( model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank ) return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Make sure everyone is synced before calling all reduce torch.distributed.barrier() # Loss tensor is worker specific so all_reduce (and SUM) torch.distributed.all_reduce(loss_tensor) # Detach and divide by the world size to get the mean on each device return loss_tensor.item() / self.world_size def grad_accum_context(self, model: torch.nn.Module): \"\"\"Return the context to wrap the gradient accumulation steps DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s)) SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object Returns ------- no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient accumulation else nullcontext \"\"\" if self._verbose and self._ddp_config.no_sync: self._print_device(\"DDP Using no sync context\") context = model.no_sync() if self._ddp_config.no_sync else nullcontext() return context def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return torch.distributed.get_rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return torch.distributed.get_world_size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return torch.distributed.is_initialized() def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group() class DistributedDeepspeed(BaseDistributed): \"\"\"Class for using Deepspeed as the distributed backend This class handles common functionality for the deepspeed backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _deepspeed_config: DeepspeedConfig Configuration object for Deepspeed backend _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init call for DistributedDeepspeed Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here deepspeed_config, grad_accum_steps or grad_clip might be passed in \"\"\" self._deepspeed_config = kwargs[\"deepspeed_config\"] # Call init first to pass local rank to super self._call_init() # Forward device to super -- should be set from MPI lookup that is called super(DistributedDeepspeed, self).__init__( device_id=int(os.environ[\"LOCAL_RANK\"]), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"Deepspeed\", verbose=verbose, ) self._deepspeed_init_config = self._handle_deepspeed_configs( grad_accum_steps=kwargs[\"grad_accum_steps\"], grad_clip=kwargs[\"grad_clip\"] ) def _call_init(self): \"\"\"Does any backend initialization work related to deepspeed setup Returns ------- None \"\"\" ds.init_distributed( dist_backend=self._deepspeed_config.dist_backend, auto_mpi_discovery=self._deepspeed_config.auto_mpi_discovery, distributed_port=self._deepspeed_config.distributed_port, verbose=self._deepspeed_config.verbose, init_method=self._deepspeed_config.init_method, ) def setup_distributed(self): \"\"\"Handles any underlying deepspeed setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Deepspeed Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") model, optimizer, _, _ = ds.initialize( model=model, optimizer=optimizer, model_parameters=filter(lambda p: p.requires_grad, model.parameters()), config_params=self._deepspeed_init_config, ) return model, optimizer def _handle_deepspeed_configs( self, grad_accum_steps: int, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], ): \"\"\"Handles building the dictionary of configs that the deepspeed initialize call expects https://www.deepspeed.ai/docs/config-json/ Parameters ---------- grad_accum_steps: int number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None gradient clipping config objects Returns ------- dict All deepspeed parameters merged together from individual pieces \"\"\" # empty dict to start ds_config = {} # Map batch size stuff -- need to define 2/3 ds_config.update(self._map_ds_batch_configs(grad_accum_steps=grad_accum_steps)) # Skip optimizer & skip scheduler # Map communication ds_config.update(self._map_ds_communication_configs()) # Map FP16 and add enabled flag if selected ds_config.update(self._map_ds_fp16_configs()) # Map grad clipping ds_config.update(self._map_ds_grad_clip_configs(grad_clip=grad_clip)) # Map zero -- internally map param offloading and optimizer offloading to zero ds_config.update(self._map_ds_zero_configs()) # Map aio ds_config.update(self._map_ds_aio_configs()) # Map logging ds_config.update(self._map_ds_logging_configs()) # Map flops -- enabled ds_config.update(self._map_ds_flops_configs()) # Map activation checkpointing ds_config.update(self._map_ds_activation_checkpointing_configs()) # Map tensorboard ds_config.update(self._map_ds_tensorboard_config()) # Map PLD ds_config.update(self._map_ds_pld_config()) return ds_config def _map_ds_pld_config(self): \"\"\"Maps progressive layer drop parameters https://www.deepspeed.ai/tutorials/progressive_layer_dropping/ https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L293 Returns ------- dict pld parameters or enabled false dict \"\"\" if self._deepspeed_config.progressive_layer_drop is not None: map_dict = { v.name: getattr(self._deepspeed_config.progressive_layer_drop, v.name) for v in self._deepspeed_config.progressive_layer_drop.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"progressive_layer_drop\": map_dict} else: return {\"progressive_layer_drop\": {\"enabled\": False}} def _map_ds_tensorboard_config(self): \"\"\"Maps tensorboard related parameters https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L268 Returns ------- dict tensorboard parameters or enabled false dict \"\"\" if self._deepspeed_config.tensorboard is not None: map_dict = { v.name: getattr(self._deepspeed_config.tensorboard, v.name) for v in self._deepspeed_config.tensorboard.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"tensorboard\": map_dict} else: return {\"tensorboard\": {\"enabled\": False}} def _map_ds_grad_clip_configs( self, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] ): \"\"\"Maps grad clipping related parameters https://www.deepspeed.ai/docs/config-json/#gradient-clipping Parameters ---------- grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None gradient clipping config objects Returns ------- dict gradient clipping parameters or empty dict \"\"\" if grad_clip is not None: if isinstance(grad_clip, ClipGradNormConfig): return {\"gradient_clipping\": grad_clip.max_norm} else: raise ValueError( f\"Deepspeed does not currently support \" f'{type(grad_clip).__name__.replace(\"Config\", \"\")}' ) else: return {} def _map_ds_logging_configs(self): \"\"\"Maps logging related parameters https://www.deepspeed.ai/docs/config-json/#logging Returns ------- dict logging parameters or empty dict \"\"\" return { \"steps_per_print\": self._deepspeed_config.steps_per_print, \"dump_state\": self._deepspeed_config.dump_state, \"wall_clock_breakdown\": self._deepspeed_config.wall_clock_breakdown, } def _map_ds_activation_checkpointing_configs(self): \"\"\"Maps activation checkpointing related parameters https://www.deepspeed.ai/docs/config-json/#activation-checkpointing Returns ------- dict activation checkpointing parameters or empty dict \"\"\" if self._deepspeed_config.activation_checkpointing is not None: map_dict = { v.name: getattr(self._deepspeed_config.activation_checkpointing, v.name) for v in self._deepspeed_config.activation_checkpointing.__attrs_attrs__ } return {\"activation_checkpointing\": map_dict} else: return {} def _map_ds_flops_configs(self): \"\"\"Maps flops related parameters https://www.deepspeed.ai/docs/config-json/#flops-profiler Returns ------- dict flops parameters or enabled false dict \"\"\" if self._deepspeed_config.flops_profiler is not None: map_dict = { v.name: getattr(self._deepspeed_config.flops_profiler, v.name) for v in self._deepspeed_config.flops_profiler.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"flops_profiler\": map_dict} else: return {\"flops_profiler\": {\"enabled\": False}} def _map_ds_aio_configs(self): \"\"\"Maps async i/o related parameters https://www.deepspeed.ai/docs/config-json/#asynchronous-io Returns ------- dict async i/o parameters or empty dict \"\"\" if self._deepspeed_config.aio is not None: map_dict = { v.name: getattr(self._deepspeed_config.aio, v.name) for v in self._deepspeed_config.aio.__attrs_attrs__ } return {\"aio\": map_dict} else: return {} def _map_ds_zero_configs(self): \"\"\"Maps ZeRO related parameters https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training Returns ------- dict ZeRO related parameters \"\"\" map_dict = {} for v in self._deepspeed_config.zero_optimization.__attrs_attrs__: if v.name == \"offload_optimizer\": map_dict.update(self._map_ds_offload_optimizer_configs()) elif v.name == \"offload_param\": map_dict.update(self._map_ds_offload_param_configs()) # Just map the rest since the name:value is correct else: map_dict.update( {v.name: getattr(self._deepspeed_config.zero_optimization, v.name)} ) # Default overlap com to True for ZeRO stage 3 map_dict[\"overlap_comm\"] = ( True if map_dict[\"stage\"] == 3 else map_dict[\"overlap_comm\"] ) return {\"zero_optimization\": map_dict} def _map_ds_offload_param_configs(self): \"\"\"Maps ZeRO parameter offload parameters https://www.deepspeed.ai/docs/config-json/#parameter-offloading Returns ------- dict ZeRO offload parameter parameters \"\"\" # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct if self._deepspeed_config.zero_optimization.offload_param is not None: map_dict = { v.name: getattr( self._deepspeed_config.zero_optimization.offload_param, v.name ) for v in self._deepspeed_config.zero_optimization.offload_param.__attrs_attrs__ } return {\"offload_param\": map_dict} else: return {\"offload_param\": None} def _map_ds_offload_optimizer_configs(self): \"\"\"Maps ZeRO optimizer offload parameters https://www.deepspeed.ai/docs/config-json/#optimizer-offloading Returns ------- dict ZeRO offload optimizer parameters \"\"\" # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct if self._deepspeed_config.zero_optimization.offload_optimizer is not None: map_dict = { v.name: getattr( self._deepspeed_config.zero_optimization.offload_optimizer, v.name ) for v in self._deepspeed_config.zero_optimization.offload_optimizer.__attrs_attrs__ } # Set some post init values map_dict[\"pipeline\"] = ( map_dict[\"pipeline_read\"] or map_dict[\"pipeline_write\"] ) return {\"offload_optimizer\": map_dict} else: return {\"offload_optimizer\": None} def _map_ds_fp16_configs(self): \"\"\"Maps FP16 related parameters https://www.deepspeed.ai/docs/config-json/#fp16-training-options Returns ------- dict fp16 related parameters or enabled false dict \"\"\" if self._deepspeed_config.fp16 is not None: # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct map_dict = { v.name: getattr(self._deepspeed_config.fp16, v.name) for v in self._deepspeed_config.fp16.__attrs_attrs__ } # Add the enabled flag map_dict.update({\"enabled\": True}) return {\"fp16\": map_dict} else: return {\"fp16\": {\"enabled\": False}} def _map_ds_batch_configs(self, grad_accum_steps: int): \"\"\"Maps batch size related parameters https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters Parameters ---------- grad_accum_steps: int number of gradient accumulation steps Returns ------- dict batch size related parameters \"\"\" # Need to define 2/3 return { \"train_micro_batch_size_per_gpu\": self._batch_size_per_device, \"gradient_accumulation_steps\": grad_accum_steps, } def _map_ds_communication_configs(self): \"\"\"Maps communication related parameters https://www.deepspeed.ai/docs/config-json/#communication-options Returns ------- dict communication related parameters \"\"\" return { \"fp32_allreduce\": self._deepspeed_config.fp32_allreduce, \"gradient_predivide_factor\": self._deepspeed_config.gradient_predivide_factor, \"prescale_gradients:\": self._deepspeed_config.prescale_gradients, \"sparse_gradients\": self._deepspeed_config.sparse_gradients, } def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Loss tensor is worker specific so all_reduce (and SUM) torch.distributed.all_reduce(loss_tensor) # Detach and divide by the world size to get the mean on each device return loss_tensor.item() / self.world_size def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return torch.distributed.get_rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return torch.distributed.get_world_size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return torch.distributed.is_initialized() def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group() class DistributedHorovod(BaseDistributed): \"\"\"Class for using Horovod as the distributed backend This class handles common functionality for the horovod backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _device_id: int, default: None Current device id _horovod_config: HorovodConfig Configuration object for Horovod backend _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\" Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here horovod_config might be passed in \"\"\" # Grab the config self._horovod_config = kwargs[\"horovod_config\"] # Initialize first so the local rank call cal be forwarded to super self._call_init() super(DistributedHorovod, self).__init__( device_id=hvd.local_rank(), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"Horovod\", verbose=verbose, ) self._multi_loss = ( len(kwargs[\"loss\"]) if isinstance(kwargs[\"loss\"], (list, tuple)) else 1 ) def _call_init(self): \"\"\"Does any backend initialization work related to horovod setup Returns ------- None \"\"\" hvd.init() def _hvd_convert_to_sync_batch_norm( self, module: torch.nn.Module, process_group=None ): \"\"\"Replaces all BatchNorm*D layers with horovod.torch.SyncBatchNorm layers https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm https://nvidia.github.io/apex/_modules/apex/parallel.html#convert_syncbn_model Parameters ---------- module: torch.nn.Module current model object process_group: default: None process group to scope synchronization, default is the whole world Returns ------- module_output: torch.nn.Module modified version of model with all BatchNorm*D layers replaced with horovod.torch.SyncBatchNorm layers Notes ----- Borrows heavily from the current torch convert_sync_batchnorm and apex convert_syncbn_model implementations only changing the underlying layer type to use the hvd implementation \"\"\" module_output = module if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): module_output = hvd.SyncBatchNorm( num_features=module.num_features, eps=module.eps, momentum=module.momentum, affine=module.affine, track_running_stats=module.track_running_stats, ) # Handle the copy of affine vars if affine if module.affine: with torch.no_grad(): module_output.weight = module.weight module_output.bias = module.bias # Handle the swap of running stats module_output.running_mean = module.running_mean module_output.running_var = module.running_var # Iterate recursively and replace for name, child in module.named_children(): module_output.add_module( name=name, module=self._hvd_convert_to_sync_batch_norm( module=child, process_group=process_group ), ) # delete and return del module return module_output def setup_distributed(self): \"\"\"Handles any underlying horovod setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Horovod Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") op_dict = {\"Average\": hvd.Average, \"Sum\": hvd.Sum, \"Adasum\": hvd.Adasum} optimizer = hvd.DistributedOptimizer( optimizer=optimizer, named_parameters=model.named_parameters(), backward_passes_per_step=grad_accum * self._multi_loss if grad_accum is not None else self._multi_loss, compression=hvd.Compression.fp16 if self._horovod_config.compression else hvd.Compression.none, gradient_predivide_factor=self._horovod_config.gradient_predivide_factor, op=op_dict.get(self._horovod_config.op), ) # Broadcast the initial variable states from rank 0 to all other processes hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Make sure everyone is synced before the all-reduce # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Loss tensor is worker specific so allreduce -- force SUM from Horovod sum_tensor = hvd.allreduce(loss_tensor, op=hvd.Sum) # Detach and divide by the world size to get the mean on each device return sum_tensor.item() / self.world_size def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Return the context to wrap the step call Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation \"\"\" # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( \"Horovod skipping synchronize as it was triggered pre grad-clip\" ) return optimizer.skip_synchronize() def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return hvd.rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return hvd.size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return hvd.is_initialized() class RunnerDistEnum(Enum): \"\"\"Enum for building the runtime object with distributed functionality\"\"\" cpu = DistributedNullCPU gpu = DistributedNullGPU ddp = DistributedDDP horovod = DistributedHorovod deepspeed = DistributedDeepspeed Classes BaseDistributed class BaseDistributed ( device_id : Union [ int , str , NoneType ], batch_size_per_device : int , info_rank : Union [ int , List [ int ]], name : str , verbose : bool = True ) Attributes Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class BaseDistributed(ABC): \"\"\"Base class for distributed backends This class handles common functionality for all of the different distributed backends including setup, loss sync, gradient accumulation context, step context and various properties/attributes related to distributed frameworks Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, device_id: Optional[Union[int, str]], batch_size_per_device: int, info_rank: Union[int, List[int]], name: str, verbose: bool = True, ): \"\"\"Init for BaseDistributed class Parameters ---------- device_id: int, default: None Current device id batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information name: str name of current backend verbose: bool, default: True flag for Stoke print verbosity \"\"\" self._batch_size_per_device = batch_size_per_device self._device_id = device_id self._info_rank = info_rank self._name = name self._verbose = verbose def _print_info(self): \"\"\"Basic print of backend initialization status Returns ------- None \"\"\" self._print_device(f\"{self._name} Initialized: {self.initialized}\") def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item() def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass def _call_init(self): \"\"\"Base init call Nothing to do here... Returns ------- None \"\"\" pass def _print_device(self, msg: Union[str, List[str]]): \"\"\"Prints a str of list of strs on the currently set _info_rank Internal version of public print_device that always points to the set _info_rank Parameters ---------- msg: Union[str, List[str]] message(s) to print Returns ------- None \"\"\" self.print_device(msg=msg, rank=self._info_rank) def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass @property def device_id(self): \"\"\"Returns the current device id\"\"\" return self._device_id @property @abstractmethod def rank(self): pass @property @abstractmethod def world_size(self): pass @property @abstractmethod def initialized(self): pass Ancestors (in MRO) abc.ABC Descendants stoke.distributed.DistributedNullCPU stoke.distributed.DistributedNullGPU stoke.distributed.DistributedDDP stoke.distributed.DistributedDeepspeed stoke.distributed.DistributedHorovod Instance variables device_id Returns the current device id initialized rank world_size Methods barrier def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass clean def clean ( self ) Base clean call Nothing to do here... Returns: Type Description None None ??? example \"View Source\" def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass detach_and_sync_loss def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item() grad_accum_context def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() print_device def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass setup_distributed def setup_distributed ( self ) Base setup distributed Does nothing as nothing needs to be wrapped Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass step_context def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() wrap_distributed def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module same as input model ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer DistributedDDP class DistributedDDP ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs ) Attributes Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _ddp_config DDPConfig Configuration object for DDP backend None _ddp_handler None wrapper method that will modify the DDP instance to use SDDP if flagged None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedDDP(BaseDistributed): \"\"\"Class for using DDP as the distributed backend This class handles common functionality for the DDP backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _ddp_config: DDPConfig Configuration object for DDP backend _ddp_handler wrapper method that will modify the DDP instance to use SDDP if flagged _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init call for DistributedDDP Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here ddp_config, sharded_config, or fully_sharded_config might be passed in \"\"\" self._ddp_config = kwargs[\"ddp_config\"] super(DistributedDDP, self).__init__( device_id=self._ddp_config.local_rank, batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch DDP\", verbose=verbose, ) # This creates the wrapper method depending on DDP or SDDP self._ddp_handler = self._create_ddp_handler(kwargs)( verbose=self._verbose, sddp_config=kwargs[\"sharded_config\"], fsdp_config=kwargs[\"fully_sharded_config\"], ddp_config=self._ddp_config, ) @staticmethod def _create_ddp_handler(kwargs: dict): \"\"\"Determines which DDP related class to use based on the kwarg config passed through Parameters ---------- kwargs: dict Extra arguments from the __init__ call Returns ------- FairscaleSDDPExtension or BaseDDP \"\"\" if kwargs[\"sharded_config\"] is not None: return DistributedHandlerEnum.sddp.value elif kwargs[\"fully_sharded_config\"] is not None: return DistributedHandlerEnum.fsdp.value else: return DistributedHandlerEnum.base.value def _call_init(self): \"\"\"Does any backend initialization work related to DDP setup Borrows code from DeepSpeed to setup DDP via openMPI https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/utils/distributed.py Returns ------- None \"\"\" # Borrowing a bit of code from deepspeed required_env = [ \"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\", ] if self._ddp_config.auto_mpi_discovery and not all( map(lambda v: v in os.environ, required_env) ): try: from mpi4py import MPI mpi_discovery(verbose=True) except ImportError as e: print( e, \": mpi4py cannot be imported -- please install Stoke with the MPI option (pip install stoke[mpi])\", ) # Initialize call for DDP torch.distributed.init_process_group( backend=self._ddp_config.backend, init_method=self._ddp_config.init_method ) def setup_distributed(self): \"\"\"Handles any underlying DDP setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) # Call the init fnc here after device id is set self._call_init() def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" self._print_device(f\"{self._name} Class: {type(self._ddp_handler).__name__}\") # Print info if verbose if self._verbose: self._print_info() self._print_device( [ f\"{self._name} -- Device ID: {torch.cuda.current_device()}\", f\"{self._name} -- Rank: {self.rank}\", ] ) if self._ddp_config.convert_to_sync_batch_norm: self.print_device( f\"Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...\" ) torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model) if self._verbose and isinstance( self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension) ): self._print_device( f\"Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}\" ) # Pass through to the handler for DDP wrappers model, optimizer = self._ddp_handler.handle_ddp( model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank ) return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Make sure everyone is synced before calling all reduce torch.distributed.barrier() # Loss tensor is worker specific so all_reduce (and SUM) torch.distributed.all_reduce(loss_tensor) # Detach and divide by the world size to get the mean on each device return loss_tensor.item() / self.world_size def grad_accum_context(self, model: torch.nn.Module): \"\"\"Return the context to wrap the gradient accumulation steps DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s)) SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object Returns ------- no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient accumulation else nullcontext \"\"\" if self._verbose and self._ddp_config.no_sync: self._print_device(\"DDP Using no sync context\") context = model.no_sync() if self._ddp_config.no_sync else nullcontext() return context def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return torch.distributed.get_rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return torch.distributed.get_world_size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return torch.distributed.is_initialized() def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group() Ancestors (in MRO) stoke.distributed.BaseDistributed abc.ABC Instance variables device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank world_size Returns current world size Methods barrier def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() clean def clean ( self ) Cleans up at the end of a DDP run ??? example \"View Source\" def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group() detach_and_sync_loss def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) grad_accum_context def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Return the context to wrap the gradient accumulation steps DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s)) SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Return the context to wrap the gradient accumulation steps DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s)) SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object Returns ------- no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient accumulation else nullcontext \"\"\" if self._verbose and self._ddp_config.no_sync: self._print_device(\"DDP Using no sync context\") context = model.no_sync() if self._ddp_config.no_sync else nullcontext() return context print_device def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass setup_distributed def setup_distributed ( self ) Handles any underlying DDP setup post init Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Handles any underlying DDP setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) # Call the init fnc here after device id is set self._call_init() step_context def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() wrap_distributed def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" self._print_device(f\"{self._name} Class: {type(self._ddp_handler).__name__}\") # Print info if verbose if self._verbose: self._print_info() self._print_device( [ f\"{self._name} -- Device ID: {torch.cuda.current_device()}\", f\"{self._name} -- Rank: {self.rank}\", ] ) if self._ddp_config.convert_to_sync_batch_norm: self.print_device( f\"Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...\" ) torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model) if self._verbose and isinstance( self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension) ): self._print_device( f\"Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}\" ) # Pass through to the handler for DDP wrappers model, optimizer = self._ddp_handler.handle_ddp( model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank ) return model, optimizer DistributedDeepspeed class DistributedDeepspeed ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs ) Attributes Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _deepspeed_config DeepspeedConfig Configuration object for Deepspeed backend None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedDeepspeed(BaseDistributed): \"\"\"Class for using Deepspeed as the distributed backend This class handles common functionality for the deepspeed backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _deepspeed_config: DeepspeedConfig Configuration object for Deepspeed backend _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init call for DistributedDeepspeed Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here deepspeed_config, grad_accum_steps or grad_clip might be passed in \"\"\" self._deepspeed_config = kwargs[\"deepspeed_config\"] # Call init first to pass local rank to super self._call_init() # Forward device to super -- should be set from MPI lookup that is called super(DistributedDeepspeed, self).__init__( device_id=int(os.environ[\"LOCAL_RANK\"]), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"Deepspeed\", verbose=verbose, ) self._deepspeed_init_config = self._handle_deepspeed_configs( grad_accum_steps=kwargs[\"grad_accum_steps\"], grad_clip=kwargs[\"grad_clip\"] ) def _call_init(self): \"\"\"Does any backend initialization work related to deepspeed setup Returns ------- None \"\"\" ds.init_distributed( dist_backend=self._deepspeed_config.dist_backend, auto_mpi_discovery=self._deepspeed_config.auto_mpi_discovery, distributed_port=self._deepspeed_config.distributed_port, verbose=self._deepspeed_config.verbose, init_method=self._deepspeed_config.init_method, ) def setup_distributed(self): \"\"\"Handles any underlying deepspeed setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Deepspeed Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") model, optimizer, _, _ = ds.initialize( model=model, optimizer=optimizer, model_parameters=filter(lambda p: p.requires_grad, model.parameters()), config_params=self._deepspeed_init_config, ) return model, optimizer def _handle_deepspeed_configs( self, grad_accum_steps: int, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], ): \"\"\"Handles building the dictionary of configs that the deepspeed initialize call expects https://www.deepspeed.ai/docs/config-json/ Parameters ---------- grad_accum_steps: int number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None gradient clipping config objects Returns ------- dict All deepspeed parameters merged together from individual pieces \"\"\" # empty dict to start ds_config = {} # Map batch size stuff -- need to define 2/3 ds_config.update(self._map_ds_batch_configs(grad_accum_steps=grad_accum_steps)) # Skip optimizer & skip scheduler # Map communication ds_config.update(self._map_ds_communication_configs()) # Map FP16 and add enabled flag if selected ds_config.update(self._map_ds_fp16_configs()) # Map grad clipping ds_config.update(self._map_ds_grad_clip_configs(grad_clip=grad_clip)) # Map zero -- internally map param offloading and optimizer offloading to zero ds_config.update(self._map_ds_zero_configs()) # Map aio ds_config.update(self._map_ds_aio_configs()) # Map logging ds_config.update(self._map_ds_logging_configs()) # Map flops -- enabled ds_config.update(self._map_ds_flops_configs()) # Map activation checkpointing ds_config.update(self._map_ds_activation_checkpointing_configs()) # Map tensorboard ds_config.update(self._map_ds_tensorboard_config()) # Map PLD ds_config.update(self._map_ds_pld_config()) return ds_config def _map_ds_pld_config(self): \"\"\"Maps progressive layer drop parameters https://www.deepspeed.ai/tutorials/progressive_layer_dropping/ https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L293 Returns ------- dict pld parameters or enabled false dict \"\"\" if self._deepspeed_config.progressive_layer_drop is not None: map_dict = { v.name: getattr(self._deepspeed_config.progressive_layer_drop, v.name) for v in self._deepspeed_config.progressive_layer_drop.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"progressive_layer_drop\": map_dict} else: return {\"progressive_layer_drop\": {\"enabled\": False}} def _map_ds_tensorboard_config(self): \"\"\"Maps tensorboard related parameters https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L268 Returns ------- dict tensorboard parameters or enabled false dict \"\"\" if self._deepspeed_config.tensorboard is not None: map_dict = { v.name: getattr(self._deepspeed_config.tensorboard, v.name) for v in self._deepspeed_config.tensorboard.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"tensorboard\": map_dict} else: return {\"tensorboard\": {\"enabled\": False}} def _map_ds_grad_clip_configs( self, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] ): \"\"\"Maps grad clipping related parameters https://www.deepspeed.ai/docs/config-json/#gradient-clipping Parameters ---------- grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None gradient clipping config objects Returns ------- dict gradient clipping parameters or empty dict \"\"\" if grad_clip is not None: if isinstance(grad_clip, ClipGradNormConfig): return {\"gradient_clipping\": grad_clip.max_norm} else: raise ValueError( f\"Deepspeed does not currently support \" f'{type(grad_clip).__name__.replace(\"Config\", \"\")}' ) else: return {} def _map_ds_logging_configs(self): \"\"\"Maps logging related parameters https://www.deepspeed.ai/docs/config-json/#logging Returns ------- dict logging parameters or empty dict \"\"\" return { \"steps_per_print\": self._deepspeed_config.steps_per_print, \"dump_state\": self._deepspeed_config.dump_state, \"wall_clock_breakdown\": self._deepspeed_config.wall_clock_breakdown, } def _map_ds_activation_checkpointing_configs(self): \"\"\"Maps activation checkpointing related parameters https://www.deepspeed.ai/docs/config-json/#activation-checkpointing Returns ------- dict activation checkpointing parameters or empty dict \"\"\" if self._deepspeed_config.activation_checkpointing is not None: map_dict = { v.name: getattr(self._deepspeed_config.activation_checkpointing, v.name) for v in self._deepspeed_config.activation_checkpointing.__attrs_attrs__ } return {\"activation_checkpointing\": map_dict} else: return {} def _map_ds_flops_configs(self): \"\"\"Maps flops related parameters https://www.deepspeed.ai/docs/config-json/#flops-profiler Returns ------- dict flops parameters or enabled false dict \"\"\" if self._deepspeed_config.flops_profiler is not None: map_dict = { v.name: getattr(self._deepspeed_config.flops_profiler, v.name) for v in self._deepspeed_config.flops_profiler.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"flops_profiler\": map_dict} else: return {\"flops_profiler\": {\"enabled\": False}} def _map_ds_aio_configs(self): \"\"\"Maps async i/o related parameters https://www.deepspeed.ai/docs/config-json/#asynchronous-io Returns ------- dict async i/o parameters or empty dict \"\"\" if self._deepspeed_config.aio is not None: map_dict = { v.name: getattr(self._deepspeed_config.aio, v.name) for v in self._deepspeed_config.aio.__attrs_attrs__ } return {\"aio\": map_dict} else: return {} def _map_ds_zero_configs(self): \"\"\"Maps ZeRO related parameters https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training Returns ------- dict ZeRO related parameters \"\"\" map_dict = {} for v in self._deepspeed_config.zero_optimization.__attrs_attrs__: if v.name == \"offload_optimizer\": map_dict.update(self._map_ds_offload_optimizer_configs()) elif v.name == \"offload_param\": map_dict.update(self._map_ds_offload_param_configs()) # Just map the rest since the name:value is correct else: map_dict.update( {v.name: getattr(self._deepspeed_config.zero_optimization, v.name)} ) # Default overlap com to True for ZeRO stage 3 map_dict[\"overlap_comm\"] = ( True if map_dict[\"stage\"] == 3 else map_dict[\"overlap_comm\"] ) return {\"zero_optimization\": map_dict} def _map_ds_offload_param_configs(self): \"\"\"Maps ZeRO parameter offload parameters https://www.deepspeed.ai/docs/config-json/#parameter-offloading Returns ------- dict ZeRO offload parameter parameters \"\"\" # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct if self._deepspeed_config.zero_optimization.offload_param is not None: map_dict = { v.name: getattr( self._deepspeed_config.zero_optimization.offload_param, v.name ) for v in self._deepspeed_config.zero_optimization.offload_param.__attrs_attrs__ } return {\"offload_param\": map_dict} else: return {\"offload_param\": None} def _map_ds_offload_optimizer_configs(self): \"\"\"Maps ZeRO optimizer offload parameters https://www.deepspeed.ai/docs/config-json/#optimizer-offloading Returns ------- dict ZeRO offload optimizer parameters \"\"\" # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct if self._deepspeed_config.zero_optimization.offload_optimizer is not None: map_dict = { v.name: getattr( self._deepspeed_config.zero_optimization.offload_optimizer, v.name ) for v in self._deepspeed_config.zero_optimization.offload_optimizer.__attrs_attrs__ } # Set some post init values map_dict[\"pipeline\"] = ( map_dict[\"pipeline_read\"] or map_dict[\"pipeline_write\"] ) return {\"offload_optimizer\": map_dict} else: return {\"offload_optimizer\": None} def _map_ds_fp16_configs(self): \"\"\"Maps FP16 related parameters https://www.deepspeed.ai/docs/config-json/#fp16-training-options Returns ------- dict fp16 related parameters or enabled false dict \"\"\" if self._deepspeed_config.fp16 is not None: # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct map_dict = { v.name: getattr(self._deepspeed_config.fp16, v.name) for v in self._deepspeed_config.fp16.__attrs_attrs__ } # Add the enabled flag map_dict.update({\"enabled\": True}) return {\"fp16\": map_dict} else: return {\"fp16\": {\"enabled\": False}} def _map_ds_batch_configs(self, grad_accum_steps: int): \"\"\"Maps batch size related parameters https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters Parameters ---------- grad_accum_steps: int number of gradient accumulation steps Returns ------- dict batch size related parameters \"\"\" # Need to define 2/3 return { \"train_micro_batch_size_per_gpu\": self._batch_size_per_device, \"gradient_accumulation_steps\": grad_accum_steps, } def _map_ds_communication_configs(self): \"\"\"Maps communication related parameters https://www.deepspeed.ai/docs/config-json/#communication-options Returns ------- dict communication related parameters \"\"\" return { \"fp32_allreduce\": self._deepspeed_config.fp32_allreduce, \"gradient_predivide_factor\": self._deepspeed_config.gradient_predivide_factor, \"prescale_gradients:\": self._deepspeed_config.prescale_gradients, \"sparse_gradients\": self._deepspeed_config.sparse_gradients, } def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Loss tensor is worker specific so all_reduce (and SUM) torch.distributed.all_reduce(loss_tensor) # Detach and divide by the world size to get the mean on each device return loss_tensor.item() / self.world_size def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return torch.distributed.get_rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return torch.distributed.get_world_size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return torch.distributed.is_initialized() def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group() Ancestors (in MRO) stoke.distributed.BaseDistributed abc.ABC Instance variables device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank world_size Returns current world size Methods barrier def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() clean def clean ( self ) Cleans up at the end of a DDP run ??? example \"View Source\" def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group() detach_and_sync_loss def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) grad_accum_context def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() print_device def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass setup_distributed def setup_distributed ( self ) Handles any underlying deepspeed setup post init Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Handles any underlying deepspeed setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) step_context def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() wrap_distributed def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Overrides base implementation for wrapping with Deepspeed Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Deepspeed Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") model, optimizer, _, _ = ds.initialize( model=model, optimizer=optimizer, model_parameters=filter(lambda p: p.requires_grad, model.parameters()), config_params=self._deepspeed_init_config, ) return model, optimizer DistributedHorovod class DistributedHorovod ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs ) Attributes Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _device_id int, default: None Current device id None _horovod_config HorovodConfig Configuration object for Horovod backend None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedHorovod(BaseDistributed): \"\"\"Class for using Horovod as the distributed backend This class handles common functionality for the horovod backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _device_id: int, default: None Current device id _horovod_config: HorovodConfig Configuration object for Horovod backend _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\" Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here horovod_config might be passed in \"\"\" # Grab the config self._horovod_config = kwargs[\"horovod_config\"] # Initialize first so the local rank call cal be forwarded to super self._call_init() super(DistributedHorovod, self).__init__( device_id=hvd.local_rank(), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"Horovod\", verbose=verbose, ) self._multi_loss = ( len(kwargs[\"loss\"]) if isinstance(kwargs[\"loss\"], (list, tuple)) else 1 ) def _call_init(self): \"\"\"Does any backend initialization work related to horovod setup Returns ------- None \"\"\" hvd.init() def _hvd_convert_to_sync_batch_norm( self, module: torch.nn.Module, process_group=None ): \"\"\"Replaces all BatchNorm*D layers with horovod.torch.SyncBatchNorm layers https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm https://nvidia.github.io/apex/_modules/apex/parallel.html#convert_syncbn_model Parameters ---------- module: torch.nn.Module current model object process_group: default: None process group to scope synchronization, default is the whole world Returns ------- module_output: torch.nn.Module modified version of model with all BatchNorm*D layers replaced with horovod.torch.SyncBatchNorm layers Notes ----- Borrows heavily from the current torch convert_sync_batchnorm and apex convert_syncbn_model implementations only changing the underlying layer type to use the hvd implementation \"\"\" module_output = module if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): module_output = hvd.SyncBatchNorm( num_features=module.num_features, eps=module.eps, momentum=module.momentum, affine=module.affine, track_running_stats=module.track_running_stats, ) # Handle the copy of affine vars if affine if module.affine: with torch.no_grad(): module_output.weight = module.weight module_output.bias = module.bias # Handle the swap of running stats module_output.running_mean = module.running_mean module_output.running_var = module.running_var # Iterate recursively and replace for name, child in module.named_children(): module_output.add_module( name=name, module=self._hvd_convert_to_sync_batch_norm( module=child, process_group=process_group ), ) # delete and return del module return module_output def setup_distributed(self): \"\"\"Handles any underlying horovod setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Horovod Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") op_dict = {\"Average\": hvd.Average, \"Sum\": hvd.Sum, \"Adasum\": hvd.Adasum} optimizer = hvd.DistributedOptimizer( optimizer=optimizer, named_parameters=model.named_parameters(), backward_passes_per_step=grad_accum * self._multi_loss if grad_accum is not None else self._multi_loss, compression=hvd.Compression.fp16 if self._horovod_config.compression else hvd.Compression.none, gradient_predivide_factor=self._horovod_config.gradient_predivide_factor, op=op_dict.get(self._horovod_config.op), ) # Broadcast the initial variable states from rank 0 to all other processes hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Make sure everyone is synced before the all-reduce # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Loss tensor is worker specific so allreduce -- force SUM from Horovod sum_tensor = hvd.allreduce(loss_tensor, op=hvd.Sum) # Detach and divide by the world size to get the mean on each device return sum_tensor.item() / self.world_size def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Return the context to wrap the step call Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation \"\"\" # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( \"Horovod skipping synchronize as it was triggered pre grad-clip\" ) return optimizer.skip_synchronize() def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return hvd.rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return hvd.size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return hvd.is_initialized() Ancestors (in MRO) stoke.distributed.BaseDistributed abc.ABC Instance variables device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank world_size Returns current world size Methods barrier def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() clean def clean ( self ) Base clean call Nothing to do here... Returns: Type Description None None ??? example \"View Source\" def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass detach_and_sync_loss def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) grad_accum_context def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() print_device def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass setup_distributed def setup_distributed ( self ) Handles any underlying horovod setup post init Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Handles any underlying horovod setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) step_context def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Return the context to wrap the step call Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Return the context to wrap the step call Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation \"\"\" # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( \"Horovod skipping synchronize as it was triggered pre grad-clip\" ) return optimizer.skip_synchronize() wrap_distributed def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Overrides base implementation for wrapping with Horovod Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Horovod Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") op_dict = {\"Average\": hvd.Average, \"Sum\": hvd.Sum, \"Adasum\": hvd.Adasum} optimizer = hvd.DistributedOptimizer( optimizer=optimizer, named_parameters=model.named_parameters(), backward_passes_per_step=grad_accum * self._multi_loss if grad_accum is not None else self._multi_loss, compression=hvd.Compression.fp16 if self._horovod_config.compression else hvd.Compression.none, gradient_predivide_factor=self._horovod_config.gradient_predivide_factor, op=op_dict.get(self._horovod_config.op), ) # Broadcast the initial variable states from rank 0 to all other processes hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) return model, optimizer DistributedNullCPU class DistributedNullCPU ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs ) Attributes Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedNullCPU(BaseDistributed): def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init for DistributedNullCPU Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Device ID set to None as it is not needed for non distributed CPU \"\"\" super(DistributedNullCPU, self).__init__( device_id=\"cpu\", batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch CPU\", verbose=verbose, ) @property def rank(self): \"\"\"Returns current distributed rank No rank so return string of cpu \"\"\" return \"cpu\" @property def world_size(self): \"\"\"Returns current world size\"\"\" return 1 @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return True Ancestors (in MRO) stoke.distributed.BaseDistributed abc.ABC Instance variables device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank No rank so return string of cpu world_size Returns current world size Methods barrier def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass clean def clean ( self ) Base clean call Nothing to do here... Returns: Type Description None None ??? example \"View Source\" def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass detach_and_sync_loss def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item() grad_accum_context def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() print_device def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass setup_distributed def setup_distributed ( self ) Base setup distributed Does nothing as nothing needs to be wrapped Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass step_context def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() wrap_distributed def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module same as input model ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer DistributedNullGPU class DistributedNullGPU ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs ) Attributes Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedNullGPU(BaseDistributed): def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init for DistributedNullCPU Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Device ID set to the current CUDA device as there is only a single GPU being used \"\"\" super(DistributedNullGPU, self).__init__( device_id=torch.cuda.current_device(), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch GPU\", verbose=verbose, ) @property def rank(self): \"\"\"Returns current distributed rank No rank so return string of gpu \"\"\" return \"gpu\" @property def world_size(self): \"\"\"Returns current world size\"\"\" return 1 @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return True Ancestors (in MRO) stoke.distributed.BaseDistributed abc.ABC Instance variables device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank No rank so return string of gpu world_size Returns current world size Methods barrier def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass clean def clean ( self ) Base clean call Nothing to do here... Returns: Type Description None None ??? example \"View Source\" def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass detach_and_sync_loss def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item() grad_accum_context def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() print_device def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass setup_distributed def setup_distributed ( self ) Base setup distributed Does nothing as nothing needs to be wrapped Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass step_context def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() wrap_distributed def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module same as input model ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer RunnerDistEnum class RunnerDistEnum ( / , * args , ** kwargs ) ??? example \"View Source\" class RunnerDistEnum(Enum): \"\"\"Enum for building the runtime object with distributed functionality\"\"\" cpu = DistributedNullCPU gpu = DistributedNullGPU ddp = DistributedDDP horovod = DistributedHorovod deepspeed = DistributedDeepspeed Ancestors (in MRO) enum.Enum Class variables cpu ddp deepspeed gpu horovod name value","title":"Distributed"},{"location":"reference/stoke/distributed/#module-stokedistributed","text":"Handles distributed related classes -- mixin style None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles distributed related classes -- mixin style\"\"\" import os from abc import ABC, abstractmethod from contextlib import nullcontext from enum import Enum from typing import List, Optional, Tuple, Union import deepspeed as ds import horovod.torch as hvd import torch from deepspeed.utils.distributed import mpi_discovery from fairscale.optim.oss import OSS from stoke.configs import ClipGradConfig, ClipGradNormConfig from stoke.extensions import ( DistributedHandlerEnum, FairscaleFSDPExtension, FairscaleSDDPExtension, ) from stoke.utils import unrolled_print class BaseDistributed(ABC): \"\"\"Base class for distributed backends This class handles common functionality for all of the different distributed backends including setup, loss sync, gradient accumulation context, step context and various properties/attributes related to distributed frameworks Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, device_id: Optional[Union[int, str]], batch_size_per_device: int, info_rank: Union[int, List[int]], name: str, verbose: bool = True, ): \"\"\"Init for BaseDistributed class Parameters ---------- device_id: int, default: None Current device id batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information name: str name of current backend verbose: bool, default: True flag for Stoke print verbosity \"\"\" self._batch_size_per_device = batch_size_per_device self._device_id = device_id self._info_rank = info_rank self._name = name self._verbose = verbose def _print_info(self): \"\"\"Basic print of backend initialization status Returns ------- None \"\"\" self._print_device(f\"{self._name} Initialized: {self.initialized}\") def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item() def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass def _call_init(self): \"\"\"Base init call Nothing to do here... Returns ------- None \"\"\" pass def _print_device(self, msg: Union[str, List[str]]): \"\"\"Prints a str of list of strs on the currently set _info_rank Internal version of public print_device that always points to the set _info_rank Parameters ---------- msg: Union[str, List[str]] message(s) to print Returns ------- None \"\"\" self.print_device(msg=msg, rank=self._info_rank) def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass @property def device_id(self): \"\"\"Returns the current device id\"\"\" return self._device_id @property @abstractmethod def rank(self): pass @property @abstractmethod def world_size(self): pass @property @abstractmethod def initialized(self): pass class DistributedNullCPU(BaseDistributed): def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init for DistributedNullCPU Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Device ID set to None as it is not needed for non distributed CPU \"\"\" super(DistributedNullCPU, self).__init__( device_id=\"cpu\", batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch CPU\", verbose=verbose, ) @property def rank(self): \"\"\"Returns current distributed rank No rank so return string of cpu \"\"\" return \"cpu\" @property def world_size(self): \"\"\"Returns current world size\"\"\" return 1 @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return True class DistributedNullGPU(BaseDistributed): def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init for DistributedNullCPU Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Device ID set to the current CUDA device as there is only a single GPU being used \"\"\" super(DistributedNullGPU, self).__init__( device_id=torch.cuda.current_device(), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch GPU\", verbose=verbose, ) @property def rank(self): \"\"\"Returns current distributed rank No rank so return string of gpu \"\"\" return \"gpu\" @property def world_size(self): \"\"\"Returns current world size\"\"\" return 1 @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return True class DistributedDDP(BaseDistributed): \"\"\"Class for using DDP as the distributed backend This class handles common functionality for the DDP backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _ddp_config: DDPConfig Configuration object for DDP backend _ddp_handler wrapper method that will modify the DDP instance to use SDDP if flagged _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init call for DistributedDDP Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here ddp_config, sharded_config, or fully_sharded_config might be passed in \"\"\" self._ddp_config = kwargs[\"ddp_config\"] super(DistributedDDP, self).__init__( device_id=self._ddp_config.local_rank, batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch DDP\", verbose=verbose, ) # This creates the wrapper method depending on DDP or SDDP self._ddp_handler = self._create_ddp_handler(kwargs)( verbose=self._verbose, sddp_config=kwargs[\"sharded_config\"], fsdp_config=kwargs[\"fully_sharded_config\"], ddp_config=self._ddp_config, ) @staticmethod def _create_ddp_handler(kwargs: dict): \"\"\"Determines which DDP related class to use based on the kwarg config passed through Parameters ---------- kwargs: dict Extra arguments from the __init__ call Returns ------- FairscaleSDDPExtension or BaseDDP \"\"\" if kwargs[\"sharded_config\"] is not None: return DistributedHandlerEnum.sddp.value elif kwargs[\"fully_sharded_config\"] is not None: return DistributedHandlerEnum.fsdp.value else: return DistributedHandlerEnum.base.value def _call_init(self): \"\"\"Does any backend initialization work related to DDP setup Borrows code from DeepSpeed to setup DDP via openMPI https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/utils/distributed.py Returns ------- None \"\"\" # Borrowing a bit of code from deepspeed required_env = [ \"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\", ] if self._ddp_config.auto_mpi_discovery and not all( map(lambda v: v in os.environ, required_env) ): try: from mpi4py import MPI mpi_discovery(verbose=True) except ImportError as e: print( e, \": mpi4py cannot be imported -- please install Stoke with the MPI option (pip install stoke[mpi])\", ) # Initialize call for DDP torch.distributed.init_process_group( backend=self._ddp_config.backend, init_method=self._ddp_config.init_method ) def setup_distributed(self): \"\"\"Handles any underlying DDP setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) # Call the init fnc here after device id is set self._call_init() def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" self._print_device(f\"{self._name} Class: {type(self._ddp_handler).__name__}\") # Print info if verbose if self._verbose: self._print_info() self._print_device( [ f\"{self._name} -- Device ID: {torch.cuda.current_device()}\", f\"{self._name} -- Rank: {self.rank}\", ] ) if self._ddp_config.convert_to_sync_batch_norm: self.print_device( f\"Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...\" ) torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model) if self._verbose and isinstance( self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension) ): self._print_device( f\"Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}\" ) # Pass through to the handler for DDP wrappers model, optimizer = self._ddp_handler.handle_ddp( model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank ) return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Make sure everyone is synced before calling all reduce torch.distributed.barrier() # Loss tensor is worker specific so all_reduce (and SUM) torch.distributed.all_reduce(loss_tensor) # Detach and divide by the world size to get the mean on each device return loss_tensor.item() / self.world_size def grad_accum_context(self, model: torch.nn.Module): \"\"\"Return the context to wrap the gradient accumulation steps DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s)) SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object Returns ------- no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient accumulation else nullcontext \"\"\" if self._verbose and self._ddp_config.no_sync: self._print_device(\"DDP Using no sync context\") context = model.no_sync() if self._ddp_config.no_sync else nullcontext() return context def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return torch.distributed.get_rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return torch.distributed.get_world_size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return torch.distributed.is_initialized() def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group() class DistributedDeepspeed(BaseDistributed): \"\"\"Class for using Deepspeed as the distributed backend This class handles common functionality for the deepspeed backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _deepspeed_config: DeepspeedConfig Configuration object for Deepspeed backend _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init call for DistributedDeepspeed Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here deepspeed_config, grad_accum_steps or grad_clip might be passed in \"\"\" self._deepspeed_config = kwargs[\"deepspeed_config\"] # Call init first to pass local rank to super self._call_init() # Forward device to super -- should be set from MPI lookup that is called super(DistributedDeepspeed, self).__init__( device_id=int(os.environ[\"LOCAL_RANK\"]), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"Deepspeed\", verbose=verbose, ) self._deepspeed_init_config = self._handle_deepspeed_configs( grad_accum_steps=kwargs[\"grad_accum_steps\"], grad_clip=kwargs[\"grad_clip\"] ) def _call_init(self): \"\"\"Does any backend initialization work related to deepspeed setup Returns ------- None \"\"\" ds.init_distributed( dist_backend=self._deepspeed_config.dist_backend, auto_mpi_discovery=self._deepspeed_config.auto_mpi_discovery, distributed_port=self._deepspeed_config.distributed_port, verbose=self._deepspeed_config.verbose, init_method=self._deepspeed_config.init_method, ) def setup_distributed(self): \"\"\"Handles any underlying deepspeed setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Deepspeed Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") model, optimizer, _, _ = ds.initialize( model=model, optimizer=optimizer, model_parameters=filter(lambda p: p.requires_grad, model.parameters()), config_params=self._deepspeed_init_config, ) return model, optimizer def _handle_deepspeed_configs( self, grad_accum_steps: int, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], ): \"\"\"Handles building the dictionary of configs that the deepspeed initialize call expects https://www.deepspeed.ai/docs/config-json/ Parameters ---------- grad_accum_steps: int number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None gradient clipping config objects Returns ------- dict All deepspeed parameters merged together from individual pieces \"\"\" # empty dict to start ds_config = {} # Map batch size stuff -- need to define 2/3 ds_config.update(self._map_ds_batch_configs(grad_accum_steps=grad_accum_steps)) # Skip optimizer & skip scheduler # Map communication ds_config.update(self._map_ds_communication_configs()) # Map FP16 and add enabled flag if selected ds_config.update(self._map_ds_fp16_configs()) # Map grad clipping ds_config.update(self._map_ds_grad_clip_configs(grad_clip=grad_clip)) # Map zero -- internally map param offloading and optimizer offloading to zero ds_config.update(self._map_ds_zero_configs()) # Map aio ds_config.update(self._map_ds_aio_configs()) # Map logging ds_config.update(self._map_ds_logging_configs()) # Map flops -- enabled ds_config.update(self._map_ds_flops_configs()) # Map activation checkpointing ds_config.update(self._map_ds_activation_checkpointing_configs()) # Map tensorboard ds_config.update(self._map_ds_tensorboard_config()) # Map PLD ds_config.update(self._map_ds_pld_config()) return ds_config def _map_ds_pld_config(self): \"\"\"Maps progressive layer drop parameters https://www.deepspeed.ai/tutorials/progressive_layer_dropping/ https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L293 Returns ------- dict pld parameters or enabled false dict \"\"\" if self._deepspeed_config.progressive_layer_drop is not None: map_dict = { v.name: getattr(self._deepspeed_config.progressive_layer_drop, v.name) for v in self._deepspeed_config.progressive_layer_drop.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"progressive_layer_drop\": map_dict} else: return {\"progressive_layer_drop\": {\"enabled\": False}} def _map_ds_tensorboard_config(self): \"\"\"Maps tensorboard related parameters https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L268 Returns ------- dict tensorboard parameters or enabled false dict \"\"\" if self._deepspeed_config.tensorboard is not None: map_dict = { v.name: getattr(self._deepspeed_config.tensorboard, v.name) for v in self._deepspeed_config.tensorboard.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"tensorboard\": map_dict} else: return {\"tensorboard\": {\"enabled\": False}} def _map_ds_grad_clip_configs( self, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] ): \"\"\"Maps grad clipping related parameters https://www.deepspeed.ai/docs/config-json/#gradient-clipping Parameters ---------- grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None gradient clipping config objects Returns ------- dict gradient clipping parameters or empty dict \"\"\" if grad_clip is not None: if isinstance(grad_clip, ClipGradNormConfig): return {\"gradient_clipping\": grad_clip.max_norm} else: raise ValueError( f\"Deepspeed does not currently support \" f'{type(grad_clip).__name__.replace(\"Config\", \"\")}' ) else: return {} def _map_ds_logging_configs(self): \"\"\"Maps logging related parameters https://www.deepspeed.ai/docs/config-json/#logging Returns ------- dict logging parameters or empty dict \"\"\" return { \"steps_per_print\": self._deepspeed_config.steps_per_print, \"dump_state\": self._deepspeed_config.dump_state, \"wall_clock_breakdown\": self._deepspeed_config.wall_clock_breakdown, } def _map_ds_activation_checkpointing_configs(self): \"\"\"Maps activation checkpointing related parameters https://www.deepspeed.ai/docs/config-json/#activation-checkpointing Returns ------- dict activation checkpointing parameters or empty dict \"\"\" if self._deepspeed_config.activation_checkpointing is not None: map_dict = { v.name: getattr(self._deepspeed_config.activation_checkpointing, v.name) for v in self._deepspeed_config.activation_checkpointing.__attrs_attrs__ } return {\"activation_checkpointing\": map_dict} else: return {} def _map_ds_flops_configs(self): \"\"\"Maps flops related parameters https://www.deepspeed.ai/docs/config-json/#flops-profiler Returns ------- dict flops parameters or enabled false dict \"\"\" if self._deepspeed_config.flops_profiler is not None: map_dict = { v.name: getattr(self._deepspeed_config.flops_profiler, v.name) for v in self._deepspeed_config.flops_profiler.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"flops_profiler\": map_dict} else: return {\"flops_profiler\": {\"enabled\": False}} def _map_ds_aio_configs(self): \"\"\"Maps async i/o related parameters https://www.deepspeed.ai/docs/config-json/#asynchronous-io Returns ------- dict async i/o parameters or empty dict \"\"\" if self._deepspeed_config.aio is not None: map_dict = { v.name: getattr(self._deepspeed_config.aio, v.name) for v in self._deepspeed_config.aio.__attrs_attrs__ } return {\"aio\": map_dict} else: return {} def _map_ds_zero_configs(self): \"\"\"Maps ZeRO related parameters https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training Returns ------- dict ZeRO related parameters \"\"\" map_dict = {} for v in self._deepspeed_config.zero_optimization.__attrs_attrs__: if v.name == \"offload_optimizer\": map_dict.update(self._map_ds_offload_optimizer_configs()) elif v.name == \"offload_param\": map_dict.update(self._map_ds_offload_param_configs()) # Just map the rest since the name:value is correct else: map_dict.update( {v.name: getattr(self._deepspeed_config.zero_optimization, v.name)} ) # Default overlap com to True for ZeRO stage 3 map_dict[\"overlap_comm\"] = ( True if map_dict[\"stage\"] == 3 else map_dict[\"overlap_comm\"] ) return {\"zero_optimization\": map_dict} def _map_ds_offload_param_configs(self): \"\"\"Maps ZeRO parameter offload parameters https://www.deepspeed.ai/docs/config-json/#parameter-offloading Returns ------- dict ZeRO offload parameter parameters \"\"\" # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct if self._deepspeed_config.zero_optimization.offload_param is not None: map_dict = { v.name: getattr( self._deepspeed_config.zero_optimization.offload_param, v.name ) for v in self._deepspeed_config.zero_optimization.offload_param.__attrs_attrs__ } return {\"offload_param\": map_dict} else: return {\"offload_param\": None} def _map_ds_offload_optimizer_configs(self): \"\"\"Maps ZeRO optimizer offload parameters https://www.deepspeed.ai/docs/config-json/#optimizer-offloading Returns ------- dict ZeRO offload optimizer parameters \"\"\" # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct if self._deepspeed_config.zero_optimization.offload_optimizer is not None: map_dict = { v.name: getattr( self._deepspeed_config.zero_optimization.offload_optimizer, v.name ) for v in self._deepspeed_config.zero_optimization.offload_optimizer.__attrs_attrs__ } # Set some post init values map_dict[\"pipeline\"] = ( map_dict[\"pipeline_read\"] or map_dict[\"pipeline_write\"] ) return {\"offload_optimizer\": map_dict} else: return {\"offload_optimizer\": None} def _map_ds_fp16_configs(self): \"\"\"Maps FP16 related parameters https://www.deepspeed.ai/docs/config-json/#fp16-training-options Returns ------- dict fp16 related parameters or enabled false dict \"\"\" if self._deepspeed_config.fp16 is not None: # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct map_dict = { v.name: getattr(self._deepspeed_config.fp16, v.name) for v in self._deepspeed_config.fp16.__attrs_attrs__ } # Add the enabled flag map_dict.update({\"enabled\": True}) return {\"fp16\": map_dict} else: return {\"fp16\": {\"enabled\": False}} def _map_ds_batch_configs(self, grad_accum_steps: int): \"\"\"Maps batch size related parameters https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters Parameters ---------- grad_accum_steps: int number of gradient accumulation steps Returns ------- dict batch size related parameters \"\"\" # Need to define 2/3 return { \"train_micro_batch_size_per_gpu\": self._batch_size_per_device, \"gradient_accumulation_steps\": grad_accum_steps, } def _map_ds_communication_configs(self): \"\"\"Maps communication related parameters https://www.deepspeed.ai/docs/config-json/#communication-options Returns ------- dict communication related parameters \"\"\" return { \"fp32_allreduce\": self._deepspeed_config.fp32_allreduce, \"gradient_predivide_factor\": self._deepspeed_config.gradient_predivide_factor, \"prescale_gradients:\": self._deepspeed_config.prescale_gradients, \"sparse_gradients\": self._deepspeed_config.sparse_gradients, } def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Loss tensor is worker specific so all_reduce (and SUM) torch.distributed.all_reduce(loss_tensor) # Detach and divide by the world size to get the mean on each device return loss_tensor.item() / self.world_size def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return torch.distributed.get_rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return torch.distributed.get_world_size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return torch.distributed.is_initialized() def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group() class DistributedHorovod(BaseDistributed): \"\"\"Class for using Horovod as the distributed backend This class handles common functionality for the horovod backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _device_id: int, default: None Current device id _horovod_config: HorovodConfig Configuration object for Horovod backend _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\" Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here horovod_config might be passed in \"\"\" # Grab the config self._horovod_config = kwargs[\"horovod_config\"] # Initialize first so the local rank call cal be forwarded to super self._call_init() super(DistributedHorovod, self).__init__( device_id=hvd.local_rank(), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"Horovod\", verbose=verbose, ) self._multi_loss = ( len(kwargs[\"loss\"]) if isinstance(kwargs[\"loss\"], (list, tuple)) else 1 ) def _call_init(self): \"\"\"Does any backend initialization work related to horovod setup Returns ------- None \"\"\" hvd.init() def _hvd_convert_to_sync_batch_norm( self, module: torch.nn.Module, process_group=None ): \"\"\"Replaces all BatchNorm*D layers with horovod.torch.SyncBatchNorm layers https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm https://nvidia.github.io/apex/_modules/apex/parallel.html#convert_syncbn_model Parameters ---------- module: torch.nn.Module current model object process_group: default: None process group to scope synchronization, default is the whole world Returns ------- module_output: torch.nn.Module modified version of model with all BatchNorm*D layers replaced with horovod.torch.SyncBatchNorm layers Notes ----- Borrows heavily from the current torch convert_sync_batchnorm and apex convert_syncbn_model implementations only changing the underlying layer type to use the hvd implementation \"\"\" module_output = module if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): module_output = hvd.SyncBatchNorm( num_features=module.num_features, eps=module.eps, momentum=module.momentum, affine=module.affine, track_running_stats=module.track_running_stats, ) # Handle the copy of affine vars if affine if module.affine: with torch.no_grad(): module_output.weight = module.weight module_output.bias = module.bias # Handle the swap of running stats module_output.running_mean = module.running_mean module_output.running_var = module.running_var # Iterate recursively and replace for name, child in module.named_children(): module_output.add_module( name=name, module=self._hvd_convert_to_sync_batch_norm( module=child, process_group=process_group ), ) # delete and return del module return module_output def setup_distributed(self): \"\"\"Handles any underlying horovod setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Horovod Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") op_dict = {\"Average\": hvd.Average, \"Sum\": hvd.Sum, \"Adasum\": hvd.Adasum} optimizer = hvd.DistributedOptimizer( optimizer=optimizer, named_parameters=model.named_parameters(), backward_passes_per_step=grad_accum * self._multi_loss if grad_accum is not None else self._multi_loss, compression=hvd.Compression.fp16 if self._horovod_config.compression else hvd.Compression.none, gradient_predivide_factor=self._horovod_config.gradient_predivide_factor, op=op_dict.get(self._horovod_config.op), ) # Broadcast the initial variable states from rank 0 to all other processes hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Make sure everyone is synced before the all-reduce # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Loss tensor is worker specific so allreduce -- force SUM from Horovod sum_tensor = hvd.allreduce(loss_tensor, op=hvd.Sum) # Detach and divide by the world size to get the mean on each device return sum_tensor.item() / self.world_size def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Return the context to wrap the step call Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation \"\"\" # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( \"Horovod skipping synchronize as it was triggered pre grad-clip\" ) return optimizer.skip_synchronize() def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return hvd.rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return hvd.size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return hvd.is_initialized() class RunnerDistEnum(Enum): \"\"\"Enum for building the runtime object with distributed functionality\"\"\" cpu = DistributedNullCPU gpu = DistributedNullGPU ddp = DistributedDDP horovod = DistributedHorovod deepspeed = DistributedDeepspeed","title":"Module stoke.distributed"},{"location":"reference/stoke/distributed/#classes","text":"","title":"Classes"},{"location":"reference/stoke/distributed/#basedistributed","text":"class BaseDistributed ( device_id : Union [ int , str , NoneType ], batch_size_per_device : int , info_rank : Union [ int , List [ int ]], name : str , verbose : bool = True )","title":"BaseDistributed"},{"location":"reference/stoke/distributed/#attributes","text":"Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class BaseDistributed(ABC): \"\"\"Base class for distributed backends This class handles common functionality for all of the different distributed backends including setup, loss sync, gradient accumulation context, step context and various properties/attributes related to distributed frameworks Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, device_id: Optional[Union[int, str]], batch_size_per_device: int, info_rank: Union[int, List[int]], name: str, verbose: bool = True, ): \"\"\"Init for BaseDistributed class Parameters ---------- device_id: int, default: None Current device id batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information name: str name of current backend verbose: bool, default: True flag for Stoke print verbosity \"\"\" self._batch_size_per_device = batch_size_per_device self._device_id = device_id self._info_rank = info_rank self._name = name self._verbose = verbose def _print_info(self): \"\"\"Basic print of backend initialization status Returns ------- None \"\"\" self._print_device(f\"{self._name} Initialized: {self.initialized}\") def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item() def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext() def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext() def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass def _call_init(self): \"\"\"Base init call Nothing to do here... Returns ------- None \"\"\" pass def _print_device(self, msg: Union[str, List[str]]): \"\"\"Prints a str of list of strs on the currently set _info_rank Internal version of public print_device that always points to the set _info_rank Parameters ---------- msg: Union[str, List[str]] message(s) to print Returns ------- None \"\"\" self.print_device(msg=msg, rank=self._info_rank) def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass @property def device_id(self): \"\"\"Returns the current device id\"\"\" return self._device_id @property @abstractmethod def rank(self): pass @property @abstractmethod def world_size(self): pass @property @abstractmethod def initialized(self): pass","title":"Attributes"},{"location":"reference/stoke/distributed/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/distributed/#descendants","text":"stoke.distributed.DistributedNullCPU stoke.distributed.DistributedNullGPU stoke.distributed.DistributedDDP stoke.distributed.DistributedDeepspeed stoke.distributed.DistributedHorovod","title":"Descendants"},{"location":"reference/stoke/distributed/#instance-variables","text":"device_id Returns the current device id initialized rank world_size","title":"Instance variables"},{"location":"reference/stoke/distributed/#methods","text":"","title":"Methods"},{"location":"reference/stoke/distributed/#barrier","text":"def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass","title":"barrier"},{"location":"reference/stoke/distributed/#clean","text":"def clean ( self ) Base clean call Nothing to do here... Returns: Type Description None None ??? example \"View Source\" def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass","title":"clean"},{"location":"reference/stoke/distributed/#detach_and_sync_loss","text":"def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item()","title":"detach_and_sync_loss"},{"location":"reference/stoke/distributed/#grad_accum_context","text":"def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"grad_accum_context"},{"location":"reference/stoke/distributed/#print_device","text":"def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass","title":"print_device"},{"location":"reference/stoke/distributed/#setup_distributed","text":"def setup_distributed ( self ) Base setup distributed Does nothing as nothing needs to be wrapped Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass","title":"setup_distributed"},{"location":"reference/stoke/distributed/#step_context","text":"def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"step_context"},{"location":"reference/stoke/distributed/#wrap_distributed","text":"def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module same as input model ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer","title":"wrap_distributed"},{"location":"reference/stoke/distributed/#distributedddp","text":"class DistributedDDP ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs )","title":"DistributedDDP"},{"location":"reference/stoke/distributed/#attributes_1","text":"Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _ddp_config DDPConfig Configuration object for DDP backend None _ddp_handler None wrapper method that will modify the DDP instance to use SDDP if flagged None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedDDP(BaseDistributed): \"\"\"Class for using DDP as the distributed backend This class handles common functionality for the DDP backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _ddp_config: DDPConfig Configuration object for DDP backend _ddp_handler wrapper method that will modify the DDP instance to use SDDP if flagged _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init call for DistributedDDP Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here ddp_config, sharded_config, or fully_sharded_config might be passed in \"\"\" self._ddp_config = kwargs[\"ddp_config\"] super(DistributedDDP, self).__init__( device_id=self._ddp_config.local_rank, batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch DDP\", verbose=verbose, ) # This creates the wrapper method depending on DDP or SDDP self._ddp_handler = self._create_ddp_handler(kwargs)( verbose=self._verbose, sddp_config=kwargs[\"sharded_config\"], fsdp_config=kwargs[\"fully_sharded_config\"], ddp_config=self._ddp_config, ) @staticmethod def _create_ddp_handler(kwargs: dict): \"\"\"Determines which DDP related class to use based on the kwarg config passed through Parameters ---------- kwargs: dict Extra arguments from the __init__ call Returns ------- FairscaleSDDPExtension or BaseDDP \"\"\" if kwargs[\"sharded_config\"] is not None: return DistributedHandlerEnum.sddp.value elif kwargs[\"fully_sharded_config\"] is not None: return DistributedHandlerEnum.fsdp.value else: return DistributedHandlerEnum.base.value def _call_init(self): \"\"\"Does any backend initialization work related to DDP setup Borrows code from DeepSpeed to setup DDP via openMPI https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/utils/distributed.py Returns ------- None \"\"\" # Borrowing a bit of code from deepspeed required_env = [ \"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\", ] if self._ddp_config.auto_mpi_discovery and not all( map(lambda v: v in os.environ, required_env) ): try: from mpi4py import MPI mpi_discovery(verbose=True) except ImportError as e: print( e, \": mpi4py cannot be imported -- please install Stoke with the MPI option (pip install stoke[mpi])\", ) # Initialize call for DDP torch.distributed.init_process_group( backend=self._ddp_config.backend, init_method=self._ddp_config.init_method ) def setup_distributed(self): \"\"\"Handles any underlying DDP setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) # Call the init fnc here after device id is set self._call_init() def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" self._print_device(f\"{self._name} Class: {type(self._ddp_handler).__name__}\") # Print info if verbose if self._verbose: self._print_info() self._print_device( [ f\"{self._name} -- Device ID: {torch.cuda.current_device()}\", f\"{self._name} -- Rank: {self.rank}\", ] ) if self._ddp_config.convert_to_sync_batch_norm: self.print_device( f\"Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...\" ) torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model) if self._verbose and isinstance( self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension) ): self._print_device( f\"Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}\" ) # Pass through to the handler for DDP wrappers model, optimizer = self._ddp_handler.handle_ddp( model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank ) return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Make sure everyone is synced before calling all reduce torch.distributed.barrier() # Loss tensor is worker specific so all_reduce (and SUM) torch.distributed.all_reduce(loss_tensor) # Detach and divide by the world size to get the mean on each device return loss_tensor.item() / self.world_size def grad_accum_context(self, model: torch.nn.Module): \"\"\"Return the context to wrap the gradient accumulation steps DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s)) SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object Returns ------- no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient accumulation else nullcontext \"\"\" if self._verbose and self._ddp_config.no_sync: self._print_device(\"DDP Using no sync context\") context = model.no_sync() if self._ddp_config.no_sync else nullcontext() return context def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return torch.distributed.get_rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return torch.distributed.get_world_size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return torch.distributed.is_initialized() def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group()","title":"Attributes"},{"location":"reference/stoke/distributed/#ancestors-in-mro_1","text":"stoke.distributed.BaseDistributed abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/distributed/#instance-variables_1","text":"device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank world_size Returns current world size","title":"Instance variables"},{"location":"reference/stoke/distributed/#methods_1","text":"","title":"Methods"},{"location":"reference/stoke/distributed/#barrier_1","text":"def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier()","title":"barrier"},{"location":"reference/stoke/distributed/#clean_1","text":"def clean ( self ) Cleans up at the end of a DDP run ??? example \"View Source\" def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group()","title":"clean"},{"location":"reference/stoke/distributed/#detach_and_sync_loss_1","text":"def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device)","title":"detach_and_sync_loss"},{"location":"reference/stoke/distributed/#grad_accum_context_1","text":"def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Return the context to wrap the gradient accumulation steps DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s)) SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Return the context to wrap the gradient accumulation steps DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s)) SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object Returns ------- no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient accumulation else nullcontext \"\"\" if self._verbose and self._ddp_config.no_sync: self._print_device(\"DDP Using no sync context\") context = model.no_sync() if self._ddp_config.no_sync else nullcontext() return context","title":"grad_accum_context"},{"location":"reference/stoke/distributed/#print_device_1","text":"def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass","title":"print_device"},{"location":"reference/stoke/distributed/#setup_distributed_1","text":"def setup_distributed ( self ) Handles any underlying DDP setup post init Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Handles any underlying DDP setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) # Call the init fnc here after device id is set self._call_init()","title":"setup_distributed"},{"location":"reference/stoke/distributed/#step_context_1","text":"def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"step_context"},{"location":"reference/stoke/distributed/#wrap_distributed_1","text":"def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" self._print_device(f\"{self._name} Class: {type(self._ddp_handler).__name__}\") # Print info if verbose if self._verbose: self._print_info() self._print_device( [ f\"{self._name} -- Device ID: {torch.cuda.current_device()}\", f\"{self._name} -- Rank: {self.rank}\", ] ) if self._ddp_config.convert_to_sync_batch_norm: self.print_device( f\"Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...\" ) torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model) if self._verbose and isinstance( self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension) ): self._print_device( f\"Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}\" ) # Pass through to the handler for DDP wrappers model, optimizer = self._ddp_handler.handle_ddp( model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank ) return model, optimizer","title":"wrap_distributed"},{"location":"reference/stoke/distributed/#distributeddeepspeed","text":"class DistributedDeepspeed ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs )","title":"DistributedDeepspeed"},{"location":"reference/stoke/distributed/#attributes_2","text":"Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _deepspeed_config DeepspeedConfig Configuration object for Deepspeed backend None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedDeepspeed(BaseDistributed): \"\"\"Class for using Deepspeed as the distributed backend This class handles common functionality for the deepspeed backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _deepspeed_config: DeepspeedConfig Configuration object for Deepspeed backend _device_id: int, default: None Current device id _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init call for DistributedDeepspeed Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here deepspeed_config, grad_accum_steps or grad_clip might be passed in \"\"\" self._deepspeed_config = kwargs[\"deepspeed_config\"] # Call init first to pass local rank to super self._call_init() # Forward device to super -- should be set from MPI lookup that is called super(DistributedDeepspeed, self).__init__( device_id=int(os.environ[\"LOCAL_RANK\"]), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"Deepspeed\", verbose=verbose, ) self._deepspeed_init_config = self._handle_deepspeed_configs( grad_accum_steps=kwargs[\"grad_accum_steps\"], grad_clip=kwargs[\"grad_clip\"] ) def _call_init(self): \"\"\"Does any backend initialization work related to deepspeed setup Returns ------- None \"\"\" ds.init_distributed( dist_backend=self._deepspeed_config.dist_backend, auto_mpi_discovery=self._deepspeed_config.auto_mpi_discovery, distributed_port=self._deepspeed_config.distributed_port, verbose=self._deepspeed_config.verbose, init_method=self._deepspeed_config.init_method, ) def setup_distributed(self): \"\"\"Handles any underlying deepspeed setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Deepspeed Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") model, optimizer, _, _ = ds.initialize( model=model, optimizer=optimizer, model_parameters=filter(lambda p: p.requires_grad, model.parameters()), config_params=self._deepspeed_init_config, ) return model, optimizer def _handle_deepspeed_configs( self, grad_accum_steps: int, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], ): \"\"\"Handles building the dictionary of configs that the deepspeed initialize call expects https://www.deepspeed.ai/docs/config-json/ Parameters ---------- grad_accum_steps: int number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None gradient clipping config objects Returns ------- dict All deepspeed parameters merged together from individual pieces \"\"\" # empty dict to start ds_config = {} # Map batch size stuff -- need to define 2/3 ds_config.update(self._map_ds_batch_configs(grad_accum_steps=grad_accum_steps)) # Skip optimizer & skip scheduler # Map communication ds_config.update(self._map_ds_communication_configs()) # Map FP16 and add enabled flag if selected ds_config.update(self._map_ds_fp16_configs()) # Map grad clipping ds_config.update(self._map_ds_grad_clip_configs(grad_clip=grad_clip)) # Map zero -- internally map param offloading and optimizer offloading to zero ds_config.update(self._map_ds_zero_configs()) # Map aio ds_config.update(self._map_ds_aio_configs()) # Map logging ds_config.update(self._map_ds_logging_configs()) # Map flops -- enabled ds_config.update(self._map_ds_flops_configs()) # Map activation checkpointing ds_config.update(self._map_ds_activation_checkpointing_configs()) # Map tensorboard ds_config.update(self._map_ds_tensorboard_config()) # Map PLD ds_config.update(self._map_ds_pld_config()) return ds_config def _map_ds_pld_config(self): \"\"\"Maps progressive layer drop parameters https://www.deepspeed.ai/tutorials/progressive_layer_dropping/ https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L293 Returns ------- dict pld parameters or enabled false dict \"\"\" if self._deepspeed_config.progressive_layer_drop is not None: map_dict = { v.name: getattr(self._deepspeed_config.progressive_layer_drop, v.name) for v in self._deepspeed_config.progressive_layer_drop.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"progressive_layer_drop\": map_dict} else: return {\"progressive_layer_drop\": {\"enabled\": False}} def _map_ds_tensorboard_config(self): \"\"\"Maps tensorboard related parameters https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L268 Returns ------- dict tensorboard parameters or enabled false dict \"\"\" if self._deepspeed_config.tensorboard is not None: map_dict = { v.name: getattr(self._deepspeed_config.tensorboard, v.name) for v in self._deepspeed_config.tensorboard.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"tensorboard\": map_dict} else: return {\"tensorboard\": {\"enabled\": False}} def _map_ds_grad_clip_configs( self, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] ): \"\"\"Maps grad clipping related parameters https://www.deepspeed.ai/docs/config-json/#gradient-clipping Parameters ---------- grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None gradient clipping config objects Returns ------- dict gradient clipping parameters or empty dict \"\"\" if grad_clip is not None: if isinstance(grad_clip, ClipGradNormConfig): return {\"gradient_clipping\": grad_clip.max_norm} else: raise ValueError( f\"Deepspeed does not currently support \" f'{type(grad_clip).__name__.replace(\"Config\", \"\")}' ) else: return {} def _map_ds_logging_configs(self): \"\"\"Maps logging related parameters https://www.deepspeed.ai/docs/config-json/#logging Returns ------- dict logging parameters or empty dict \"\"\" return { \"steps_per_print\": self._deepspeed_config.steps_per_print, \"dump_state\": self._deepspeed_config.dump_state, \"wall_clock_breakdown\": self._deepspeed_config.wall_clock_breakdown, } def _map_ds_activation_checkpointing_configs(self): \"\"\"Maps activation checkpointing related parameters https://www.deepspeed.ai/docs/config-json/#activation-checkpointing Returns ------- dict activation checkpointing parameters or empty dict \"\"\" if self._deepspeed_config.activation_checkpointing is not None: map_dict = { v.name: getattr(self._deepspeed_config.activation_checkpointing, v.name) for v in self._deepspeed_config.activation_checkpointing.__attrs_attrs__ } return {\"activation_checkpointing\": map_dict} else: return {} def _map_ds_flops_configs(self): \"\"\"Maps flops related parameters https://www.deepspeed.ai/docs/config-json/#flops-profiler Returns ------- dict flops parameters or enabled false dict \"\"\" if self._deepspeed_config.flops_profiler is not None: map_dict = { v.name: getattr(self._deepspeed_config.flops_profiler, v.name) for v in self._deepspeed_config.flops_profiler.__attrs_attrs__ } map_dict.update({\"enabled\": True}) return {\"flops_profiler\": map_dict} else: return {\"flops_profiler\": {\"enabled\": False}} def _map_ds_aio_configs(self): \"\"\"Maps async i/o related parameters https://www.deepspeed.ai/docs/config-json/#asynchronous-io Returns ------- dict async i/o parameters or empty dict \"\"\" if self._deepspeed_config.aio is not None: map_dict = { v.name: getattr(self._deepspeed_config.aio, v.name) for v in self._deepspeed_config.aio.__attrs_attrs__ } return {\"aio\": map_dict} else: return {} def _map_ds_zero_configs(self): \"\"\"Maps ZeRO related parameters https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training Returns ------- dict ZeRO related parameters \"\"\" map_dict = {} for v in self._deepspeed_config.zero_optimization.__attrs_attrs__: if v.name == \"offload_optimizer\": map_dict.update(self._map_ds_offload_optimizer_configs()) elif v.name == \"offload_param\": map_dict.update(self._map_ds_offload_param_configs()) # Just map the rest since the name:value is correct else: map_dict.update( {v.name: getattr(self._deepspeed_config.zero_optimization, v.name)} ) # Default overlap com to True for ZeRO stage 3 map_dict[\"overlap_comm\"] = ( True if map_dict[\"stage\"] == 3 else map_dict[\"overlap_comm\"] ) return {\"zero_optimization\": map_dict} def _map_ds_offload_param_configs(self): \"\"\"Maps ZeRO parameter offload parameters https://www.deepspeed.ai/docs/config-json/#parameter-offloading Returns ------- dict ZeRO offload parameter parameters \"\"\" # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct if self._deepspeed_config.zero_optimization.offload_param is not None: map_dict = { v.name: getattr( self._deepspeed_config.zero_optimization.offload_param, v.name ) for v in self._deepspeed_config.zero_optimization.offload_param.__attrs_attrs__ } return {\"offload_param\": map_dict} else: return {\"offload_param\": None} def _map_ds_offload_optimizer_configs(self): \"\"\"Maps ZeRO optimizer offload parameters https://www.deepspeed.ai/docs/config-json/#optimizer-offloading Returns ------- dict ZeRO offload optimizer parameters \"\"\" # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct if self._deepspeed_config.zero_optimization.offload_optimizer is not None: map_dict = { v.name: getattr( self._deepspeed_config.zero_optimization.offload_optimizer, v.name ) for v in self._deepspeed_config.zero_optimization.offload_optimizer.__attrs_attrs__ } # Set some post init values map_dict[\"pipeline\"] = ( map_dict[\"pipeline_read\"] or map_dict[\"pipeline_write\"] ) return {\"offload_optimizer\": map_dict} else: return {\"offload_optimizer\": None} def _map_ds_fp16_configs(self): \"\"\"Maps FP16 related parameters https://www.deepspeed.ai/docs/config-json/#fp16-training-options Returns ------- dict fp16 related parameters or enabled false dict \"\"\" if self._deepspeed_config.fp16 is not None: # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct map_dict = { v.name: getattr(self._deepspeed_config.fp16, v.name) for v in self._deepspeed_config.fp16.__attrs_attrs__ } # Add the enabled flag map_dict.update({\"enabled\": True}) return {\"fp16\": map_dict} else: return {\"fp16\": {\"enabled\": False}} def _map_ds_batch_configs(self, grad_accum_steps: int): \"\"\"Maps batch size related parameters https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters Parameters ---------- grad_accum_steps: int number of gradient accumulation steps Returns ------- dict batch size related parameters \"\"\" # Need to define 2/3 return { \"train_micro_batch_size_per_gpu\": self._batch_size_per_device, \"gradient_accumulation_steps\": grad_accum_steps, } def _map_ds_communication_configs(self): \"\"\"Maps communication related parameters https://www.deepspeed.ai/docs/config-json/#communication-options Returns ------- dict communication related parameters \"\"\" return { \"fp32_allreduce\": self._deepspeed_config.fp32_allreduce, \"gradient_predivide_factor\": self._deepspeed_config.gradient_predivide_factor, \"prescale_gradients:\": self._deepspeed_config.prescale_gradients, \"sparse_gradients\": self._deepspeed_config.sparse_gradients, } def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Loss tensor is worker specific so all_reduce (and SUM) torch.distributed.all_reduce(loss_tensor) # Detach and divide by the world size to get the mean on each device return loss_tensor.item() / self.world_size def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return torch.distributed.get_rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return torch.distributed.get_world_size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return torch.distributed.is_initialized() def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group()","title":"Attributes"},{"location":"reference/stoke/distributed/#ancestors-in-mro_2","text":"stoke.distributed.BaseDistributed abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/distributed/#instance-variables_2","text":"device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank world_size Returns current world size","title":"Instance variables"},{"location":"reference/stoke/distributed/#methods_2","text":"","title":"Methods"},{"location":"reference/stoke/distributed/#barrier_2","text":"def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" torch.distributed.barrier()","title":"barrier"},{"location":"reference/stoke/distributed/#clean_2","text":"def clean ( self ) Cleans up at the end of a DDP run ??? example \"View Source\" def clean(self): \"\"\"Cleans up at the end of a DDP run\"\"\" torch.distributed.destroy_process_group()","title":"clean"},{"location":"reference/stoke/distributed/#detach_and_sync_loss_2","text":"def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device)","title":"detach_and_sync_loss"},{"location":"reference/stoke/distributed/#grad_accum_context_2","text":"def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"grad_accum_context"},{"location":"reference/stoke/distributed/#print_device_2","text":"def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass","title":"print_device"},{"location":"reference/stoke/distributed/#setup_distributed_2","text":"def setup_distributed ( self ) Handles any underlying deepspeed setup post init Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Handles any underlying deepspeed setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id)","title":"setup_distributed"},{"location":"reference/stoke/distributed/#step_context_2","text":"def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"step_context"},{"location":"reference/stoke/distributed/#wrap_distributed_2","text":"def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Overrides base implementation for wrapping with Deepspeed Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Deepspeed Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") model, optimizer, _, _ = ds.initialize( model=model, optimizer=optimizer, model_parameters=filter(lambda p: p.requires_grad, model.parameters()), config_params=self._deepspeed_init_config, ) return model, optimizer","title":"wrap_distributed"},{"location":"reference/stoke/distributed/#distributedhorovod","text":"class DistributedHorovod ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs )","title":"DistributedHorovod"},{"location":"reference/stoke/distributed/#attributes_3","text":"Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _device_id int, default: None Current device id None _horovod_config HorovodConfig Configuration object for Horovod backend None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedHorovod(BaseDistributed): \"\"\"Class for using Horovod as the distributed backend This class handles common functionality for the horovod backend including setup, loss sync, gradient accumulation context, step context and various properties/attributes Attributes ---------- device_id initialized rank world_size _batch_size_per_device: int batch size per device or for non-distributed the overall batch size _device_id: int, default: None Current device id _horovod_config: HorovodConfig Configuration object for Horovod backend _info_rank: Union[int, List[int]] Which device(s) to print information _name: str name of current backend _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\" Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here horovod_config might be passed in \"\"\" # Grab the config self._horovod_config = kwargs[\"horovod_config\"] # Initialize first so the local rank call cal be forwarded to super self._call_init() super(DistributedHorovod, self).__init__( device_id=hvd.local_rank(), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"Horovod\", verbose=verbose, ) self._multi_loss = ( len(kwargs[\"loss\"]) if isinstance(kwargs[\"loss\"], (list, tuple)) else 1 ) def _call_init(self): \"\"\"Does any backend initialization work related to horovod setup Returns ------- None \"\"\" hvd.init() def _hvd_convert_to_sync_batch_norm( self, module: torch.nn.Module, process_group=None ): \"\"\"Replaces all BatchNorm*D layers with horovod.torch.SyncBatchNorm layers https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm https://nvidia.github.io/apex/_modules/apex/parallel.html#convert_syncbn_model Parameters ---------- module: torch.nn.Module current model object process_group: default: None process group to scope synchronization, default is the whole world Returns ------- module_output: torch.nn.Module modified version of model with all BatchNorm*D layers replaced with horovod.torch.SyncBatchNorm layers Notes ----- Borrows heavily from the current torch convert_sync_batchnorm and apex convert_syncbn_model implementations only changing the underlying layer type to use the hvd implementation \"\"\" module_output = module if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): module_output = hvd.SyncBatchNorm( num_features=module.num_features, eps=module.eps, momentum=module.momentum, affine=module.affine, track_running_stats=module.track_running_stats, ) # Handle the copy of affine vars if affine if module.affine: with torch.no_grad(): module_output.weight = module.weight module_output.bias = module.bias # Handle the swap of running stats module_output.running_mean = module.running_mean module_output.running_var = module.running_var # Iterate recursively and replace for name, child in module.named_children(): module_output.add_module( name=name, module=self._hvd_convert_to_sync_batch_norm( module=child, process_group=process_group ), ) # delete and return del module return module_output def setup_distributed(self): \"\"\"Handles any underlying horovod setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id) def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Horovod Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") op_dict = {\"Average\": hvd.Average, \"Sum\": hvd.Sum, \"Adasum\": hvd.Adasum} optimizer = hvd.DistributedOptimizer( optimizer=optimizer, named_parameters=model.named_parameters(), backward_passes_per_step=grad_accum * self._multi_loss if grad_accum is not None else self._multi_loss, compression=hvd.Compression.fp16 if self._horovod_config.compression else hvd.Compression.none, gradient_predivide_factor=self._horovod_config.gradient_predivide_factor, op=op_dict.get(self._horovod_config.op), ) # Broadcast the initial variable states from rank 0 to all other processes hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) return model, optimizer def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device) def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None): \"\"\"Take a single loss and detach it from the compute graph and sync across devices if needed Parameters ---------- loss: torch.Tensor current loss(es) on the device device: default: None output device of the sync call Returns ------- float detached, synced, and mean calculated across devices \"\"\" # map to the same device the loss is on pre detach if not set if device is None: device = loss.device detached_loss = loss.item() with torch.no_grad(): loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype) # Make sure everyone is synced before the all-reduce # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Loss tensor is worker specific so allreduce -- force SUM from Horovod sum_tensor = hvd.allreduce(loss_tensor, op=hvd.Sum) # Detach and divide by the world size to get the mean on each device return sum_tensor.item() / self.world_size def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Return the context to wrap the step call Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation \"\"\" # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( \"Horovod skipping synchronize as it was triggered pre grad-clip\" ) return optimizer.skip_synchronize() def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() @property def rank(self): \"\"\"Returns current distributed rank\"\"\" return hvd.rank() @property def world_size(self): \"\"\"Returns current world size\"\"\" return hvd.size() @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return hvd.is_initialized()","title":"Attributes"},{"location":"reference/stoke/distributed/#ancestors-in-mro_3","text":"stoke.distributed.BaseDistributed abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/distributed/#instance-variables_3","text":"device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank world_size Returns current world size","title":"Instance variables"},{"location":"reference/stoke/distributed/#methods_3","text":"","title":"Methods"},{"location":"reference/stoke/distributed/#barrier_3","text":"def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join()","title":"barrier"},{"location":"reference/stoke/distributed/#clean_3","text":"def clean ( self ) Base clean call Nothing to do here... Returns: Type Description None None ??? example \"View Source\" def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass","title":"clean"},{"location":"reference/stoke/distributed/#detach_and_sync_loss_3","text":"def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been synced across multiple devices and detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)( self._single_detach_and_sync_loss(val, device) for val in loss ) else: return self._single_detach_and_sync_loss(loss, device)","title":"detach_and_sync_loss"},{"location":"reference/stoke/distributed/#grad_accum_context_3","text":"def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"grad_accum_context"},{"location":"reference/stoke/distributed/#print_device_3","text":"def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass","title":"print_device"},{"location":"reference/stoke/distributed/#setup_distributed_3","text":"def setup_distributed ( self ) Handles any underlying horovod setup post init Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Handles any underlying horovod setup post init Returns ------- None \"\"\" # Set the device rank torch.cuda.set_device(self._device_id)","title":"setup_distributed"},{"location":"reference/stoke/distributed/#step_context_3","text":"def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Return the context to wrap the step call Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Return the context to wrap the step call Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation \"\"\" # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( \"Horovod skipping synchronize as it was triggered pre grad-clip\" ) return optimizer.skip_synchronize()","title":"step_context"},{"location":"reference/stoke/distributed/#wrap_distributed_3","text":"def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Overrides base implementation for wrapping with Horovod Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Overrides base implementation for wrapping with Horovod Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] Wrapped optimizer object \"\"\" # Print info if verbose if self._verbose: self._print_info() self._print_device( f\"{self._name} -- Device ID: {torch.cuda.current_device()}\" ) self._print_device(f\"{self._name} -- Rank: {self.rank}\") op_dict = {\"Average\": hvd.Average, \"Sum\": hvd.Sum, \"Adasum\": hvd.Adasum} optimizer = hvd.DistributedOptimizer( optimizer=optimizer, named_parameters=model.named_parameters(), backward_passes_per_step=grad_accum * self._multi_loss if grad_accum is not None else self._multi_loss, compression=hvd.Compression.fp16 if self._horovod_config.compression else hvd.Compression.none, gradient_predivide_factor=self._horovod_config.gradient_predivide_factor, op=op_dict.get(self._horovod_config.op), ) # Broadcast the initial variable states from rank 0 to all other processes hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) return model, optimizer","title":"wrap_distributed"},{"location":"reference/stoke/distributed/#distributednullcpu","text":"class DistributedNullCPU ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs )","title":"DistributedNullCPU"},{"location":"reference/stoke/distributed/#attributes_4","text":"Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedNullCPU(BaseDistributed): def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init for DistributedNullCPU Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Device ID set to None as it is not needed for non distributed CPU \"\"\" super(DistributedNullCPU, self).__init__( device_id=\"cpu\", batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch CPU\", verbose=verbose, ) @property def rank(self): \"\"\"Returns current distributed rank No rank so return string of cpu \"\"\" return \"cpu\" @property def world_size(self): \"\"\"Returns current world size\"\"\" return 1 @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return True","title":"Attributes"},{"location":"reference/stoke/distributed/#ancestors-in-mro_4","text":"stoke.distributed.BaseDistributed abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/distributed/#instance-variables_4","text":"device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank No rank so return string of cpu world_size Returns current world size","title":"Instance variables"},{"location":"reference/stoke/distributed/#methods_4","text":"","title":"Methods"},{"location":"reference/stoke/distributed/#barrier_4","text":"def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass","title":"barrier"},{"location":"reference/stoke/distributed/#clean_4","text":"def clean ( self ) Base clean call Nothing to do here... Returns: Type Description None None ??? example \"View Source\" def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass","title":"clean"},{"location":"reference/stoke/distributed/#detach_and_sync_loss_4","text":"def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item()","title":"detach_and_sync_loss"},{"location":"reference/stoke/distributed/#grad_accum_context_4","text":"def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"grad_accum_context"},{"location":"reference/stoke/distributed/#print_device_4","text":"def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass","title":"print_device"},{"location":"reference/stoke/distributed/#setup_distributed_4","text":"def setup_distributed ( self ) Base setup distributed Does nothing as nothing needs to be wrapped Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass","title":"setup_distributed"},{"location":"reference/stoke/distributed/#step_context_4","text":"def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"step_context"},{"location":"reference/stoke/distributed/#wrap_distributed_4","text":"def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module same as input model ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer","title":"wrap_distributed"},{"location":"reference/stoke/distributed/#distributednullgpu","text":"class DistributedNullGPU ( batch_size_per_device : int , info_rank : Union [ int , List [ int ]], verbose : bool = True , ** kwargs )","title":"DistributedNullGPU"},{"location":"reference/stoke/distributed/#attributes_5","text":"Name Type Description Default device_id None None None initialized None None None rank None None None world_size None None None _batch_size_per_device int batch size per device or for non-distributed the overall batch size None _device_id int, default: None Current device id None _info_rank Union[int, List[int]] Which device(s) to print information None _name str name of current backend None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DistributedNullGPU(BaseDistributed): def __init__( self, batch_size_per_device: int, info_rank: Union[int, List[int]], verbose: bool = True, **kwargs, ): \"\"\"Init for DistributedNullCPU Parameters ---------- batch_size_per_device: int batch size per device or for non-distributed the overall batch size info_rank: Union[int, List[int]] Which device(s) to print information verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Device ID set to the current CUDA device as there is only a single GPU being used \"\"\" super(DistributedNullGPU, self).__init__( device_id=torch.cuda.current_device(), batch_size_per_device=batch_size_per_device, info_rank=info_rank, name=\"PyTorch GPU\", verbose=verbose, ) @property def rank(self): \"\"\"Returns current distributed rank No rank so return string of gpu \"\"\" return \"gpu\" @property def world_size(self): \"\"\"Returns current world size\"\"\" return 1 @property def initialized(self): \"\"\"Returns if distributed backend is initialized correctly\"\"\" return True","title":"Attributes"},{"location":"reference/stoke/distributed/#ancestors-in-mro_5","text":"stoke.distributed.BaseDistributed abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/distributed/#instance-variables_5","text":"device_id Returns the current device id initialized Returns if distributed backend is initialized correctly rank Returns current distributed rank No rank so return string of gpu world_size Returns current world size","title":"Instance variables"},{"location":"reference/stoke/distributed/#methods_5","text":"","title":"Methods"},{"location":"reference/stoke/distributed/#barrier_5","text":"def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" pass","title":"barrier"},{"location":"reference/stoke/distributed/#clean_5","text":"def clean ( self ) Base clean call Nothing to do here... Returns: Type Description None None ??? example \"View Source\" def clean(self): \"\"\"Base clean call Nothing to do here... Returns ------- None \"\"\" pass","title":"clean"},{"location":"reference/stoke/distributed/#detach_and_sync_loss_5","text":"def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None device default: None output device of the sync call None Returns: Type Description Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce) Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device device: default: None output device of the sync call Returns ------- Union[float, List[float], Tuple[float]] loss(es) that has(have) been detached from the graph \"\"\" if isinstance(loss, (list, tuple)): return type(loss)(val.item() for val in loss) else: return loss.item()","title":"detach_and_sync_loss"},{"location":"reference/stoke/distributed/#grad_accum_context_5","text":"def grad_accum_context ( self , model : torch . nn . modules . module . Module ) Returns base context for gradient accumulation By default no context is used Parameters: Name Type Description Default model torch.nn.Module current model object None Returns: Type Description nullcontext() None ??? example \"View Source\" def grad_accum_context(self, model: torch.nn.Module): \"\"\"Returns base context for gradient accumulation By default no context is used Parameters ---------- model: torch.nn.Module current model object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"grad_accum_context"},{"location":"reference/stoke/distributed/#print_device_5","text":"def print_device ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 , single_line : bool = False ) Public facing method to print on specific device ranks Parameters: Name Type Description Default msg Union[str, List[str]] message(s) to print None rank Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_device( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0, single_line: bool = False, ): \"\"\"Public facing method to print on specific device ranks Parameters ---------- msg: Union[str, List[str]] message(s) to print rank: Optional[Union[int, List[int]]], default: 0 device rank to print to (prevents printing on multiple devices in distributed mode) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" # Ignore the rank check if the current rank is a non-distributed version if self.rank == \"cpu\" or self.rank == \"gpu\": unrolled_print(msg, single_line=single_line) # if it's a list then check the rank against the list elif isinstance(rank, list) and self.rank in rank: unrolled_print(msg, single_line=single_line) # If its an int then check the equality elif isinstance(rank, int) and rank == self.rank: unrolled_print(msg, single_line=single_line) # the else is essentially skip print else: pass","title":"print_device"},{"location":"reference/stoke/distributed/#setup_distributed_5","text":"def setup_distributed ( self ) Base setup distributed Does nothing as nothing needs to be wrapped Returns: Type Description None None ??? example \"View Source\" def setup_distributed(self): \"\"\"Base setup distributed Does nothing as nothing needs to be wrapped Returns ------- None \"\"\" pass","title":"setup_distributed"},{"location":"reference/stoke/distributed/#step_context_5","text":"def step_context ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Returns base context for the step call By default no context is used Parameters: Name Type Description Default optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description nullcontext() None ??? example \"View Source\" def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]): \"\"\"Returns base context for the step call By default no context is used Parameters ---------- optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- nullcontext() \"\"\" return nullcontext()","title":"step_context"},{"location":"reference/stoke/distributed/#wrap_distributed_5","text":"def wrap_distributed ( self , model : torch . nn . modules . module . Module , grad_accum : Union [ int , NoneType ], optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None Returns: Type Description torch.nn.Module same as input model ??? example \"View Source\" def wrap_distributed( self, model: torch.nn.Module, grad_accum: Optional[int], optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Base wrapper for distributed backends Does nothing but print as nothing needs to be wrapped Parameters ---------- model: torch.nn.Module current model object optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None current optimizer object grad_accum: int, default: None Number of gradient accumulation steps Returns ------- model: torch.nn.Module same as input model optimizer: Union[torch.optim.Optimizer, OSS]] same as input optimizer \"\"\" # Print info if verbose if self._verbose: self._print_info() return model, optimizer","title":"wrap_distributed"},{"location":"reference/stoke/distributed/#runnerdistenum","text":"class RunnerDistEnum ( / , * args , ** kwargs ) ??? example \"View Source\" class RunnerDistEnum(Enum): \"\"\"Enum for building the runtime object with distributed functionality\"\"\" cpu = DistributedNullCPU gpu = DistributedNullGPU ddp = DistributedDDP horovod = DistributedHorovod deepspeed = DistributedDeepspeed","title":"RunnerDistEnum"},{"location":"reference/stoke/distributed/#ancestors-in-mro_6","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/distributed/#class-variables","text":"cpu ddp deepspeed gpu horovod name value","title":"Class variables"},{"location":"reference/stoke/extensions/","text":"Module stoke.extensions Handles extension wrapper related classes -- mixin style None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles extension wrapper related classes -- mixin style\"\"\" from abc import ABC from enum import Enum from typing import Dict, Optional, Tuple, Type, Union import attr import torch from fairscale.nn.data_parallel import FullyShardedDataParallel, ShardedDataParallel from fairscale.optim.oss import OSS from stoke.configs import ( DDPConfig, FairscaleFSDPConfig, FairscaleOSSConfig, FairscaleSDDPConfig, ) @attr.s(auto_attribs=True) class _FairscaleFSDPConfig(FairscaleFSDPConfig): mixed_precision: bool = False class BaseOptimizer(ABC): \"\"\"Base class for creating an optimizer Attributes ---------- _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for BaseOptimizer class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> torch.optim.Optimizer: \"\"\"Instantiates a torch optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- torch.optim.Optimizer instantiated torch optimizer object \"\"\" if self._verbose: self._print_device(f\"Creating basic torch optimizer: {optimizer.__name__}\") return optimizer(params=model.parameters(), **optimizer_kwargs) class FairscaleOSSExtension(BaseOptimizer): \"\"\"Inherits from BaseOptimizer for OSS class creation Attributes ---------- _oss_config: FairscaleOSSConfig, Configuration object for Fairscale OSS _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, oss_config: FairscaleOSSConfig, verbose: bool = True, **kwargs): \"\"\"Init for FairscaleOSSExtension class Parameters ---------- oss_config: FairscaleOSSConfig Configuration object for Fairscale OSS verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" super(FairscaleOSSExtension, self).__init__(verbose=verbose) self._oss_config = oss_config def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> OSS: \"\"\"Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- OSS instantiated Fairscale OSS optimizer object \"\"\" if self._verbose: self._print_device( f\"Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}\" ) return OSS( params=model.parameters(), optim=optimizer, broadcast_fp16=self._oss_config.broadcast_fp16, **optimizer_kwargs, ) class RunnerOptimizerEnum(Enum): \"\"\"Enum for optimizer creation\"\"\" oss = FairscaleOSSExtension base = BaseOptimizer class BaseDDP: \"\"\"Base class for using the DDP backend Attributes ---------- _ddp_config: DDPConfig Base DDP configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, ddp_config: DDPConfig, verbose: bool = True, **kwargs): \"\"\"Init for BaseDDP Parameters ---------- ddp_config: DDPConfig Base DDP configuration object verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._ddp_config = ddp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the base DDP call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = torch.nn.parallel.DistributedDataParallel( module=model, device_ids=[rank], output_device=rank, bucket_cap_mb=self._ddp_config.bucket_cap_mb, broadcast_buffers=self._ddp_config.broadcast_buffers, find_unused_parameters=self._ddp_config.find_unused_parameters, gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view, ) return model, optimizer class FairscaleSDDPExtension: \"\"\"Class for using the Fairscale SDDP backend Attributes ---------- _sddp_config: FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, sddp_config: FairscaleSDDPConfig, verbose: bool = True, **kwargs ): \"\"\"Init for FairscaleSDDPExtension Parameters ---------- sddp_config: FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration objet verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._sddp_config = sddp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the ShardedDataParallel call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = ShardedDataParallel( module=model, sharded_optimizer=optimizer, broadcast_buffers=self._sddp_config.broadcast_buffers, sync_models_at_startup=self._sddp_config.sync_models_at_startup, reduce_buffer_size=self._sddp_config.reduce_buffer_size, auto_refresh_trainable=self._sddp_config.auto_refresh_trainable, reduce_fp16=self._sddp_config.reduce_fp16, ) return model, optimizer class FairscaleFSDPExtension: \"\"\"Class for using the Fairscale FSDP backend Attributes ---------- _fsdp_config: _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, fsdp_config: _FairscaleFSDPConfig, verbose: bool = True, **kwargs ): \"\"\"Init for FairscaleSDDPExtension Parameters ---------- _fsdp_config: _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._fsdpp_config = fsdp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the FullyShardedDataParallel call Also sets grad divide factors https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = FullyShardedDataParallel( module=model, reshard_after_forward=self._fsdpp_config.reshard_after_forward, mixed_precision=self._fsdpp_config.mixed_precision, fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter, flatten_parameters=self._fsdpp_config.flatten_parameters, move_params_to_cpu=self._fsdpp_config.move_params_to_cpu, compute_dtype=self._fsdpp_config.compute_dtype, buffer_dtype=self._fsdpp_config.buffer_dtype, move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu, bucket_cap_mb=self._fsdpp_config.bucket_cap_mb, no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state, clear_autocast_cache=self._fsdpp_config.clear_autocast_cache, force_input_to_fp32=self._fsdpp_config.force_input_to_fp32, verbose=self._fsdpp_config.verbose, ) # Trigger the set of pre-divide or post-divide factors if set in the config model.set_gradient_divide_factors( pre=self._fsdpp_config.gradient_predivide_factor if self._fsdpp_config.gradient_predivide_factor is not None else model.gradient_predivide_factor, post=self._fsdpp_config.gradient_postdivide_factor if self._fsdpp_config.gradient_postdivide_factor is not None else model.gradient_postdivide_factor, recursive=True, ) return model, optimizer class DistributedHandlerEnum(Enum): \"\"\"Enum for DDP use\"\"\" sddp = FairscaleSDDPExtension fsdp = FairscaleFSDPExtension base = BaseDDP Classes BaseDDP class BaseDDP ( ddp_config : stoke . configs . DDPConfig , verbose : bool = True , ** kwargs ) Attributes Name Type Description Default _ddp_config DDPConfig Base DDP configuration object None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class BaseDDP: \"\"\"Base class for using the DDP backend Attributes ---------- _ddp_config: DDPConfig Base DDP configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, ddp_config: DDPConfig, verbose: bool = True, **kwargs): \"\"\"Init for BaseDDP Parameters ---------- ddp_config: DDPConfig Base DDP configuration object verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._ddp_config = ddp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the base DDP call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = torch.nn.parallel.DistributedDataParallel( module=model, device_ids=[rank], output_device=rank, bucket_cap_mb=self._ddp_config.bucket_cap_mb, broadcast_buffers=self._ddp_config.broadcast_buffers, find_unused_parameters=self._ddp_config.find_unused_parameters, gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view, ) return model, optimizer Methods handle_ddp def handle_ddp ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], grad_accum : Union [ int , NoneType ], rank : int ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps the model in the base DDP call Parameters: Name Type Description Default model torch.nn.Module Current model object None optimizer Union[torch.optim.Optimizer, OSS] Current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None rank int Current CUDA device rank in the distributed setup None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the base DDP call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = torch.nn.parallel.DistributedDataParallel( module=model, device_ids=[rank], output_device=rank, bucket_cap_mb=self._ddp_config.bucket_cap_mb, broadcast_buffers=self._ddp_config.broadcast_buffers, find_unused_parameters=self._ddp_config.find_unused_parameters, gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view, ) return model, optimizer BaseOptimizer class BaseOptimizer ( verbose : bool = True , ** kwargs ) Attributes Name Type Description Default _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class BaseOptimizer(ABC): \"\"\"Base class for creating an optimizer Attributes ---------- _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for BaseOptimizer class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> torch.optim.Optimizer: \"\"\"Instantiates a torch optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- torch.optim.Optimizer instantiated torch optimizer object \"\"\" if self._verbose: self._print_device(f\"Creating basic torch optimizer: {optimizer.__name__}\") return optimizer(params=model.parameters(), **optimizer_kwargs) Ancestors (in MRO) abc.ABC Descendants stoke.extensions.FairscaleOSSExtension Methods build_optimizer def build_optimizer ( self , optimizer : Type [ torch . optim . optimizer . Optimizer ], optimizer_kwargs : Dict , model : torch . nn . modules . module . Module ) -> torch . optim . optimizer . Optimizer Instantiates a torch optimizer object from the type and optimizer kwargs Parameters: Name Type Description Default optimizer Type[torch.optim.Optimizer] type of torch optimizer None optimizer_kwargs Dict dictionary of all kwargs to pass to the optimizer None model torch.nn.Module model object None Returns: Type Description torch.optim.Optimizer instantiated torch optimizer object ??? example \"View Source\" def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> torch.optim.Optimizer: \"\"\"Instantiates a torch optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- torch.optim.Optimizer instantiated torch optimizer object \"\"\" if self._verbose: self._print_device(f\"Creating basic torch optimizer: {optimizer.__name__}\") return optimizer(params=model.parameters(), **optimizer_kwargs) DistributedHandlerEnum class DistributedHandlerEnum ( / , * args , ** kwargs ) ??? example \"View Source\" class DistributedHandlerEnum(Enum): \"\"\"Enum for DDP use\"\"\" sddp = FairscaleSDDPExtension fsdp = FairscaleFSDPExtension base = BaseDDP Ancestors (in MRO) enum.Enum Class variables base fsdp name sddp value FairscaleFSDPExtension class FairscaleFSDPExtension ( fsdp_config : stoke . extensions . _FairscaleFSDPConfig , verbose : bool = True , ** kwargs ) Attributes Name Type Description Default _fsdp_config _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class FairscaleFSDPExtension: \"\"\"Class for using the Fairscale FSDP backend Attributes ---------- _fsdp_config: _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, fsdp_config: _FairscaleFSDPConfig, verbose: bool = True, **kwargs ): \"\"\"Init for FairscaleSDDPExtension Parameters ---------- _fsdp_config: _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._fsdpp_config = fsdp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the FullyShardedDataParallel call Also sets grad divide factors https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = FullyShardedDataParallel( module=model, reshard_after_forward=self._fsdpp_config.reshard_after_forward, mixed_precision=self._fsdpp_config.mixed_precision, fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter, flatten_parameters=self._fsdpp_config.flatten_parameters, move_params_to_cpu=self._fsdpp_config.move_params_to_cpu, compute_dtype=self._fsdpp_config.compute_dtype, buffer_dtype=self._fsdpp_config.buffer_dtype, move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu, bucket_cap_mb=self._fsdpp_config.bucket_cap_mb, no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state, clear_autocast_cache=self._fsdpp_config.clear_autocast_cache, force_input_to_fp32=self._fsdpp_config.force_input_to_fp32, verbose=self._fsdpp_config.verbose, ) # Trigger the set of pre-divide or post-divide factors if set in the config model.set_gradient_divide_factors( pre=self._fsdpp_config.gradient_predivide_factor if self._fsdpp_config.gradient_predivide_factor is not None else model.gradient_predivide_factor, post=self._fsdpp_config.gradient_postdivide_factor if self._fsdpp_config.gradient_postdivide_factor is not None else model.gradient_postdivide_factor, recursive=True, ) return model, optimizer Methods handle_ddp def handle_ddp ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], grad_accum : Union [ int , NoneType ], rank : int ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps the model in the FullyShardedDataParallel call Also sets grad divide factors https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors Parameters: Name Type Description Default model torch.nn.Module Current model object None optimizer Union[torch.optim.Optimizer, OSS] Current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None rank int Current CUDA device rank in the distributed setup None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the FullyShardedDataParallel call Also sets grad divide factors https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = FullyShardedDataParallel( module=model, reshard_after_forward=self._fsdpp_config.reshard_after_forward, mixed_precision=self._fsdpp_config.mixed_precision, fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter, flatten_parameters=self._fsdpp_config.flatten_parameters, move_params_to_cpu=self._fsdpp_config.move_params_to_cpu, compute_dtype=self._fsdpp_config.compute_dtype, buffer_dtype=self._fsdpp_config.buffer_dtype, move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu, bucket_cap_mb=self._fsdpp_config.bucket_cap_mb, no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state, clear_autocast_cache=self._fsdpp_config.clear_autocast_cache, force_input_to_fp32=self._fsdpp_config.force_input_to_fp32, verbose=self._fsdpp_config.verbose, ) # Trigger the set of pre-divide or post-divide factors if set in the config model.set_gradient_divide_factors( pre=self._fsdpp_config.gradient_predivide_factor if self._fsdpp_config.gradient_predivide_factor is not None else model.gradient_predivide_factor, post=self._fsdpp_config.gradient_postdivide_factor if self._fsdpp_config.gradient_postdivide_factor is not None else model.gradient_postdivide_factor, recursive=True, ) return model, optimizer FairscaleOSSExtension class FairscaleOSSExtension ( oss_config : stoke . configs . FairscaleOSSConfig , verbose : bool = True , ** kwargs ) Attributes Name Type Description Default _oss_config FairscaleOSSConfig, Configuration object for Fairscale OSS None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class FairscaleOSSExtension(BaseOptimizer): \"\"\"Inherits from BaseOptimizer for OSS class creation Attributes ---------- _oss_config: FairscaleOSSConfig, Configuration object for Fairscale OSS _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, oss_config: FairscaleOSSConfig, verbose: bool = True, **kwargs): \"\"\"Init for FairscaleOSSExtension class Parameters ---------- oss_config: FairscaleOSSConfig Configuration object for Fairscale OSS verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" super(FairscaleOSSExtension, self).__init__(verbose=verbose) self._oss_config = oss_config def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> OSS: \"\"\"Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- OSS instantiated Fairscale OSS optimizer object \"\"\" if self._verbose: self._print_device( f\"Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}\" ) return OSS( params=model.parameters(), optim=optimizer, broadcast_fp16=self._oss_config.broadcast_fp16, **optimizer_kwargs, ) Ancestors (in MRO) stoke.extensions.BaseOptimizer abc.ABC Methods build_optimizer def build_optimizer ( self , optimizer : Type [ torch . optim . optimizer . Optimizer ], optimizer_kwargs : Dict , model : torch . nn . modules . module . Module ) -> fairscale . optim . oss . OSS Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs Parameters: Name Type Description Default optimizer Type[torch.optim.Optimizer] type of torch optimizer None optimizer_kwargs Dict dictionary of all kwargs to pass to the optimizer None model torch.nn.Module model object None Returns: Type Description OSS instantiated Fairscale OSS optimizer object ??? example \"View Source\" def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> OSS: \"\"\"Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- OSS instantiated Fairscale OSS optimizer object \"\"\" if self._verbose: self._print_device( f\"Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}\" ) return OSS( params=model.parameters(), optim=optimizer, broadcast_fp16=self._oss_config.broadcast_fp16, **optimizer_kwargs, ) FairscaleSDDPExtension class FairscaleSDDPExtension ( sddp_config : stoke . configs . FairscaleSDDPConfig , verbose : bool = True , ** kwargs ) Attributes Name Type Description Default _sddp_config FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration object None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class FairscaleSDDPExtension: \"\"\"Class for using the Fairscale SDDP backend Attributes ---------- _sddp_config: FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, sddp_config: FairscaleSDDPConfig, verbose: bool = True, **kwargs ): \"\"\"Init for FairscaleSDDPExtension Parameters ---------- sddp_config: FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration objet verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._sddp_config = sddp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the ShardedDataParallel call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = ShardedDataParallel( module=model, sharded_optimizer=optimizer, broadcast_buffers=self._sddp_config.broadcast_buffers, sync_models_at_startup=self._sddp_config.sync_models_at_startup, reduce_buffer_size=self._sddp_config.reduce_buffer_size, auto_refresh_trainable=self._sddp_config.auto_refresh_trainable, reduce_fp16=self._sddp_config.reduce_fp16, ) return model, optimizer Methods handle_ddp def handle_ddp ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], grad_accum : Union [ int , NoneType ], rank : int ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps the model in the ShardedDataParallel call Parameters: Name Type Description Default model torch.nn.Module Current model object None optimizer Union[torch.optim.Optimizer, OSS] Current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None rank int Current CUDA device rank in the distributed setup None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the ShardedDataParallel call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = ShardedDataParallel( module=model, sharded_optimizer=optimizer, broadcast_buffers=self._sddp_config.broadcast_buffers, sync_models_at_startup=self._sddp_config.sync_models_at_startup, reduce_buffer_size=self._sddp_config.reduce_buffer_size, auto_refresh_trainable=self._sddp_config.auto_refresh_trainable, reduce_fp16=self._sddp_config.reduce_fp16, ) return model, optimizer RunnerOptimizerEnum class RunnerOptimizerEnum ( / , * args , ** kwargs ) ??? example \"View Source\" class RunnerOptimizerEnum(Enum): \"\"\"Enum for optimizer creation\"\"\" oss = FairscaleOSSExtension base = BaseOptimizer Ancestors (in MRO) enum.Enum Class variables base name oss value","title":"Extensions"},{"location":"reference/stoke/extensions/#module-stokeextensions","text":"Handles extension wrapper related classes -- mixin style None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles extension wrapper related classes -- mixin style\"\"\" from abc import ABC from enum import Enum from typing import Dict, Optional, Tuple, Type, Union import attr import torch from fairscale.nn.data_parallel import FullyShardedDataParallel, ShardedDataParallel from fairscale.optim.oss import OSS from stoke.configs import ( DDPConfig, FairscaleFSDPConfig, FairscaleOSSConfig, FairscaleSDDPConfig, ) @attr.s(auto_attribs=True) class _FairscaleFSDPConfig(FairscaleFSDPConfig): mixed_precision: bool = False class BaseOptimizer(ABC): \"\"\"Base class for creating an optimizer Attributes ---------- _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for BaseOptimizer class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> torch.optim.Optimizer: \"\"\"Instantiates a torch optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- torch.optim.Optimizer instantiated torch optimizer object \"\"\" if self._verbose: self._print_device(f\"Creating basic torch optimizer: {optimizer.__name__}\") return optimizer(params=model.parameters(), **optimizer_kwargs) class FairscaleOSSExtension(BaseOptimizer): \"\"\"Inherits from BaseOptimizer for OSS class creation Attributes ---------- _oss_config: FairscaleOSSConfig, Configuration object for Fairscale OSS _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, oss_config: FairscaleOSSConfig, verbose: bool = True, **kwargs): \"\"\"Init for FairscaleOSSExtension class Parameters ---------- oss_config: FairscaleOSSConfig Configuration object for Fairscale OSS verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" super(FairscaleOSSExtension, self).__init__(verbose=verbose) self._oss_config = oss_config def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> OSS: \"\"\"Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- OSS instantiated Fairscale OSS optimizer object \"\"\" if self._verbose: self._print_device( f\"Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}\" ) return OSS( params=model.parameters(), optim=optimizer, broadcast_fp16=self._oss_config.broadcast_fp16, **optimizer_kwargs, ) class RunnerOptimizerEnum(Enum): \"\"\"Enum for optimizer creation\"\"\" oss = FairscaleOSSExtension base = BaseOptimizer class BaseDDP: \"\"\"Base class for using the DDP backend Attributes ---------- _ddp_config: DDPConfig Base DDP configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, ddp_config: DDPConfig, verbose: bool = True, **kwargs): \"\"\"Init for BaseDDP Parameters ---------- ddp_config: DDPConfig Base DDP configuration object verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._ddp_config = ddp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the base DDP call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = torch.nn.parallel.DistributedDataParallel( module=model, device_ids=[rank], output_device=rank, bucket_cap_mb=self._ddp_config.bucket_cap_mb, broadcast_buffers=self._ddp_config.broadcast_buffers, find_unused_parameters=self._ddp_config.find_unused_parameters, gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view, ) return model, optimizer class FairscaleSDDPExtension: \"\"\"Class for using the Fairscale SDDP backend Attributes ---------- _sddp_config: FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, sddp_config: FairscaleSDDPConfig, verbose: bool = True, **kwargs ): \"\"\"Init for FairscaleSDDPExtension Parameters ---------- sddp_config: FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration objet verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._sddp_config = sddp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the ShardedDataParallel call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = ShardedDataParallel( module=model, sharded_optimizer=optimizer, broadcast_buffers=self._sddp_config.broadcast_buffers, sync_models_at_startup=self._sddp_config.sync_models_at_startup, reduce_buffer_size=self._sddp_config.reduce_buffer_size, auto_refresh_trainable=self._sddp_config.auto_refresh_trainable, reduce_fp16=self._sddp_config.reduce_fp16, ) return model, optimizer class FairscaleFSDPExtension: \"\"\"Class for using the Fairscale FSDP backend Attributes ---------- _fsdp_config: _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, fsdp_config: _FairscaleFSDPConfig, verbose: bool = True, **kwargs ): \"\"\"Init for FairscaleSDDPExtension Parameters ---------- _fsdp_config: _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._fsdpp_config = fsdp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the FullyShardedDataParallel call Also sets grad divide factors https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = FullyShardedDataParallel( module=model, reshard_after_forward=self._fsdpp_config.reshard_after_forward, mixed_precision=self._fsdpp_config.mixed_precision, fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter, flatten_parameters=self._fsdpp_config.flatten_parameters, move_params_to_cpu=self._fsdpp_config.move_params_to_cpu, compute_dtype=self._fsdpp_config.compute_dtype, buffer_dtype=self._fsdpp_config.buffer_dtype, move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu, bucket_cap_mb=self._fsdpp_config.bucket_cap_mb, no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state, clear_autocast_cache=self._fsdpp_config.clear_autocast_cache, force_input_to_fp32=self._fsdpp_config.force_input_to_fp32, verbose=self._fsdpp_config.verbose, ) # Trigger the set of pre-divide or post-divide factors if set in the config model.set_gradient_divide_factors( pre=self._fsdpp_config.gradient_predivide_factor if self._fsdpp_config.gradient_predivide_factor is not None else model.gradient_predivide_factor, post=self._fsdpp_config.gradient_postdivide_factor if self._fsdpp_config.gradient_postdivide_factor is not None else model.gradient_postdivide_factor, recursive=True, ) return model, optimizer class DistributedHandlerEnum(Enum): \"\"\"Enum for DDP use\"\"\" sddp = FairscaleSDDPExtension fsdp = FairscaleFSDPExtension base = BaseDDP","title":"Module stoke.extensions"},{"location":"reference/stoke/extensions/#classes","text":"","title":"Classes"},{"location":"reference/stoke/extensions/#baseddp","text":"class BaseDDP ( ddp_config : stoke . configs . DDPConfig , verbose : bool = True , ** kwargs )","title":"BaseDDP"},{"location":"reference/stoke/extensions/#attributes","text":"Name Type Description Default _ddp_config DDPConfig Base DDP configuration object None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class BaseDDP: \"\"\"Base class for using the DDP backend Attributes ---------- _ddp_config: DDPConfig Base DDP configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, ddp_config: DDPConfig, verbose: bool = True, **kwargs): \"\"\"Init for BaseDDP Parameters ---------- ddp_config: DDPConfig Base DDP configuration object verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._ddp_config = ddp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the base DDP call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = torch.nn.parallel.DistributedDataParallel( module=model, device_ids=[rank], output_device=rank, bucket_cap_mb=self._ddp_config.bucket_cap_mb, broadcast_buffers=self._ddp_config.broadcast_buffers, find_unused_parameters=self._ddp_config.find_unused_parameters, gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view, ) return model, optimizer","title":"Attributes"},{"location":"reference/stoke/extensions/#methods","text":"","title":"Methods"},{"location":"reference/stoke/extensions/#handle_ddp","text":"def handle_ddp ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], grad_accum : Union [ int , NoneType ], rank : int ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps the model in the base DDP call Parameters: Name Type Description Default model torch.nn.Module Current model object None optimizer Union[torch.optim.Optimizer, OSS] Current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None rank int Current CUDA device rank in the distributed setup None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the base DDP call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = torch.nn.parallel.DistributedDataParallel( module=model, device_ids=[rank], output_device=rank, bucket_cap_mb=self._ddp_config.bucket_cap_mb, broadcast_buffers=self._ddp_config.broadcast_buffers, find_unused_parameters=self._ddp_config.find_unused_parameters, gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view, ) return model, optimizer","title":"handle_ddp"},{"location":"reference/stoke/extensions/#baseoptimizer","text":"class BaseOptimizer ( verbose : bool = True , ** kwargs )","title":"BaseOptimizer"},{"location":"reference/stoke/extensions/#attributes_1","text":"Name Type Description Default _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class BaseOptimizer(ABC): \"\"\"Base class for creating an optimizer Attributes ---------- _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for BaseOptimizer class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> torch.optim.Optimizer: \"\"\"Instantiates a torch optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- torch.optim.Optimizer instantiated torch optimizer object \"\"\" if self._verbose: self._print_device(f\"Creating basic torch optimizer: {optimizer.__name__}\") return optimizer(params=model.parameters(), **optimizer_kwargs)","title":"Attributes"},{"location":"reference/stoke/extensions/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/extensions/#descendants","text":"stoke.extensions.FairscaleOSSExtension","title":"Descendants"},{"location":"reference/stoke/extensions/#methods_1","text":"","title":"Methods"},{"location":"reference/stoke/extensions/#build_optimizer","text":"def build_optimizer ( self , optimizer : Type [ torch . optim . optimizer . Optimizer ], optimizer_kwargs : Dict , model : torch . nn . modules . module . Module ) -> torch . optim . optimizer . Optimizer Instantiates a torch optimizer object from the type and optimizer kwargs Parameters: Name Type Description Default optimizer Type[torch.optim.Optimizer] type of torch optimizer None optimizer_kwargs Dict dictionary of all kwargs to pass to the optimizer None model torch.nn.Module model object None Returns: Type Description torch.optim.Optimizer instantiated torch optimizer object ??? example \"View Source\" def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> torch.optim.Optimizer: \"\"\"Instantiates a torch optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- torch.optim.Optimizer instantiated torch optimizer object \"\"\" if self._verbose: self._print_device(f\"Creating basic torch optimizer: {optimizer.__name__}\") return optimizer(params=model.parameters(), **optimizer_kwargs)","title":"build_optimizer"},{"location":"reference/stoke/extensions/#distributedhandlerenum","text":"class DistributedHandlerEnum ( / , * args , ** kwargs ) ??? example \"View Source\" class DistributedHandlerEnum(Enum): \"\"\"Enum for DDP use\"\"\" sddp = FairscaleSDDPExtension fsdp = FairscaleFSDPExtension base = BaseDDP","title":"DistributedHandlerEnum"},{"location":"reference/stoke/extensions/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/extensions/#class-variables","text":"base fsdp name sddp value","title":"Class variables"},{"location":"reference/stoke/extensions/#fairscalefsdpextension","text":"class FairscaleFSDPExtension ( fsdp_config : stoke . extensions . _FairscaleFSDPConfig , verbose : bool = True , ** kwargs )","title":"FairscaleFSDPExtension"},{"location":"reference/stoke/extensions/#attributes_2","text":"Name Type Description Default _fsdp_config _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class FairscaleFSDPExtension: \"\"\"Class for using the Fairscale FSDP backend Attributes ---------- _fsdp_config: _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, fsdp_config: _FairscaleFSDPConfig, verbose: bool = True, **kwargs ): \"\"\"Init for FairscaleSDDPExtension Parameters ---------- _fsdp_config: _FairscaleFSDPConfig Base Fairscale Fully Sharded Data Parallel configuration object verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._fsdpp_config = fsdp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the FullyShardedDataParallel call Also sets grad divide factors https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = FullyShardedDataParallel( module=model, reshard_after_forward=self._fsdpp_config.reshard_after_forward, mixed_precision=self._fsdpp_config.mixed_precision, fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter, flatten_parameters=self._fsdpp_config.flatten_parameters, move_params_to_cpu=self._fsdpp_config.move_params_to_cpu, compute_dtype=self._fsdpp_config.compute_dtype, buffer_dtype=self._fsdpp_config.buffer_dtype, move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu, bucket_cap_mb=self._fsdpp_config.bucket_cap_mb, no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state, clear_autocast_cache=self._fsdpp_config.clear_autocast_cache, force_input_to_fp32=self._fsdpp_config.force_input_to_fp32, verbose=self._fsdpp_config.verbose, ) # Trigger the set of pre-divide or post-divide factors if set in the config model.set_gradient_divide_factors( pre=self._fsdpp_config.gradient_predivide_factor if self._fsdpp_config.gradient_predivide_factor is not None else model.gradient_predivide_factor, post=self._fsdpp_config.gradient_postdivide_factor if self._fsdpp_config.gradient_postdivide_factor is not None else model.gradient_postdivide_factor, recursive=True, ) return model, optimizer","title":"Attributes"},{"location":"reference/stoke/extensions/#methods_2","text":"","title":"Methods"},{"location":"reference/stoke/extensions/#handle_ddp_1","text":"def handle_ddp ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], grad_accum : Union [ int , NoneType ], rank : int ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps the model in the FullyShardedDataParallel call Also sets grad divide factors https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors Parameters: Name Type Description Default model torch.nn.Module Current model object None optimizer Union[torch.optim.Optimizer, OSS] Current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None rank int Current CUDA device rank in the distributed setup None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the FullyShardedDataParallel call Also sets grad divide factors https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = FullyShardedDataParallel( module=model, reshard_after_forward=self._fsdpp_config.reshard_after_forward, mixed_precision=self._fsdpp_config.mixed_precision, fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter, flatten_parameters=self._fsdpp_config.flatten_parameters, move_params_to_cpu=self._fsdpp_config.move_params_to_cpu, compute_dtype=self._fsdpp_config.compute_dtype, buffer_dtype=self._fsdpp_config.buffer_dtype, move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu, bucket_cap_mb=self._fsdpp_config.bucket_cap_mb, no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state, clear_autocast_cache=self._fsdpp_config.clear_autocast_cache, force_input_to_fp32=self._fsdpp_config.force_input_to_fp32, verbose=self._fsdpp_config.verbose, ) # Trigger the set of pre-divide or post-divide factors if set in the config model.set_gradient_divide_factors( pre=self._fsdpp_config.gradient_predivide_factor if self._fsdpp_config.gradient_predivide_factor is not None else model.gradient_predivide_factor, post=self._fsdpp_config.gradient_postdivide_factor if self._fsdpp_config.gradient_postdivide_factor is not None else model.gradient_postdivide_factor, recursive=True, ) return model, optimizer","title":"handle_ddp"},{"location":"reference/stoke/extensions/#fairscaleossextension","text":"class FairscaleOSSExtension ( oss_config : stoke . configs . FairscaleOSSConfig , verbose : bool = True , ** kwargs )","title":"FairscaleOSSExtension"},{"location":"reference/stoke/extensions/#attributes_3","text":"Name Type Description Default _oss_config FairscaleOSSConfig, Configuration object for Fairscale OSS None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class FairscaleOSSExtension(BaseOptimizer): \"\"\"Inherits from BaseOptimizer for OSS class creation Attributes ---------- _oss_config: FairscaleOSSConfig, Configuration object for Fairscale OSS _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, oss_config: FairscaleOSSConfig, verbose: bool = True, **kwargs): \"\"\"Init for FairscaleOSSExtension class Parameters ---------- oss_config: FairscaleOSSConfig Configuration object for Fairscale OSS verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" super(FairscaleOSSExtension, self).__init__(verbose=verbose) self._oss_config = oss_config def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> OSS: \"\"\"Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- OSS instantiated Fairscale OSS optimizer object \"\"\" if self._verbose: self._print_device( f\"Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}\" ) return OSS( params=model.parameters(), optim=optimizer, broadcast_fp16=self._oss_config.broadcast_fp16, **optimizer_kwargs, )","title":"Attributes"},{"location":"reference/stoke/extensions/#ancestors-in-mro_2","text":"stoke.extensions.BaseOptimizer abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/extensions/#methods_3","text":"","title":"Methods"},{"location":"reference/stoke/extensions/#build_optimizer_1","text":"def build_optimizer ( self , optimizer : Type [ torch . optim . optimizer . Optimizer ], optimizer_kwargs : Dict , model : torch . nn . modules . module . Module ) -> fairscale . optim . oss . OSS Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs Parameters: Name Type Description Default optimizer Type[torch.optim.Optimizer] type of torch optimizer None optimizer_kwargs Dict dictionary of all kwargs to pass to the optimizer None model torch.nn.Module model object None Returns: Type Description OSS instantiated Fairscale OSS optimizer object ??? example \"View Source\" def build_optimizer( self, optimizer: Type[torch.optim.Optimizer], optimizer_kwargs: Dict, model: torch.nn.Module, ) -> OSS: \"\"\"Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs Parameters ---------- optimizer: Type[torch.optim.Optimizer] type of torch optimizer optimizer_kwargs: Dict dictionary of all kwargs to pass to the optimizer model: torch.nn.Module model object Returns ------- OSS instantiated Fairscale OSS optimizer object \"\"\" if self._verbose: self._print_device( f\"Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}\" ) return OSS( params=model.parameters(), optim=optimizer, broadcast_fp16=self._oss_config.broadcast_fp16, **optimizer_kwargs, )","title":"build_optimizer"},{"location":"reference/stoke/extensions/#fairscalesddpextension","text":"class FairscaleSDDPExtension ( sddp_config : stoke . configs . FairscaleSDDPConfig , verbose : bool = True , ** kwargs )","title":"FairscaleSDDPExtension"},{"location":"reference/stoke/extensions/#attributes_4","text":"Name Type Description Default _sddp_config FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration object None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class FairscaleSDDPExtension: \"\"\"Class for using the Fairscale SDDP backend Attributes ---------- _sddp_config: FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration object _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__( self, sddp_config: FairscaleSDDPConfig, verbose: bool = True, **kwargs ): \"\"\"Init for FairscaleSDDPExtension Parameters ---------- sddp_config: FairscaleSDDPConfig Base Fairscale ShardedDataParallel configuration objet verbose: bool, default: True flag for Stoke print verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call \"\"\" self._verbose = verbose self._sddp_config = sddp_config def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the ShardedDataParallel call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = ShardedDataParallel( module=model, sharded_optimizer=optimizer, broadcast_buffers=self._sddp_config.broadcast_buffers, sync_models_at_startup=self._sddp_config.sync_models_at_startup, reduce_buffer_size=self._sddp_config.reduce_buffer_size, auto_refresh_trainable=self._sddp_config.auto_refresh_trainable, reduce_fp16=self._sddp_config.reduce_fp16, ) return model, optimizer","title":"Attributes"},{"location":"reference/stoke/extensions/#methods_4","text":"","title":"Methods"},{"location":"reference/stoke/extensions/#handle_ddp_2","text":"def handle_ddp ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], grad_accum : Union [ int , NoneType ], rank : int ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps the model in the ShardedDataParallel call Parameters: Name Type Description Default model torch.nn.Module Current model object None optimizer Union[torch.optim.Optimizer, OSS] Current optimizer object None grad_accum int, default: None Number of gradient accumulation steps None rank int Current CUDA device rank in the distributed setup None Returns: Type Description torch.nn.Module Wrapped model object ??? example \"View Source\" def handle_ddp( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], grad_accum: Optional[int], rank: int, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps the model in the ShardedDataParallel call Parameters ---------- model: torch.nn.Module Current model object optimizer: Union[torch.optim.Optimizer, OSS] Current optimizer object grad_accum: int, default: None Number of gradient accumulation steps rank: int Current CUDA device rank in the distributed setup Returns ------- model: torch.nn.Module Wrapped model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object \"\"\" model = ShardedDataParallel( module=model, sharded_optimizer=optimizer, broadcast_buffers=self._sddp_config.broadcast_buffers, sync_models_at_startup=self._sddp_config.sync_models_at_startup, reduce_buffer_size=self._sddp_config.reduce_buffer_size, auto_refresh_trainable=self._sddp_config.auto_refresh_trainable, reduce_fp16=self._sddp_config.reduce_fp16, ) return model, optimizer","title":"handle_ddp"},{"location":"reference/stoke/extensions/#runneroptimizerenum","text":"class RunnerOptimizerEnum ( / , * args , ** kwargs ) ??? example \"View Source\" class RunnerOptimizerEnum(Enum): \"\"\"Enum for optimizer creation\"\"\" oss = FairscaleOSSExtension base = BaseOptimizer","title":"RunnerOptimizerEnum"},{"location":"reference/stoke/extensions/#ancestors-in-mro_3","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/extensions/#class-variables_1","text":"base name oss value","title":"Class variables"},{"location":"reference/stoke/fp16/","text":"Module stoke.fp16 Handles FP16/mixed-precision related classes -- mixin style None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles FP16/mixed-precision related classes -- mixin style\"\"\" from abc import ABC from contextlib import nullcontext from enum import Enum from typing import List, Optional, Tuple, Union import torch from fairscale.optim.grad_scaler import ShardedGradScaler from fairscale.optim.oss import OSS from stoke.configs import ClipGradConfig, ClipGradNormConfig class BaseFP16(ABC): \"\"\"Base class for mixed precision and FP16 functionality This class handles base and common functionality for all of the different mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _scaler: default: None scaler object for backends that require one _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, scaler=None, verbose: bool = True): \"\"\"Init for BaseFP16 class Parameters ---------- scaler: default: None scaler object for backends that require one verbose: bool, default: True flag for verbosity \"\"\" self._scaler = scaler self._verbose = verbose def _scaler_info(self): if self._verbose and self._scaler is not None: self._print_device( f\"FP16 Mixin: Initialized scaler of type {type(self._scaler).__name__}\" ) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value) def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type ) @property def scaler(self): \"\"\"Returns grad scaler\"\"\" return self._scaler @property def loss_context(self): \"\"\"Returns the base context wrapper for the loss call\"\"\" return nullcontext() @property def model_context(self): \"\"\"Returns the base context wrapper for the model call\"\"\" return nullcontext() def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Base wrapped backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): val.backward(retain_graph=(idx == 0)) else: loss.backward() def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step() class NullFP16(BaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for NullFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Scaler set to None as it is not needed \"\"\" super(NullFP16, self).__init__(scaler=None, verbose=verbose) class DeepspeedFP16(NullFP16): def __init__(self, verbose: bool = True, **kwargs): super(DeepspeedFP16, self).__init__(verbose=verbose) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls backward via the model engine instead of the loss Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): model.backward(val, retain_graph=(idx == 0)) else: model.backward(loss) def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls step via the model engine instead of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" model.step() class ApexBaseFP16(BaseFP16): \"\"\"Base class for Apex FP16 methods This class handles base and common functionality for O1 and O2 Apex mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _apex_config: ApexConfig Configuration object for Apex _multi_loss: int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) _scaler: default: None scaler object for backends that require one _verbose bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexBaseFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexBaseFP16, self).__init__(scaler=None, verbose=verbose) self._conditional_import() self._apex_config = kwargs[\"apex_config\"] self._multi_loss = ( len(kwargs[\"loss\"]) if isinstance(kwargs[\"loss\"], (list, tuple)) else 1 ) @staticmethod def _conditional_import(): \"\"\"Attempts to conditionally import apex if the functionality is required Raises ------ ImportError If apex cannot be imported Returns ------- None \"\"\" try: global amp from apex import amp except ImportError as e: print( e, \": apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) def _apex_convert_to_sync_batch_norm(self, model: torch.nn.Module): \"\"\"Replaces all BatchNorm*D layers with apex.parallel.SyncBatchNorm layers Parameters ---------- model: torch.nn.Module current model object Returns ------- model: torch.nn.Module modified version of model with all BatchNorm*D layers replaced with apex.parallel.SyncBatchNorm layers \"\"\" self.print_device( f\"Converting all BatchNorm*D layers to apex.parallel.SyncBatchNorm layers...\" ) try: from apex.parallel import convert_syncbn_model model = convert_syncbn_model(module=model) except ImportError as e: print( e, \": apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) return model def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value ) def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type ) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() class ApexO2AmpFP16(ApexBaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexO2AmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexO2AmpFP16, self).__init__(verbose=verbose, **kwargs) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O2\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer class ApexO1AmpFP16(ApexBaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexO1AmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexO1AmpFP16, self).__init__(verbose=verbose, **kwargs) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O1\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer class NativeAmpFP16(BaseFP16): \"\"\"Base class for PyTorch Native AMP FP16 methods This class handles base and common functionality for native PyTorch AMP mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _amp_config: AMPConfig Configuration object for Apex _scaler: default: torch.cuda.amp.GradScaler scaler object for loss _verbose bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for NativeAmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here amp_config or sharded_config might be passed in Notes ----- Scaler set between torch.cuda.amp.GradScaler and ShardedGradScaler depending on if a sharded config is passed via kwargs \"\"\" self._amp_config = kwargs[\"amp_config\"] # Switch the scaler obj ref depending on fairscale sharding scaler = ( ShardedGradScaler if (kwargs[\"sharded_config\"] is not None) or (kwargs[\"fully_sharded_config\"] is not None) else torch.cuda.amp.GradScaler ) super(NativeAmpFP16, self).__init__( scaler=scaler( backoff_factor=self._amp_config.backoff_factor, enabled=True, growth_factor=self._amp_config.growth_factor, growth_interval=self._amp_config.growth_interval, init_scale=self._amp_config.init_scale, ), verbose=verbose, ) @property def loss_context(self): \"\"\"Overrides base and returns the native AMP autocast context\"\"\" return torch.cuda.amp.autocast(enabled=True) @property def model_context(self): \"\"\"Overrides base and returns the native AMP autocast context\"\"\" return torch.cuda.amp.autocast(enabled=True) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Overrides base wrapped backward call for AMP scaled backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): self._scaler.scale(val).backward(retain_graph=(idx == 0)) else: self._scaler.scale(loss).backward() def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Overrides base wrapped step of the optimizer with the AMP scaler version Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" self.scaler.step(optimizer) self.scaler.update() class RunnerFP16Enum(Enum): \"\"\"Enum for building the runtime object with mixed-precision functionality\"\"\" full = NullFP16 apex_O1 = ApexO1AmpFP16 apex_O2 = ApexO2AmpFP16 amp = NativeAmpFP16 deepspeed = DeepspeedFP16 Classes ApexBaseFP16 class ApexBaseFP16 ( verbose : bool = True , ** kwargs ) Attributes Name Type Description Default loss_context None None None model_context None None None scaler None None None _apex_config ApexConfig Configuration object for Apex None _multi_loss int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) None _scaler default: None scaler object for backends that require one None _verbose bool, default True flag for Stoke print verbosity None ??? example \"View Source\" class ApexBaseFP16(BaseFP16): \"\"\"Base class for Apex FP16 methods This class handles base and common functionality for O1 and O2 Apex mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _apex_config: ApexConfig Configuration object for Apex _multi_loss: int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) _scaler: default: None scaler object for backends that require one _verbose bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexBaseFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexBaseFP16, self).__init__(scaler=None, verbose=verbose) self._conditional_import() self._apex_config = kwargs[\"apex_config\"] self._multi_loss = ( len(kwargs[\"loss\"]) if isinstance(kwargs[\"loss\"], (list, tuple)) else 1 ) @staticmethod def _conditional_import(): \"\"\"Attempts to conditionally import apex if the functionality is required Raises ------ ImportError If apex cannot be imported Returns ------- None \"\"\" try: global amp from apex import amp except ImportError as e: print( e, \": apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) def _apex_convert_to_sync_batch_norm(self, model: torch.nn.Module): \"\"\"Replaces all BatchNorm*D layers with apex.parallel.SyncBatchNorm layers Parameters ---------- model: torch.nn.Module current model object Returns ------- model: torch.nn.Module modified version of model with all BatchNorm*D layers replaced with apex.parallel.SyncBatchNorm layers \"\"\" self.print_device( f\"Converting all BatchNorm*D layers to apex.parallel.SyncBatchNorm layers...\" ) try: from apex.parallel import convert_syncbn_model model = convert_syncbn_model(module=model) except ImportError as e: print( e, \": apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) return model def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value ) def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type ) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() Ancestors (in MRO) stoke.fp16.BaseFP16 abc.ABC Descendants stoke.fp16.ApexO2AmpFP16 stoke.fp16.ApexO1AmpFP16 Instance variables loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler Methods backward_call def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() clip_grad def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) clip_grad_norm def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type ) clip_grad_value def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value ) step_call def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step() wrap_fp16 def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer ApexO1AmpFP16 class ApexO1AmpFP16 ( verbose : bool = True , ** kwargs ) Attributes Name Type Description Default loss_context None None None model_context None None None scaler None None None _apex_config ApexConfig Configuration object for Apex None _multi_loss int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) None _scaler default: None scaler object for backends that require one None _verbose bool, default True flag for Stoke print verbosity None ??? example \"View Source\" class ApexO1AmpFP16(ApexBaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexO1AmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexO1AmpFP16, self).__init__(verbose=verbose, **kwargs) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O1\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer Ancestors (in MRO) stoke.fp16.ApexBaseFP16 stoke.fp16.BaseFP16 abc.ABC Instance variables loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler Methods backward_call def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() clip_grad def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) clip_grad_norm def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type ) clip_grad_value def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value ) step_call def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step() wrap_fp16 def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O1\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer ApexO2AmpFP16 class ApexO2AmpFP16 ( verbose : bool = True , ** kwargs ) Attributes Name Type Description Default loss_context None None None model_context None None None scaler None None None _apex_config ApexConfig Configuration object for Apex None _multi_loss int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) None _scaler default: None scaler object for backends that require one None _verbose bool, default True flag for Stoke print verbosity None ??? example \"View Source\" class ApexO2AmpFP16(ApexBaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexO2AmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexO2AmpFP16, self).__init__(verbose=verbose, **kwargs) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O2\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer Ancestors (in MRO) stoke.fp16.ApexBaseFP16 stoke.fp16.BaseFP16 abc.ABC Instance variables loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler Methods backward_call def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() clip_grad def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) clip_grad_norm def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type ) clip_grad_value def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value ) step_call def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step() wrap_fp16 def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O2\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer BaseFP16 class BaseFP16 ( scaler = None , verbose : bool = True ) Attributes Name Type Description Default loss_context None None None model_context None None None scaler None None None _scaler default: None scaler object for backends that require one None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class BaseFP16(ABC): \"\"\"Base class for mixed precision and FP16 functionality This class handles base and common functionality for all of the different mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _scaler: default: None scaler object for backends that require one _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, scaler=None, verbose: bool = True): \"\"\"Init for BaseFP16 class Parameters ---------- scaler: default: None scaler object for backends that require one verbose: bool, default: True flag for verbosity \"\"\" self._scaler = scaler self._verbose = verbose def _scaler_info(self): if self._verbose and self._scaler is not None: self._print_device( f\"FP16 Mixin: Initialized scaler of type {type(self._scaler).__name__}\" ) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value) def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type ) @property def scaler(self): \"\"\"Returns grad scaler\"\"\" return self._scaler @property def loss_context(self): \"\"\"Returns the base context wrapper for the loss call\"\"\" return nullcontext() @property def model_context(self): \"\"\"Returns the base context wrapper for the model call\"\"\" return nullcontext() def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Base wrapped backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): val.backward(retain_graph=(idx == 0)) else: loss.backward() def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step() Ancestors (in MRO) abc.ABC Descendants stoke.fp16.NullFP16 stoke.fp16.ApexBaseFP16 stoke.fp16.NativeAmpFP16 Instance variables loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler Methods backward_call def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Base wrapped backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): val.backward(retain_graph=(idx == 0)) else: loss.backward() clip_grad def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) clip_grad_norm def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type ) clip_grad_value def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Base handle clip gradients by value Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value) step_call def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step() wrap_fp16 def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer DeepspeedFP16 class DeepspeedFP16 ( verbose : bool = True , ** kwargs ) Attributes Name Type Description Default loss_context None None None model_context None None None scaler None None None _scaler default: None scaler object for backends that require one None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DeepspeedFP16(NullFP16): def __init__(self, verbose: bool = True, **kwargs): super(DeepspeedFP16, self).__init__(verbose=verbose) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls backward via the model engine instead of the loss Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): model.backward(val, retain_graph=(idx == 0)) else: model.backward(loss) def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls step via the model engine instead of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" model.step() Ancestors (in MRO) stoke.fp16.NullFP16 stoke.fp16.BaseFP16 abc.ABC Instance variables loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler Methods backward_call def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override of deepspeed wrapped backward call Deepspeed calls backward via the model engine instead of the loss Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls backward via the model engine instead of the loss Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): model.backward(val, retain_graph=(idx == 0)) else: model.backward(loss) clip_grad def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) clip_grad_norm def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type ) clip_grad_value def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Base handle clip gradients by value Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value) step_call def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override of deepspeed wrapped backward call Deepspeed calls step via the model engine instead of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls step via the model engine instead of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" model.step() wrap_fp16 def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer NativeAmpFP16 class NativeAmpFP16 ( verbose : bool = True , ** kwargs ) Attributes Name Type Description Default loss_context None None None model_context None None None scaler None None None _amp_config AMPConfig Configuration object for Apex None _scaler default: torch.cuda.amp.GradScaler scaler object for loss None _verbose bool, default True flag for Stoke print verbosity None ??? example \"View Source\" class NativeAmpFP16(BaseFP16): \"\"\"Base class for PyTorch Native AMP FP16 methods This class handles base and common functionality for native PyTorch AMP mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _amp_config: AMPConfig Configuration object for Apex _scaler: default: torch.cuda.amp.GradScaler scaler object for loss _verbose bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for NativeAmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here amp_config or sharded_config might be passed in Notes ----- Scaler set between torch.cuda.amp.GradScaler and ShardedGradScaler depending on if a sharded config is passed via kwargs \"\"\" self._amp_config = kwargs[\"amp_config\"] # Switch the scaler obj ref depending on fairscale sharding scaler = ( ShardedGradScaler if (kwargs[\"sharded_config\"] is not None) or (kwargs[\"fully_sharded_config\"] is not None) else torch.cuda.amp.GradScaler ) super(NativeAmpFP16, self).__init__( scaler=scaler( backoff_factor=self._amp_config.backoff_factor, enabled=True, growth_factor=self._amp_config.growth_factor, growth_interval=self._amp_config.growth_interval, init_scale=self._amp_config.init_scale, ), verbose=verbose, ) @property def loss_context(self): \"\"\"Overrides base and returns the native AMP autocast context\"\"\" return torch.cuda.amp.autocast(enabled=True) @property def model_context(self): \"\"\"Overrides base and returns the native AMP autocast context\"\"\" return torch.cuda.amp.autocast(enabled=True) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Overrides base wrapped backward call for AMP scaled backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): self._scaler.scale(val).backward(retain_graph=(idx == 0)) else: self._scaler.scale(loss).backward() def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Overrides base wrapped step of the optimizer with the AMP scaler version Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" self.scaler.step(optimizer) self.scaler.update() Ancestors (in MRO) stoke.fp16.BaseFP16 abc.ABC Instance variables loss_context Overrides base and returns the native AMP autocast context model_context Overrides base and returns the native AMP autocast context scaler Returns grad scaler Methods backward_call def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Overrides base wrapped backward call for AMP scaled backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Overrides base wrapped backward call for AMP scaled backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): self._scaler.scale(val).backward(retain_graph=(idx == 0)) else: self._scaler.scale(loss).backward() clip_grad def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) clip_grad_norm def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type ) clip_grad_value def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Base handle clip gradients by value Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value) step_call def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Overrides base wrapped step of the optimizer with the AMP scaler version Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Overrides base wrapped step of the optimizer with the AMP scaler version Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" self.scaler.step(optimizer) self.scaler.update() wrap_fp16 def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer NullFP16 class NullFP16 ( verbose : bool = True , ** kwargs ) Attributes Name Type Description Default loss_context None None None model_context None None None scaler None None None _scaler default: None scaler object for backends that require one None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class NullFP16(BaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for NullFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Scaler set to None as it is not needed \"\"\" super(NullFP16, self).__init__(scaler=None, verbose=verbose) Ancestors (in MRO) stoke.fp16.BaseFP16 abc.ABC Descendants stoke.fp16.DeepspeedFP16 Instance variables loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler Methods backward_call def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Base wrapped backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): val.backward(retain_graph=(idx == 0)) else: loss.backward() clip_grad def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) clip_grad_norm def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type ) clip_grad_value def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Base handle clip gradients by value Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value) step_call def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step() wrap_fp16 def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer RunnerFP16Enum class RunnerFP16Enum ( / , * args , ** kwargs ) ??? example \"View Source\" class RunnerFP16Enum(Enum): \"\"\"Enum for building the runtime object with mixed-precision functionality\"\"\" full = NullFP16 apex_O1 = ApexO1AmpFP16 apex_O2 = ApexO2AmpFP16 amp = NativeAmpFP16 deepspeed = DeepspeedFP16 Ancestors (in MRO) enum.Enum Class variables amp apex_O1 apex_O2 deepspeed full name value","title":"Fp16"},{"location":"reference/stoke/fp16/#module-stokefp16","text":"Handles FP16/mixed-precision related classes -- mixin style None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles FP16/mixed-precision related classes -- mixin style\"\"\" from abc import ABC from contextlib import nullcontext from enum import Enum from typing import List, Optional, Tuple, Union import torch from fairscale.optim.grad_scaler import ShardedGradScaler from fairscale.optim.oss import OSS from stoke.configs import ClipGradConfig, ClipGradNormConfig class BaseFP16(ABC): \"\"\"Base class for mixed precision and FP16 functionality This class handles base and common functionality for all of the different mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _scaler: default: None scaler object for backends that require one _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, scaler=None, verbose: bool = True): \"\"\"Init for BaseFP16 class Parameters ---------- scaler: default: None scaler object for backends that require one verbose: bool, default: True flag for verbosity \"\"\" self._scaler = scaler self._verbose = verbose def _scaler_info(self): if self._verbose and self._scaler is not None: self._print_device( f\"FP16 Mixin: Initialized scaler of type {type(self._scaler).__name__}\" ) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value) def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type ) @property def scaler(self): \"\"\"Returns grad scaler\"\"\" return self._scaler @property def loss_context(self): \"\"\"Returns the base context wrapper for the loss call\"\"\" return nullcontext() @property def model_context(self): \"\"\"Returns the base context wrapper for the model call\"\"\" return nullcontext() def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Base wrapped backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): val.backward(retain_graph=(idx == 0)) else: loss.backward() def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step() class NullFP16(BaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for NullFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Scaler set to None as it is not needed \"\"\" super(NullFP16, self).__init__(scaler=None, verbose=verbose) class DeepspeedFP16(NullFP16): def __init__(self, verbose: bool = True, **kwargs): super(DeepspeedFP16, self).__init__(verbose=verbose) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls backward via the model engine instead of the loss Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): model.backward(val, retain_graph=(idx == 0)) else: model.backward(loss) def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls step via the model engine instead of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" model.step() class ApexBaseFP16(BaseFP16): \"\"\"Base class for Apex FP16 methods This class handles base and common functionality for O1 and O2 Apex mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _apex_config: ApexConfig Configuration object for Apex _multi_loss: int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) _scaler: default: None scaler object for backends that require one _verbose bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexBaseFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexBaseFP16, self).__init__(scaler=None, verbose=verbose) self._conditional_import() self._apex_config = kwargs[\"apex_config\"] self._multi_loss = ( len(kwargs[\"loss\"]) if isinstance(kwargs[\"loss\"], (list, tuple)) else 1 ) @staticmethod def _conditional_import(): \"\"\"Attempts to conditionally import apex if the functionality is required Raises ------ ImportError If apex cannot be imported Returns ------- None \"\"\" try: global amp from apex import amp except ImportError as e: print( e, \": apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) def _apex_convert_to_sync_batch_norm(self, model: torch.nn.Module): \"\"\"Replaces all BatchNorm*D layers with apex.parallel.SyncBatchNorm layers Parameters ---------- model: torch.nn.Module current model object Returns ------- model: torch.nn.Module modified version of model with all BatchNorm*D layers replaced with apex.parallel.SyncBatchNorm layers \"\"\" self.print_device( f\"Converting all BatchNorm*D layers to apex.parallel.SyncBatchNorm layers...\" ) try: from apex.parallel import convert_syncbn_model model = convert_syncbn_model(module=model) except ImportError as e: print( e, \": apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) return model def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value ) def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type ) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() class ApexO2AmpFP16(ApexBaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexO2AmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexO2AmpFP16, self).__init__(verbose=verbose, **kwargs) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O2\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer class ApexO1AmpFP16(ApexBaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexO1AmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexO1AmpFP16, self).__init__(verbose=verbose, **kwargs) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O1\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer class NativeAmpFP16(BaseFP16): \"\"\"Base class for PyTorch Native AMP FP16 methods This class handles base and common functionality for native PyTorch AMP mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _amp_config: AMPConfig Configuration object for Apex _scaler: default: torch.cuda.amp.GradScaler scaler object for loss _verbose bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for NativeAmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here amp_config or sharded_config might be passed in Notes ----- Scaler set between torch.cuda.amp.GradScaler and ShardedGradScaler depending on if a sharded config is passed via kwargs \"\"\" self._amp_config = kwargs[\"amp_config\"] # Switch the scaler obj ref depending on fairscale sharding scaler = ( ShardedGradScaler if (kwargs[\"sharded_config\"] is not None) or (kwargs[\"fully_sharded_config\"] is not None) else torch.cuda.amp.GradScaler ) super(NativeAmpFP16, self).__init__( scaler=scaler( backoff_factor=self._amp_config.backoff_factor, enabled=True, growth_factor=self._amp_config.growth_factor, growth_interval=self._amp_config.growth_interval, init_scale=self._amp_config.init_scale, ), verbose=verbose, ) @property def loss_context(self): \"\"\"Overrides base and returns the native AMP autocast context\"\"\" return torch.cuda.amp.autocast(enabled=True) @property def model_context(self): \"\"\"Overrides base and returns the native AMP autocast context\"\"\" return torch.cuda.amp.autocast(enabled=True) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Overrides base wrapped backward call for AMP scaled backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): self._scaler.scale(val).backward(retain_graph=(idx == 0)) else: self._scaler.scale(loss).backward() def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Overrides base wrapped step of the optimizer with the AMP scaler version Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" self.scaler.step(optimizer) self.scaler.update() class RunnerFP16Enum(Enum): \"\"\"Enum for building the runtime object with mixed-precision functionality\"\"\" full = NullFP16 apex_O1 = ApexO1AmpFP16 apex_O2 = ApexO2AmpFP16 amp = NativeAmpFP16 deepspeed = DeepspeedFP16","title":"Module stoke.fp16"},{"location":"reference/stoke/fp16/#classes","text":"","title":"Classes"},{"location":"reference/stoke/fp16/#apexbasefp16","text":"class ApexBaseFP16 ( verbose : bool = True , ** kwargs )","title":"ApexBaseFP16"},{"location":"reference/stoke/fp16/#attributes","text":"Name Type Description Default loss_context None None None model_context None None None scaler None None None _apex_config ApexConfig Configuration object for Apex None _multi_loss int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) None _scaler default: None scaler object for backends that require one None _verbose bool, default True flag for Stoke print verbosity None ??? example \"View Source\" class ApexBaseFP16(BaseFP16): \"\"\"Base class for Apex FP16 methods This class handles base and common functionality for O1 and O2 Apex mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _apex_config: ApexConfig Configuration object for Apex _multi_loss: int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) _scaler: default: None scaler object for backends that require one _verbose bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexBaseFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexBaseFP16, self).__init__(scaler=None, verbose=verbose) self._conditional_import() self._apex_config = kwargs[\"apex_config\"] self._multi_loss = ( len(kwargs[\"loss\"]) if isinstance(kwargs[\"loss\"], (list, tuple)) else 1 ) @staticmethod def _conditional_import(): \"\"\"Attempts to conditionally import apex if the functionality is required Raises ------ ImportError If apex cannot be imported Returns ------- None \"\"\" try: global amp from apex import amp except ImportError as e: print( e, \": apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) def _apex_convert_to_sync_batch_norm(self, model: torch.nn.Module): \"\"\"Replaces all BatchNorm*D layers with apex.parallel.SyncBatchNorm layers Parameters ---------- model: torch.nn.Module current model object Returns ------- model: torch.nn.Module modified version of model with all BatchNorm*D layers replaced with apex.parallel.SyncBatchNorm layers \"\"\" self.print_device( f\"Converting all BatchNorm*D layers to apex.parallel.SyncBatchNorm layers...\" ) try: from apex.parallel import convert_syncbn_model model = convert_syncbn_model(module=model) except ImportError as e: print( e, \": apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) return model def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value ) def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type ) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward()","title":"Attributes"},{"location":"reference/stoke/fp16/#ancestors-in-mro","text":"stoke.fp16.BaseFP16 abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/fp16/#descendants","text":"stoke.fp16.ApexO2AmpFP16 stoke.fp16.ApexO1AmpFP16","title":"Descendants"},{"location":"reference/stoke/fp16/#instance-variables","text":"loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler","title":"Instance variables"},{"location":"reference/stoke/fp16/#methods","text":"","title":"Methods"},{"location":"reference/stoke/fp16/#backward_call","text":"def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward()","title":"backward_call"},{"location":"reference/stoke/fp16/#clip_grad","text":"def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" )","title":"clip_grad"},{"location":"reference/stoke/fp16/#clip_grad_norm","text":"def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type )","title":"clip_grad_norm"},{"location":"reference/stoke/fp16/#clip_grad_value","text":"def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value )","title":"clip_grad_value"},{"location":"reference/stoke/fp16/#step_call","text":"def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step()","title":"step_call"},{"location":"reference/stoke/fp16/#wrap_fp16","text":"def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer","title":"wrap_fp16"},{"location":"reference/stoke/fp16/#apexo1ampfp16","text":"class ApexO1AmpFP16 ( verbose : bool = True , ** kwargs )","title":"ApexO1AmpFP16"},{"location":"reference/stoke/fp16/#attributes_1","text":"Name Type Description Default loss_context None None None model_context None None None scaler None None None _apex_config ApexConfig Configuration object for Apex None _multi_loss int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) None _scaler default: None scaler object for backends that require one None _verbose bool, default True flag for Stoke print verbosity None ??? example \"View Source\" class ApexO1AmpFP16(ApexBaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexO1AmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexO1AmpFP16, self).__init__(verbose=verbose, **kwargs) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O1\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer","title":"Attributes"},{"location":"reference/stoke/fp16/#ancestors-in-mro_1","text":"stoke.fp16.ApexBaseFP16 stoke.fp16.BaseFP16 abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/fp16/#instance-variables_1","text":"loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler","title":"Instance variables"},{"location":"reference/stoke/fp16/#methods_1","text":"","title":"Methods"},{"location":"reference/stoke/fp16/#backward_call_1","text":"def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward()","title":"backward_call"},{"location":"reference/stoke/fp16/#clip_grad_1","text":"def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" )","title":"clip_grad"},{"location":"reference/stoke/fp16/#clip_grad_norm_1","text":"def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type )","title":"clip_grad_norm"},{"location":"reference/stoke/fp16/#clip_grad_value_1","text":"def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value )","title":"clip_grad_value"},{"location":"reference/stoke/fp16/#step_call_1","text":"def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step()","title":"step_call"},{"location":"reference/stoke/fp16/#wrap_fp16_1","text":"def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O1\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer","title":"wrap_fp16"},{"location":"reference/stoke/fp16/#apexo2ampfp16","text":"class ApexO2AmpFP16 ( verbose : bool = True , ** kwargs )","title":"ApexO2AmpFP16"},{"location":"reference/stoke/fp16/#attributes_2","text":"Name Type Description Default loss_context None None None model_context None None None scaler None None None _apex_config ApexConfig Configuration object for Apex None _multi_loss int, default: 1 Holds the number of losses to use (apex can use multiple scalers per loss) None _scaler default: None scaler object for backends that require one None _verbose bool, default True flag for Stoke print verbosity None ??? example \"View Source\" class ApexO2AmpFP16(ApexBaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for ApexO2AmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in Notes ----- Scaler set to None as it is not needed \"\"\" super(ApexO2AmpFP16, self).__init__(verbose=verbose, **kwargs) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O2\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer","title":"Attributes"},{"location":"reference/stoke/fp16/#ancestors-in-mro_2","text":"stoke.fp16.ApexBaseFP16 stoke.fp16.BaseFP16 abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/fp16/#instance-variables_2","text":"loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler","title":"Instance variables"},{"location":"reference/stoke/fp16/#methods_2","text":"","title":"Methods"},{"location":"reference/stoke/fp16/#backward_call_2","text":"def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override wrapped backward call for APEX Need to use APEX scale_loss context with backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): with amp.scale_loss( val, optimizer, loss_id=idx if self._apex_config.scaler_per_loss else 0, ) as scaled_loss: scaled_loss.backward(retain_graph=(idx == 0)) else: with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward()","title":"backward_call"},{"location":"reference/stoke/fp16/#clip_grad_2","text":"def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" )","title":"clip_grad"},{"location":"reference/stoke/fp16/#clip_grad_norm_2","text":"def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Override handle clip gradients by the norm for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_norm_( amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type )","title":"clip_grad_norm"},{"location":"reference/stoke/fp16/#clip_grad_value_2","text":"def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Override handle clip gradients by value for APEX Need to call master_params within APEX to clip correctly Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self._verbose: self._print_device( f\"Automatically clipping calculated/accumulated gradients...\" ) torch.nn.utils.clip_grad_value_( amp.master_params(optimizer), clip_value=clip_value )","title":"clip_grad_value"},{"location":"reference/stoke/fp16/#step_call_2","text":"def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step()","title":"step_call"},{"location":"reference/stoke/fp16/#wrap_fp16_2","text":"def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() if self._apex_config.convert_to_sync_batch_norm: model = self._apex_convert_to_sync_batch_norm(model=model) model, optimizer = amp.initialize( model, optimizer, opt_level=\"O2\", cast_model_outputs=self._apex_config.cast_model_outputs, max_loss_scale=self._apex_config.max_loss_scale, min_loss_scale=self._apex_config.min_loss_scale, verbosity=self._apex_config.verbosity, num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1, ) return model, optimizer","title":"wrap_fp16"},{"location":"reference/stoke/fp16/#basefp16","text":"class BaseFP16 ( scaler = None , verbose : bool = True )","title":"BaseFP16"},{"location":"reference/stoke/fp16/#attributes_3","text":"Name Type Description Default loss_context None None None model_context None None None scaler None None None _scaler default: None scaler object for backends that require one None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class BaseFP16(ABC): \"\"\"Base class for mixed precision and FP16 functionality This class handles base and common functionality for all of the different mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _scaler: default: None scaler object for backends that require one _verbose: bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, scaler=None, verbose: bool = True): \"\"\"Init for BaseFP16 class Parameters ---------- scaler: default: None scaler object for backends that require one verbose: bool, default: True flag for verbosity \"\"\" self._scaler = scaler self._verbose = verbose def _scaler_info(self): if self._verbose and self._scaler is not None: self._print_device( f\"FP16 Mixin: Initialized scaler of type {type(self._scaler).__name__}\" ) def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" ) def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value) def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type ) @property def scaler(self): \"\"\"Returns grad scaler\"\"\" return self._scaler @property def loss_context(self): \"\"\"Returns the base context wrapper for the loss call\"\"\" return nullcontext() @property def model_context(self): \"\"\"Returns the base context wrapper for the model call\"\"\" return nullcontext() def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Base wrapped backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): val.backward(retain_graph=(idx == 0)) else: loss.backward() def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step()","title":"Attributes"},{"location":"reference/stoke/fp16/#ancestors-in-mro_3","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/fp16/#descendants_1","text":"stoke.fp16.NullFP16 stoke.fp16.ApexBaseFP16 stoke.fp16.NativeAmpFP16","title":"Descendants"},{"location":"reference/stoke/fp16/#instance-variables_3","text":"loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler","title":"Instance variables"},{"location":"reference/stoke/fp16/#methods_3","text":"","title":"Methods"},{"location":"reference/stoke/fp16/#backward_call_3","text":"def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Base wrapped backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): val.backward(retain_graph=(idx == 0)) else: loss.backward()","title":"backward_call"},{"location":"reference/stoke/fp16/#clip_grad_3","text":"def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" )","title":"clip_grad"},{"location":"reference/stoke/fp16/#clip_grad_norm_3","text":"def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type )","title":"clip_grad_norm"},{"location":"reference/stoke/fp16/#clip_grad_value_3","text":"def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Base handle clip gradients by value Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)","title":"clip_grad_value"},{"location":"reference/stoke/fp16/#step_call_3","text":"def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step()","title":"step_call"},{"location":"reference/stoke/fp16/#wrap_fp16_3","text":"def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer","title":"wrap_fp16"},{"location":"reference/stoke/fp16/#deepspeedfp16","text":"class DeepspeedFP16 ( verbose : bool = True , ** kwargs )","title":"DeepspeedFP16"},{"location":"reference/stoke/fp16/#attributes_4","text":"Name Type Description Default loss_context None None None model_context None None None scaler None None None _scaler default: None scaler object for backends that require one None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class DeepspeedFP16(NullFP16): def __init__(self, verbose: bool = True, **kwargs): super(DeepspeedFP16, self).__init__(verbose=verbose) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls backward via the model engine instead of the loss Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): model.backward(val, retain_graph=(idx == 0)) else: model.backward(loss) def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls step via the model engine instead of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" model.step()","title":"Attributes"},{"location":"reference/stoke/fp16/#ancestors-in-mro_4","text":"stoke.fp16.NullFP16 stoke.fp16.BaseFP16 abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/fp16/#instance-variables_4","text":"loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler","title":"Instance variables"},{"location":"reference/stoke/fp16/#methods_4","text":"","title":"Methods"},{"location":"reference/stoke/fp16/#backward_call_4","text":"def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override of deepspeed wrapped backward call Deepspeed calls backward via the model engine instead of the loss Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls backward via the model engine instead of the loss Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): model.backward(val, retain_graph=(idx == 0)) else: model.backward(loss)","title":"backward_call"},{"location":"reference/stoke/fp16/#clip_grad_4","text":"def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" )","title":"clip_grad"},{"location":"reference/stoke/fp16/#clip_grad_norm_4","text":"def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type )","title":"clip_grad_norm"},{"location":"reference/stoke/fp16/#clip_grad_value_4","text":"def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Base handle clip gradients by value Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)","title":"clip_grad_value"},{"location":"reference/stoke/fp16/#step_call_4","text":"def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Override of deepspeed wrapped backward call Deepspeed calls step via the model engine instead of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Override of deepspeed wrapped backward call Deepspeed calls step via the model engine instead of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" model.step()","title":"step_call"},{"location":"reference/stoke/fp16/#wrap_fp16_4","text":"def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer","title":"wrap_fp16"},{"location":"reference/stoke/fp16/#nativeampfp16","text":"class NativeAmpFP16 ( verbose : bool = True , ** kwargs )","title":"NativeAmpFP16"},{"location":"reference/stoke/fp16/#attributes_5","text":"Name Type Description Default loss_context None None None model_context None None None scaler None None None _amp_config AMPConfig Configuration object for Apex None _scaler default: torch.cuda.amp.GradScaler scaler object for loss None _verbose bool, default True flag for Stoke print verbosity None ??? example \"View Source\" class NativeAmpFP16(BaseFP16): \"\"\"Base class for PyTorch Native AMP FP16 methods This class handles base and common functionality for native PyTorch AMP mixed-precision backends. Contains functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward and loss calls Attributes ---------- loss_context model_context scaler _amp_config: AMPConfig Configuration object for Apex _scaler: default: torch.cuda.amp.GradScaler scaler object for loss _verbose bool, default: True flag for Stoke print verbosity \"\"\" def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for NativeAmpFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call -- here amp_config or sharded_config might be passed in Notes ----- Scaler set between torch.cuda.amp.GradScaler and ShardedGradScaler depending on if a sharded config is passed via kwargs \"\"\" self._amp_config = kwargs[\"amp_config\"] # Switch the scaler obj ref depending on fairscale sharding scaler = ( ShardedGradScaler if (kwargs[\"sharded_config\"] is not None) or (kwargs[\"fully_sharded_config\"] is not None) else torch.cuda.amp.GradScaler ) super(NativeAmpFP16, self).__init__( scaler=scaler( backoff_factor=self._amp_config.backoff_factor, enabled=True, growth_factor=self._amp_config.growth_factor, growth_interval=self._amp_config.growth_interval, init_scale=self._amp_config.init_scale, ), verbose=verbose, ) @property def loss_context(self): \"\"\"Overrides base and returns the native AMP autocast context\"\"\" return torch.cuda.amp.autocast(enabled=True) @property def model_context(self): \"\"\"Overrides base and returns the native AMP autocast context\"\"\" return torch.cuda.amp.autocast(enabled=True) def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Overrides base wrapped backward call for AMP scaled backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): self._scaler.scale(val).backward(retain_graph=(idx == 0)) else: self._scaler.scale(loss).backward() def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Overrides base wrapped step of the optimizer with the AMP scaler version Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" self.scaler.step(optimizer) self.scaler.update()","title":"Attributes"},{"location":"reference/stoke/fp16/#ancestors-in-mro_5","text":"stoke.fp16.BaseFP16 abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/fp16/#instance-variables_5","text":"loss_context Overrides base and returns the native AMP autocast context model_context Overrides base and returns the native AMP autocast context scaler Returns grad scaler","title":"Instance variables"},{"location":"reference/stoke/fp16/#methods_5","text":"","title":"Methods"},{"location":"reference/stoke/fp16/#backward_call_5","text":"def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Overrides base wrapped backward call for AMP scaled backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Overrides base wrapped backward call for AMP scaled backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): self._scaler.scale(val).backward(retain_graph=(idx == 0)) else: self._scaler.scale(loss).backward()","title":"backward_call"},{"location":"reference/stoke/fp16/#clip_grad_5","text":"def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" )","title":"clip_grad"},{"location":"reference/stoke/fp16/#clip_grad_norm_5","text":"def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type )","title":"clip_grad_norm"},{"location":"reference/stoke/fp16/#clip_grad_value_5","text":"def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Base handle clip gradients by value Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)","title":"clip_grad_value"},{"location":"reference/stoke/fp16/#step_call_5","text":"def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Overrides base wrapped step of the optimizer with the AMP scaler version Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Overrides base wrapped step of the optimizer with the AMP scaler version Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" self.scaler.step(optimizer) self.scaler.update()","title":"step_call"},{"location":"reference/stoke/fp16/#wrap_fp16_5","text":"def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer","title":"wrap_fp16"},{"location":"reference/stoke/fp16/#nullfp16","text":"class NullFP16 ( verbose : bool = True , ** kwargs )","title":"NullFP16"},{"location":"reference/stoke/fp16/#attributes_6","text":"Name Type Description Default loss_context None None None model_context None None None scaler None None None _scaler default: None scaler object for backends that require one None _verbose bool, default: True flag for Stoke print verbosity None ??? example \"View Source\" class NullFP16(BaseFP16): def __init__(self, verbose: bool = True, **kwargs): \"\"\"Init for NullFP16 class Parameters ---------- verbose: bool, default: True flag for verbosity **kwargs: dict, optional Extra arguments passed to the __init__ call Notes ----- Scaler set to None as it is not needed \"\"\" super(NullFP16, self).__init__(scaler=None, verbose=verbose)","title":"Attributes"},{"location":"reference/stoke/fp16/#ancestors-in-mro_6","text":"stoke.fp16.BaseFP16 abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/fp16/#descendants_2","text":"stoke.fp16.DeepspeedFP16","title":"Descendants"},{"location":"reference/stoke/fp16/#instance-variables_6","text":"loss_context Returns the base context wrapper for the loss call model_context Returns the base context wrapper for the model call scaler Returns grad scaler","title":"Instance variables"},{"location":"reference/stoke/fp16/#methods_6","text":"","title":"Methods"},{"location":"reference/stoke/fp16/#backward_call_6","text":"def backward_call ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped backward call Parameters: Name Type Description Default loss loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def backward_call( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], ): \"\"\"Base wrapped backward call Parameters ---------- loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] loss tensor(s) model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): for idx, val in enumerate(loss): val.backward(retain_graph=(idx == 0)) else: loss.backward()","title":"backward_call"},{"location":"reference/stoke/fp16/#clip_grad_6","text":"def clip_grad ( self , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig ], model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], oss : bool , horovod : bool , deepspeed : bool , fsdp : bool ) Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters: Name Type Description Default grad_clip Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use None model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None oss bool optimizer state sharding flag None horovod bool horovod flag None deepspeed bool deepspeed flag None fsdp bool fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad( self, grad_clip: Union[ClipGradConfig, ClipGradNormConfig], model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], oss: bool, horovod: bool, deepspeed: bool, fsdp: bool, ): \"\"\"Base handle clipping the current gradients Determines which method to use based on the gradient clipping config and the current runtime state Parameters ---------- grad_clip: Union[ClipGradConfig, ClipGradNormConfig] gradient clipping config that will determine which method to use model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object oss: bool optimizer state sharding flag horovod: bool horovod flag deepspeed: bool deepspeed flag fsdp: bool fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if deepspeed: if self._verbose: self._print_device( \"Letting deepspeed internally handle clipping calculated/accumulated \" \"gradients...\" ) else: if self._verbose: self._print_device( f'{type(grad_clip).__name__.replace(\"Config\", \"\")} ' f\"is automatically clipping calculated/accumulated gradients...\" ) if horovod: # Hidden here -- Horovod docs are terrible # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer if self._verbose: self._print_device( f\"Calling Horovod optimizer.synchronize() pre grad-clip\" ) optimizer.synchronize() if isinstance(grad_clip, ClipGradConfig): self.clip_grad_value( model=model, optimizer=optimizer, clip_value=grad_clip.clip_value ) elif isinstance(grad_clip, ClipGradNormConfig): self.clip_grad_norm( model=model, optimizer=optimizer, max_norm=grad_clip.max_norm, norm_type=grad_clip.norm_type, oss=oss, fsdp=fsdp, ) else: raise ValueError( f\"Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}\" )","title":"clip_grad"},{"location":"reference/stoke/fp16/#clip_grad_norm_6","text":"def clip_grad_norm ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], max_norm : Union [ float , int ], norm_type : Union [ float , int ], oss : bool = False , fsdp : bool = False ) Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None max_norm Union[float, int] max norm of the gradients None norm_type Union[float, int] type of the used p-norm None oss bool, default: False optimizer state sharding flag None fsdp bool, default: False fully sharded data parallel flag for Fairscale None Returns: Type Description None None ??? example \"View Source\" def clip_grad_norm( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], max_norm: Union[float, int], norm_type: Union[float, int], oss: bool = False, fsdp: bool = False, ): \"\"\"Base handle clip gradients by the norm Depending on some extension flags switch between the correct clip_grad_norm calls OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object max_norm: Union[float, int] max norm of the gradients norm_type: Union[float, int] type of the used p-norm oss: bool, default: False optimizer state sharding flag fsdp: bool, default: False fully sharded data parallel flag for Fairscale Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm if oss: optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type) # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm elif fsdp: model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type) else: torch.nn.utils.clip_grad_norm_( model.parameters(), max_norm=max_norm, norm_type=norm_type )","title":"clip_grad_norm"},{"location":"reference/stoke/fp16/#clip_grad_value_6","text":"def clip_grad_value ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], clip_value : float ) Base handle clip gradients by value Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None clip_value float absolute value to clip grads None Returns: Type Description None None ??? example \"View Source\" def clip_grad_value( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], clip_value: float, ): \"\"\"Base handle clip gradients by value Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object clip_value: float absolute value to clip grads Returns ------- None \"\"\" if self.scaler is not None: if self._verbose: self._print_device(f\"Automatically unscaling gradients...\") self._scaler.unscale_(optimizer) torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)","title":"clip_grad_value"},{"location":"reference/stoke/fp16/#step_call_6","text":"def step_call ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ] ) Base wrapped step of the optimizer Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description None None ??? example \"View Source\" def step_call( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS] ): \"\"\"Base wrapped step of the optimizer Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- None \"\"\" # Step the optimizer optimizer.step()","title":"step_call"},{"location":"reference/stoke/fp16/#wrap_fp16_6","text":"def wrap_fp16 ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS , NoneType ] = None ) -> Tuple [ torch . nn . modules . module . Module , Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ]] Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None Returns: Type Description torch.nn.Module modified version of model object for mixed-precision backends ??? example \"View Source\" def wrap_fp16( self, model: torch.nn.Module, optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None, ) -> Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]: \"\"\"Wraps model and optimizer with specific mixed-precision related backend wrappers Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object Returns ------- model: torch.nn.Module modified version of model object for mixed-precision backends optimizer: Union[torch.optim.Optimizer, OSS]] modified version of optimizer object for mixed-precision backends \"\"\" self._scaler_info() return model, optimizer","title":"wrap_fp16"},{"location":"reference/stoke/fp16/#runnerfp16enum","text":"class RunnerFP16Enum ( / , * args , ** kwargs ) ??? example \"View Source\" class RunnerFP16Enum(Enum): \"\"\"Enum for building the runtime object with mixed-precision functionality\"\"\" full = NullFP16 apex_O1 = ApexO1AmpFP16 apex_O2 = ApexO2AmpFP16 amp = NativeAmpFP16 deepspeed = DeepspeedFP16","title":"RunnerFP16Enum"},{"location":"reference/stoke/fp16/#ancestors-in-mro_7","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/fp16/#class-variables","text":"amp apex_O1 apex_O2 deepspeed full name value","title":"Class variables"},{"location":"reference/stoke/io/","text":"Module stoke.io Handles i/o related functions -- mixin style None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles i/o related functions -- mixin style\"\"\" from abc import ABC from enum import Enum from typing import Callable, Dict, Optional, Union import horovod.torch as hvd import torch from fairscale.nn.data_parallel import FullyShardedDataParallel from fairscale.optim.oss import OSS from stoke.utils import make_folder class BaseStokeIO(ABC): \"\"\"Base class for handling IO for different backends Attributes ---------- _save_rank: int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) _prefix: str prefix to append to all checkpoints _verbose: bool, default: True Flag for verbosity \"\"\" def __init__(self, save_rank: int = 0, verbose: bool = True, **kwargs): \"\"\"Init for BaseStokeIO class Parameters ---------- save_rank: int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) verbose: bool, default: True Flag for verbosity \"\"\" self._save_rank = save_rank self._prefix = \"stoke\" self._verbose = verbose def _make_tag(self, name: str, backward_step: int): \"\"\"Constructs the save tag Parameters ---------- name: str name used to save checkpoint file backward_step: int current number of backward calls (for saving unique name/tag) Returns ------- str \"\"\" return f\"{self._prefix}-{name}-backward-step-{backward_step}\" def _make_full_save_path( self, path: str, name: str, backward_step: int, extension: str ): \"\"\"Constructs the full string path from each piece and appends a stoke prefix Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str name used to save checkpoint file backward_step: int current number of backward calls (for saving unique name/tag) extension: str extension used to save PyTorch model checkpoint Returns ------- str \"\"\" return f\"{path}/{self._make_tag(name=name, backward_step=backward_step)}.{extension}\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- out_path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Call private as no logic is needed for the base save call out_path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) return out_path, tag def _save( self, model_dict: Dict, optimizer_dict: Dict, path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: Dict, scaler_dict: Optional[Dict], extension: str, create_directory: bool, extras: Optional[Dict], ): \"\"\"Private base implementation for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: Dict current model object dictionary optimizer: Dict current optimizer object dictionary scaler_dict: Optional[Dict] state_dict from native PyTorch AMP, Fairscale, or APEX path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: Dict current stoke status dictionary extension: str extension used to save PyTorch model checkpoint create_directory: bool flag to create the directory path if it doesn't exist extras: Dict a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Save the model with the constructed path try: if create_directory: make_folder(path) torch.save( { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"model_state_dict\": model_dict, \"optimizer_state_dict\": optimizer_dict, \"scaler_state_dict\": scaler_dict, \"extras\": extras, }, save_path, ) except OSError as e: self._print_device(f\"Unable to save model to given path: {save_path}\") raise e return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def _load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], map_loc: str, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Private base implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object map_loc: str device map gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary try: load_dict = torch.load(f\"{path}/{tag}\", map_location=map_loc) # Load the model state dict model.load_state_dict( state_dict=load_dict[\"model_state_dict\"], strict=strict ) # Handle the fully sharded data parallel case where the shard needs to be pulled from the full state dict if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling loading of correct optimizer sharded state for Fairscale FSDP\" ) optimizer.load_state_dict( state_dict=model.get_shard_from_optim_state_dict( load_dict[\"optimizer_state_dict\"] ) ) # Fallback to the default load form the fully state dict else: # Load the optimizer state dict optimizer.load_state_dict(state_dict=load_dict[\"optimizer_state_dict\"]) # Load the scaler state if needed if scaler_dict_fn is not None: scaler_dict_fn(load_dict[\"scaler_state_dict\"]) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( load_dict[\"backward_step\"], load_dict[\"grad_accum_step\"], load_dict[\"optimizer_step\"], load_dict[\"extras\"], ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id or cpu no matter what (covers CPU->GPU and GPU->GPU) # this should be functional for cuda:0 since this will catch the single GPU case only map_loc = f\"cuda:{self.device_id}\" if gpu else self.device_id self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) return backward_step, grad_accum_step, optimizer_step, extras class DeepspeedIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(DeepspeedIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Deepspeed override implementation for saving a PyTorch model checkpoint Deepspeed maintains it's own wrapper for saving so it needs to be called here. It looks like it will save multiple pieces depending on sharding but I'm not sure https://www.deepspeed.ai/getting-started/#model-checkpointing https://github.com/microsoft/DeepSpeed/blob/ed3de0c21b1fea330de9c1a78a23ca33f340ef20/deepspeed/runtime/engine.py#L1822 Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: Callable state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint (Note: Deepspeed will ignore this due to it's internal implementation) create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as Notes ----- From deepspeed save_checkpoint doc_string: all processes must call this method and not just the process with rank 0. It is because each process needs to save its master weights and scheduler+optimizer states. This method will hang waiting to synchronize with other processes if it's called just for the process with rank 0. \"\"\" # Construct the tag for deepspeed tag = self._make_tag(name=name, backward_step=backward_step) # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # Save the model with the constructed path try: client_sd = { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"extras\": extras, } _ = model.save_checkpoint( path, tag, client_state=client_sd, save_latest=False ) except OSError as e: self._print_device(f\"Unable to save model to given path: {path}\") raise e # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return path, tag def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Deepspeed override implementation for loading a PyTorch model checkpoint https://www.deepspeed.ai/getting-started/#model-checkpointing Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") try: _, client_sd = model.load_checkpoint( path, tag, load_module_strict=strict, load_optimizer_states=True ) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( client_sd[\"backward_step\"], client_sd[\"grad_accum_step\"], client_sd[\"optimizer_step\"], client_sd[\"extras\"], ) class DDPIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(DDPIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # FSDP needs different syntax for saving if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling consolidation of optimizer sharded states for Fairscale FSDP\" ) # Need to be called on all ranks model_state = model.state_dict() optimizer_state = model.gather_full_optim_state_dict(optimizer) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model_state, optimizer_dict=optimizer_state, path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) else: # If OSS then make sure it's consolidated before saving as norm PyTorch checkpoint # This needs to be called on all ranks but can be given a recipient_rank if isinstance(optimizer, OSS): self._print_device( f\"Consolidating optimizer sharded states onto device {self._save_rank}\" ) optimizer.consolidate_state_dict(recipient_rank=self._save_rank) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls torch.distributed.barrier() # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices torch.distributed.barrier() return backward_step, grad_accum_step, optimizer_step, extras class HorovodIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(HorovodIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # map to cuda:device_id -- horovod will only load on cuda:0 and then broadcast instead of loading on multiple # devices? TODO: Check if this is necessary or could we just load like DDP and skip the broadcast? # Terrible Horovod docs strike again -- load on dev 0 and sync -- but this doesn't deal with amp/apex # https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_imagenet_resnet50.py # I think we can just ignore this and load on all devices map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return backward_step, grad_accum_step, optimizer_step, extras class RunnerIOEnum(Enum): base = BaseStokeIO deepspeed = DeepspeedIO ddp = DDPIO horovod = HorovodIO Classes BaseStokeIO class BaseStokeIO ( save_rank : int = 0 , verbose : bool = True , ** kwargs ) Attributes Name Type Description Default _save_rank int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) None _prefix str prefix to append to all checkpoints None _verbose bool, default: True Flag for verbosity None ??? example \"View Source\" class BaseStokeIO(ABC): \"\"\"Base class for handling IO for different backends Attributes ---------- _save_rank: int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) _prefix: str prefix to append to all checkpoints _verbose: bool, default: True Flag for verbosity \"\"\" def __init__(self, save_rank: int = 0, verbose: bool = True, **kwargs): \"\"\"Init for BaseStokeIO class Parameters ---------- save_rank: int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) verbose: bool, default: True Flag for verbosity \"\"\" self._save_rank = save_rank self._prefix = \"stoke\" self._verbose = verbose def _make_tag(self, name: str, backward_step: int): \"\"\"Constructs the save tag Parameters ---------- name: str name used to save checkpoint file backward_step: int current number of backward calls (for saving unique name/tag) Returns ------- str \"\"\" return f\"{self._prefix}-{name}-backward-step-{backward_step}\" def _make_full_save_path( self, path: str, name: str, backward_step: int, extension: str ): \"\"\"Constructs the full string path from each piece and appends a stoke prefix Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str name used to save checkpoint file backward_step: int current number of backward calls (for saving unique name/tag) extension: str extension used to save PyTorch model checkpoint Returns ------- str \"\"\" return f\"{path}/{self._make_tag(name=name, backward_step=backward_step)}.{extension}\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- out_path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Call private as no logic is needed for the base save call out_path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) return out_path, tag def _save( self, model_dict: Dict, optimizer_dict: Dict, path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: Dict, scaler_dict: Optional[Dict], extension: str, create_directory: bool, extras: Optional[Dict], ): \"\"\"Private base implementation for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: Dict current model object dictionary optimizer: Dict current optimizer object dictionary scaler_dict: Optional[Dict] state_dict from native PyTorch AMP, Fairscale, or APEX path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: Dict current stoke status dictionary extension: str extension used to save PyTorch model checkpoint create_directory: bool flag to create the directory path if it doesn't exist extras: Dict a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Save the model with the constructed path try: if create_directory: make_folder(path) torch.save( { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"model_state_dict\": model_dict, \"optimizer_state_dict\": optimizer_dict, \"scaler_state_dict\": scaler_dict, \"extras\": extras, }, save_path, ) except OSError as e: self._print_device(f\"Unable to save model to given path: {save_path}\") raise e return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def _load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], map_loc: str, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Private base implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object map_loc: str device map gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary try: load_dict = torch.load(f\"{path}/{tag}\", map_location=map_loc) # Load the model state dict model.load_state_dict( state_dict=load_dict[\"model_state_dict\"], strict=strict ) # Handle the fully sharded data parallel case where the shard needs to be pulled from the full state dict if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling loading of correct optimizer sharded state for Fairscale FSDP\" ) optimizer.load_state_dict( state_dict=model.get_shard_from_optim_state_dict( load_dict[\"optimizer_state_dict\"] ) ) # Fallback to the default load form the fully state dict else: # Load the optimizer state dict optimizer.load_state_dict(state_dict=load_dict[\"optimizer_state_dict\"]) # Load the scaler state if needed if scaler_dict_fn is not None: scaler_dict_fn(load_dict[\"scaler_state_dict\"]) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( load_dict[\"backward_step\"], load_dict[\"grad_accum_step\"], load_dict[\"optimizer_step\"], load_dict[\"extras\"], ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id or cpu no matter what (covers CPU->GPU and GPU->GPU) # this should be functional for cuda:0 since this will catch the single GPU case only map_loc = f\"cuda:{self.device_id}\" if gpu else self.device_id self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) return backward_step, grad_accum_step, optimizer_step, extras Ancestors (in MRO) abc.ABC Descendants stoke.io.DeepspeedIO stoke.io.DDPIO stoke.io.HorovodIO Methods load def load ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], gpu : bool , path : str , tag : str , scaler_dict_fn : Union [ Callable , NoneType ] = None , strict : bool = True ) Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None gpu bool if using gpu device or not None path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None scaler_dict_fn Callable, default: None callable function to load the scaler state dict None strict bool ignore non-matching keys None Returns: Type Description int current number of backward calls (for resuming training correctly) ??? example \"View Source\" def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id or cpu no matter what (covers CPU->GPU and GPU->GPU) # this should be functional for cuda:0 since this will catch the single GPU case only map_loc = f\"cuda:{self.device_id}\" if gpu else self.device_id self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) return backward_step, grad_accum_step, optimizer_step, extras save def save ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], path : str , backward_step : int , grad_accum_step : int , optimizer_step : int , name : str , status : dict , scaler_dict : Union [ dict , NoneType ] = None , extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None backward_step int current number of backward calls (for resuming training correctly) None grad_accum_step int, current step of gradient accumulation (for resuming training correctly) None optimizer_step int current number of optimizer calls (for resuming training correctly) None name str name used to save checkpoint file None status dict current stoke status dictionary None scaler_dict dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- out_path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Call private as no logic is needed for the base save call out_path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) return out_path, tag DDPIO class DDPIO ( save_rank : int = 0 , ** kwargs ) Attributes Name Type Description Default _save_rank int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) None _prefix str prefix to append to all checkpoints None _verbose bool, default: True Flag for verbosity None ??? example \"View Source\" class DDPIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(DDPIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # FSDP needs different syntax for saving if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling consolidation of optimizer sharded states for Fairscale FSDP\" ) # Need to be called on all ranks model_state = model.state_dict() optimizer_state = model.gather_full_optim_state_dict(optimizer) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model_state, optimizer_dict=optimizer_state, path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) else: # If OSS then make sure it's consolidated before saving as norm PyTorch checkpoint # This needs to be called on all ranks but can be given a recipient_rank if isinstance(optimizer, OSS): self._print_device( f\"Consolidating optimizer sharded states onto device {self._save_rank}\" ) optimizer.consolidate_state_dict(recipient_rank=self._save_rank) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls torch.distributed.barrier() # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices torch.distributed.barrier() return backward_step, grad_accum_step, optimizer_step, extras Ancestors (in MRO) stoke.io.BaseStokeIO abc.ABC Methods load def load ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], gpu : bool , path : str , tag : str , scaler_dict_fn : Union [ Callable , NoneType ] = None , strict : bool = True ) Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None gpu bool if using gpu device or not None path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None scaler_dict_fn Callable, default: None callable function to load the scaler state dict None strict bool ignore non-matching keys None Returns: Type Description int current number of backward calls (for resuming training correctly) ??? example \"View Source\" def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls torch.distributed.barrier() # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices torch.distributed.barrier() return backward_step, grad_accum_step, optimizer_step, extras save def save ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], path : str , backward_step : int , grad_accum_step : int , optimizer_step : int , name : str , status : dict , scaler_dict : Union [ dict , NoneType ] = None , extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None backward_step int current number of backward calls (for resuming training correctly) None grad_accum_step int, current step of gradient accumulation (for resuming training correctly) None optimizer_step int current number of optimizer calls (for resuming training correctly) None name str name used to save checkpoint file None status dict current stoke status dictionary None scaler_dict dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # FSDP needs different syntax for saving if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling consolidation of optimizer sharded states for Fairscale FSDP\" ) # Need to be called on all ranks model_state = model.state_dict() optimizer_state = model.gather_full_optim_state_dict(optimizer) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model_state, optimizer_dict=optimizer_state, path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) else: # If OSS then make sure it's consolidated before saving as norm PyTorch checkpoint # This needs to be called on all ranks but can be given a recipient_rank if isinstance(optimizer, OSS): self._print_device( f\"Consolidating optimizer sharded states onto device {self._save_rank}\" ) optimizer.consolidate_state_dict(recipient_rank=self._save_rank) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) DeepspeedIO class DeepspeedIO ( save_rank : int = 0 , ** kwargs ) Attributes Name Type Description Default _save_rank int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) None _prefix str prefix to append to all checkpoints None _verbose bool, default: True Flag for verbosity None ??? example \"View Source\" class DeepspeedIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(DeepspeedIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Deepspeed override implementation for saving a PyTorch model checkpoint Deepspeed maintains it's own wrapper for saving so it needs to be called here. It looks like it will save multiple pieces depending on sharding but I'm not sure https://www.deepspeed.ai/getting-started/#model-checkpointing https://github.com/microsoft/DeepSpeed/blob/ed3de0c21b1fea330de9c1a78a23ca33f340ef20/deepspeed/runtime/engine.py#L1822 Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: Callable state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint (Note: Deepspeed will ignore this due to it's internal implementation) create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as Notes ----- From deepspeed save_checkpoint doc_string: all processes must call this method and not just the process with rank 0. It is because each process needs to save its master weights and scheduler+optimizer states. This method will hang waiting to synchronize with other processes if it's called just for the process with rank 0. \"\"\" # Construct the tag for deepspeed tag = self._make_tag(name=name, backward_step=backward_step) # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # Save the model with the constructed path try: client_sd = { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"extras\": extras, } _ = model.save_checkpoint( path, tag, client_state=client_sd, save_latest=False ) except OSError as e: self._print_device(f\"Unable to save model to given path: {path}\") raise e # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return path, tag def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Deepspeed override implementation for loading a PyTorch model checkpoint https://www.deepspeed.ai/getting-started/#model-checkpointing Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") try: _, client_sd = model.load_checkpoint( path, tag, load_module_strict=strict, load_optimizer_states=True ) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( client_sd[\"backward_step\"], client_sd[\"grad_accum_step\"], client_sd[\"optimizer_step\"], client_sd[\"extras\"], ) Ancestors (in MRO) stoke.io.BaseStokeIO abc.ABC Methods load def load ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], gpu : bool , path : str , tag : str , scaler_dict_fn : Union [ Callable , NoneType ] = None , strict : bool = True ) Deepspeed override implementation for loading a PyTorch model checkpoint https://www.deepspeed.ai/getting-started/#model-checkpointing Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None gpu bool if using gpu device or not None path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None scaler_dict_fn Callable, default: None callable function to load the scaler state dict None strict bool ignore non-matching keys None Returns: Type Description int current number of backward calls (for resuming training correctly) ??? example \"View Source\" def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Deepspeed override implementation for loading a PyTorch model checkpoint https://www.deepspeed.ai/getting-started/#model-checkpointing Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") try: _, client_sd = model.load_checkpoint( path, tag, load_module_strict=strict, load_optimizer_states=True ) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( client_sd[\"backward_step\"], client_sd[\"grad_accum_step\"], client_sd[\"optimizer_step\"], client_sd[\"extras\"], ) save def save ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], path : str , backward_step : int , grad_accum_step : int , optimizer_step : int , name : str , status : dict , scaler_dict : Union [ dict , NoneType ] = None , extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Deepspeed override implementation for saving a PyTorch model checkpoint Deepspeed maintains it's own wrapper for saving so it needs to be called here. It looks like it will save multiple pieces depending on sharding but I'm not sure https://www.deepspeed.ai/getting-started/#model-checkpointing https://github.com/microsoft/DeepSpeed/blob/ed3de0c21b1fea330de9c1a78a23ca33f340ef20/deepspeed/runtime/engine.py#L1822 Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None backward_step int current number of backward calls (for resuming training correctly) None grad_accum_step int, current step of gradient accumulation None optimizer_step int current number of optimizer calls (for resuming training correctly) None name str name used to save checkpoint file None status dict current stoke status dictionary None scaler_dict Callable state_dict from native PyTorch AMP, Fairscale, or APEX None extension str, default: '.pt' extension used to save PyTorch model checkpoint (Note: Deepspeed will ignore this due to it's internal implementation) None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Deepspeed override implementation for saving a PyTorch model checkpoint Deepspeed maintains it's own wrapper for saving so it needs to be called here. It looks like it will save multiple pieces depending on sharding but I'm not sure https://www.deepspeed.ai/getting-started/#model-checkpointing https://github.com/microsoft/DeepSpeed/blob/ed3de0c21b1fea330de9c1a78a23ca33f340ef20/deepspeed/runtime/engine.py#L1822 Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: Callable state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint (Note: Deepspeed will ignore this due to it's internal implementation) create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as Notes ----- From deepspeed save_checkpoint doc_string: all processes must call this method and not just the process with rank 0. It is because each process needs to save its master weights and scheduler+optimizer states. This method will hang waiting to synchronize with other processes if it's called just for the process with rank 0. \"\"\" # Construct the tag for deepspeed tag = self._make_tag(name=name, backward_step=backward_step) # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # Save the model with the constructed path try: client_sd = { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"extras\": extras, } _ = model.save_checkpoint( path, tag, client_state=client_sd, save_latest=False ) except OSError as e: self._print_device(f\"Unable to save model to given path: {path}\") raise e # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return path, tag HorovodIO class HorovodIO ( save_rank : int = 0 , ** kwargs ) Attributes Name Type Description Default _save_rank int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) None _prefix str prefix to append to all checkpoints None _verbose bool, default: True Flag for verbosity None ??? example \"View Source\" class HorovodIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(HorovodIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # map to cuda:device_id -- horovod will only load on cuda:0 and then broadcast instead of loading on multiple # devices? TODO: Check if this is necessary or could we just load like DDP and skip the broadcast? # Terrible Horovod docs strike again -- load on dev 0 and sync -- but this doesn't deal with amp/apex # https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_imagenet_resnet50.py # I think we can just ignore this and load on all devices map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return backward_step, grad_accum_step, optimizer_step, extras Ancestors (in MRO) stoke.io.BaseStokeIO abc.ABC Methods load def load ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], gpu : bool , path : str , tag : str , scaler_dict_fn : Union [ Callable , NoneType ] = None , strict : bool = True ) Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None gpu bool if using gpu device or not None path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None scaler_dict_fn Callable, default: None callable function to load the scaler state dict None strict bool ignore non-matching keys None Returns: Type Description int current number of backward calls (for resuming training correctly) ??? example \"View Source\" def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # map to cuda:device_id -- horovod will only load on cuda:0 and then broadcast instead of loading on multiple # devices? TODO: Check if this is necessary or could we just load like DDP and skip the broadcast? # Terrible Horovod docs strike again -- load on dev 0 and sync -- but this doesn't deal with amp/apex # https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_imagenet_resnet50.py # I think we can just ignore this and load on all devices map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return backward_step, grad_accum_step, optimizer_step, extras save def save ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], path : str , backward_step : int , grad_accum_step : int , optimizer_step : int , name : str , status : dict , scaler_dict : Union [ dict , NoneType ] = None , extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None backward_step int current number of backward calls (for resuming training correctly) None grad_accum_step int, current step of gradient accumulation (for resuming training correctly) None optimizer_step int current number of optimizer calls (for resuming training correctly) None name str name used to save checkpoint file None status dict current stoke status dictionary None scaler_dict dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) RunnerIOEnum class RunnerIOEnum ( / , * args , ** kwargs ) ??? example \"View Source\" class RunnerIOEnum(Enum): base = BaseStokeIO deepspeed = DeepspeedIO ddp = DDPIO horovod = HorovodIO Ancestors (in MRO) enum.Enum Class variables base ddp deepspeed horovod name value","title":"Io"},{"location":"reference/stoke/io/#module-stokeio","text":"Handles i/o related functions -- mixin style None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles i/o related functions -- mixin style\"\"\" from abc import ABC from enum import Enum from typing import Callable, Dict, Optional, Union import horovod.torch as hvd import torch from fairscale.nn.data_parallel import FullyShardedDataParallel from fairscale.optim.oss import OSS from stoke.utils import make_folder class BaseStokeIO(ABC): \"\"\"Base class for handling IO for different backends Attributes ---------- _save_rank: int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) _prefix: str prefix to append to all checkpoints _verbose: bool, default: True Flag for verbosity \"\"\" def __init__(self, save_rank: int = 0, verbose: bool = True, **kwargs): \"\"\"Init for BaseStokeIO class Parameters ---------- save_rank: int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) verbose: bool, default: True Flag for verbosity \"\"\" self._save_rank = save_rank self._prefix = \"stoke\" self._verbose = verbose def _make_tag(self, name: str, backward_step: int): \"\"\"Constructs the save tag Parameters ---------- name: str name used to save checkpoint file backward_step: int current number of backward calls (for saving unique name/tag) Returns ------- str \"\"\" return f\"{self._prefix}-{name}-backward-step-{backward_step}\" def _make_full_save_path( self, path: str, name: str, backward_step: int, extension: str ): \"\"\"Constructs the full string path from each piece and appends a stoke prefix Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str name used to save checkpoint file backward_step: int current number of backward calls (for saving unique name/tag) extension: str extension used to save PyTorch model checkpoint Returns ------- str \"\"\" return f\"{path}/{self._make_tag(name=name, backward_step=backward_step)}.{extension}\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- out_path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Call private as no logic is needed for the base save call out_path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) return out_path, tag def _save( self, model_dict: Dict, optimizer_dict: Dict, path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: Dict, scaler_dict: Optional[Dict], extension: str, create_directory: bool, extras: Optional[Dict], ): \"\"\"Private base implementation for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: Dict current model object dictionary optimizer: Dict current optimizer object dictionary scaler_dict: Optional[Dict] state_dict from native PyTorch AMP, Fairscale, or APEX path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: Dict current stoke status dictionary extension: str extension used to save PyTorch model checkpoint create_directory: bool flag to create the directory path if it doesn't exist extras: Dict a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Save the model with the constructed path try: if create_directory: make_folder(path) torch.save( { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"model_state_dict\": model_dict, \"optimizer_state_dict\": optimizer_dict, \"scaler_state_dict\": scaler_dict, \"extras\": extras, }, save_path, ) except OSError as e: self._print_device(f\"Unable to save model to given path: {save_path}\") raise e return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def _load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], map_loc: str, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Private base implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object map_loc: str device map gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary try: load_dict = torch.load(f\"{path}/{tag}\", map_location=map_loc) # Load the model state dict model.load_state_dict( state_dict=load_dict[\"model_state_dict\"], strict=strict ) # Handle the fully sharded data parallel case where the shard needs to be pulled from the full state dict if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling loading of correct optimizer sharded state for Fairscale FSDP\" ) optimizer.load_state_dict( state_dict=model.get_shard_from_optim_state_dict( load_dict[\"optimizer_state_dict\"] ) ) # Fallback to the default load form the fully state dict else: # Load the optimizer state dict optimizer.load_state_dict(state_dict=load_dict[\"optimizer_state_dict\"]) # Load the scaler state if needed if scaler_dict_fn is not None: scaler_dict_fn(load_dict[\"scaler_state_dict\"]) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( load_dict[\"backward_step\"], load_dict[\"grad_accum_step\"], load_dict[\"optimizer_step\"], load_dict[\"extras\"], ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id or cpu no matter what (covers CPU->GPU and GPU->GPU) # this should be functional for cuda:0 since this will catch the single GPU case only map_loc = f\"cuda:{self.device_id}\" if gpu else self.device_id self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) return backward_step, grad_accum_step, optimizer_step, extras class DeepspeedIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(DeepspeedIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Deepspeed override implementation for saving a PyTorch model checkpoint Deepspeed maintains it's own wrapper for saving so it needs to be called here. It looks like it will save multiple pieces depending on sharding but I'm not sure https://www.deepspeed.ai/getting-started/#model-checkpointing https://github.com/microsoft/DeepSpeed/blob/ed3de0c21b1fea330de9c1a78a23ca33f340ef20/deepspeed/runtime/engine.py#L1822 Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: Callable state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint (Note: Deepspeed will ignore this due to it's internal implementation) create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as Notes ----- From deepspeed save_checkpoint doc_string: all processes must call this method and not just the process with rank 0. It is because each process needs to save its master weights and scheduler+optimizer states. This method will hang waiting to synchronize with other processes if it's called just for the process with rank 0. \"\"\" # Construct the tag for deepspeed tag = self._make_tag(name=name, backward_step=backward_step) # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # Save the model with the constructed path try: client_sd = { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"extras\": extras, } _ = model.save_checkpoint( path, tag, client_state=client_sd, save_latest=False ) except OSError as e: self._print_device(f\"Unable to save model to given path: {path}\") raise e # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return path, tag def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Deepspeed override implementation for loading a PyTorch model checkpoint https://www.deepspeed.ai/getting-started/#model-checkpointing Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") try: _, client_sd = model.load_checkpoint( path, tag, load_module_strict=strict, load_optimizer_states=True ) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( client_sd[\"backward_step\"], client_sd[\"grad_accum_step\"], client_sd[\"optimizer_step\"], client_sd[\"extras\"], ) class DDPIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(DDPIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # FSDP needs different syntax for saving if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling consolidation of optimizer sharded states for Fairscale FSDP\" ) # Need to be called on all ranks model_state = model.state_dict() optimizer_state = model.gather_full_optim_state_dict(optimizer) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model_state, optimizer_dict=optimizer_state, path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) else: # If OSS then make sure it's consolidated before saving as norm PyTorch checkpoint # This needs to be called on all ranks but can be given a recipient_rank if isinstance(optimizer, OSS): self._print_device( f\"Consolidating optimizer sharded states onto device {self._save_rank}\" ) optimizer.consolidate_state_dict(recipient_rank=self._save_rank) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls torch.distributed.barrier() # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices torch.distributed.barrier() return backward_step, grad_accum_step, optimizer_step, extras class HorovodIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(HorovodIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # map to cuda:device_id -- horovod will only load on cuda:0 and then broadcast instead of loading on multiple # devices? TODO: Check if this is necessary or could we just load like DDP and skip the broadcast? # Terrible Horovod docs strike again -- load on dev 0 and sync -- but this doesn't deal with amp/apex # https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_imagenet_resnet50.py # I think we can just ignore this and load on all devices map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return backward_step, grad_accum_step, optimizer_step, extras class RunnerIOEnum(Enum): base = BaseStokeIO deepspeed = DeepspeedIO ddp = DDPIO horovod = HorovodIO","title":"Module stoke.io"},{"location":"reference/stoke/io/#classes","text":"","title":"Classes"},{"location":"reference/stoke/io/#basestokeio","text":"class BaseStokeIO ( save_rank : int = 0 , verbose : bool = True , ** kwargs )","title":"BaseStokeIO"},{"location":"reference/stoke/io/#attributes","text":"Name Type Description Default _save_rank int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) None _prefix str prefix to append to all checkpoints None _verbose bool, default: True Flag for verbosity None ??? example \"View Source\" class BaseStokeIO(ABC): \"\"\"Base class for handling IO for different backends Attributes ---------- _save_rank: int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) _prefix: str prefix to append to all checkpoints _verbose: bool, default: True Flag for verbosity \"\"\" def __init__(self, save_rank: int = 0, verbose: bool = True, **kwargs): \"\"\"Init for BaseStokeIO class Parameters ---------- save_rank: int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) verbose: bool, default: True Flag for verbosity \"\"\" self._save_rank = save_rank self._prefix = \"stoke\" self._verbose = verbose def _make_tag(self, name: str, backward_step: int): \"\"\"Constructs the save tag Parameters ---------- name: str name used to save checkpoint file backward_step: int current number of backward calls (for saving unique name/tag) Returns ------- str \"\"\" return f\"{self._prefix}-{name}-backward-step-{backward_step}\" def _make_full_save_path( self, path: str, name: str, backward_step: int, extension: str ): \"\"\"Constructs the full string path from each piece and appends a stoke prefix Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str name used to save checkpoint file backward_step: int current number of backward calls (for saving unique name/tag) extension: str extension used to save PyTorch model checkpoint Returns ------- str \"\"\" return f\"{path}/{self._make_tag(name=name, backward_step=backward_step)}.{extension}\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- out_path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Call private as no logic is needed for the base save call out_path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) return out_path, tag def _save( self, model_dict: Dict, optimizer_dict: Dict, path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: Dict, scaler_dict: Optional[Dict], extension: str, create_directory: bool, extras: Optional[Dict], ): \"\"\"Private base implementation for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: Dict current model object dictionary optimizer: Dict current optimizer object dictionary scaler_dict: Optional[Dict] state_dict from native PyTorch AMP, Fairscale, or APEX path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: Dict current stoke status dictionary extension: str extension used to save PyTorch model checkpoint create_directory: bool flag to create the directory path if it doesn't exist extras: Dict a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Save the model with the constructed path try: if create_directory: make_folder(path) torch.save( { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"model_state_dict\": model_dict, \"optimizer_state_dict\": optimizer_dict, \"scaler_state_dict\": scaler_dict, \"extras\": extras, }, save_path, ) except OSError as e: self._print_device(f\"Unable to save model to given path: {save_path}\") raise e return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def _load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], map_loc: str, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Private base implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object map_loc: str device map gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary try: load_dict = torch.load(f\"{path}/{tag}\", map_location=map_loc) # Load the model state dict model.load_state_dict( state_dict=load_dict[\"model_state_dict\"], strict=strict ) # Handle the fully sharded data parallel case where the shard needs to be pulled from the full state dict if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling loading of correct optimizer sharded state for Fairscale FSDP\" ) optimizer.load_state_dict( state_dict=model.get_shard_from_optim_state_dict( load_dict[\"optimizer_state_dict\"] ) ) # Fallback to the default load form the fully state dict else: # Load the optimizer state dict optimizer.load_state_dict(state_dict=load_dict[\"optimizer_state_dict\"]) # Load the scaler state if needed if scaler_dict_fn is not None: scaler_dict_fn(load_dict[\"scaler_state_dict\"]) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( load_dict[\"backward_step\"], load_dict[\"grad_accum_step\"], load_dict[\"optimizer_step\"], load_dict[\"extras\"], ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id or cpu no matter what (covers CPU->GPU and GPU->GPU) # this should be functional for cuda:0 since this will catch the single GPU case only map_loc = f\"cuda:{self.device_id}\" if gpu else self.device_id self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) return backward_step, grad_accum_step, optimizer_step, extras","title":"Attributes"},{"location":"reference/stoke/io/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/io/#descendants","text":"stoke.io.DeepspeedIO stoke.io.DDPIO stoke.io.HorovodIO","title":"Descendants"},{"location":"reference/stoke/io/#methods","text":"","title":"Methods"},{"location":"reference/stoke/io/#load","text":"def load ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], gpu : bool , path : str , tag : str , scaler_dict_fn : Union [ Callable , NoneType ] = None , strict : bool = True ) Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None gpu bool if using gpu device or not None path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None scaler_dict_fn Callable, default: None callable function to load the scaler state dict None strict bool ignore non-matching keys None Returns: Type Description int current number of backward calls (for resuming training correctly) ??? example \"View Source\" def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id or cpu no matter what (covers CPU->GPU and GPU->GPU) # this should be functional for cuda:0 since this will catch the single GPU case only map_loc = f\"cuda:{self.device_id}\" if gpu else self.device_id self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) return backward_step, grad_accum_step, optimizer_step, extras","title":"load"},{"location":"reference/stoke/io/#save","text":"def save ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], path : str , backward_step : int , grad_accum_step : int , optimizer_step : int , name : str , status : dict , scaler_dict : Union [ dict , NoneType ] = None , extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None backward_step int current number of backward calls (for resuming training correctly) None grad_accum_step int, current step of gradient accumulation (for resuming training correctly) None optimizer_step int current number of optimizer calls (for resuming training correctly) None name str name used to save checkpoint file None status dict current stoke status dictionary None scaler_dict dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- out_path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" # Call private as no logic is needed for the base save call out_path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) return out_path, tag","title":"save"},{"location":"reference/stoke/io/#ddpio","text":"class DDPIO ( save_rank : int = 0 , ** kwargs )","title":"DDPIO"},{"location":"reference/stoke/io/#attributes_1","text":"Name Type Description Default _save_rank int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) None _prefix str prefix to append to all checkpoints None _verbose bool, default: True Flag for verbosity None ??? example \"View Source\" class DDPIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(DDPIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # FSDP needs different syntax for saving if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling consolidation of optimizer sharded states for Fairscale FSDP\" ) # Need to be called on all ranks model_state = model.state_dict() optimizer_state = model.gather_full_optim_state_dict(optimizer) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model_state, optimizer_dict=optimizer_state, path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) else: # If OSS then make sure it's consolidated before saving as norm PyTorch checkpoint # This needs to be called on all ranks but can be given a recipient_rank if isinstance(optimizer, OSS): self._print_device( f\"Consolidating optimizer sharded states onto device {self._save_rank}\" ) optimizer.consolidate_state_dict(recipient_rank=self._save_rank) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls torch.distributed.barrier() # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices torch.distributed.barrier() return backward_step, grad_accum_step, optimizer_step, extras","title":"Attributes"},{"location":"reference/stoke/io/#ancestors-in-mro_1","text":"stoke.io.BaseStokeIO abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/io/#methods_1","text":"","title":"Methods"},{"location":"reference/stoke/io/#load_1","text":"def load ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], gpu : bool , path : str , tag : str , scaler_dict_fn : Union [ Callable , NoneType ] = None , strict : bool = True ) Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None gpu bool if using gpu device or not None path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None scaler_dict_fn Callable, default: None callable function to load the scaler state dict None strict bool ignore non-matching keys None Returns: Type Description int current number of backward calls (for resuming training correctly) ??? example \"View Source\" def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls torch.distributed.barrier() # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") # Call the private load interface backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices torch.distributed.barrier() return backward_step, grad_accum_step, optimizer_step, extras","title":"load"},{"location":"reference/stoke/io/#save_1","text":"def save ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], path : str , backward_step : int , grad_accum_step : int , optimizer_step : int , name : str , status : dict , scaler_dict : Union [ dict , NoneType ] = None , extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None backward_step int current number of backward calls (for resuming training correctly) None grad_accum_step int, current step of gradient accumulation (for resuming training correctly) None optimizer_step int current number of optimizer calls (for resuming training correctly) None name str name used to save checkpoint file None status dict current stoke status dictionary None scaler_dict dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # FSDP needs different syntax for saving if isinstance(model, FullyShardedDataParallel): self._print_device( \"Handling consolidation of optimizer sharded states for Fairscale FSDP\" ) # Need to be called on all ranks model_state = model.state_dict() optimizer_state = model.gather_full_optim_state_dict(optimizer) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model_state, optimizer_dict=optimizer_state, path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) else: # If OSS then make sure it's consolidated before saving as norm PyTorch checkpoint # This needs to be called on all ranks but can be given a recipient_rank if isinstance(optimizer, OSS): self._print_device( f\"Consolidating optimizer sharded states onto device {self._save_rank}\" ) optimizer.consolidate_state_dict(recipient_rank=self._save_rank) # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", )","title":"save"},{"location":"reference/stoke/io/#deepspeedio","text":"class DeepspeedIO ( save_rank : int = 0 , ** kwargs )","title":"DeepspeedIO"},{"location":"reference/stoke/io/#attributes_2","text":"Name Type Description Default _save_rank int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) None _prefix str prefix to append to all checkpoints None _verbose bool, default: True Flag for verbosity None ??? example \"View Source\" class DeepspeedIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(DeepspeedIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Deepspeed override implementation for saving a PyTorch model checkpoint Deepspeed maintains it's own wrapper for saving so it needs to be called here. It looks like it will save multiple pieces depending on sharding but I'm not sure https://www.deepspeed.ai/getting-started/#model-checkpointing https://github.com/microsoft/DeepSpeed/blob/ed3de0c21b1fea330de9c1a78a23ca33f340ef20/deepspeed/runtime/engine.py#L1822 Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: Callable state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint (Note: Deepspeed will ignore this due to it's internal implementation) create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as Notes ----- From deepspeed save_checkpoint doc_string: all processes must call this method and not just the process with rank 0. It is because each process needs to save its master weights and scheduler+optimizer states. This method will hang waiting to synchronize with other processes if it's called just for the process with rank 0. \"\"\" # Construct the tag for deepspeed tag = self._make_tag(name=name, backward_step=backward_step) # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # Save the model with the constructed path try: client_sd = { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"extras\": extras, } _ = model.save_checkpoint( path, tag, client_state=client_sd, save_latest=False ) except OSError as e: self._print_device(f\"Unable to save model to given path: {path}\") raise e # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return path, tag def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Deepspeed override implementation for loading a PyTorch model checkpoint https://www.deepspeed.ai/getting-started/#model-checkpointing Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") try: _, client_sd = model.load_checkpoint( path, tag, load_module_strict=strict, load_optimizer_states=True ) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( client_sd[\"backward_step\"], client_sd[\"grad_accum_step\"], client_sd[\"optimizer_step\"], client_sd[\"extras\"], )","title":"Attributes"},{"location":"reference/stoke/io/#ancestors-in-mro_2","text":"stoke.io.BaseStokeIO abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/io/#methods_2","text":"","title":"Methods"},{"location":"reference/stoke/io/#load_2","text":"def load ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], gpu : bool , path : str , tag : str , scaler_dict_fn : Union [ Callable , NoneType ] = None , strict : bool = True ) Deepspeed override implementation for loading a PyTorch model checkpoint https://www.deepspeed.ai/getting-started/#model-checkpointing Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None gpu bool if using gpu device or not None path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None scaler_dict_fn Callable, default: None callable function to load the scaler state dict None strict bool ignore non-matching keys None Returns: Type Description int current number of backward calls (for resuming training correctly) ??? example \"View Source\" def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): \"\"\"Deepspeed override implementation for loading a PyTorch model checkpoint https://www.deepspeed.ai/getting-started/#model-checkpointing Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object gpu: bool if using gpu device or not path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as scaler_dict_fn: Callable, default: None callable function to load the scaler state dict strict: bool ignore non-matching keys Returns ------- backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation (for resuming training correctly) optimizer_step: int current number of optimizer calls (for resuming training correctly) extras: dict a dictionary of any extra things that were saved \"\"\" # Load the dictionary # map to cuda:device_id (as this will prevent the save on device 0 from clashing with the current device id) map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") try: _, client_sd = model.load_checkpoint( path, tag, load_module_strict=strict, load_optimizer_states=True ) except OSError as e: self._print_device(f\"Unable to load model from given path: {path}/{tag}\") raise e return ( client_sd[\"backward_step\"], client_sd[\"grad_accum_step\"], client_sd[\"optimizer_step\"], client_sd[\"extras\"], )","title":"load"},{"location":"reference/stoke/io/#save_2","text":"def save ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], path : str , backward_step : int , grad_accum_step : int , optimizer_step : int , name : str , status : dict , scaler_dict : Union [ dict , NoneType ] = None , extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Deepspeed override implementation for saving a PyTorch model checkpoint Deepspeed maintains it's own wrapper for saving so it needs to be called here. It looks like it will save multiple pieces depending on sharding but I'm not sure https://www.deepspeed.ai/getting-started/#model-checkpointing https://github.com/microsoft/DeepSpeed/blob/ed3de0c21b1fea330de9c1a78a23ca33f340ef20/deepspeed/runtime/engine.py#L1822 Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None backward_step int current number of backward calls (for resuming training correctly) None grad_accum_step int, current step of gradient accumulation None optimizer_step int current number of optimizer calls (for resuming training correctly) None name str name used to save checkpoint file None status dict current stoke status dictionary None scaler_dict Callable state_dict from native PyTorch AMP, Fairscale, or APEX None extension str, default: '.pt' extension used to save PyTorch model checkpoint (Note: Deepspeed will ignore this due to it's internal implementation) None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Deepspeed override implementation for saving a PyTorch model checkpoint Deepspeed maintains it's own wrapper for saving so it needs to be called here. It looks like it will save multiple pieces depending on sharding but I'm not sure https://www.deepspeed.ai/getting-started/#model-checkpointing https://github.com/microsoft/DeepSpeed/blob/ed3de0c21b1fea330de9c1a78a23ca33f340ef20/deepspeed/runtime/engine.py#L1822 Parameters ---------- model: torch.nn.Module current model object optimizer: Union[torch.optim.Optimizer, OSS] current optimizer object path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) backward_step: int current number of backward calls (for resuming training correctly) grad_accum_step: int, current step of gradient accumulation optimizer_step: int current number of optimizer calls (for resuming training correctly) name: str name used to save checkpoint file status: dict current stoke status dictionary scaler_dict: Callable state_dict from native PyTorch AMP, Fairscale, or APEX extension: str, default: '.pt' extension used to save PyTorch model checkpoint (Note: Deepspeed will ignore this due to it's internal implementation) create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as Notes ----- From deepspeed save_checkpoint doc_string: all processes must call this method and not just the process with rank 0. It is because each process needs to save its master weights and scheduler+optimizer states. This method will hang waiting to synchronize with other processes if it's called just for the process with rank 0. \"\"\" # Construct the tag for deepspeed tag = self._make_tag(name=name, backward_step=backward_step) # Construct the path save_path = self._make_full_save_path( path=path, name=name, backward_step=backward_step, extension=extension ) if self._verbose: self._print_device(f\"Attempting to save model checkpoint to {save_path}\") # Use a barrier to make sure the save is done only when all devices are finished with prior calls torch.distributed.barrier() # Save the model with the constructed path try: client_sd = { \"backward_step\": backward_step, \"grad_accum_step\": grad_accum_step, \"optimizer_step\": optimizer_step, \"stoke_status\": status, \"extras\": extras, } _ = model.save_checkpoint( path, tag, client_state=client_sd, save_latest=False ) except OSError as e: self._print_device(f\"Unable to save model to given path: {path}\") raise e # Use a barrier to make sure no one exits until the save is complete torch.distributed.barrier() return path, tag","title":"save"},{"location":"reference/stoke/io/#horovodio","text":"class HorovodIO ( save_rank : int = 0 , ** kwargs )","title":"HorovodIO"},{"location":"reference/stoke/io/#attributes_3","text":"Name Type Description Default _save_rank int, default: 0 device to restrict calls to if necessary (e.g. horovod, ddp) None _prefix str prefix to append to all checkpoints None _verbose bool, default: True Flag for verbosity None ??? example \"View Source\" class HorovodIO(BaseStokeIO): def __init__(self, save_rank: int = 0, **kwargs): super(HorovodIO, self).__init__(save_rank=save_rank, **kwargs) def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", ) def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # map to cuda:device_id -- horovod will only load on cuda:0 and then broadcast instead of loading on multiple # devices? TODO: Check if this is necessary or could we just load like DDP and skip the broadcast? # Terrible Horovod docs strike again -- load on dev 0 and sync -- but this doesn't deal with amp/apex # https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_imagenet_resnet50.py # I think we can just ignore this and load on all devices map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return backward_step, grad_accum_step, optimizer_step, extras","title":"Attributes"},{"location":"reference/stoke/io/#ancestors-in-mro_3","text":"stoke.io.BaseStokeIO abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/stoke/io/#methods_3","text":"","title":"Methods"},{"location":"reference/stoke/io/#load_3","text":"def load ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], gpu : bool , path : str , tag : str , scaler_dict_fn : Union [ Callable , NoneType ] = None , strict : bool = True ) Implementation for loading a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None gpu bool if using gpu device or not None path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None scaler_dict_fn Callable, default: None callable function to load the scaler state dict None strict bool ignore non-matching keys None Returns: Type Description int current number of backward calls (for resuming training correctly) ??? example \"View Source\" def load( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], gpu: bool, path: str, tag: str, scaler_dict_fn: Optional[Callable] = None, strict: bool = True, ): # Use a barrier to make sure the load is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # map to cuda:device_id -- horovod will only load on cuda:0 and then broadcast instead of loading on multiple # devices? TODO: Check if this is necessary or could we just load like DDP and skip the broadcast? # Terrible Horovod docs strike again -- load on dev 0 and sync -- but this doesn't deal with amp/apex # https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_imagenet_resnet50.py # I think we can just ignore this and load on all devices map_loc = f\"cuda:{self.device_id}\" self._print_device(f\"Load is mapping to {map_loc}\") backward_step, grad_accum_step, optimizer_step, extras = self._load( model=model, optimizer=optimizer, map_loc=map_loc, path=path, tag=tag, scaler_dict_fn=scaler_dict_fn, strict=strict, ) # Use a barrier to make sure no one exits until the load is complete across all devices # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return backward_step, grad_accum_step, optimizer_step, extras","title":"load"},{"location":"reference/stoke/io/#save_3","text":"def save ( self , model : torch . nn . modules . module . Module , optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], path : str , backward_step : int , grad_accum_step : int , optimizer_step : int , name : str , status : dict , scaler_dict : Union [ dict , NoneType ] = None , extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Implementation(s) for saving a PyTorch model checkpoint https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference Parameters: Name Type Description Default model torch.nn.Module current model object None optimizer Union[torch.optim.Optimizer, OSS] current optimizer object None path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None backward_step int current number of backward calls (for resuming training correctly) None grad_accum_step int, current step of gradient accumulation (for resuming training correctly) None optimizer_step int current number of optimizer calls (for resuming training correctly) None name str name used to save checkpoint file None status dict current stoke status dictionary None scaler_dict dict, default: None state_dict from native PyTorch AMP, Fairscale, or APEX None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS], path: str, backward_step: int, grad_accum_step: int, optimizer_step: int, name: str, status: dict, scaler_dict: Optional[dict] = None, extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): # Use a barrier to make sure the save is done only when all devices are finished with prior calls # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() # Use a logical barrier to only save on the 0 idx device if self.rank == self._save_rank: # Dispatch to private save method if logic is met path, tag = self._save( model_dict=model.state_dict(), optimizer_dict=optimizer.state_dict(), path=path, backward_step=backward_step, optimizer_step=optimizer_step, name=name, scaler_dict=scaler_dict, extension=extension, create_directory=create_directory, extras=extras, grad_accum_step=grad_accum_step, status=status, ) # Use a barrier to make sure no one exits until the save is complete # Horovod doesn't have a native barrier so lean on join to take care of it # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join hvd.join() return ( path, f\"{self._make_tag(name=name, backward_step=backward_step)}.{extension}\", )","title":"save"},{"location":"reference/stoke/io/#runnerioenum","text":"class RunnerIOEnum ( / , * args , ** kwargs ) ??? example \"View Source\" class RunnerIOEnum(Enum): base = BaseStokeIO deepspeed = DeepspeedIO ddp = DDPIO horovod = HorovodIO","title":"RunnerIOEnum"},{"location":"reference/stoke/io/#ancestors-in-mro_4","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/io/#class-variables","text":"base ddp deepspeed horovod name value","title":"Class variables"},{"location":"reference/stoke/status/","text":"Module stoke.status Handles setting the status/state of Stoke None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles setting the status/state of Stoke\"\"\" import os from enum import Enum from typing import List, Optional, Union import attr import torch from stoke.configs import ( AMPConfig, ApexConfig, ClipGradConfig, ClipGradNormConfig, DDPConfig, DeepspeedConfig, DeepspeedFP16Config, FairscaleFSDPConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig, ) from stoke.extensions import _FairscaleFSDPConfig class DistributedOptions(Enum): \"\"\"Enum that defines the options for Distributed backends\"\"\" horovod = \"horovod\" ddp = \"ddp\" deepspeed = \"deepspeed\" class FP16Options(Enum): \"\"\"Enum that defines the options for FP16 backends\"\"\" apex_O1 = \"apex_O1\" apex_O2 = \"apex_O2\" amp = \"amp\" deepspeed = \"deepspeed\" class _MissingLocalRankException(Exception): \"\"\"Custom exception for when local rank cannot be found\"\"\" pass class StokeStatus: \"\"\"Low level stoke object that manages and sets the status of the overall run time configuration Based on the set flags this object checks for valid combinations (as there are a lot that will not work together) and builds a status object whose attributes are forwarded on via property decorators. Handles managing init of backend config objects and any post init modifications. Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_distributed_ddp is_distributed_deepspeed is_distributed_horovod is_fairscale is_fp16_apex is_fp16_deepspeed nccl oss oss_config sddp_config sharded status zero _configs: dict dictionary of config objects _key_list: list valid config objects to check against _status: dict dictionary that is the current requested state of Stoke \"\"\" def __init__( self, batch_size_per_device: int, grad_accum: Optional[int], grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], gpu: bool, fp16: Optional[FP16Options], distributed: Optional[DistributedOptions], fairscale_oss: bool, fairscale_sddp: bool, fairscale_fsdp: bool, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ], ): \"\"\"Init for StokeStatus class object Parameters ---------- batch_size_per_device: int Batch size at the single device level grad_accum: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig]], default: None Configuration objects for runtimes \"\"\" # Allowed keys for configs self._key_list = [ \"AMPConfig\", \"ApexConfig\", \"DDPConfig\", \"DeepspeedConfig\", \"FairscaleOSSConfig\", \"FairscaleSDDPConfig\", \"FairscaleFSDPConfig\" \"HorovodConfig\", ] # Set the configs first which allows for checking of some config vars later self._configs = self._set_configs(configs=configs) # Set simple state vars -- post combo check so validity is fine to set self._status = { \"cuda\": torch.cuda.is_available(), \"nccl\": torch.distributed.is_nccl_available(), \"batch_size\": batch_size_per_device, \"grad_accum\": grad_accum if grad_accum is not None else 1, \"grad_clip\": grad_clip, \"gpu\": gpu, \"distributed\": distributed, \"zero\": self._configs.get(\"DeepspeedConfig\").zero_optimization.stage if self._configs.get(\"DeepspeedConfig\") else None, \"oss\": fairscale_oss, \"sharded\": fairscale_sddp, \"fully_sharded\": fairscale_fsdp, \"world_size\": -1, } # Check fp16 since it might need APEX imports and update state dict self._status.update({\"fp16\": self._set_fp16(fp16=fp16)}) # Check all the invalid combinations self._check_all_raised_combinations() def _check_all_raised_combinations(self): \"\"\"Checks all acceptable/restricted combinations and raises exceptions for any invalid combinations README.md should have a table of acceptable combinations Returns ------- None \"\"\" # No gpu if no CUDA if self.gpu and not self.cuda: raise ValueError(\"Stoke -- GPU(s) cannot be used as CUDA is not available\") # No fairscale and deepspeed if self.is_fairscale and ( self.is_distributed_deepspeed or self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Cannot use both fairscale extensions \" f\"(currently: oss: {self.oss}, sddp: {self.sharded}) \" f\"and deepspeed (currently: distributed: {self.is_distributed_deepspeed}, \" f\"fp16: {self.is_fp16_deepspeed})\" ) # No Distributed without gpu, cuda, and nccl if ( not self.cuda or not self.gpu or not self.nccl ) and self.distributed is not None: raise ValueError( f\"Stoke -- Distributed requires CUDA (currently: {self.cuda}), GPU (currently: {self.gpu}), \" f\"and NCCL (currently: {self.nccl})\" ) # No FP16 without CUDA if not self.cuda and (self.fp16 is not None): raise ValueError(f\"Stoke -- FP16 training requires CUDA availability\") # No fairscale without gpu, cuda, and nccl and DDP (will catch Horovod) if ( not self.cuda or not self.gpu or not self.nccl or not self.is_distributed_ddp ) and self.is_fairscale: raise ValueError( f\"Stoke -- Fairscale extensions (currently: oss: {self.oss}, sddp: {self.sharded}) \" f\"requires CUDA (currently: {self.cuda}), \" f\"GPU (currently: {self.gpu}), \" f\"DDP (currently: {self.is_distributed_ddp}) and NCCL (currently: {self.nccl})\" ) # No SDDP w/o OSS if self.sharded and not self.oss: raise ValueError( f\"Stoke -- Fairscale SDDP requires OSS (currently: oss: {self.oss}, sddp: {self.sharded})\" ) # FSDP stands alone if (self.sharded or self.oss) and self.fully_sharded: raise ValueError( f\"Stoke -- Fairscale FSDP does not require SDDP or OSS as it manages OSS itself\" f\"(currently: oss: {self.oss}, sddp: {self.sharded}. fsdp: {self.fully_sharded})\" ) # No fairscale with APEX if self.is_fairscale and self.is_fp16_apex: raise ValueError( f\"Stoke -- Fairscale does not currently support APEX (currently: {self.is_fp16_apex}) \" f\"for mixed precision\" ) # No fairscale oss with grad clip by value if (self.oss or self.fully_sharded) and isinstance( self.grad_clip, ClipGradConfig ): raise ValueError( f\"Stoke -- Fairscale OSS and FSDP do not currently support torch.nn.utils.clip_grad_value_ \" f\"(currently: {type(self.grad_clip).__name__})\" ) # No deepspeed FP16 without deepspeed distributed if self.is_fp16_deepspeed and not self.is_distributed_deepspeed: raise ValueError( f\"Stoke -- Deepspeed FP16 (currently: {self.is_fp16_deepspeed}) requires the use of \" f\"Deepspeed distributed (currently: {self.is_distributed_deepspeed})\" ) # No other FP16 with deepspeed distributed if ( self.is_distributed_deepspeed and self.fp16 is not None and not self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Deepspeed distributed (currently: {self.is_distributed_deepspeed}) only \" f\"supports its own internal FP16 implementation (currently: {self.fp16})\" ) # No zero > 0 without deepspeed FP16 if ( self.is_distributed_deepspeed and self.zero > 0 and not self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Deepspeed ZeRO extension (currently: Stage-{self.zero}) requires Deepspeed\" f\"FP16 extension (currently: {self.is_fp16_deepspeed})\" ) def _set_fp16(self, fp16: Optional[FP16Options]): \"\"\"Sets the state of the FP16 backend Seeing as the APEX install is not packaged currently with Stoke (or if it is requires building some things from source it's liable to fail). Handling it this way allows Stoke not to break if APEX isn't installed correctly Parameters ---------- fp16: FP16Options, optional Enum that defines the options for FP16 backends Returns ------- FP16Options or None \"\"\" if self._status.get(\"cuda\") and (fp16 is not None): if fp16 == \"apex_O1\" or fp16 == \"apex_O2\": # Try/Except the apex import to see if it's available try: from apex import amp except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) return fp16 else: return None def _set_configs(self, configs): \"\"\"Determines which configs were set from user input and sets all others to None Parameters ---------- configs: list List of any user specified run time configs Returns ------- config_dict: dict or None dictionary of config objects or None \"\"\" # Set those that are specified within a dict if configs is not None: config_dict = {type(val).__name__: val for val in configs} else: config_dict = {} # Set those missing within the existing config dict to None so property accessors work correctly none_dict = {val: None for val in self._key_list if val not in config_dict} config_dict.update(none_dict) return config_dict def set_post_init_values(self, world_size: int): \"\"\"Sets post-init values that cannot be set prior to run-time instantiation Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet Parameters ---------- world_size: int current distributed world size Returns ------- None \"\"\" self._status.update({\"world_size\": world_size}) @property def status(self): \"\"\"Shortcut to status dict\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.get(\"batch_size\") @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self.batch_size * self.grad_accum * self._status.get(\"world_size\") @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.get(\"grad_clip\") @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.get(\"grad_accum\") @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.get(\"gpu\") @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.get(\"cuda\") @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.get(\"nccl\") @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.get(\"fp16\") @property def is_fp16_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self.fp16 == \"apex_O1\" or self.fp16 == \"apex_O2\" @property def is_fp16_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self.fp16 == \"amp\" @property def is_fp16_deepspeed(self): \"\"\"Returns if Deepspeed FP16 is activated\"\"\" return self.fp16 == \"deepspeed\" @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.get(\"oss\") @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.get(\"sharded\") @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.get(\"fully_sharded\") @property def world_size(self): \"\"\"Returns the current world size\"\"\" return self._status.get(\"world_size\") @property def zero(self): \"\"\"Returns what stage of ZeRO Deepspeed is using\"\"\" return self._status.get(\"zero\") @property def is_fairscale(self): \"\"\"Returns if any part of Fairscale is activated\"\"\" return self.oss or self.sharded or self.fully_sharded @property def distributed(self): \"\"\"Shortcut to distributed setting\"\"\" return self._status.get(\"distributed\") @property def is_distributed_deepspeed(self): \"\"\"Returns if Deepspeed is activated\"\"\" return self.distributed == \"deepspeed\" @property def is_distributed_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self.distributed == \"ddp\" @property def is_distributed_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self.distributed == \"horovod\" @property def apex_config(self): \"\"\"Checks for user defined ApexConfig and/or sets a default config object Returns ------- ApexConfig User set ApexConfig or the defaulted version \"\"\" config = self._configs.get(\"ApexConfig\") return config if config is not None else ApexConfig() @property def amp_config(self): \"\"\"Checks for user defined AMPConfig and/or sets a default config object Returns ------- AMPConfig User set AMPConfig or the defaulted version \"\"\" config = self._configs.get(\"AMPConfig\") return config if config is not None else AMPConfig() @property def ddp_config(self): \"\"\"Checks for user defined DDPConfig and/or sets a default config object Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it https://pytorch.org/docs/stable/distributed.html#launch-utility Returns ------- DDPConfig User set DDPConfig or the defaulted version \"\"\" config = self._configs.get(\"DDPConfig\") # Here need to check if the config passed through defined the local rank or not... # Assuming that it's being caught from the arg parser... if not try and grab it from # the env (set from the launcher) if config is not None and config.local_rank is None: try: local_rank = int(os.environ[\"LOCAL_RANK\"]) except _MissingLocalRankException: raise _MissingLocalRankException( f\"Stoke -- Device local rank must be defined within the DDPConfig \" f\" (handled by parsing --local_arg from the torch.distributed.launch \" f\"command) or defined as env variable LOCAL_RANK (handled by calling \" f\"torch.distributed.launch with the --use_env flag)\" ) # Evolve the config if grabbing from the env variable config = attr.evolve(config, local_rank=local_rank) elif config is None: try: local_rank = int(os.environ[\"LOCAL_RANK\"]) except _MissingLocalRankException: raise _MissingLocalRankException( f\"Stoke -- Device local rank must be defined within the DDPConfig \" f\" (handled by parsing --local_arg from the torch.distributed.launch \" f\"command) or defined as env variable LOCAL_RANK (handled by calling \" f\"torch.distributed.launch with the --use_env flag)\" ) # Set a default config with the local rank from the env config = DDPConfig(local_rank=local_rank) return config @property def deepspeed_config(self): \"\"\"Checks for user defined DeepspeedConfig and/or sets a default config object Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object like AMP or APEX Returns ------- DeepspeedConfig User set DeepspeedConfig or the defaulted version \"\"\" config = self._configs.get(\"DeepspeedConfig\") # Deepspeed only has a single config so FP16 needs to be handled here based on the status flag if no config # is passed through # Fall back to basics of both if no config if self.fp16 == \"deepspeed\" and config is None: config = DeepspeedConfig(fp16=DeepspeedFP16Config()) # Fall back to defaults if a config is passed but the FP16 Config wasn't set elif self.fp16 == \"deepspeed\" and config is not None and config.fp16 is None: config = attr.evolve(config, fp16=DeepspeedFP16Config()) # Fall back to hard defaults if just using distributed elif config is None: config = DeepspeedConfig() else: config = config return config @property def oss_config(self): \"\"\"Checks for user defined FairscaleOSSConfig and/or sets a default config object Returns ------- FairscaleOSSConfig User set FairscaleOSSConfig or the defaulted version \"\"\" config = self._configs.get(\"FairscaleOSSConfig\") return config if config is not None else FairscaleOSSConfig() @property def sddp_config(self): \"\"\"Checks for user defined FairscaleSDDPConfig and/or sets a default config object Returns ------- FairscaleSDDPConfig User set FairscaleSDDPConfig or the defaulted version \"\"\" config = self._configs.get(\"FairscaleSDDPConfig\") return config if config is not None else FairscaleSDDPConfig() @property def fsdp_config(self): \"\"\"Checks for user defined FairscaleFSDPConfig and/or sets a default config object Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings Returns ------- FairscaleFSDPConfig mutated with mixed-precision state \"\"\" config = self._configs.get(\"FairscaleFSDPConfig\") # Swap in a default config if none if config is None: config = FairscaleFSDPConfig() # Handle FP16 settings if set via constructor -- these need to be morphed at runtime to a new attr class config_dict = attr.asdict(config) config_dict.update({\"mixed_precision\": self.is_fp16_amp}) return _FairscaleFSDPConfig(**config_dict) @property def horovod_config(self): \"\"\"Checks for user defined HorovodConfig and/or sets a default config object Returns ------- HorovodConfig User set HorovodConfig or the defaulted version \"\"\" config = self._configs.get(\"HorovodConfig\") return config if config is not None else HorovodConfig() def __repr__(self): \"\"\"Formats the status for pretty printing Returns ------- str pretty formatted status string \"\"\" return ( f\"STOKE STATE:\\n\" f\" CUDA AVAILABLE: {self.cuda}\\n\" f\" NCCL AVAILABLE: {self.nccl}\\n\" f\" GPU FLAG: {self.gpu}\\n\" f\" FP16 FLAG: {self.fp16}\\n\" f\" DISTRIBUTED BACKEND: {self.distributed}\\n\" f\" FAIRSCALE OSS: {self.oss}\\n\" f\" FAIRSCALE SDDP: {self.sharded}\\n\" f\" FAIRSCALE FSDP: {self.fully_sharded}\\n\" f' DEEPSPEED ZeRO: {f\"Stage {self.zero}\" if self.is_distributed_deepspeed else f\"False\"}\\n' f\" WORLD SIZE: {self.world_size}\\n\" f\" GRAD ACCUMULATION STEPS: {self.grad_accum}\\n\" f\" BATCH SIZE (PER DEVICE): {self.batch_size}\\n\" f\" EFFECTIVE BATCH SIZE (ALL DEVICES): {self.effective_batch_size}\\n\" f' GRAD CLIP: ({\", \".join(f\"{k}: {v}\" for k, v in attr.asdict(self.grad_clip).items()) if self.grad_clip is not None else \"None\"})' ) Classes DistributedOptions class DistributedOptions ( / , * args , ** kwargs ) ??? example \"View Source\" class DistributedOptions(Enum): \"\"\"Enum that defines the options for Distributed backends\"\"\" horovod = \"horovod\" ddp = \"ddp\" deepspeed = \"deepspeed\" Ancestors (in MRO) enum.Enum Class variables ddp deepspeed horovod name value FP16Options class FP16Options ( / , * args , ** kwargs ) ??? example \"View Source\" class FP16Options(Enum): \"\"\"Enum that defines the options for FP16 backends\"\"\" apex_O1 = \"apex_O1\" apex_O2 = \"apex_O2\" amp = \"amp\" deepspeed = \"deepspeed\" Ancestors (in MRO) enum.Enum Class variables amp apex_O1 apex_O2 deepspeed name value StokeStatus class StokeStatus ( batch_size_per_device : int , grad_accum : Union [ int , NoneType ], grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig , NoneType ], gpu : bool , fp16 : Union [ stoke . status . FP16Options , NoneType ], distributed : Union [ stoke . status . DistributedOptions , NoneType ], fairscale_oss : bool , fairscale_sddp : bool , fairscale_fsdp : bool , configs : Union [ List [ Union [ stoke . configs . AMPConfig , stoke . configs . ApexConfig , stoke . configs . DDPConfig , stoke . configs . DeepspeedConfig , stoke . configs . FairscaleOSSConfig , stoke . configs . FairscaleSDDPConfig , stoke . configs . FairscaleFSDPConfig , stoke . configs . HorovodConfig ]], NoneType ] ) Attributes Name Type Description Default amp_config None None None apex_config None None None batch_size None None None cuda None None None ddp_config None None None deepspeed_config None None None distributed None None None effective_batch_size None None None fp16 None None None fsdp_config None None None fully_sharded None None None gpu None None None grad_accum None None None grad_clip None None None horovod_config None None None is_distributed_ddp None None None is_distributed_deepspeed None None None is_distributed_horovod None None None is_fairscale None None None is_fp16_apex None None None is_fp16_deepspeed None None None nccl None None None oss None None None oss_config None None None sddp_config None None None sharded None None None status None None None zero None None None _configs dict dictionary of config objects None _key_list list valid config objects to check against None _status dict dictionary that is the current requested state of Stoke None ??? example \"View Source\" class StokeStatus: \"\"\"Low level stoke object that manages and sets the status of the overall run time configuration Based on the set flags this object checks for valid combinations (as there are a lot that will not work together) and builds a status object whose attributes are forwarded on via property decorators. Handles managing init of backend config objects and any post init modifications. Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_distributed_ddp is_distributed_deepspeed is_distributed_horovod is_fairscale is_fp16_apex is_fp16_deepspeed nccl oss oss_config sddp_config sharded status zero _configs: dict dictionary of config objects _key_list: list valid config objects to check against _status: dict dictionary that is the current requested state of Stoke \"\"\" def __init__( self, batch_size_per_device: int, grad_accum: Optional[int], grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], gpu: bool, fp16: Optional[FP16Options], distributed: Optional[DistributedOptions], fairscale_oss: bool, fairscale_sddp: bool, fairscale_fsdp: bool, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ], ): \"\"\"Init for StokeStatus class object Parameters ---------- batch_size_per_device: int Batch size at the single device level grad_accum: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig]], default: None Configuration objects for runtimes \"\"\" # Allowed keys for configs self._key_list = [ \"AMPConfig\", \"ApexConfig\", \"DDPConfig\", \"DeepspeedConfig\", \"FairscaleOSSConfig\", \"FairscaleSDDPConfig\", \"FairscaleFSDPConfig\" \"HorovodConfig\", ] # Set the configs first which allows for checking of some config vars later self._configs = self._set_configs(configs=configs) # Set simple state vars -- post combo check so validity is fine to set self._status = { \"cuda\": torch.cuda.is_available(), \"nccl\": torch.distributed.is_nccl_available(), \"batch_size\": batch_size_per_device, \"grad_accum\": grad_accum if grad_accum is not None else 1, \"grad_clip\": grad_clip, \"gpu\": gpu, \"distributed\": distributed, \"zero\": self._configs.get(\"DeepspeedConfig\").zero_optimization.stage if self._configs.get(\"DeepspeedConfig\") else None, \"oss\": fairscale_oss, \"sharded\": fairscale_sddp, \"fully_sharded\": fairscale_fsdp, \"world_size\": -1, } # Check fp16 since it might need APEX imports and update state dict self._status.update({\"fp16\": self._set_fp16(fp16=fp16)}) # Check all the invalid combinations self._check_all_raised_combinations() def _check_all_raised_combinations(self): \"\"\"Checks all acceptable/restricted combinations and raises exceptions for any invalid combinations README.md should have a table of acceptable combinations Returns ------- None \"\"\" # No gpu if no CUDA if self.gpu and not self.cuda: raise ValueError(\"Stoke -- GPU(s) cannot be used as CUDA is not available\") # No fairscale and deepspeed if self.is_fairscale and ( self.is_distributed_deepspeed or self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Cannot use both fairscale extensions \" f\"(currently: oss: {self.oss}, sddp: {self.sharded}) \" f\"and deepspeed (currently: distributed: {self.is_distributed_deepspeed}, \" f\"fp16: {self.is_fp16_deepspeed})\" ) # No Distributed without gpu, cuda, and nccl if ( not self.cuda or not self.gpu or not self.nccl ) and self.distributed is not None: raise ValueError( f\"Stoke -- Distributed requires CUDA (currently: {self.cuda}), GPU (currently: {self.gpu}), \" f\"and NCCL (currently: {self.nccl})\" ) # No FP16 without CUDA if not self.cuda and (self.fp16 is not None): raise ValueError(f\"Stoke -- FP16 training requires CUDA availability\") # No fairscale without gpu, cuda, and nccl and DDP (will catch Horovod) if ( not self.cuda or not self.gpu or not self.nccl or not self.is_distributed_ddp ) and self.is_fairscale: raise ValueError( f\"Stoke -- Fairscale extensions (currently: oss: {self.oss}, sddp: {self.sharded}) \" f\"requires CUDA (currently: {self.cuda}), \" f\"GPU (currently: {self.gpu}), \" f\"DDP (currently: {self.is_distributed_ddp}) and NCCL (currently: {self.nccl})\" ) # No SDDP w/o OSS if self.sharded and not self.oss: raise ValueError( f\"Stoke -- Fairscale SDDP requires OSS (currently: oss: {self.oss}, sddp: {self.sharded})\" ) # FSDP stands alone if (self.sharded or self.oss) and self.fully_sharded: raise ValueError( f\"Stoke -- Fairscale FSDP does not require SDDP or OSS as it manages OSS itself\" f\"(currently: oss: {self.oss}, sddp: {self.sharded}. fsdp: {self.fully_sharded})\" ) # No fairscale with APEX if self.is_fairscale and self.is_fp16_apex: raise ValueError( f\"Stoke -- Fairscale does not currently support APEX (currently: {self.is_fp16_apex}) \" f\"for mixed precision\" ) # No fairscale oss with grad clip by value if (self.oss or self.fully_sharded) and isinstance( self.grad_clip, ClipGradConfig ): raise ValueError( f\"Stoke -- Fairscale OSS and FSDP do not currently support torch.nn.utils.clip_grad_value_ \" f\"(currently: {type(self.grad_clip).__name__})\" ) # No deepspeed FP16 without deepspeed distributed if self.is_fp16_deepspeed and not self.is_distributed_deepspeed: raise ValueError( f\"Stoke -- Deepspeed FP16 (currently: {self.is_fp16_deepspeed}) requires the use of \" f\"Deepspeed distributed (currently: {self.is_distributed_deepspeed})\" ) # No other FP16 with deepspeed distributed if ( self.is_distributed_deepspeed and self.fp16 is not None and not self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Deepspeed distributed (currently: {self.is_distributed_deepspeed}) only \" f\"supports its own internal FP16 implementation (currently: {self.fp16})\" ) # No zero > 0 without deepspeed FP16 if ( self.is_distributed_deepspeed and self.zero > 0 and not self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Deepspeed ZeRO extension (currently: Stage-{self.zero}) requires Deepspeed\" f\"FP16 extension (currently: {self.is_fp16_deepspeed})\" ) def _set_fp16(self, fp16: Optional[FP16Options]): \"\"\"Sets the state of the FP16 backend Seeing as the APEX install is not packaged currently with Stoke (or if it is requires building some things from source it's liable to fail). Handling it this way allows Stoke not to break if APEX isn't installed correctly Parameters ---------- fp16: FP16Options, optional Enum that defines the options for FP16 backends Returns ------- FP16Options or None \"\"\" if self._status.get(\"cuda\") and (fp16 is not None): if fp16 == \"apex_O1\" or fp16 == \"apex_O2\": # Try/Except the apex import to see if it's available try: from apex import amp except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) return fp16 else: return None def _set_configs(self, configs): \"\"\"Determines which configs were set from user input and sets all others to None Parameters ---------- configs: list List of any user specified run time configs Returns ------- config_dict: dict or None dictionary of config objects or None \"\"\" # Set those that are specified within a dict if configs is not None: config_dict = {type(val).__name__: val for val in configs} else: config_dict = {} # Set those missing within the existing config dict to None so property accessors work correctly none_dict = {val: None for val in self._key_list if val not in config_dict} config_dict.update(none_dict) return config_dict def set_post_init_values(self, world_size: int): \"\"\"Sets post-init values that cannot be set prior to run-time instantiation Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet Parameters ---------- world_size: int current distributed world size Returns ------- None \"\"\" self._status.update({\"world_size\": world_size}) @property def status(self): \"\"\"Shortcut to status dict\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.get(\"batch_size\") @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self.batch_size * self.grad_accum * self._status.get(\"world_size\") @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.get(\"grad_clip\") @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.get(\"grad_accum\") @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.get(\"gpu\") @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.get(\"cuda\") @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.get(\"nccl\") @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.get(\"fp16\") @property def is_fp16_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self.fp16 == \"apex_O1\" or self.fp16 == \"apex_O2\" @property def is_fp16_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self.fp16 == \"amp\" @property def is_fp16_deepspeed(self): \"\"\"Returns if Deepspeed FP16 is activated\"\"\" return self.fp16 == \"deepspeed\" @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.get(\"oss\") @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.get(\"sharded\") @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.get(\"fully_sharded\") @property def world_size(self): \"\"\"Returns the current world size\"\"\" return self._status.get(\"world_size\") @property def zero(self): \"\"\"Returns what stage of ZeRO Deepspeed is using\"\"\" return self._status.get(\"zero\") @property def is_fairscale(self): \"\"\"Returns if any part of Fairscale is activated\"\"\" return self.oss or self.sharded or self.fully_sharded @property def distributed(self): \"\"\"Shortcut to distributed setting\"\"\" return self._status.get(\"distributed\") @property def is_distributed_deepspeed(self): \"\"\"Returns if Deepspeed is activated\"\"\" return self.distributed == \"deepspeed\" @property def is_distributed_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self.distributed == \"ddp\" @property def is_distributed_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self.distributed == \"horovod\" @property def apex_config(self): \"\"\"Checks for user defined ApexConfig and/or sets a default config object Returns ------- ApexConfig User set ApexConfig or the defaulted version \"\"\" config = self._configs.get(\"ApexConfig\") return config if config is not None else ApexConfig() @property def amp_config(self): \"\"\"Checks for user defined AMPConfig and/or sets a default config object Returns ------- AMPConfig User set AMPConfig or the defaulted version \"\"\" config = self._configs.get(\"AMPConfig\") return config if config is not None else AMPConfig() @property def ddp_config(self): \"\"\"Checks for user defined DDPConfig and/or sets a default config object Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it https://pytorch.org/docs/stable/distributed.html#launch-utility Returns ------- DDPConfig User set DDPConfig or the defaulted version \"\"\" config = self._configs.get(\"DDPConfig\") # Here need to check if the config passed through defined the local rank or not... # Assuming that it's being caught from the arg parser... if not try and grab it from # the env (set from the launcher) if config is not None and config.local_rank is None: try: local_rank = int(os.environ[\"LOCAL_RANK\"]) except _MissingLocalRankException: raise _MissingLocalRankException( f\"Stoke -- Device local rank must be defined within the DDPConfig \" f\" (handled by parsing --local_arg from the torch.distributed.launch \" f\"command) or defined as env variable LOCAL_RANK (handled by calling \" f\"torch.distributed.launch with the --use_env flag)\" ) # Evolve the config if grabbing from the env variable config = attr.evolve(config, local_rank=local_rank) elif config is None: try: local_rank = int(os.environ[\"LOCAL_RANK\"]) except _MissingLocalRankException: raise _MissingLocalRankException( f\"Stoke -- Device local rank must be defined within the DDPConfig \" f\" (handled by parsing --local_arg from the torch.distributed.launch \" f\"command) or defined as env variable LOCAL_RANK (handled by calling \" f\"torch.distributed.launch with the --use_env flag)\" ) # Set a default config with the local rank from the env config = DDPConfig(local_rank=local_rank) return config @property def deepspeed_config(self): \"\"\"Checks for user defined DeepspeedConfig and/or sets a default config object Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object like AMP or APEX Returns ------- DeepspeedConfig User set DeepspeedConfig or the defaulted version \"\"\" config = self._configs.get(\"DeepspeedConfig\") # Deepspeed only has a single config so FP16 needs to be handled here based on the status flag if no config # is passed through # Fall back to basics of both if no config if self.fp16 == \"deepspeed\" and config is None: config = DeepspeedConfig(fp16=DeepspeedFP16Config()) # Fall back to defaults if a config is passed but the FP16 Config wasn't set elif self.fp16 == \"deepspeed\" and config is not None and config.fp16 is None: config = attr.evolve(config, fp16=DeepspeedFP16Config()) # Fall back to hard defaults if just using distributed elif config is None: config = DeepspeedConfig() else: config = config return config @property def oss_config(self): \"\"\"Checks for user defined FairscaleOSSConfig and/or sets a default config object Returns ------- FairscaleOSSConfig User set FairscaleOSSConfig or the defaulted version \"\"\" config = self._configs.get(\"FairscaleOSSConfig\") return config if config is not None else FairscaleOSSConfig() @property def sddp_config(self): \"\"\"Checks for user defined FairscaleSDDPConfig and/or sets a default config object Returns ------- FairscaleSDDPConfig User set FairscaleSDDPConfig or the defaulted version \"\"\" config = self._configs.get(\"FairscaleSDDPConfig\") return config if config is not None else FairscaleSDDPConfig() @property def fsdp_config(self): \"\"\"Checks for user defined FairscaleFSDPConfig and/or sets a default config object Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings Returns ------- FairscaleFSDPConfig mutated with mixed-precision state \"\"\" config = self._configs.get(\"FairscaleFSDPConfig\") # Swap in a default config if none if config is None: config = FairscaleFSDPConfig() # Handle FP16 settings if set via constructor -- these need to be morphed at runtime to a new attr class config_dict = attr.asdict(config) config_dict.update({\"mixed_precision\": self.is_fp16_amp}) return _FairscaleFSDPConfig(**config_dict) @property def horovod_config(self): \"\"\"Checks for user defined HorovodConfig and/or sets a default config object Returns ------- HorovodConfig User set HorovodConfig or the defaulted version \"\"\" config = self._configs.get(\"HorovodConfig\") return config if config is not None else HorovodConfig() def __repr__(self): \"\"\"Formats the status for pretty printing Returns ------- str pretty formatted status string \"\"\" return ( f\"STOKE STATE:\\n\" f\" CUDA AVAILABLE: {self.cuda}\\n\" f\" NCCL AVAILABLE: {self.nccl}\\n\" f\" GPU FLAG: {self.gpu}\\n\" f\" FP16 FLAG: {self.fp16}\\n\" f\" DISTRIBUTED BACKEND: {self.distributed}\\n\" f\" FAIRSCALE OSS: {self.oss}\\n\" f\" FAIRSCALE SDDP: {self.sharded}\\n\" f\" FAIRSCALE FSDP: {self.fully_sharded}\\n\" f' DEEPSPEED ZeRO: {f\"Stage {self.zero}\" if self.is_distributed_deepspeed else f\"False\"}\\n' f\" WORLD SIZE: {self.world_size}\\n\" f\" GRAD ACCUMULATION STEPS: {self.grad_accum}\\n\" f\" BATCH SIZE (PER DEVICE): {self.batch_size}\\n\" f\" EFFECTIVE BATCH SIZE (ALL DEVICES): {self.effective_batch_size}\\n\" f' GRAD CLIP: ({\", \".join(f\"{k}: {v}\" for k, v in attr.asdict(self.grad_clip).items()) if self.grad_clip is not None else \"None\"})' ) Instance variables amp_config Checks for user defined AMPConfig and/or sets a default config object apex_config Checks for user defined ApexConfig and/or sets a default config object batch_size Shortcut to batch size cuda Shortcut to get cuda status ddp_config Checks for user defined DDPConfig and/or sets a default config object Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it https://pytorch.org/docs/stable/distributed.html#launch-utility deepspeed_config Checks for user defined DeepspeedConfig and/or sets a default config object Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object like AMP or APEX distributed Shortcut to distributed setting effective_batch_size Shortcut to effective batch size fp16 Shortcut to get FP16 status fsdp_config Checks for user defined FairscaleFSDPConfig and/or sets a default config object Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings fully_sharded Returns if Fairscale fully sharded DDP status gpu Shortcut to get GPU status grad_accum Shortcut to get grad accumulation grad_clip Shortcut to get grad clip horovod_config Checks for user defined HorovodConfig and/or sets a default config object is_distributed_ddp Returns if DDP is activated is_distributed_deepspeed Returns if Deepspeed is activated is_distributed_horovod Returns if Horovod is activated is_fairscale Returns if any part of Fairscale is activated is_fp16_amp Returns if AMP is activated is_fp16_apex Returns if APEX is activated is_fp16_deepspeed Returns if Deepspeed FP16 is activated nccl Shortcut to get nccl status oss Returns if Fairscale optimizer state sharding status oss_config Checks for user defined FairscaleOSSConfig and/or sets a default config object sddp_config Checks for user defined FairscaleSDDPConfig and/or sets a default config object sharded Returns if Fairscale sharded DDP status status Shortcut to status dict world_size Returns the current world size zero Returns what stage of ZeRO Deepspeed is using Methods set_post_init_values def set_post_init_values ( self , world_size : int ) Sets post-init values that cannot be set prior to run-time instantiation Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet Parameters: Name Type Description Default world_size int current distributed world size None Returns: Type Description None None ??? example \"View Source\" def set_post_init_values(self, world_size: int): \"\"\"Sets post-init values that cannot be set prior to run-time instantiation Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet Parameters ---------- world_size: int current distributed world size Returns ------- None \"\"\" self._status.update({\"world_size\": world_size})","title":"Status"},{"location":"reference/stoke/status/#module-stokestatus","text":"Handles setting the status/state of Stoke None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Handles setting the status/state of Stoke\"\"\" import os from enum import Enum from typing import List, Optional, Union import attr import torch from stoke.configs import ( AMPConfig, ApexConfig, ClipGradConfig, ClipGradNormConfig, DDPConfig, DeepspeedConfig, DeepspeedFP16Config, FairscaleFSDPConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig, ) from stoke.extensions import _FairscaleFSDPConfig class DistributedOptions(Enum): \"\"\"Enum that defines the options for Distributed backends\"\"\" horovod = \"horovod\" ddp = \"ddp\" deepspeed = \"deepspeed\" class FP16Options(Enum): \"\"\"Enum that defines the options for FP16 backends\"\"\" apex_O1 = \"apex_O1\" apex_O2 = \"apex_O2\" amp = \"amp\" deepspeed = \"deepspeed\" class _MissingLocalRankException(Exception): \"\"\"Custom exception for when local rank cannot be found\"\"\" pass class StokeStatus: \"\"\"Low level stoke object that manages and sets the status of the overall run time configuration Based on the set flags this object checks for valid combinations (as there are a lot that will not work together) and builds a status object whose attributes are forwarded on via property decorators. Handles managing init of backend config objects and any post init modifications. Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_distributed_ddp is_distributed_deepspeed is_distributed_horovod is_fairscale is_fp16_apex is_fp16_deepspeed nccl oss oss_config sddp_config sharded status zero _configs: dict dictionary of config objects _key_list: list valid config objects to check against _status: dict dictionary that is the current requested state of Stoke \"\"\" def __init__( self, batch_size_per_device: int, grad_accum: Optional[int], grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], gpu: bool, fp16: Optional[FP16Options], distributed: Optional[DistributedOptions], fairscale_oss: bool, fairscale_sddp: bool, fairscale_fsdp: bool, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ], ): \"\"\"Init for StokeStatus class object Parameters ---------- batch_size_per_device: int Batch size at the single device level grad_accum: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig]], default: None Configuration objects for runtimes \"\"\" # Allowed keys for configs self._key_list = [ \"AMPConfig\", \"ApexConfig\", \"DDPConfig\", \"DeepspeedConfig\", \"FairscaleOSSConfig\", \"FairscaleSDDPConfig\", \"FairscaleFSDPConfig\" \"HorovodConfig\", ] # Set the configs first which allows for checking of some config vars later self._configs = self._set_configs(configs=configs) # Set simple state vars -- post combo check so validity is fine to set self._status = { \"cuda\": torch.cuda.is_available(), \"nccl\": torch.distributed.is_nccl_available(), \"batch_size\": batch_size_per_device, \"grad_accum\": grad_accum if grad_accum is not None else 1, \"grad_clip\": grad_clip, \"gpu\": gpu, \"distributed\": distributed, \"zero\": self._configs.get(\"DeepspeedConfig\").zero_optimization.stage if self._configs.get(\"DeepspeedConfig\") else None, \"oss\": fairscale_oss, \"sharded\": fairscale_sddp, \"fully_sharded\": fairscale_fsdp, \"world_size\": -1, } # Check fp16 since it might need APEX imports and update state dict self._status.update({\"fp16\": self._set_fp16(fp16=fp16)}) # Check all the invalid combinations self._check_all_raised_combinations() def _check_all_raised_combinations(self): \"\"\"Checks all acceptable/restricted combinations and raises exceptions for any invalid combinations README.md should have a table of acceptable combinations Returns ------- None \"\"\" # No gpu if no CUDA if self.gpu and not self.cuda: raise ValueError(\"Stoke -- GPU(s) cannot be used as CUDA is not available\") # No fairscale and deepspeed if self.is_fairscale and ( self.is_distributed_deepspeed or self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Cannot use both fairscale extensions \" f\"(currently: oss: {self.oss}, sddp: {self.sharded}) \" f\"and deepspeed (currently: distributed: {self.is_distributed_deepspeed}, \" f\"fp16: {self.is_fp16_deepspeed})\" ) # No Distributed without gpu, cuda, and nccl if ( not self.cuda or not self.gpu or not self.nccl ) and self.distributed is not None: raise ValueError( f\"Stoke -- Distributed requires CUDA (currently: {self.cuda}), GPU (currently: {self.gpu}), \" f\"and NCCL (currently: {self.nccl})\" ) # No FP16 without CUDA if not self.cuda and (self.fp16 is not None): raise ValueError(f\"Stoke -- FP16 training requires CUDA availability\") # No fairscale without gpu, cuda, and nccl and DDP (will catch Horovod) if ( not self.cuda or not self.gpu or not self.nccl or not self.is_distributed_ddp ) and self.is_fairscale: raise ValueError( f\"Stoke -- Fairscale extensions (currently: oss: {self.oss}, sddp: {self.sharded}) \" f\"requires CUDA (currently: {self.cuda}), \" f\"GPU (currently: {self.gpu}), \" f\"DDP (currently: {self.is_distributed_ddp}) and NCCL (currently: {self.nccl})\" ) # No SDDP w/o OSS if self.sharded and not self.oss: raise ValueError( f\"Stoke -- Fairscale SDDP requires OSS (currently: oss: {self.oss}, sddp: {self.sharded})\" ) # FSDP stands alone if (self.sharded or self.oss) and self.fully_sharded: raise ValueError( f\"Stoke -- Fairscale FSDP does not require SDDP or OSS as it manages OSS itself\" f\"(currently: oss: {self.oss}, sddp: {self.sharded}. fsdp: {self.fully_sharded})\" ) # No fairscale with APEX if self.is_fairscale and self.is_fp16_apex: raise ValueError( f\"Stoke -- Fairscale does not currently support APEX (currently: {self.is_fp16_apex}) \" f\"for mixed precision\" ) # No fairscale oss with grad clip by value if (self.oss or self.fully_sharded) and isinstance( self.grad_clip, ClipGradConfig ): raise ValueError( f\"Stoke -- Fairscale OSS and FSDP do not currently support torch.nn.utils.clip_grad_value_ \" f\"(currently: {type(self.grad_clip).__name__})\" ) # No deepspeed FP16 without deepspeed distributed if self.is_fp16_deepspeed and not self.is_distributed_deepspeed: raise ValueError( f\"Stoke -- Deepspeed FP16 (currently: {self.is_fp16_deepspeed}) requires the use of \" f\"Deepspeed distributed (currently: {self.is_distributed_deepspeed})\" ) # No other FP16 with deepspeed distributed if ( self.is_distributed_deepspeed and self.fp16 is not None and not self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Deepspeed distributed (currently: {self.is_distributed_deepspeed}) only \" f\"supports its own internal FP16 implementation (currently: {self.fp16})\" ) # No zero > 0 without deepspeed FP16 if ( self.is_distributed_deepspeed and self.zero > 0 and not self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Deepspeed ZeRO extension (currently: Stage-{self.zero}) requires Deepspeed\" f\"FP16 extension (currently: {self.is_fp16_deepspeed})\" ) def _set_fp16(self, fp16: Optional[FP16Options]): \"\"\"Sets the state of the FP16 backend Seeing as the APEX install is not packaged currently with Stoke (or if it is requires building some things from source it's liable to fail). Handling it this way allows Stoke not to break if APEX isn't installed correctly Parameters ---------- fp16: FP16Options, optional Enum that defines the options for FP16 backends Returns ------- FP16Options or None \"\"\" if self._status.get(\"cuda\") and (fp16 is not None): if fp16 == \"apex_O1\" or fp16 == \"apex_O2\": # Try/Except the apex import to see if it's available try: from apex import amp except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) return fp16 else: return None def _set_configs(self, configs): \"\"\"Determines which configs were set from user input and sets all others to None Parameters ---------- configs: list List of any user specified run time configs Returns ------- config_dict: dict or None dictionary of config objects or None \"\"\" # Set those that are specified within a dict if configs is not None: config_dict = {type(val).__name__: val for val in configs} else: config_dict = {} # Set those missing within the existing config dict to None so property accessors work correctly none_dict = {val: None for val in self._key_list if val not in config_dict} config_dict.update(none_dict) return config_dict def set_post_init_values(self, world_size: int): \"\"\"Sets post-init values that cannot be set prior to run-time instantiation Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet Parameters ---------- world_size: int current distributed world size Returns ------- None \"\"\" self._status.update({\"world_size\": world_size}) @property def status(self): \"\"\"Shortcut to status dict\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.get(\"batch_size\") @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self.batch_size * self.grad_accum * self._status.get(\"world_size\") @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.get(\"grad_clip\") @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.get(\"grad_accum\") @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.get(\"gpu\") @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.get(\"cuda\") @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.get(\"nccl\") @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.get(\"fp16\") @property def is_fp16_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self.fp16 == \"apex_O1\" or self.fp16 == \"apex_O2\" @property def is_fp16_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self.fp16 == \"amp\" @property def is_fp16_deepspeed(self): \"\"\"Returns if Deepspeed FP16 is activated\"\"\" return self.fp16 == \"deepspeed\" @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.get(\"oss\") @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.get(\"sharded\") @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.get(\"fully_sharded\") @property def world_size(self): \"\"\"Returns the current world size\"\"\" return self._status.get(\"world_size\") @property def zero(self): \"\"\"Returns what stage of ZeRO Deepspeed is using\"\"\" return self._status.get(\"zero\") @property def is_fairscale(self): \"\"\"Returns if any part of Fairscale is activated\"\"\" return self.oss or self.sharded or self.fully_sharded @property def distributed(self): \"\"\"Shortcut to distributed setting\"\"\" return self._status.get(\"distributed\") @property def is_distributed_deepspeed(self): \"\"\"Returns if Deepspeed is activated\"\"\" return self.distributed == \"deepspeed\" @property def is_distributed_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self.distributed == \"ddp\" @property def is_distributed_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self.distributed == \"horovod\" @property def apex_config(self): \"\"\"Checks for user defined ApexConfig and/or sets a default config object Returns ------- ApexConfig User set ApexConfig or the defaulted version \"\"\" config = self._configs.get(\"ApexConfig\") return config if config is not None else ApexConfig() @property def amp_config(self): \"\"\"Checks for user defined AMPConfig and/or sets a default config object Returns ------- AMPConfig User set AMPConfig or the defaulted version \"\"\" config = self._configs.get(\"AMPConfig\") return config if config is not None else AMPConfig() @property def ddp_config(self): \"\"\"Checks for user defined DDPConfig and/or sets a default config object Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it https://pytorch.org/docs/stable/distributed.html#launch-utility Returns ------- DDPConfig User set DDPConfig or the defaulted version \"\"\" config = self._configs.get(\"DDPConfig\") # Here need to check if the config passed through defined the local rank or not... # Assuming that it's being caught from the arg parser... if not try and grab it from # the env (set from the launcher) if config is not None and config.local_rank is None: try: local_rank = int(os.environ[\"LOCAL_RANK\"]) except _MissingLocalRankException: raise _MissingLocalRankException( f\"Stoke -- Device local rank must be defined within the DDPConfig \" f\" (handled by parsing --local_arg from the torch.distributed.launch \" f\"command) or defined as env variable LOCAL_RANK (handled by calling \" f\"torch.distributed.launch with the --use_env flag)\" ) # Evolve the config if grabbing from the env variable config = attr.evolve(config, local_rank=local_rank) elif config is None: try: local_rank = int(os.environ[\"LOCAL_RANK\"]) except _MissingLocalRankException: raise _MissingLocalRankException( f\"Stoke -- Device local rank must be defined within the DDPConfig \" f\" (handled by parsing --local_arg from the torch.distributed.launch \" f\"command) or defined as env variable LOCAL_RANK (handled by calling \" f\"torch.distributed.launch with the --use_env flag)\" ) # Set a default config with the local rank from the env config = DDPConfig(local_rank=local_rank) return config @property def deepspeed_config(self): \"\"\"Checks for user defined DeepspeedConfig and/or sets a default config object Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object like AMP or APEX Returns ------- DeepspeedConfig User set DeepspeedConfig or the defaulted version \"\"\" config = self._configs.get(\"DeepspeedConfig\") # Deepspeed only has a single config so FP16 needs to be handled here based on the status flag if no config # is passed through # Fall back to basics of both if no config if self.fp16 == \"deepspeed\" and config is None: config = DeepspeedConfig(fp16=DeepspeedFP16Config()) # Fall back to defaults if a config is passed but the FP16 Config wasn't set elif self.fp16 == \"deepspeed\" and config is not None and config.fp16 is None: config = attr.evolve(config, fp16=DeepspeedFP16Config()) # Fall back to hard defaults if just using distributed elif config is None: config = DeepspeedConfig() else: config = config return config @property def oss_config(self): \"\"\"Checks for user defined FairscaleOSSConfig and/or sets a default config object Returns ------- FairscaleOSSConfig User set FairscaleOSSConfig or the defaulted version \"\"\" config = self._configs.get(\"FairscaleOSSConfig\") return config if config is not None else FairscaleOSSConfig() @property def sddp_config(self): \"\"\"Checks for user defined FairscaleSDDPConfig and/or sets a default config object Returns ------- FairscaleSDDPConfig User set FairscaleSDDPConfig or the defaulted version \"\"\" config = self._configs.get(\"FairscaleSDDPConfig\") return config if config is not None else FairscaleSDDPConfig() @property def fsdp_config(self): \"\"\"Checks for user defined FairscaleFSDPConfig and/or sets a default config object Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings Returns ------- FairscaleFSDPConfig mutated with mixed-precision state \"\"\" config = self._configs.get(\"FairscaleFSDPConfig\") # Swap in a default config if none if config is None: config = FairscaleFSDPConfig() # Handle FP16 settings if set via constructor -- these need to be morphed at runtime to a new attr class config_dict = attr.asdict(config) config_dict.update({\"mixed_precision\": self.is_fp16_amp}) return _FairscaleFSDPConfig(**config_dict) @property def horovod_config(self): \"\"\"Checks for user defined HorovodConfig and/or sets a default config object Returns ------- HorovodConfig User set HorovodConfig or the defaulted version \"\"\" config = self._configs.get(\"HorovodConfig\") return config if config is not None else HorovodConfig() def __repr__(self): \"\"\"Formats the status for pretty printing Returns ------- str pretty formatted status string \"\"\" return ( f\"STOKE STATE:\\n\" f\" CUDA AVAILABLE: {self.cuda}\\n\" f\" NCCL AVAILABLE: {self.nccl}\\n\" f\" GPU FLAG: {self.gpu}\\n\" f\" FP16 FLAG: {self.fp16}\\n\" f\" DISTRIBUTED BACKEND: {self.distributed}\\n\" f\" FAIRSCALE OSS: {self.oss}\\n\" f\" FAIRSCALE SDDP: {self.sharded}\\n\" f\" FAIRSCALE FSDP: {self.fully_sharded}\\n\" f' DEEPSPEED ZeRO: {f\"Stage {self.zero}\" if self.is_distributed_deepspeed else f\"False\"}\\n' f\" WORLD SIZE: {self.world_size}\\n\" f\" GRAD ACCUMULATION STEPS: {self.grad_accum}\\n\" f\" BATCH SIZE (PER DEVICE): {self.batch_size}\\n\" f\" EFFECTIVE BATCH SIZE (ALL DEVICES): {self.effective_batch_size}\\n\" f' GRAD CLIP: ({\", \".join(f\"{k}: {v}\" for k, v in attr.asdict(self.grad_clip).items()) if self.grad_clip is not None else \"None\"})' )","title":"Module stoke.status"},{"location":"reference/stoke/status/#classes","text":"","title":"Classes"},{"location":"reference/stoke/status/#distributedoptions","text":"class DistributedOptions ( / , * args , ** kwargs ) ??? example \"View Source\" class DistributedOptions(Enum): \"\"\"Enum that defines the options for Distributed backends\"\"\" horovod = \"horovod\" ddp = \"ddp\" deepspeed = \"deepspeed\"","title":"DistributedOptions"},{"location":"reference/stoke/status/#ancestors-in-mro","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/status/#class-variables","text":"ddp deepspeed horovod name value","title":"Class variables"},{"location":"reference/stoke/status/#fp16options","text":"class FP16Options ( / , * args , ** kwargs ) ??? example \"View Source\" class FP16Options(Enum): \"\"\"Enum that defines the options for FP16 backends\"\"\" apex_O1 = \"apex_O1\" apex_O2 = \"apex_O2\" amp = \"amp\" deepspeed = \"deepspeed\"","title":"FP16Options"},{"location":"reference/stoke/status/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/status/#class-variables_1","text":"amp apex_O1 apex_O2 deepspeed name value","title":"Class variables"},{"location":"reference/stoke/status/#stokestatus","text":"class StokeStatus ( batch_size_per_device : int , grad_accum : Union [ int , NoneType ], grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig , NoneType ], gpu : bool , fp16 : Union [ stoke . status . FP16Options , NoneType ], distributed : Union [ stoke . status . DistributedOptions , NoneType ], fairscale_oss : bool , fairscale_sddp : bool , fairscale_fsdp : bool , configs : Union [ List [ Union [ stoke . configs . AMPConfig , stoke . configs . ApexConfig , stoke . configs . DDPConfig , stoke . configs . DeepspeedConfig , stoke . configs . FairscaleOSSConfig , stoke . configs . FairscaleSDDPConfig , stoke . configs . FairscaleFSDPConfig , stoke . configs . HorovodConfig ]], NoneType ] )","title":"StokeStatus"},{"location":"reference/stoke/status/#attributes","text":"Name Type Description Default amp_config None None None apex_config None None None batch_size None None None cuda None None None ddp_config None None None deepspeed_config None None None distributed None None None effective_batch_size None None None fp16 None None None fsdp_config None None None fully_sharded None None None gpu None None None grad_accum None None None grad_clip None None None horovod_config None None None is_distributed_ddp None None None is_distributed_deepspeed None None None is_distributed_horovod None None None is_fairscale None None None is_fp16_apex None None None is_fp16_deepspeed None None None nccl None None None oss None None None oss_config None None None sddp_config None None None sharded None None None status None None None zero None None None _configs dict dictionary of config objects None _key_list list valid config objects to check against None _status dict dictionary that is the current requested state of Stoke None ??? example \"View Source\" class StokeStatus: \"\"\"Low level stoke object that manages and sets the status of the overall run time configuration Based on the set flags this object checks for valid combinations (as there are a lot that will not work together) and builds a status object whose attributes are forwarded on via property decorators. Handles managing init of backend config objects and any post init modifications. Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_distributed_ddp is_distributed_deepspeed is_distributed_horovod is_fairscale is_fp16_apex is_fp16_deepspeed nccl oss oss_config sddp_config sharded status zero _configs: dict dictionary of config objects _key_list: list valid config objects to check against _status: dict dictionary that is the current requested state of Stoke \"\"\" def __init__( self, batch_size_per_device: int, grad_accum: Optional[int], grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], gpu: bool, fp16: Optional[FP16Options], distributed: Optional[DistributedOptions], fairscale_oss: bool, fairscale_sddp: bool, fairscale_fsdp: bool, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ], ): \"\"\"Init for StokeStatus class object Parameters ---------- batch_size_per_device: int Batch size at the single device level grad_accum: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig]], default: None Configuration objects for runtimes \"\"\" # Allowed keys for configs self._key_list = [ \"AMPConfig\", \"ApexConfig\", \"DDPConfig\", \"DeepspeedConfig\", \"FairscaleOSSConfig\", \"FairscaleSDDPConfig\", \"FairscaleFSDPConfig\" \"HorovodConfig\", ] # Set the configs first which allows for checking of some config vars later self._configs = self._set_configs(configs=configs) # Set simple state vars -- post combo check so validity is fine to set self._status = { \"cuda\": torch.cuda.is_available(), \"nccl\": torch.distributed.is_nccl_available(), \"batch_size\": batch_size_per_device, \"grad_accum\": grad_accum if grad_accum is not None else 1, \"grad_clip\": grad_clip, \"gpu\": gpu, \"distributed\": distributed, \"zero\": self._configs.get(\"DeepspeedConfig\").zero_optimization.stage if self._configs.get(\"DeepspeedConfig\") else None, \"oss\": fairscale_oss, \"sharded\": fairscale_sddp, \"fully_sharded\": fairscale_fsdp, \"world_size\": -1, } # Check fp16 since it might need APEX imports and update state dict self._status.update({\"fp16\": self._set_fp16(fp16=fp16)}) # Check all the invalid combinations self._check_all_raised_combinations() def _check_all_raised_combinations(self): \"\"\"Checks all acceptable/restricted combinations and raises exceptions for any invalid combinations README.md should have a table of acceptable combinations Returns ------- None \"\"\" # No gpu if no CUDA if self.gpu and not self.cuda: raise ValueError(\"Stoke -- GPU(s) cannot be used as CUDA is not available\") # No fairscale and deepspeed if self.is_fairscale and ( self.is_distributed_deepspeed or self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Cannot use both fairscale extensions \" f\"(currently: oss: {self.oss}, sddp: {self.sharded}) \" f\"and deepspeed (currently: distributed: {self.is_distributed_deepspeed}, \" f\"fp16: {self.is_fp16_deepspeed})\" ) # No Distributed without gpu, cuda, and nccl if ( not self.cuda or not self.gpu or not self.nccl ) and self.distributed is not None: raise ValueError( f\"Stoke -- Distributed requires CUDA (currently: {self.cuda}), GPU (currently: {self.gpu}), \" f\"and NCCL (currently: {self.nccl})\" ) # No FP16 without CUDA if not self.cuda and (self.fp16 is not None): raise ValueError(f\"Stoke -- FP16 training requires CUDA availability\") # No fairscale without gpu, cuda, and nccl and DDP (will catch Horovod) if ( not self.cuda or not self.gpu or not self.nccl or not self.is_distributed_ddp ) and self.is_fairscale: raise ValueError( f\"Stoke -- Fairscale extensions (currently: oss: {self.oss}, sddp: {self.sharded}) \" f\"requires CUDA (currently: {self.cuda}), \" f\"GPU (currently: {self.gpu}), \" f\"DDP (currently: {self.is_distributed_ddp}) and NCCL (currently: {self.nccl})\" ) # No SDDP w/o OSS if self.sharded and not self.oss: raise ValueError( f\"Stoke -- Fairscale SDDP requires OSS (currently: oss: {self.oss}, sddp: {self.sharded})\" ) # FSDP stands alone if (self.sharded or self.oss) and self.fully_sharded: raise ValueError( f\"Stoke -- Fairscale FSDP does not require SDDP or OSS as it manages OSS itself\" f\"(currently: oss: {self.oss}, sddp: {self.sharded}. fsdp: {self.fully_sharded})\" ) # No fairscale with APEX if self.is_fairscale and self.is_fp16_apex: raise ValueError( f\"Stoke -- Fairscale does not currently support APEX (currently: {self.is_fp16_apex}) \" f\"for mixed precision\" ) # No fairscale oss with grad clip by value if (self.oss or self.fully_sharded) and isinstance( self.grad_clip, ClipGradConfig ): raise ValueError( f\"Stoke -- Fairscale OSS and FSDP do not currently support torch.nn.utils.clip_grad_value_ \" f\"(currently: {type(self.grad_clip).__name__})\" ) # No deepspeed FP16 without deepspeed distributed if self.is_fp16_deepspeed and not self.is_distributed_deepspeed: raise ValueError( f\"Stoke -- Deepspeed FP16 (currently: {self.is_fp16_deepspeed}) requires the use of \" f\"Deepspeed distributed (currently: {self.is_distributed_deepspeed})\" ) # No other FP16 with deepspeed distributed if ( self.is_distributed_deepspeed and self.fp16 is not None and not self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Deepspeed distributed (currently: {self.is_distributed_deepspeed}) only \" f\"supports its own internal FP16 implementation (currently: {self.fp16})\" ) # No zero > 0 without deepspeed FP16 if ( self.is_distributed_deepspeed and self.zero > 0 and not self.is_fp16_deepspeed ): raise ValueError( f\"Stoke -- Deepspeed ZeRO extension (currently: Stage-{self.zero}) requires Deepspeed\" f\"FP16 extension (currently: {self.is_fp16_deepspeed})\" ) def _set_fp16(self, fp16: Optional[FP16Options]): \"\"\"Sets the state of the FP16 backend Seeing as the APEX install is not packaged currently with Stoke (or if it is requires building some things from source it's liable to fail). Handling it this way allows Stoke not to break if APEX isn't installed correctly Parameters ---------- fp16: FP16Options, optional Enum that defines the options for FP16 backends Returns ------- FP16Options or None \"\"\" if self._status.get(\"cuda\") and (fp16 is not None): if fp16 == \"apex_O1\" or fp16 == \"apex_O2\": # Try/Except the apex import to see if it's available try: from apex import amp except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) return fp16 else: return None def _set_configs(self, configs): \"\"\"Determines which configs were set from user input and sets all others to None Parameters ---------- configs: list List of any user specified run time configs Returns ------- config_dict: dict or None dictionary of config objects or None \"\"\" # Set those that are specified within a dict if configs is not None: config_dict = {type(val).__name__: val for val in configs} else: config_dict = {} # Set those missing within the existing config dict to None so property accessors work correctly none_dict = {val: None for val in self._key_list if val not in config_dict} config_dict.update(none_dict) return config_dict def set_post_init_values(self, world_size: int): \"\"\"Sets post-init values that cannot be set prior to run-time instantiation Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet Parameters ---------- world_size: int current distributed world size Returns ------- None \"\"\" self._status.update({\"world_size\": world_size}) @property def status(self): \"\"\"Shortcut to status dict\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.get(\"batch_size\") @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self.batch_size * self.grad_accum * self._status.get(\"world_size\") @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.get(\"grad_clip\") @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.get(\"grad_accum\") @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.get(\"gpu\") @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.get(\"cuda\") @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.get(\"nccl\") @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.get(\"fp16\") @property def is_fp16_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self.fp16 == \"apex_O1\" or self.fp16 == \"apex_O2\" @property def is_fp16_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self.fp16 == \"amp\" @property def is_fp16_deepspeed(self): \"\"\"Returns if Deepspeed FP16 is activated\"\"\" return self.fp16 == \"deepspeed\" @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.get(\"oss\") @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.get(\"sharded\") @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.get(\"fully_sharded\") @property def world_size(self): \"\"\"Returns the current world size\"\"\" return self._status.get(\"world_size\") @property def zero(self): \"\"\"Returns what stage of ZeRO Deepspeed is using\"\"\" return self._status.get(\"zero\") @property def is_fairscale(self): \"\"\"Returns if any part of Fairscale is activated\"\"\" return self.oss or self.sharded or self.fully_sharded @property def distributed(self): \"\"\"Shortcut to distributed setting\"\"\" return self._status.get(\"distributed\") @property def is_distributed_deepspeed(self): \"\"\"Returns if Deepspeed is activated\"\"\" return self.distributed == \"deepspeed\" @property def is_distributed_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self.distributed == \"ddp\" @property def is_distributed_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self.distributed == \"horovod\" @property def apex_config(self): \"\"\"Checks for user defined ApexConfig and/or sets a default config object Returns ------- ApexConfig User set ApexConfig or the defaulted version \"\"\" config = self._configs.get(\"ApexConfig\") return config if config is not None else ApexConfig() @property def amp_config(self): \"\"\"Checks for user defined AMPConfig and/or sets a default config object Returns ------- AMPConfig User set AMPConfig or the defaulted version \"\"\" config = self._configs.get(\"AMPConfig\") return config if config is not None else AMPConfig() @property def ddp_config(self): \"\"\"Checks for user defined DDPConfig and/or sets a default config object Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it https://pytorch.org/docs/stable/distributed.html#launch-utility Returns ------- DDPConfig User set DDPConfig or the defaulted version \"\"\" config = self._configs.get(\"DDPConfig\") # Here need to check if the config passed through defined the local rank or not... # Assuming that it's being caught from the arg parser... if not try and grab it from # the env (set from the launcher) if config is not None and config.local_rank is None: try: local_rank = int(os.environ[\"LOCAL_RANK\"]) except _MissingLocalRankException: raise _MissingLocalRankException( f\"Stoke -- Device local rank must be defined within the DDPConfig \" f\" (handled by parsing --local_arg from the torch.distributed.launch \" f\"command) or defined as env variable LOCAL_RANK (handled by calling \" f\"torch.distributed.launch with the --use_env flag)\" ) # Evolve the config if grabbing from the env variable config = attr.evolve(config, local_rank=local_rank) elif config is None: try: local_rank = int(os.environ[\"LOCAL_RANK\"]) except _MissingLocalRankException: raise _MissingLocalRankException( f\"Stoke -- Device local rank must be defined within the DDPConfig \" f\" (handled by parsing --local_arg from the torch.distributed.launch \" f\"command) or defined as env variable LOCAL_RANK (handled by calling \" f\"torch.distributed.launch with the --use_env flag)\" ) # Set a default config with the local rank from the env config = DDPConfig(local_rank=local_rank) return config @property def deepspeed_config(self): \"\"\"Checks for user defined DeepspeedConfig and/or sets a default config object Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object like AMP or APEX Returns ------- DeepspeedConfig User set DeepspeedConfig or the defaulted version \"\"\" config = self._configs.get(\"DeepspeedConfig\") # Deepspeed only has a single config so FP16 needs to be handled here based on the status flag if no config # is passed through # Fall back to basics of both if no config if self.fp16 == \"deepspeed\" and config is None: config = DeepspeedConfig(fp16=DeepspeedFP16Config()) # Fall back to defaults if a config is passed but the FP16 Config wasn't set elif self.fp16 == \"deepspeed\" and config is not None and config.fp16 is None: config = attr.evolve(config, fp16=DeepspeedFP16Config()) # Fall back to hard defaults if just using distributed elif config is None: config = DeepspeedConfig() else: config = config return config @property def oss_config(self): \"\"\"Checks for user defined FairscaleOSSConfig and/or sets a default config object Returns ------- FairscaleOSSConfig User set FairscaleOSSConfig or the defaulted version \"\"\" config = self._configs.get(\"FairscaleOSSConfig\") return config if config is not None else FairscaleOSSConfig() @property def sddp_config(self): \"\"\"Checks for user defined FairscaleSDDPConfig and/or sets a default config object Returns ------- FairscaleSDDPConfig User set FairscaleSDDPConfig or the defaulted version \"\"\" config = self._configs.get(\"FairscaleSDDPConfig\") return config if config is not None else FairscaleSDDPConfig() @property def fsdp_config(self): \"\"\"Checks for user defined FairscaleFSDPConfig and/or sets a default config object Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings Returns ------- FairscaleFSDPConfig mutated with mixed-precision state \"\"\" config = self._configs.get(\"FairscaleFSDPConfig\") # Swap in a default config if none if config is None: config = FairscaleFSDPConfig() # Handle FP16 settings if set via constructor -- these need to be morphed at runtime to a new attr class config_dict = attr.asdict(config) config_dict.update({\"mixed_precision\": self.is_fp16_amp}) return _FairscaleFSDPConfig(**config_dict) @property def horovod_config(self): \"\"\"Checks for user defined HorovodConfig and/or sets a default config object Returns ------- HorovodConfig User set HorovodConfig or the defaulted version \"\"\" config = self._configs.get(\"HorovodConfig\") return config if config is not None else HorovodConfig() def __repr__(self): \"\"\"Formats the status for pretty printing Returns ------- str pretty formatted status string \"\"\" return ( f\"STOKE STATE:\\n\" f\" CUDA AVAILABLE: {self.cuda}\\n\" f\" NCCL AVAILABLE: {self.nccl}\\n\" f\" GPU FLAG: {self.gpu}\\n\" f\" FP16 FLAG: {self.fp16}\\n\" f\" DISTRIBUTED BACKEND: {self.distributed}\\n\" f\" FAIRSCALE OSS: {self.oss}\\n\" f\" FAIRSCALE SDDP: {self.sharded}\\n\" f\" FAIRSCALE FSDP: {self.fully_sharded}\\n\" f' DEEPSPEED ZeRO: {f\"Stage {self.zero}\" if self.is_distributed_deepspeed else f\"False\"}\\n' f\" WORLD SIZE: {self.world_size}\\n\" f\" GRAD ACCUMULATION STEPS: {self.grad_accum}\\n\" f\" BATCH SIZE (PER DEVICE): {self.batch_size}\\n\" f\" EFFECTIVE BATCH SIZE (ALL DEVICES): {self.effective_batch_size}\\n\" f' GRAD CLIP: ({\", \".join(f\"{k}: {v}\" for k, v in attr.asdict(self.grad_clip).items()) if self.grad_clip is not None else \"None\"})' )","title":"Attributes"},{"location":"reference/stoke/status/#instance-variables","text":"amp_config Checks for user defined AMPConfig and/or sets a default config object apex_config Checks for user defined ApexConfig and/or sets a default config object batch_size Shortcut to batch size cuda Shortcut to get cuda status ddp_config Checks for user defined DDPConfig and/or sets a default config object Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it https://pytorch.org/docs/stable/distributed.html#launch-utility deepspeed_config Checks for user defined DeepspeedConfig and/or sets a default config object Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object like AMP or APEX distributed Shortcut to distributed setting effective_batch_size Shortcut to effective batch size fp16 Shortcut to get FP16 status fsdp_config Checks for user defined FairscaleFSDPConfig and/or sets a default config object Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings fully_sharded Returns if Fairscale fully sharded DDP status gpu Shortcut to get GPU status grad_accum Shortcut to get grad accumulation grad_clip Shortcut to get grad clip horovod_config Checks for user defined HorovodConfig and/or sets a default config object is_distributed_ddp Returns if DDP is activated is_distributed_deepspeed Returns if Deepspeed is activated is_distributed_horovod Returns if Horovod is activated is_fairscale Returns if any part of Fairscale is activated is_fp16_amp Returns if AMP is activated is_fp16_apex Returns if APEX is activated is_fp16_deepspeed Returns if Deepspeed FP16 is activated nccl Shortcut to get nccl status oss Returns if Fairscale optimizer state sharding status oss_config Checks for user defined FairscaleOSSConfig and/or sets a default config object sddp_config Checks for user defined FairscaleSDDPConfig and/or sets a default config object sharded Returns if Fairscale sharded DDP status status Shortcut to status dict world_size Returns the current world size zero Returns what stage of ZeRO Deepspeed is using","title":"Instance variables"},{"location":"reference/stoke/status/#methods","text":"","title":"Methods"},{"location":"reference/stoke/status/#set_post_init_values","text":"def set_post_init_values ( self , world_size : int ) Sets post-init values that cannot be set prior to run-time instantiation Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet Parameters: Name Type Description Default world_size int current distributed world size None Returns: Type Description None None ??? example \"View Source\" def set_post_init_values(self, world_size: int): \"\"\"Sets post-init values that cannot be set prior to run-time instantiation Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet Parameters ---------- world_size: int current distributed world size Returns ------- None \"\"\" self._status.update({\"world_size\": world_size})","title":"set_post_init_values"},{"location":"reference/stoke/stoke/","text":"Module stoke.stoke API interface to Stoke that handles any necessary config, context, setup etc. None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"API interface to Stoke that handles any necessary config, context, setup etc.\"\"\" from contextlib import nullcontext from typing import Callable, Dict, List, Optional, Sequence, Tuple, Type, Union from uuid import uuid4 import torch from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP from fairscale.nn.data_parallel import ShardedDataParallel as SDDP from torch.nn.parallel import DataParallel as DP from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import Dataset from torch.utils.data.distributed import Sampler from stoke.configs import ( AMPConfig, ApexConfig, ClipGradConfig, ClipGradNormConfig, DDPConfig, DeepspeedConfig, FairscaleFSDPConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig, StokeOptimizer, ) from stoke.data import StokeDataLoader from stoke.distributed import RunnerDistEnum from stoke.extensions import RunnerOptimizerEnum from stoke.fp16 import RunnerFP16Enum from stoke.io import RunnerIOEnum from stoke.status import DistributedOptions, FP16Options, StokeStatus from stoke.utils import ( ParamNormalize, T_co, _collate_fn_t, _worker_init_fn_t, zero_optimizer_grads, ) class Stoke: \"\"\"High level stoke object that manages all necessary configs and provides a unified interface to ops This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model, loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale), mixed-precision (AMP or APEX) and devices (CPU or GPU) Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size ema_loss fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_amp is_apex is_ddp is_deepspeed is_horovod loss_access model_access nccl num_model_parameters optimizer oss oss_config rank scaler sddp_config sharded status world_size _agg_loss: Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) _backward_steps: int Number of times gradients have been calculated on a batch of samples (calls to backward) _grad_accum_counter: int counter for grad accumulation steps _loss: Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs _last_step_loss: list, tuple, or float last loss step calculation aggregated over device(s) _model: torch.nn.Module instance of torch.nn.Module for Stoke to handle _optimizer: StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs _optimizer_steps: int Number of times step has been called on the optimizer _runner: StokeRunner the dynamically created runtime object that handles all ops _status: StokeStatus StokeStatus object that sets and maintains the current configuration _verbose: bool print verbosity _rolling_loss_steps: int number of steps that have been called for the rolling loss _rolling_mean_loss: list, tuple, or float current ema loss _ema_weight: float weight used for any ema calculation on metrics \"\"\" def __init__( self, model: torch.nn.Module, optimizer: StokeOptimizer, loss: Union[Callable, List[Callable], Tuple[Callable]], batch_size_per_device: int, grad_accum_steps: Optional[int] = 1, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None, gpu: bool = False, fp16: Optional[FP16Options] = None, distributed: Optional[DistributedOptions] = None, fairscale_oss: bool = False, fairscale_sddp: bool = False, fairscale_fsdp: bool = False, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ] = None, info_rank: Optional[Union[int, List[int]]] = 0, verbose: bool = True, ema_weight: float = 0.1, ): \"\"\"Init for Stoke class object Parameters ---------- model: torch.nn.Module PyTorch model optimizer: StokeOptimizer Optimizer configuration loss: Union[Callable, List[Callable], Tuple[Callable]] Callable loss function or functions batch_size_per_device: int Batch size at the single device level grad_accum_steps: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None Configuration objects for runtimes info_rank: Optional[Union[int, List[int]]], default = 0 Constrain prints to specific devices verbose: bool, default: True Flag for verbosity ema_weight: float, default: 0.5 weight used for any ema calculation on metrics \"\"\" # Verbosity self._verbose = verbose # Info rank self._info_rank = info_rank # EMA self._ema_weight = ema_weight # Setup the StokeState self._status = StokeStatus( batch_size_per_device=batch_size_per_device, grad_accum=grad_accum_steps, grad_clip=grad_clip, gpu=gpu, fp16=fp16, distributed=distributed, fairscale_oss=fairscale_oss, fairscale_sddp=fairscale_sddp, fairscale_fsdp=fairscale_fsdp, configs=configs, ) # Run some checks self._model = self._check_model(model) self._optimizer = self._check_optimizer(optimizer) self._loss = self._check_loss(loss) # Dynamically construct the StokeRunner from the StokeStatus self._runner, class_info = self._build_runner() # Setup distributed backend self._runner.setup_distributed() # Post here the runner will have the print_device function that is mapped to the self.print here # as it needs rank to be accessible before working if self._verbose: dev_id = ( self.rank if (self.rank == \"cpu\" or self.rank == \"gpu\") else self._info_rank ) self.print(f\"Printing verbose information on rank(s): {dev_id}\") # Print the runner class info from the mixins self.print(class_info) # Possibly place model on GPU depending on StokeStatus -- before wrap calls self._place_model_on_gpu() # Handle the wrap ops in the correct order self._handle_ordered_wrap_ops(optimizer=optimizer) # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 # Set post-init status variables self._status.set_post_init_values(world_size=self.world_size) # Print the final configuration if self._verbose: self.print(msg=self._status) def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of optimizer then the model This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped methods are called Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed through self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer) # Wrap with distributed backend -- in this case the optimizer is passed through self._model, self._optimizer = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer ) def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of model then optimizer Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn't exist yet # don't use the return for the optimizer in this case self._model, _ = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=None ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn't exist yet self._runner.wrap_fp16(model=self._model, optimizer=None) # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed) if (self.sharded and self.oss) or self.is_apex or self.is_horovod: self._wrap_optimizer_then_model(optimizer=optimizer) else: self._wrap_model_then_optimizer(optimizer=optimizer) def _check_accum(self): \"\"\"Checks if the current step is the last accumulation step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0 def _check_pre_accum(self): \"\"\"Checks if we are at the pre-accumulate step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum def _set_loss_to_zero(self): \"\"\"Used to set a loss tracker to zero depending on the type Returns ------- float or list or tuple of reset loss \"\"\" return ( type(self._loss)([0.0] * len(self._loss)) if isinstance(self._loss, (list, tuple)) else 0.0 ) def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\") def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\") def _scale_agg_loss(self): \"\"\"Scales the mean aggregated loss by grad accum Returns ------- scale_vals: list or float of mean aggregated loss \"\"\" if isinstance(self._agg_loss, (list, tuple)): scale_vals = [ val / self.grad_accum for idx, val in enumerate(self._agg_loss) ] else: scale_vals = self._agg_loss / self.grad_accum return scale_vals def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\") def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank) def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line ) @staticmethod def _check_model(model: torch.nn.Module): \"\"\"Verifies the type of the model Parameters ---------- model: torch.nn.Module current torch model Returns ------- None \"\"\" # Check if the model is an nn.Module such that it has a forward method if not isinstance(model, torch.nn.Module): raise TypeError( f\"Stoke -- Model is not of type torch.nn.Module, currently {type(model)}\" ) return model @staticmethod def _check_optimizer(optimizer: StokeOptimizer): \"\"\"Verifies the type of the optimizer Parameters ---------- optimizer: StokeOptimizer Current optimizer configuration TypedDict (aka dict) Returns ------- None \"\"\" if not isinstance(optimizer, dict): raise TypeError( f\"Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}\" ) return optimizer def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]): \"\"\"Checks to make sure the loss function(s) is/are callable Parameters ---------- loss: Union[Callable, List[Callable], Tuple[Callable]] Current callable loss(es) Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): loss = [self._check_loss(val) for val in loss] return loss elif isinstance(loss, Callable): return loss else: raise TypeError( f\"Stoke -- Loss is not of type Callable, currently {type(loss)}\" ) def _place_model_on_gpu(self): \"\"\"Automatically moves the model to GPU device(s) Returns ------- None \"\"\" if self.gpu and not self.is_deepspeed: if self._verbose: self.print(f\"Automatically handling moving model to GPU(s)...\") self._model.cuda() def _build_runner(self): \"\"\"Builds the runtime object from the mixin style classes Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a a single interface. Might prevent some IDE type-hinting as it's dynamic Returns ------- StokeRunner runtime runner object \"\"\" # Get the classes dist_class = self._get_distributed_mixin() fp16_class = self._get_fp16_mixin() optimizer_class = self._get_optimizer_mixin() io_class = self._get_io_mixin() # Python MRO hack to make sure the inits of all the Mixin classes get called def __multiple_mixin_init__(*args, **kwargs): dist_class.__init__(*args, **kwargs) fp16_class.__init__(*args, **kwargs) optimizer_class.__init__(*args, **kwargs) io_class.__init__(*args, **kwargs) # Configs pass through kwargs_dict = { \"amp_config\": self.amp_config, \"apex_config\": self.apex_config, \"ddp_config\": self.ddp_config, \"deepspeed_config\": self.deepspeed_config, \"horovod_config\": self.horovod_config, \"oss_config\": self.oss_config, \"sharded_config\": self.sddp_config, \"fully_sharded_config\": self.fsdp_config, } # Generate the runner class from the mixins based on the StokeStatus runner_class = type( \"StokeRunner\", (dist_class, fp16_class, optimizer_class, io_class), {\"__init__\": __multiple_mixin_init__}, )( verbose=self._verbose, batch_size_per_device=self.batch_size, grad_accum_steps=self.grad_accum, grad_clip=self.grad_clip, info_rank=self._info_rank, loss=self._loss, **kwargs_dict, ) # Make a list of class info for print later class_info = [ f\"Distributed Mixin: {dist_class.__name__}\", f\"Optimizer Mixin: {dist_class.__name__}\", f\"FP16 Mixin: {fp16_class.__name__}\", f\"IO Mixin: {io_class.__name__}\", ] return runner_class, class_info def _get_io_mixin(self): \"\"\"Determines which IO class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated ioclass \"\"\" if self.is_deepspeed: return_class = RunnerIOEnum.deepspeed.value elif self.is_horovod: return_class = RunnerIOEnum.horovod.value elif self.is_ddp: return_class = RunnerIOEnum.ddp.value else: return_class = RunnerIOEnum.base.value return return_class def _get_optimizer_mixin(self): \"\"\"Determines which optimizer class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated optimizer class \"\"\" if self.oss: return_class = RunnerOptimizerEnum.oss.value else: return_class = RunnerOptimizerEnum.base.value return return_class def _get_distributed_mixin(self): \"\"\"Determines which distributed class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated distributed class \"\"\" # if not gpu then fall to cpu single if not self.gpu: return_class = RunnerDistEnum.cpu.value # if gpu but no distributed then fall to single gpu elif self.gpu and (self.distributed is None): return_class = RunnerDistEnum.gpu.value elif self.gpu and (self.distributed is not None): return_class = RunnerDistEnum[self.distributed].value else: raise ValueError(\"Stoke -- Cannot map to a valid distributed class\") return return_class def _get_fp16_mixin(self): \"\"\"Determines which fp16 class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated fp16 class \"\"\" if self.fp16 is not None: return_class = RunnerFP16Enum[self.fp16].value else: return_class = RunnerFP16Enum.full.value return return_class def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) def model(self, *args, **kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs) def loss(self, *args, **kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]): \"\"\"Handles calculating the ema loss Parameters ---------- loss: Union[float, List[float], Tuple[float]] current calculated loss list, tuple or float Returns ------- None \"\"\" self._rolling_loss_steps += 1 if isinstance(loss, (list, tuple)): self._rolling_mean_loss = type(self._rolling_mean_loss)( self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx]) for idx, val in enumerate(loss) ) else: self._rolling_mean_loss = self._ema_loss( value=loss, current_mean=self._rolling_mean_loss ) def _ema_loss(self, value: float, current_mean: float): \"\"\"Calculate the ema of the loss Parameters ---------- value: float current loss value current_mean: float current mean value Returns ------- current ema value: float \"\"\" if self._rolling_loss_steps == 1: return value else: return (self._ema_weight * value) + ( (1.0 - self._ema_weight) * current_mean ) def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1 def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) def _reset(self): \"\"\"Resets the state post optimizer step call Returns ------- None \"\"\" if self._verbose: self.print(\"Resetting all grad/variables for next optimizer step\") # Zero the grads if not deepspeed if not self.is_deepspeed: self.zero_grads() # Reset counter self._grad_accum_counter = 0 # Reset agg loss -- single or mutiple losses self._agg_loss = self._set_loss_to_zero() def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" ) def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device) def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod ) def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset() def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" ) def _load_fp16_state_dict_fn(self): \"\"\"Returns the function to load the sacler state dict Returns ------- mp_state_dict_fn: Callable, default: None callable function to load the scaler state dict \"\"\" mp_state_dict_fn = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict_fn = amp.load_state_dict except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) else: mp_state_dict_fn = self.scaler.load_state_dict return mp_state_dict_fn def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier() @property def step_loss(self): \"\"\"Gets the last step loss synced across device(s) (unscaled)\"\"\" return self._last_step_loss @property def model_access(self): \"\"\"Interface for model access due to the different types between the DP, DDP, and SDDP implementations\"\"\" if isinstance(self._model, (DDP, DP, SDDP, FSDP)): return self._model.module else: return self._model @property def loss_access(self): \"\"\"Gets loss tensor(s)\"\"\" return self._loss @property def optimizer(self): \"\"\"Gets the optimizer\"\"\" return self._optimizer @property def scaler(self): \"\"\"Gets the current scaler object\"\"\" return self._runner.scaler @property def fp16_state_dict(self): \"\"\"Gets the fp16 state dict from various methods\"\"\" mp_state_dict = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict = amp.state_dict() except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) elif self.is_amp: mp_state_dict = self.scaler.state_dict() return mp_state_dict @property def status(self): \"\"\"Gets the StokeStatus object\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.batch_size @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self._status.effective_batch_size @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.grad_clip @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.grad_accum @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.gpu @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.cuda @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.nccl @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.fp16 @property def is_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self._status.is_fp16_apex @property def is_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self._status.is_fp16_amp @property def distributed(self): \"\"\"Shortcut to distributed status\"\"\" return self._status.distributed @property def is_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self._status.is_distributed_ddp @property def is_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self._status.is_distributed_horovod @property def is_deepspeed(self): \"\"\"Returns if Deepspeed is acticated\"\"\" return self._status.is_distributed_deepspeed @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.oss @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.sharded @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.fully_sharded @property def world_size(self): \"\"\"Shortcut to get world size\"\"\" return self._runner.world_size @property def rank(self): \"\"\"Shortcut to get rank\"\"\" return self._runner.rank @property def amp_config(self): \"\"\"Returns amp config or None based on amp state\"\"\" return self._status.amp_config if self.is_amp else None @property def apex_config(self): \"\"\"Returns apex config or None based on apex state\"\"\" return self._status.apex_config if self.is_apex else None @property def ddp_config(self): \"\"\"Returns ddp config or None based on ddp state\"\"\" return self._status.ddp_config if self.is_ddp else None @property def deepspeed_config(self): \"\"\"Returns deepspeed config or None based on deepspeed state\"\"\" return self._status.deepspeed_config if self.is_deepspeed else None @property def oss_config(self): \"\"\"Returns oss config or None based on ossstate\"\"\" return self._status.oss_config if self.oss else None @property def sddp_config(self): \"\"\"Returns sddp config or None based on sddp state\"\"\" return self._status.sddp_config if self.sharded else None @property def fsdp_config(self): \"\"\"Returns fsdp config or None based on fsdp state\"\"\" return self._status.fsdp_config if self.fully_sharded else None @property def horovod_config(self): \"\"\"Returns horovod config or None based on horovod state\"\"\" return self._status.horovod_config if self.is_horovod else None @property def num_model_parameters(self): \"\"\"Returns number of parameters that require gradients\"\"\" return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad) @property def ema_loss(self): \"\"\"Returns the current rolling mean loss\"\"\" return self._rolling_mean_loss Classes Stoke class Stoke ( model : torch . nn . modules . module . Module , optimizer : stoke . configs . StokeOptimizer , loss : Union [ Callable , List [ Callable ], Tuple [ Callable ]], batch_size_per_device : int , grad_accum_steps : Union [ int , NoneType ] = 1 , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig , NoneType ] = None , gpu : bool = False , fp16 : Union [ stoke . status . FP16Options , NoneType ] = None , distributed : Union [ stoke . status . DistributedOptions , NoneType ] = None , fairscale_oss : bool = False , fairscale_sddp : bool = False , fairscale_fsdp : bool = False , configs : Union [ List [ Union [ stoke . configs . AMPConfig , stoke . configs . ApexConfig , stoke . configs . DDPConfig , stoke . configs . DeepspeedConfig , stoke . configs . FairscaleOSSConfig , stoke . configs . FairscaleSDDPConfig , stoke . configs . FairscaleFSDPConfig , stoke . configs . HorovodConfig ]], NoneType ] = None , info_rank : Union [ int , List [ int ], NoneType ] = 0 , verbose : bool = True , ema_weight : float = 0.1 ) Attributes Name Type Description Default amp_config None None None apex_config None None None batch_size None None None cuda None None None ddp_config None None None deepspeed_config None None None distributed None None None effective_batch_size None None None ema_loss None None None fp16 None None None fsdp_config None None None fully_sharded None None None gpu None None None grad_accum None None None grad_clip None None None horovod_config None None None is_amp None None None is_apex None None None is_ddp None None None is_deepspeed None None None is_horovod None None None loss_access None None None model_access None None None nccl None None None num_model_parameters None None None optimizer None None None oss None None None oss_config None None None rank None None None scaler None None None sddp_config None None None sharded None None None status None None None world_size None None None _agg_loss Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) None _backward_steps int Number of times gradients have been calculated on a batch of samples (calls to backward) None _grad_accum_counter int counter for grad accumulation steps None _loss Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs None _last_step_loss list, tuple, or float last loss step calculation aggregated over device(s) None _model torch.nn.Module instance of torch.nn.Module for Stoke to handle None _optimizer StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs None _optimizer_steps int Number of times step has been called on the optimizer None _runner StokeRunner the dynamically created runtime object that handles all ops None _status StokeStatus StokeStatus object that sets and maintains the current configuration None _verbose bool print verbosity None _rolling_loss_steps int number of steps that have been called for the rolling loss None _rolling_mean_loss list, tuple, or float current ema loss None _ema_weight float weight used for any ema calculation on metrics None ??? example \"View Source\" class Stoke: \"\"\"High level stoke object that manages all necessary configs and provides a unified interface to ops This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model, loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale), mixed-precision (AMP or APEX) and devices (CPU or GPU) Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size ema_loss fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_amp is_apex is_ddp is_deepspeed is_horovod loss_access model_access nccl num_model_parameters optimizer oss oss_config rank scaler sddp_config sharded status world_size _agg_loss: Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) _backward_steps: int Number of times gradients have been calculated on a batch of samples (calls to backward) _grad_accum_counter: int counter for grad accumulation steps _loss: Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs _last_step_loss: list, tuple, or float last loss step calculation aggregated over device(s) _model: torch.nn.Module instance of torch.nn.Module for Stoke to handle _optimizer: StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs _optimizer_steps: int Number of times step has been called on the optimizer _runner: StokeRunner the dynamically created runtime object that handles all ops _status: StokeStatus StokeStatus object that sets and maintains the current configuration _verbose: bool print verbosity _rolling_loss_steps: int number of steps that have been called for the rolling loss _rolling_mean_loss: list, tuple, or float current ema loss _ema_weight: float weight used for any ema calculation on metrics \"\"\" def __init__( self, model: torch.nn.Module, optimizer: StokeOptimizer, loss: Union[Callable, List[Callable], Tuple[Callable]], batch_size_per_device: int, grad_accum_steps: Optional[int] = 1, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None, gpu: bool = False, fp16: Optional[FP16Options] = None, distributed: Optional[DistributedOptions] = None, fairscale_oss: bool = False, fairscale_sddp: bool = False, fairscale_fsdp: bool = False, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ] = None, info_rank: Optional[Union[int, List[int]]] = 0, verbose: bool = True, ema_weight: float = 0.1, ): \"\"\"Init for Stoke class object Parameters ---------- model: torch.nn.Module PyTorch model optimizer: StokeOptimizer Optimizer configuration loss: Union[Callable, List[Callable], Tuple[Callable]] Callable loss function or functions batch_size_per_device: int Batch size at the single device level grad_accum_steps: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None Configuration objects for runtimes info_rank: Optional[Union[int, List[int]]], default = 0 Constrain prints to specific devices verbose: bool, default: True Flag for verbosity ema_weight: float, default: 0.5 weight used for any ema calculation on metrics \"\"\" # Verbosity self._verbose = verbose # Info rank self._info_rank = info_rank # EMA self._ema_weight = ema_weight # Setup the StokeState self._status = StokeStatus( batch_size_per_device=batch_size_per_device, grad_accum=grad_accum_steps, grad_clip=grad_clip, gpu=gpu, fp16=fp16, distributed=distributed, fairscale_oss=fairscale_oss, fairscale_sddp=fairscale_sddp, fairscale_fsdp=fairscale_fsdp, configs=configs, ) # Run some checks self._model = self._check_model(model) self._optimizer = self._check_optimizer(optimizer) self._loss = self._check_loss(loss) # Dynamically construct the StokeRunner from the StokeStatus self._runner, class_info = self._build_runner() # Setup distributed backend self._runner.setup_distributed() # Post here the runner will have the print_device function that is mapped to the self.print here # as it needs rank to be accessible before working if self._verbose: dev_id = ( self.rank if (self.rank == \"cpu\" or self.rank == \"gpu\") else self._info_rank ) self.print(f\"Printing verbose information on rank(s): {dev_id}\") # Print the runner class info from the mixins self.print(class_info) # Possibly place model on GPU depending on StokeStatus -- before wrap calls self._place_model_on_gpu() # Handle the wrap ops in the correct order self._handle_ordered_wrap_ops(optimizer=optimizer) # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 # Set post-init status variables self._status.set_post_init_values(world_size=self.world_size) # Print the final configuration if self._verbose: self.print(msg=self._status) def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of optimizer then the model This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped methods are called Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed through self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer) # Wrap with distributed backend -- in this case the optimizer is passed through self._model, self._optimizer = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer ) def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of model then optimizer Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn't exist yet # don't use the return for the optimizer in this case self._model, _ = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=None ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn't exist yet self._runner.wrap_fp16(model=self._model, optimizer=None) # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed) if (self.sharded and self.oss) or self.is_apex or self.is_horovod: self._wrap_optimizer_then_model(optimizer=optimizer) else: self._wrap_model_then_optimizer(optimizer=optimizer) def _check_accum(self): \"\"\"Checks if the current step is the last accumulation step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0 def _check_pre_accum(self): \"\"\"Checks if we are at the pre-accumulate step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum def _set_loss_to_zero(self): \"\"\"Used to set a loss tracker to zero depending on the type Returns ------- float or list or tuple of reset loss \"\"\" return ( type(self._loss)([0.0] * len(self._loss)) if isinstance(self._loss, (list, tuple)) else 0.0 ) def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\") def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\") def _scale_agg_loss(self): \"\"\"Scales the mean aggregated loss by grad accum Returns ------- scale_vals: list or float of mean aggregated loss \"\"\" if isinstance(self._agg_loss, (list, tuple)): scale_vals = [ val / self.grad_accum for idx, val in enumerate(self._agg_loss) ] else: scale_vals = self._agg_loss / self.grad_accum return scale_vals def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\") def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank) def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line ) @staticmethod def _check_model(model: torch.nn.Module): \"\"\"Verifies the type of the model Parameters ---------- model: torch.nn.Module current torch model Returns ------- None \"\"\" # Check if the model is an nn.Module such that it has a forward method if not isinstance(model, torch.nn.Module): raise TypeError( f\"Stoke -- Model is not of type torch.nn.Module, currently {type(model)}\" ) return model @staticmethod def _check_optimizer(optimizer: StokeOptimizer): \"\"\"Verifies the type of the optimizer Parameters ---------- optimizer: StokeOptimizer Current optimizer configuration TypedDict (aka dict) Returns ------- None \"\"\" if not isinstance(optimizer, dict): raise TypeError( f\"Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}\" ) return optimizer def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]): \"\"\"Checks to make sure the loss function(s) is/are callable Parameters ---------- loss: Union[Callable, List[Callable], Tuple[Callable]] Current callable loss(es) Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): loss = [self._check_loss(val) for val in loss] return loss elif isinstance(loss, Callable): return loss else: raise TypeError( f\"Stoke -- Loss is not of type Callable, currently {type(loss)}\" ) def _place_model_on_gpu(self): \"\"\"Automatically moves the model to GPU device(s) Returns ------- None \"\"\" if self.gpu and not self.is_deepspeed: if self._verbose: self.print(f\"Automatically handling moving model to GPU(s)...\") self._model.cuda() def _build_runner(self): \"\"\"Builds the runtime object from the mixin style classes Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a a single interface. Might prevent some IDE type-hinting as it's dynamic Returns ------- StokeRunner runtime runner object \"\"\" # Get the classes dist_class = self._get_distributed_mixin() fp16_class = self._get_fp16_mixin() optimizer_class = self._get_optimizer_mixin() io_class = self._get_io_mixin() # Python MRO hack to make sure the inits of all the Mixin classes get called def __multiple_mixin_init__(*args, **kwargs): dist_class.__init__(*args, **kwargs) fp16_class.__init__(*args, **kwargs) optimizer_class.__init__(*args, **kwargs) io_class.__init__(*args, **kwargs) # Configs pass through kwargs_dict = { \"amp_config\": self.amp_config, \"apex_config\": self.apex_config, \"ddp_config\": self.ddp_config, \"deepspeed_config\": self.deepspeed_config, \"horovod_config\": self.horovod_config, \"oss_config\": self.oss_config, \"sharded_config\": self.sddp_config, \"fully_sharded_config\": self.fsdp_config, } # Generate the runner class from the mixins based on the StokeStatus runner_class = type( \"StokeRunner\", (dist_class, fp16_class, optimizer_class, io_class), {\"__init__\": __multiple_mixin_init__}, )( verbose=self._verbose, batch_size_per_device=self.batch_size, grad_accum_steps=self.grad_accum, grad_clip=self.grad_clip, info_rank=self._info_rank, loss=self._loss, **kwargs_dict, ) # Make a list of class info for print later class_info = [ f\"Distributed Mixin: {dist_class.__name__}\", f\"Optimizer Mixin: {dist_class.__name__}\", f\"FP16 Mixin: {fp16_class.__name__}\", f\"IO Mixin: {io_class.__name__}\", ] return runner_class, class_info def _get_io_mixin(self): \"\"\"Determines which IO class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated ioclass \"\"\" if self.is_deepspeed: return_class = RunnerIOEnum.deepspeed.value elif self.is_horovod: return_class = RunnerIOEnum.horovod.value elif self.is_ddp: return_class = RunnerIOEnum.ddp.value else: return_class = RunnerIOEnum.base.value return return_class def _get_optimizer_mixin(self): \"\"\"Determines which optimizer class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated optimizer class \"\"\" if self.oss: return_class = RunnerOptimizerEnum.oss.value else: return_class = RunnerOptimizerEnum.base.value return return_class def _get_distributed_mixin(self): \"\"\"Determines which distributed class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated distributed class \"\"\" # if not gpu then fall to cpu single if not self.gpu: return_class = RunnerDistEnum.cpu.value # if gpu but no distributed then fall to single gpu elif self.gpu and (self.distributed is None): return_class = RunnerDistEnum.gpu.value elif self.gpu and (self.distributed is not None): return_class = RunnerDistEnum[self.distributed].value else: raise ValueError(\"Stoke -- Cannot map to a valid distributed class\") return return_class def _get_fp16_mixin(self): \"\"\"Determines which fp16 class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated fp16 class \"\"\" if self.fp16 is not None: return_class = RunnerFP16Enum[self.fp16].value else: return_class = RunnerFP16Enum.full.value return return_class def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) def model(self, *args, **kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs) def loss(self, *args, **kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]): \"\"\"Handles calculating the ema loss Parameters ---------- loss: Union[float, List[float], Tuple[float]] current calculated loss list, tuple or float Returns ------- None \"\"\" self._rolling_loss_steps += 1 if isinstance(loss, (list, tuple)): self._rolling_mean_loss = type(self._rolling_mean_loss)( self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx]) for idx, val in enumerate(loss) ) else: self._rolling_mean_loss = self._ema_loss( value=loss, current_mean=self._rolling_mean_loss ) def _ema_loss(self, value: float, current_mean: float): \"\"\"Calculate the ema of the loss Parameters ---------- value: float current loss value current_mean: float current mean value Returns ------- current ema value: float \"\"\" if self._rolling_loss_steps == 1: return value else: return (self._ema_weight * value) + ( (1.0 - self._ema_weight) * current_mean ) def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1 def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) def _reset(self): \"\"\"Resets the state post optimizer step call Returns ------- None \"\"\" if self._verbose: self.print(\"Resetting all grad/variables for next optimizer step\") # Zero the grads if not deepspeed if not self.is_deepspeed: self.zero_grads() # Reset counter self._grad_accum_counter = 0 # Reset agg loss -- single or mutiple losses self._agg_loss = self._set_loss_to_zero() def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" ) def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device) def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod ) def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset() def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" ) def _load_fp16_state_dict_fn(self): \"\"\"Returns the function to load the sacler state dict Returns ------- mp_state_dict_fn: Callable, default: None callable function to load the scaler state dict \"\"\" mp_state_dict_fn = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict_fn = amp.load_state_dict except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) else: mp_state_dict_fn = self.scaler.load_state_dict return mp_state_dict_fn def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier() @property def step_loss(self): \"\"\"Gets the last step loss synced across device(s) (unscaled)\"\"\" return self._last_step_loss @property def model_access(self): \"\"\"Interface for model access due to the different types between the DP, DDP, and SDDP implementations\"\"\" if isinstance(self._model, (DDP, DP, SDDP, FSDP)): return self._model.module else: return self._model @property def loss_access(self): \"\"\"Gets loss tensor(s)\"\"\" return self._loss @property def optimizer(self): \"\"\"Gets the optimizer\"\"\" return self._optimizer @property def scaler(self): \"\"\"Gets the current scaler object\"\"\" return self._runner.scaler @property def fp16_state_dict(self): \"\"\"Gets the fp16 state dict from various methods\"\"\" mp_state_dict = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict = amp.state_dict() except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) elif self.is_amp: mp_state_dict = self.scaler.state_dict() return mp_state_dict @property def status(self): \"\"\"Gets the StokeStatus object\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.batch_size @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self._status.effective_batch_size @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.grad_clip @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.grad_accum @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.gpu @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.cuda @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.nccl @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.fp16 @property def is_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self._status.is_fp16_apex @property def is_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self._status.is_fp16_amp @property def distributed(self): \"\"\"Shortcut to distributed status\"\"\" return self._status.distributed @property def is_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self._status.is_distributed_ddp @property def is_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self._status.is_distributed_horovod @property def is_deepspeed(self): \"\"\"Returns if Deepspeed is acticated\"\"\" return self._status.is_distributed_deepspeed @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.oss @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.sharded @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.fully_sharded @property def world_size(self): \"\"\"Shortcut to get world size\"\"\" return self._runner.world_size @property def rank(self): \"\"\"Shortcut to get rank\"\"\" return self._runner.rank @property def amp_config(self): \"\"\"Returns amp config or None based on amp state\"\"\" return self._status.amp_config if self.is_amp else None @property def apex_config(self): \"\"\"Returns apex config or None based on apex state\"\"\" return self._status.apex_config if self.is_apex else None @property def ddp_config(self): \"\"\"Returns ddp config or None based on ddp state\"\"\" return self._status.ddp_config if self.is_ddp else None @property def deepspeed_config(self): \"\"\"Returns deepspeed config or None based on deepspeed state\"\"\" return self._status.deepspeed_config if self.is_deepspeed else None @property def oss_config(self): \"\"\"Returns oss config or None based on ossstate\"\"\" return self._status.oss_config if self.oss else None @property def sddp_config(self): \"\"\"Returns sddp config or None based on sddp state\"\"\" return self._status.sddp_config if self.sharded else None @property def fsdp_config(self): \"\"\"Returns fsdp config or None based on fsdp state\"\"\" return self._status.fsdp_config if self.fully_sharded else None @property def horovod_config(self): \"\"\"Returns horovod config or None based on horovod state\"\"\" return self._status.horovod_config if self.is_horovod else None @property def num_model_parameters(self): \"\"\"Returns number of parameters that require gradients\"\"\" return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad) @property def ema_loss(self): \"\"\"Returns the current rolling mean loss\"\"\" return self._rolling_mean_loss Instance variables amp_config Returns amp config or None based on amp state apex_config Returns apex config or None based on apex state batch_size Shortcut to batch size cuda Shortcut to get cuda status ddp_config Returns ddp config or None based on ddp state deepspeed_config Returns deepspeed config or None based on deepspeed state distributed Shortcut to distributed status effective_batch_size Shortcut to effective batch size ema_loss Returns the current rolling mean loss fp16 Shortcut to get FP16 status fp16_state_dict Gets the fp16 state dict from various methods fsdp_config Returns fsdp config or None based on fsdp state fully_sharded Returns if Fairscale fully sharded DDP status gpu Shortcut to get GPU status grad_accum Shortcut to get grad accumulation grad_clip Shortcut to get grad clip horovod_config Returns horovod config or None based on horovod state is_amp Returns if AMP is activated is_apex Returns if APEX is activated is_ddp Returns if DDP is activated is_deepspeed Returns if Deepspeed is acticated is_horovod Returns if Horovod is activated loss_access Gets loss tensor(s) model_access Interface for model access due to the different types between the DP, DDP, and SDDP implementations nccl Shortcut to get nccl status num_model_parameters Returns number of parameters that require gradients optimizer Gets the optimizer oss Returns if Fairscale optimizer state sharding status oss_config Returns oss config or None based on ossstate rank Shortcut to get rank scaler Gets the current scaler object sddp_config Returns sddp config or None based on sddp state sharded Returns if Fairscale sharded DDP status status Gets the StokeStatus object step_loss Gets the last step loss synced across device(s) (unscaled) world_size Shortcut to get world size Methods DataLoader def DataLoader ( self , dataset : torch . utils . data . dataset . Dataset [ + T_co ], shuffle : bool = False , sampler : Union [ torch . utils . data . sampler . Sampler [ int ], NoneType ] = None , batch_sampler : Union [ torch . utils . data . sampler . Sampler [ Sequence [ int ]], NoneType ] = None , num_workers : int = 0 , collate_fn : Callable [[ List [ ~ T ]], Any ] = None , pin_memory : bool = False , drop_last : bool = False , timeout : float = 0 , worker_init_fn : Callable [[ int ], NoneType ] = None , multiprocessing_context = None , generator = None , * , prefetch_factor : int = 2 , persistent_workers : bool = False ) Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters: Name Type Description Default dataset Dataset dataset from which to load the data. None shuffle bool, default: False set to True to have the data reshuffled at every epoch. None sampler Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, :attr: shuffle must not be specified. None batch_sampler Sampler or Iterable, default: None: like :attr: sampler , but returns a batch of indices at a time. Mutually exclusive with :attr: batch_size , :attr: shuffle , :attr: sampler , and :attr: drop_last . None num_workers int, default: 0 how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. None collate_fn callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. None pin_memory bool, default: False: If True , the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr: collate_fn returns a batch that is a custom type, see the example below. None drop_last bool, default: False set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. None timeout numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. None worker_init_fn callable, default: None If not None , this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1] ) as input, after seeding and before data loading. None prefetch_factor int, default: 2 Number of samples loaded in advance by each worker. 2 means there will be a total of 2 * num_workers samples prefetched across all workers. None persistent_workers bool, default: False If True , the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. None Returns: Type Description StokeDataLoader wrapped torch.utils.data.DataLoader object ??? example \"View Source\" def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) backward def backward ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]] ) Wrapped backwards call Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) None Returns: Type Description None None ??? example \"View Source\" def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1 barrier def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier() detach_and_sync_loss def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) None device default: None device to sync across None Returns: Type Description loss that is synced across devices and all_reduced w/ SUM None ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device) dump_model_parameter_info def dump_model_parameter_info ( self ) Dumps all parameter information for named parameters (shape, device, dtype) Returns: Type Description None None ??? example \"View Source\" def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" ) load def load ( self , path : str , tag : str , strict : bool = True ) Loads a model checkpoint using the correct backend interface Parameters: Name Type Description Default path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None strict bool ignore non-matching keys None Returns: Type Description dict, default: None a dictionary of any custom fields the user passed to the save function ??? example \"View Source\" def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras loss def loss ( self , * args , ** kwargs ) Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters: Name Type Description Default *args list or tuple Additional arguments should be passed as keyword arguments None **kwargs dict Extra arguments passed to the loss function call(s) None Returns: Type Description outputs of callable loss function(s) None ??? example \"View Source\" def loss(self, args, *kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss model def model ( self , * args , ** kwargs ) Wrapped model forward call Parameters: Name Type Description Default *args list or tuple Additional arguments should be passed as keyword arguments None **kwargs dict Extra arguments passed to the model forward call None Returns: Type Description model forward output None ??? example \"View Source\" def model(self, args, *kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs) print def print ( self , msg : Union [ str , List [ str ]], single_line : bool = False ) Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters: Name Type Description Default msg str message to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line ) print_ema_loss def print_ema_loss ( self , prepend_msg : str = 'Current EMA Loss' , single_line : bool = False ) Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default prepend_msg str, default: \"Current EMA Loss\" message prepend to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\") print_mean_accumulated_synced_loss def print_mean_accumulated_synced_loss ( self , prepend_msg : str = 'Mean Accumulated & Synced Loss' , pre_backwards : bool = True , single_line : bool = False ) Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default prepend_msg str, default: \"Mean Accumulated & Synced Loss\" message prepend to print None pre_backwards bool, default: True if being called pre backward step None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\") print_num_model_parameters def print_num_model_parameters ( self , normalize : stoke . utils . ParamNormalize = < ParamNormalize . MILLION : 1000000.0 > ) Parameters: Name Type Description Default normalize ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing None Returns: Type Description None None ??? example \"View Source\" def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" ) print_on_devices def print_on_devices ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 ) Wraps runner print interface for shorter semantics Parameters: Name Type Description Default msg str message to print None rank Union[int, List[int]], default: 0 which ranks to print on None Returns: Type Description None None ??? example \"View Source\" def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank) print_synced_loss def print_synced_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], prepend_msg : str = 'Step Synced Loss' , device = None , single_line : bool = False ) Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None prepend_msg str, default: \"Step Synced Loss\" message prepend to print None device default: None specify the device to place the synced loss on (defaults to same device) same single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\") reset def reset ( self ) Public method for resetting the underlying stoke state Returns: Type Description None None ??? example \"View Source\" def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset() reset_ema def reset_ema ( self ) Used to reset the current state of the rolling mean loss Returns: Type Description None None ??? example \"View Source\" def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 reset_tracking def reset_tracking ( self ) Public method for resetting all underlying stoke tracked variables Returns: Type Description None None ??? example \"View Source\" def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 save def save ( self , path : str , name : str = UUID ( '1bec68f4-7df7-48d2-a526-14685e92f54f' ), extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Saves a model checkpoint using the correct backend interface Parameters: Name Type Description Default path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None name str, default: uuid4() name used to save checkpoint file None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag step def step ( self ) Wrapped step call Handles grad clipping internally Returns: Type Description None None ??? example \"View Source\" def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) zero_grads def zero_grads ( self ) Zeros the optimizer grads depending on the optimizer type Returns: Type Description None None ??? example \"View Source\" def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod )","title":"Stoke"},{"location":"reference/stoke/stoke/#module-stokestoke","text":"API interface to Stoke that handles any necessary config, context, setup etc. None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"API interface to Stoke that handles any necessary config, context, setup etc.\"\"\" from contextlib import nullcontext from typing import Callable, Dict, List, Optional, Sequence, Tuple, Type, Union from uuid import uuid4 import torch from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP from fairscale.nn.data_parallel import ShardedDataParallel as SDDP from torch.nn.parallel import DataParallel as DP from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import Dataset from torch.utils.data.distributed import Sampler from stoke.configs import ( AMPConfig, ApexConfig, ClipGradConfig, ClipGradNormConfig, DDPConfig, DeepspeedConfig, FairscaleFSDPConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig, StokeOptimizer, ) from stoke.data import StokeDataLoader from stoke.distributed import RunnerDistEnum from stoke.extensions import RunnerOptimizerEnum from stoke.fp16 import RunnerFP16Enum from stoke.io import RunnerIOEnum from stoke.status import DistributedOptions, FP16Options, StokeStatus from stoke.utils import ( ParamNormalize, T_co, _collate_fn_t, _worker_init_fn_t, zero_optimizer_grads, ) class Stoke: \"\"\"High level stoke object that manages all necessary configs and provides a unified interface to ops This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model, loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale), mixed-precision (AMP or APEX) and devices (CPU or GPU) Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size ema_loss fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_amp is_apex is_ddp is_deepspeed is_horovod loss_access model_access nccl num_model_parameters optimizer oss oss_config rank scaler sddp_config sharded status world_size _agg_loss: Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) _backward_steps: int Number of times gradients have been calculated on a batch of samples (calls to backward) _grad_accum_counter: int counter for grad accumulation steps _loss: Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs _last_step_loss: list, tuple, or float last loss step calculation aggregated over device(s) _model: torch.nn.Module instance of torch.nn.Module for Stoke to handle _optimizer: StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs _optimizer_steps: int Number of times step has been called on the optimizer _runner: StokeRunner the dynamically created runtime object that handles all ops _status: StokeStatus StokeStatus object that sets and maintains the current configuration _verbose: bool print verbosity _rolling_loss_steps: int number of steps that have been called for the rolling loss _rolling_mean_loss: list, tuple, or float current ema loss _ema_weight: float weight used for any ema calculation on metrics \"\"\" def __init__( self, model: torch.nn.Module, optimizer: StokeOptimizer, loss: Union[Callable, List[Callable], Tuple[Callable]], batch_size_per_device: int, grad_accum_steps: Optional[int] = 1, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None, gpu: bool = False, fp16: Optional[FP16Options] = None, distributed: Optional[DistributedOptions] = None, fairscale_oss: bool = False, fairscale_sddp: bool = False, fairscale_fsdp: bool = False, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ] = None, info_rank: Optional[Union[int, List[int]]] = 0, verbose: bool = True, ema_weight: float = 0.1, ): \"\"\"Init for Stoke class object Parameters ---------- model: torch.nn.Module PyTorch model optimizer: StokeOptimizer Optimizer configuration loss: Union[Callable, List[Callable], Tuple[Callable]] Callable loss function or functions batch_size_per_device: int Batch size at the single device level grad_accum_steps: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None Configuration objects for runtimes info_rank: Optional[Union[int, List[int]]], default = 0 Constrain prints to specific devices verbose: bool, default: True Flag for verbosity ema_weight: float, default: 0.5 weight used for any ema calculation on metrics \"\"\" # Verbosity self._verbose = verbose # Info rank self._info_rank = info_rank # EMA self._ema_weight = ema_weight # Setup the StokeState self._status = StokeStatus( batch_size_per_device=batch_size_per_device, grad_accum=grad_accum_steps, grad_clip=grad_clip, gpu=gpu, fp16=fp16, distributed=distributed, fairscale_oss=fairscale_oss, fairscale_sddp=fairscale_sddp, fairscale_fsdp=fairscale_fsdp, configs=configs, ) # Run some checks self._model = self._check_model(model) self._optimizer = self._check_optimizer(optimizer) self._loss = self._check_loss(loss) # Dynamically construct the StokeRunner from the StokeStatus self._runner, class_info = self._build_runner() # Setup distributed backend self._runner.setup_distributed() # Post here the runner will have the print_device function that is mapped to the self.print here # as it needs rank to be accessible before working if self._verbose: dev_id = ( self.rank if (self.rank == \"cpu\" or self.rank == \"gpu\") else self._info_rank ) self.print(f\"Printing verbose information on rank(s): {dev_id}\") # Print the runner class info from the mixins self.print(class_info) # Possibly place model on GPU depending on StokeStatus -- before wrap calls self._place_model_on_gpu() # Handle the wrap ops in the correct order self._handle_ordered_wrap_ops(optimizer=optimizer) # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 # Set post-init status variables self._status.set_post_init_values(world_size=self.world_size) # Print the final configuration if self._verbose: self.print(msg=self._status) def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of optimizer then the model This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped methods are called Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed through self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer) # Wrap with distributed backend -- in this case the optimizer is passed through self._model, self._optimizer = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer ) def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of model then optimizer Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn't exist yet # don't use the return for the optimizer in this case self._model, _ = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=None ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn't exist yet self._runner.wrap_fp16(model=self._model, optimizer=None) # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed) if (self.sharded and self.oss) or self.is_apex or self.is_horovod: self._wrap_optimizer_then_model(optimizer=optimizer) else: self._wrap_model_then_optimizer(optimizer=optimizer) def _check_accum(self): \"\"\"Checks if the current step is the last accumulation step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0 def _check_pre_accum(self): \"\"\"Checks if we are at the pre-accumulate step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum def _set_loss_to_zero(self): \"\"\"Used to set a loss tracker to zero depending on the type Returns ------- float or list or tuple of reset loss \"\"\" return ( type(self._loss)([0.0] * len(self._loss)) if isinstance(self._loss, (list, tuple)) else 0.0 ) def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\") def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\") def _scale_agg_loss(self): \"\"\"Scales the mean aggregated loss by grad accum Returns ------- scale_vals: list or float of mean aggregated loss \"\"\" if isinstance(self._agg_loss, (list, tuple)): scale_vals = [ val / self.grad_accum for idx, val in enumerate(self._agg_loss) ] else: scale_vals = self._agg_loss / self.grad_accum return scale_vals def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\") def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank) def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line ) @staticmethod def _check_model(model: torch.nn.Module): \"\"\"Verifies the type of the model Parameters ---------- model: torch.nn.Module current torch model Returns ------- None \"\"\" # Check if the model is an nn.Module such that it has a forward method if not isinstance(model, torch.nn.Module): raise TypeError( f\"Stoke -- Model is not of type torch.nn.Module, currently {type(model)}\" ) return model @staticmethod def _check_optimizer(optimizer: StokeOptimizer): \"\"\"Verifies the type of the optimizer Parameters ---------- optimizer: StokeOptimizer Current optimizer configuration TypedDict (aka dict) Returns ------- None \"\"\" if not isinstance(optimizer, dict): raise TypeError( f\"Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}\" ) return optimizer def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]): \"\"\"Checks to make sure the loss function(s) is/are callable Parameters ---------- loss: Union[Callable, List[Callable], Tuple[Callable]] Current callable loss(es) Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): loss = [self._check_loss(val) for val in loss] return loss elif isinstance(loss, Callable): return loss else: raise TypeError( f\"Stoke -- Loss is not of type Callable, currently {type(loss)}\" ) def _place_model_on_gpu(self): \"\"\"Automatically moves the model to GPU device(s) Returns ------- None \"\"\" if self.gpu and not self.is_deepspeed: if self._verbose: self.print(f\"Automatically handling moving model to GPU(s)...\") self._model.cuda() def _build_runner(self): \"\"\"Builds the runtime object from the mixin style classes Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a a single interface. Might prevent some IDE type-hinting as it's dynamic Returns ------- StokeRunner runtime runner object \"\"\" # Get the classes dist_class = self._get_distributed_mixin() fp16_class = self._get_fp16_mixin() optimizer_class = self._get_optimizer_mixin() io_class = self._get_io_mixin() # Python MRO hack to make sure the inits of all the Mixin classes get called def __multiple_mixin_init__(*args, **kwargs): dist_class.__init__(*args, **kwargs) fp16_class.__init__(*args, **kwargs) optimizer_class.__init__(*args, **kwargs) io_class.__init__(*args, **kwargs) # Configs pass through kwargs_dict = { \"amp_config\": self.amp_config, \"apex_config\": self.apex_config, \"ddp_config\": self.ddp_config, \"deepspeed_config\": self.deepspeed_config, \"horovod_config\": self.horovod_config, \"oss_config\": self.oss_config, \"sharded_config\": self.sddp_config, \"fully_sharded_config\": self.fsdp_config, } # Generate the runner class from the mixins based on the StokeStatus runner_class = type( \"StokeRunner\", (dist_class, fp16_class, optimizer_class, io_class), {\"__init__\": __multiple_mixin_init__}, )( verbose=self._verbose, batch_size_per_device=self.batch_size, grad_accum_steps=self.grad_accum, grad_clip=self.grad_clip, info_rank=self._info_rank, loss=self._loss, **kwargs_dict, ) # Make a list of class info for print later class_info = [ f\"Distributed Mixin: {dist_class.__name__}\", f\"Optimizer Mixin: {dist_class.__name__}\", f\"FP16 Mixin: {fp16_class.__name__}\", f\"IO Mixin: {io_class.__name__}\", ] return runner_class, class_info def _get_io_mixin(self): \"\"\"Determines which IO class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated ioclass \"\"\" if self.is_deepspeed: return_class = RunnerIOEnum.deepspeed.value elif self.is_horovod: return_class = RunnerIOEnum.horovod.value elif self.is_ddp: return_class = RunnerIOEnum.ddp.value else: return_class = RunnerIOEnum.base.value return return_class def _get_optimizer_mixin(self): \"\"\"Determines which optimizer class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated optimizer class \"\"\" if self.oss: return_class = RunnerOptimizerEnum.oss.value else: return_class = RunnerOptimizerEnum.base.value return return_class def _get_distributed_mixin(self): \"\"\"Determines which distributed class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated distributed class \"\"\" # if not gpu then fall to cpu single if not self.gpu: return_class = RunnerDistEnum.cpu.value # if gpu but no distributed then fall to single gpu elif self.gpu and (self.distributed is None): return_class = RunnerDistEnum.gpu.value elif self.gpu and (self.distributed is not None): return_class = RunnerDistEnum[self.distributed].value else: raise ValueError(\"Stoke -- Cannot map to a valid distributed class\") return return_class def _get_fp16_mixin(self): \"\"\"Determines which fp16 class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated fp16 class \"\"\" if self.fp16 is not None: return_class = RunnerFP16Enum[self.fp16].value else: return_class = RunnerFP16Enum.full.value return return_class def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) def model(self, *args, **kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs) def loss(self, *args, **kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]): \"\"\"Handles calculating the ema loss Parameters ---------- loss: Union[float, List[float], Tuple[float]] current calculated loss list, tuple or float Returns ------- None \"\"\" self._rolling_loss_steps += 1 if isinstance(loss, (list, tuple)): self._rolling_mean_loss = type(self._rolling_mean_loss)( self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx]) for idx, val in enumerate(loss) ) else: self._rolling_mean_loss = self._ema_loss( value=loss, current_mean=self._rolling_mean_loss ) def _ema_loss(self, value: float, current_mean: float): \"\"\"Calculate the ema of the loss Parameters ---------- value: float current loss value current_mean: float current mean value Returns ------- current ema value: float \"\"\" if self._rolling_loss_steps == 1: return value else: return (self._ema_weight * value) + ( (1.0 - self._ema_weight) * current_mean ) def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1 def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) def _reset(self): \"\"\"Resets the state post optimizer step call Returns ------- None \"\"\" if self._verbose: self.print(\"Resetting all grad/variables for next optimizer step\") # Zero the grads if not deepspeed if not self.is_deepspeed: self.zero_grads() # Reset counter self._grad_accum_counter = 0 # Reset agg loss -- single or mutiple losses self._agg_loss = self._set_loss_to_zero() def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" ) def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device) def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod ) def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset() def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" ) def _load_fp16_state_dict_fn(self): \"\"\"Returns the function to load the sacler state dict Returns ------- mp_state_dict_fn: Callable, default: None callable function to load the scaler state dict \"\"\" mp_state_dict_fn = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict_fn = amp.load_state_dict except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) else: mp_state_dict_fn = self.scaler.load_state_dict return mp_state_dict_fn def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier() @property def step_loss(self): \"\"\"Gets the last step loss synced across device(s) (unscaled)\"\"\" return self._last_step_loss @property def model_access(self): \"\"\"Interface for model access due to the different types between the DP, DDP, and SDDP implementations\"\"\" if isinstance(self._model, (DDP, DP, SDDP, FSDP)): return self._model.module else: return self._model @property def loss_access(self): \"\"\"Gets loss tensor(s)\"\"\" return self._loss @property def optimizer(self): \"\"\"Gets the optimizer\"\"\" return self._optimizer @property def scaler(self): \"\"\"Gets the current scaler object\"\"\" return self._runner.scaler @property def fp16_state_dict(self): \"\"\"Gets the fp16 state dict from various methods\"\"\" mp_state_dict = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict = amp.state_dict() except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) elif self.is_amp: mp_state_dict = self.scaler.state_dict() return mp_state_dict @property def status(self): \"\"\"Gets the StokeStatus object\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.batch_size @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self._status.effective_batch_size @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.grad_clip @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.grad_accum @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.gpu @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.cuda @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.nccl @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.fp16 @property def is_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self._status.is_fp16_apex @property def is_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self._status.is_fp16_amp @property def distributed(self): \"\"\"Shortcut to distributed status\"\"\" return self._status.distributed @property def is_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self._status.is_distributed_ddp @property def is_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self._status.is_distributed_horovod @property def is_deepspeed(self): \"\"\"Returns if Deepspeed is acticated\"\"\" return self._status.is_distributed_deepspeed @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.oss @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.sharded @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.fully_sharded @property def world_size(self): \"\"\"Shortcut to get world size\"\"\" return self._runner.world_size @property def rank(self): \"\"\"Shortcut to get rank\"\"\" return self._runner.rank @property def amp_config(self): \"\"\"Returns amp config or None based on amp state\"\"\" return self._status.amp_config if self.is_amp else None @property def apex_config(self): \"\"\"Returns apex config or None based on apex state\"\"\" return self._status.apex_config if self.is_apex else None @property def ddp_config(self): \"\"\"Returns ddp config or None based on ddp state\"\"\" return self._status.ddp_config if self.is_ddp else None @property def deepspeed_config(self): \"\"\"Returns deepspeed config or None based on deepspeed state\"\"\" return self._status.deepspeed_config if self.is_deepspeed else None @property def oss_config(self): \"\"\"Returns oss config or None based on ossstate\"\"\" return self._status.oss_config if self.oss else None @property def sddp_config(self): \"\"\"Returns sddp config or None based on sddp state\"\"\" return self._status.sddp_config if self.sharded else None @property def fsdp_config(self): \"\"\"Returns fsdp config or None based on fsdp state\"\"\" return self._status.fsdp_config if self.fully_sharded else None @property def horovod_config(self): \"\"\"Returns horovod config or None based on horovod state\"\"\" return self._status.horovod_config if self.is_horovod else None @property def num_model_parameters(self): \"\"\"Returns number of parameters that require gradients\"\"\" return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad) @property def ema_loss(self): \"\"\"Returns the current rolling mean loss\"\"\" return self._rolling_mean_loss","title":"Module stoke.stoke"},{"location":"reference/stoke/stoke/#classes","text":"","title":"Classes"},{"location":"reference/stoke/stoke/#stoke","text":"class Stoke ( model : torch . nn . modules . module . Module , optimizer : stoke . configs . StokeOptimizer , loss : Union [ Callable , List [ Callable ], Tuple [ Callable ]], batch_size_per_device : int , grad_accum_steps : Union [ int , NoneType ] = 1 , grad_clip : Union [ stoke . configs . ClipGradConfig , stoke . configs . ClipGradNormConfig , NoneType ] = None , gpu : bool = False , fp16 : Union [ stoke . status . FP16Options , NoneType ] = None , distributed : Union [ stoke . status . DistributedOptions , NoneType ] = None , fairscale_oss : bool = False , fairscale_sddp : bool = False , fairscale_fsdp : bool = False , configs : Union [ List [ Union [ stoke . configs . AMPConfig , stoke . configs . ApexConfig , stoke . configs . DDPConfig , stoke . configs . DeepspeedConfig , stoke . configs . FairscaleOSSConfig , stoke . configs . FairscaleSDDPConfig , stoke . configs . FairscaleFSDPConfig , stoke . configs . HorovodConfig ]], NoneType ] = None , info_rank : Union [ int , List [ int ], NoneType ] = 0 , verbose : bool = True , ema_weight : float = 0.1 )","title":"Stoke"},{"location":"reference/stoke/stoke/#attributes","text":"Name Type Description Default amp_config None None None apex_config None None None batch_size None None None cuda None None None ddp_config None None None deepspeed_config None None None distributed None None None effective_batch_size None None None ema_loss None None None fp16 None None None fsdp_config None None None fully_sharded None None None gpu None None None grad_accum None None None grad_clip None None None horovod_config None None None is_amp None None None is_apex None None None is_ddp None None None is_deepspeed None None None is_horovod None None None loss_access None None None model_access None None None nccl None None None num_model_parameters None None None optimizer None None None oss None None None oss_config None None None rank None None None scaler None None None sddp_config None None None sharded None None None status None None None world_size None None None _agg_loss Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) None _backward_steps int Number of times gradients have been calculated on a batch of samples (calls to backward) None _grad_accum_counter int counter for grad accumulation steps None _loss Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs None _last_step_loss list, tuple, or float last loss step calculation aggregated over device(s) None _model torch.nn.Module instance of torch.nn.Module for Stoke to handle None _optimizer StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs None _optimizer_steps int Number of times step has been called on the optimizer None _runner StokeRunner the dynamically created runtime object that handles all ops None _status StokeStatus StokeStatus object that sets and maintains the current configuration None _verbose bool print verbosity None _rolling_loss_steps int number of steps that have been called for the rolling loss None _rolling_mean_loss list, tuple, or float current ema loss None _ema_weight float weight used for any ema calculation on metrics None ??? example \"View Source\" class Stoke: \"\"\"High level stoke object that manages all necessary configs and provides a unified interface to ops This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model, loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale), mixed-precision (AMP or APEX) and devices (CPU or GPU) Attributes ---------- amp_config apex_config batch_size cuda ddp_config deepspeed_config distributed effective_batch_size ema_loss fp16 fsdp_config fully_sharded gpu grad_accum grad_clip horovod_config is_amp is_apex is_ddp is_deepspeed is_horovod loss_access model_access nccl num_model_parameters optimizer oss oss_config rank scaler sddp_config sharded status world_size _agg_loss: Union[float, List[float], Tuple[float]] aggregated loss for grad accumulation (single or multiple losses) _backward_steps: int Number of times gradients have been calculated on a batch of samples (calls to backward) _grad_accum_counter: int counter for grad accumulation steps _loss: Union[Callable, List[Callable], Tuple[Callable]] callable function that calculates a loss from the model outputs _last_step_loss: list, tuple, or float last loss step calculation aggregated over device(s) _model: torch.nn.Module instance of torch.nn.Module for Stoke to handle _optimizer: StokeOptimizer StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs _optimizer_steps: int Number of times step has been called on the optimizer _runner: StokeRunner the dynamically created runtime object that handles all ops _status: StokeStatus StokeStatus object that sets and maintains the current configuration _verbose: bool print verbosity _rolling_loss_steps: int number of steps that have been called for the rolling loss _rolling_mean_loss: list, tuple, or float current ema loss _ema_weight: float weight used for any ema calculation on metrics \"\"\" def __init__( self, model: torch.nn.Module, optimizer: StokeOptimizer, loss: Union[Callable, List[Callable], Tuple[Callable]], batch_size_per_device: int, grad_accum_steps: Optional[int] = 1, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None, gpu: bool = False, fp16: Optional[FP16Options] = None, distributed: Optional[DistributedOptions] = None, fairscale_oss: bool = False, fairscale_sddp: bool = False, fairscale_fsdp: bool = False, configs: Optional[ List[ Union[ AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig, ] ] ] = None, info_rank: Optional[Union[int, List[int]]] = 0, verbose: bool = True, ema_weight: float = 0.1, ): \"\"\"Init for Stoke class object Parameters ---------- model: torch.nn.Module PyTorch model optimizer: StokeOptimizer Optimizer configuration loss: Union[Callable, List[Callable], Tuple[Callable]] Callable loss function or functions batch_size_per_device: int Batch size at the single device level grad_accum_steps: Optional[int], default: 1 Number of gradient accumulation steps grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None Gradient clipping configuration gpu: bool, default: False flag to use GPU device(s) fp16: Optional[FP16Options], default: None Choice of mixed-precision backend distributed: Optional[DistributedOptions], default: None Choice of distributed backend fairscale_oss: bool, default: False Flag to activate optimizer state sharding using Fairscale fairscale_sddp: bool, default: False Flag to activate sharded DDP using Fairscale fairscale_fsdp: bool, default: False Flag to activate fully sharded DDP using Fairscale configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None Configuration objects for runtimes info_rank: Optional[Union[int, List[int]]], default = 0 Constrain prints to specific devices verbose: bool, default: True Flag for verbosity ema_weight: float, default: 0.5 weight used for any ema calculation on metrics \"\"\" # Verbosity self._verbose = verbose # Info rank self._info_rank = info_rank # EMA self._ema_weight = ema_weight # Setup the StokeState self._status = StokeStatus( batch_size_per_device=batch_size_per_device, grad_accum=grad_accum_steps, grad_clip=grad_clip, gpu=gpu, fp16=fp16, distributed=distributed, fairscale_oss=fairscale_oss, fairscale_sddp=fairscale_sddp, fairscale_fsdp=fairscale_fsdp, configs=configs, ) # Run some checks self._model = self._check_model(model) self._optimizer = self._check_optimizer(optimizer) self._loss = self._check_loss(loss) # Dynamically construct the StokeRunner from the StokeStatus self._runner, class_info = self._build_runner() # Setup distributed backend self._runner.setup_distributed() # Post here the runner will have the print_device function that is mapped to the self.print here # as it needs rank to be accessible before working if self._verbose: dev_id = ( self.rank if (self.rank == \"cpu\" or self.rank == \"gpu\") else self._info_rank ) self.print(f\"Printing verbose information on rank(s): {dev_id}\") # Print the runner class info from the mixins self.print(class_info) # Possibly place model on GPU depending on StokeStatus -- before wrap calls self._place_model_on_gpu() # Handle the wrap ops in the correct order self._handle_ordered_wrap_ops(optimizer=optimizer) # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 # Set post-init status variables self._status.set_post_init_values(world_size=self.world_size) # Print the final configuration if self._verbose: self.print(msg=self._status) def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of optimizer then the model This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped methods are called Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed through self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer) # Wrap with distributed backend -- in this case the optimizer is passed through self._model, self._optimizer = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer ) def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping of model then optimizer Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn't exist yet # don't use the return for the optimizer in this case self._model, _ = self._runner.wrap_distributed( model=self._model, grad_accum=self.grad_accum, optimizer=None ) # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn't exist yet self._runner.wrap_fp16(model=self._model, optimizer=None) # Build the optimizer self._optimizer = self._runner.build_optimizer( optimizer=optimizer[\"optimizer\"], optimizer_kwargs=optimizer[\"optimizer_kwargs\"], model=self._model, ) def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer): \"\"\"Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status Parameters ---------- optimizer: StokeOptimizer Optimizer configuration Returns ------- None \"\"\" # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed) if (self.sharded and self.oss) or self.is_apex or self.is_horovod: self._wrap_optimizer_then_model(optimizer=optimizer) else: self._wrap_model_then_optimizer(optimizer=optimizer) def _check_accum(self): \"\"\"Checks if the current step is the last accumulation step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0 def _check_pre_accum(self): \"\"\"Checks if we are at the pre-accumulate step Returns ------- bool \"\"\" return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum def _set_loss_to_zero(self): \"\"\"Used to set a loss tracker to zero depending on the type Returns ------- float or list or tuple of reset loss \"\"\" return ( type(self._loss)([0.0] * len(self._loss)) if isinstance(self._loss, (list, tuple)) else 0.0 ) def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\") def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\") def _scale_agg_loss(self): \"\"\"Scales the mean aggregated loss by grad accum Returns ------- scale_vals: list or float of mean aggregated loss \"\"\" if isinstance(self._agg_loss, (list, tuple)): scale_vals = [ val / self.grad_accum for idx, val in enumerate(self._agg_loss) ] else: scale_vals = self._agg_loss / self.grad_accum return scale_vals def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\") def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank) def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line ) @staticmethod def _check_model(model: torch.nn.Module): \"\"\"Verifies the type of the model Parameters ---------- model: torch.nn.Module current torch model Returns ------- None \"\"\" # Check if the model is an nn.Module such that it has a forward method if not isinstance(model, torch.nn.Module): raise TypeError( f\"Stoke -- Model is not of type torch.nn.Module, currently {type(model)}\" ) return model @staticmethod def _check_optimizer(optimizer: StokeOptimizer): \"\"\"Verifies the type of the optimizer Parameters ---------- optimizer: StokeOptimizer Current optimizer configuration TypedDict (aka dict) Returns ------- None \"\"\" if not isinstance(optimizer, dict): raise TypeError( f\"Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}\" ) return optimizer def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]): \"\"\"Checks to make sure the loss function(s) is/are callable Parameters ---------- loss: Union[Callable, List[Callable], Tuple[Callable]] Current callable loss(es) Returns ------- None \"\"\" if isinstance(loss, (list, tuple)): loss = [self._check_loss(val) for val in loss] return loss elif isinstance(loss, Callable): return loss else: raise TypeError( f\"Stoke -- Loss is not of type Callable, currently {type(loss)}\" ) def _place_model_on_gpu(self): \"\"\"Automatically moves the model to GPU device(s) Returns ------- None \"\"\" if self.gpu and not self.is_deepspeed: if self._verbose: self.print(f\"Automatically handling moving model to GPU(s)...\") self._model.cuda() def _build_runner(self): \"\"\"Builds the runtime object from the mixin style classes Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a a single interface. Might prevent some IDE type-hinting as it's dynamic Returns ------- StokeRunner runtime runner object \"\"\" # Get the classes dist_class = self._get_distributed_mixin() fp16_class = self._get_fp16_mixin() optimizer_class = self._get_optimizer_mixin() io_class = self._get_io_mixin() # Python MRO hack to make sure the inits of all the Mixin classes get called def __multiple_mixin_init__(*args, **kwargs): dist_class.__init__(*args, **kwargs) fp16_class.__init__(*args, **kwargs) optimizer_class.__init__(*args, **kwargs) io_class.__init__(*args, **kwargs) # Configs pass through kwargs_dict = { \"amp_config\": self.amp_config, \"apex_config\": self.apex_config, \"ddp_config\": self.ddp_config, \"deepspeed_config\": self.deepspeed_config, \"horovod_config\": self.horovod_config, \"oss_config\": self.oss_config, \"sharded_config\": self.sddp_config, \"fully_sharded_config\": self.fsdp_config, } # Generate the runner class from the mixins based on the StokeStatus runner_class = type( \"StokeRunner\", (dist_class, fp16_class, optimizer_class, io_class), {\"__init__\": __multiple_mixin_init__}, )( verbose=self._verbose, batch_size_per_device=self.batch_size, grad_accum_steps=self.grad_accum, grad_clip=self.grad_clip, info_rank=self._info_rank, loss=self._loss, **kwargs_dict, ) # Make a list of class info for print later class_info = [ f\"Distributed Mixin: {dist_class.__name__}\", f\"Optimizer Mixin: {dist_class.__name__}\", f\"FP16 Mixin: {fp16_class.__name__}\", f\"IO Mixin: {io_class.__name__}\", ] return runner_class, class_info def _get_io_mixin(self): \"\"\"Determines which IO class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated ioclass \"\"\" if self.is_deepspeed: return_class = RunnerIOEnum.deepspeed.value elif self.is_horovod: return_class = RunnerIOEnum.horovod.value elif self.is_ddp: return_class = RunnerIOEnum.ddp.value else: return_class = RunnerIOEnum.base.value return return_class def _get_optimizer_mixin(self): \"\"\"Determines which optimizer class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated optimizer class \"\"\" if self.oss: return_class = RunnerOptimizerEnum.oss.value else: return_class = RunnerOptimizerEnum.base.value return return_class def _get_distributed_mixin(self): \"\"\"Determines which distributed class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated distributed class \"\"\" # if not gpu then fall to cpu single if not self.gpu: return_class = RunnerDistEnum.cpu.value # if gpu but no distributed then fall to single gpu elif self.gpu and (self.distributed is None): return_class = RunnerDistEnum.gpu.value elif self.gpu and (self.distributed is not None): return_class = RunnerDistEnum[self.distributed].value else: raise ValueError(\"Stoke -- Cannot map to a valid distributed class\") return return_class def _get_fp16_mixin(self): \"\"\"Determines which fp16 class to use Embedded logic based on the enum class Returns ------- ABCMeta un-instantiated fp16 class \"\"\" if self.fp16 is not None: return_class = RunnerFP16Enum[self.fp16].value else: return_class = RunnerFP16Enum.full.value return return_class def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, ) def model(self, *args, **kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs) def loss(self, *args, **kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]): \"\"\"Handles calculating the ema loss Parameters ---------- loss: Union[float, List[float], Tuple[float]] current calculated loss list, tuple or float Returns ------- None \"\"\" self._rolling_loss_steps += 1 if isinstance(loss, (list, tuple)): self._rolling_mean_loss = type(self._rolling_mean_loss)( self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx]) for idx, val in enumerate(loss) ) else: self._rolling_mean_loss = self._ema_loss( value=loss, current_mean=self._rolling_mean_loss ) def _ema_loss(self, value: float, current_mean: float): \"\"\"Calculate the ema of the loss Parameters ---------- value: float current loss value current_mean: float current mean value Returns ------- current ema value: float \"\"\" if self._rolling_loss_steps == 1: return value else: return (self._ema_weight * value) + ( (1.0 - self._ema_weight) * current_mean ) def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1 def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) def _reset(self): \"\"\"Resets the state post optimizer step call Returns ------- None \"\"\" if self._verbose: self.print(\"Resetting all grad/variables for next optimizer step\") # Zero the grads if not deepspeed if not self.is_deepspeed: self.zero_grads() # Reset counter self._grad_accum_counter = 0 # Reset agg loss -- single or mutiple losses self._agg_loss = self._set_loss_to_zero() def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" ) def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device) def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod ) def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset() def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0 def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" ) def _load_fp16_state_dict_fn(self): \"\"\"Returns the function to load the sacler state dict Returns ------- mp_state_dict_fn: Callable, default: None callable function to load the scaler state dict \"\"\" mp_state_dict_fn = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict_fn = amp.load_state_dict except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) else: mp_state_dict_fn = self.scaler.load_state_dict return mp_state_dict_fn def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier() @property def step_loss(self): \"\"\"Gets the last step loss synced across device(s) (unscaled)\"\"\" return self._last_step_loss @property def model_access(self): \"\"\"Interface for model access due to the different types between the DP, DDP, and SDDP implementations\"\"\" if isinstance(self._model, (DDP, DP, SDDP, FSDP)): return self._model.module else: return self._model @property def loss_access(self): \"\"\"Gets loss tensor(s)\"\"\" return self._loss @property def optimizer(self): \"\"\"Gets the optimizer\"\"\" return self._optimizer @property def scaler(self): \"\"\"Gets the current scaler object\"\"\" return self._runner.scaler @property def fp16_state_dict(self): \"\"\"Gets the fp16 state dict from various methods\"\"\" mp_state_dict = None if self.scaler is not None: if self.is_apex: try: from apex import amp mp_state_dict = amp.state_dict() except ImportError as e: print( e, \": Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)\", ) elif self.is_amp: mp_state_dict = self.scaler.state_dict() return mp_state_dict @property def status(self): \"\"\"Gets the StokeStatus object\"\"\" return self._status @property def batch_size(self): \"\"\"Shortcut to batch size\"\"\" return self._status.batch_size @property def effective_batch_size(self): \"\"\"Shortcut to effective batch size\"\"\" return self._status.effective_batch_size @property def grad_clip(self): \"\"\"Shortcut to get grad clip\"\"\" return self._status.grad_clip @property def grad_accum(self): \"\"\"Shortcut to get grad accumulation\"\"\" return self._status.grad_accum @property def gpu(self): \"\"\"Shortcut to get GPU status\"\"\" return self._status.gpu @property def cuda(self): \"\"\"Shortcut to get cuda status\"\"\" return self._status.cuda @property def nccl(self): \"\"\"Shortcut to get nccl status\"\"\" return self._status.nccl @property def fp16(self): \"\"\"Shortcut to get FP16 status\"\"\" return self._status.fp16 @property def is_apex(self): \"\"\"Returns if APEX is activated\"\"\" return self._status.is_fp16_apex @property def is_amp(self): \"\"\"Returns if AMP is activated\"\"\" return self._status.is_fp16_amp @property def distributed(self): \"\"\"Shortcut to distributed status\"\"\" return self._status.distributed @property def is_ddp(self): \"\"\"Returns if DDP is activated\"\"\" return self._status.is_distributed_ddp @property def is_horovod(self): \"\"\"Returns if Horovod is activated\"\"\" return self._status.is_distributed_horovod @property def is_deepspeed(self): \"\"\"Returns if Deepspeed is acticated\"\"\" return self._status.is_distributed_deepspeed @property def oss(self): \"\"\"Returns if Fairscale optimizer state sharding status\"\"\" return self._status.oss @property def sharded(self): \"\"\"Returns if Fairscale sharded DDP status\"\"\" return self._status.sharded @property def fully_sharded(self): \"\"\"Returns if Fairscale fully sharded DDP status\"\"\" return self._status.fully_sharded @property def world_size(self): \"\"\"Shortcut to get world size\"\"\" return self._runner.world_size @property def rank(self): \"\"\"Shortcut to get rank\"\"\" return self._runner.rank @property def amp_config(self): \"\"\"Returns amp config or None based on amp state\"\"\" return self._status.amp_config if self.is_amp else None @property def apex_config(self): \"\"\"Returns apex config or None based on apex state\"\"\" return self._status.apex_config if self.is_apex else None @property def ddp_config(self): \"\"\"Returns ddp config or None based on ddp state\"\"\" return self._status.ddp_config if self.is_ddp else None @property def deepspeed_config(self): \"\"\"Returns deepspeed config or None based on deepspeed state\"\"\" return self._status.deepspeed_config if self.is_deepspeed else None @property def oss_config(self): \"\"\"Returns oss config or None based on ossstate\"\"\" return self._status.oss_config if self.oss else None @property def sddp_config(self): \"\"\"Returns sddp config or None based on sddp state\"\"\" return self._status.sddp_config if self.sharded else None @property def fsdp_config(self): \"\"\"Returns fsdp config or None based on fsdp state\"\"\" return self._status.fsdp_config if self.fully_sharded else None @property def horovod_config(self): \"\"\"Returns horovod config or None based on horovod state\"\"\" return self._status.horovod_config if self.is_horovod else None @property def num_model_parameters(self): \"\"\"Returns number of parameters that require gradients\"\"\" return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad) @property def ema_loss(self): \"\"\"Returns the current rolling mean loss\"\"\" return self._rolling_mean_loss","title":"Attributes"},{"location":"reference/stoke/stoke/#instance-variables","text":"amp_config Returns amp config or None based on amp state apex_config Returns apex config or None based on apex state batch_size Shortcut to batch size cuda Shortcut to get cuda status ddp_config Returns ddp config or None based on ddp state deepspeed_config Returns deepspeed config or None based on deepspeed state distributed Shortcut to distributed status effective_batch_size Shortcut to effective batch size ema_loss Returns the current rolling mean loss fp16 Shortcut to get FP16 status fp16_state_dict Gets the fp16 state dict from various methods fsdp_config Returns fsdp config or None based on fsdp state fully_sharded Returns if Fairscale fully sharded DDP status gpu Shortcut to get GPU status grad_accum Shortcut to get grad accumulation grad_clip Shortcut to get grad clip horovod_config Returns horovod config or None based on horovod state is_amp Returns if AMP is activated is_apex Returns if APEX is activated is_ddp Returns if DDP is activated is_deepspeed Returns if Deepspeed is acticated is_horovod Returns if Horovod is activated loss_access Gets loss tensor(s) model_access Interface for model access due to the different types between the DP, DDP, and SDDP implementations nccl Shortcut to get nccl status num_model_parameters Returns number of parameters that require gradients optimizer Gets the optimizer oss Returns if Fairscale optimizer state sharding status oss_config Returns oss config or None based on ossstate rank Shortcut to get rank scaler Gets the current scaler object sddp_config Returns sddp config or None based on sddp state sharded Returns if Fairscale sharded DDP status status Gets the StokeStatus object step_loss Gets the last step loss synced across device(s) (unscaled) world_size Shortcut to get world size","title":"Instance variables"},{"location":"reference/stoke/stoke/#methods","text":"","title":"Methods"},{"location":"reference/stoke/stoke/#dataloader","text":"def DataLoader ( self , dataset : torch . utils . data . dataset . Dataset [ + T_co ], shuffle : bool = False , sampler : Union [ torch . utils . data . sampler . Sampler [ int ], NoneType ] = None , batch_sampler : Union [ torch . utils . data . sampler . Sampler [ Sequence [ int ]], NoneType ] = None , num_workers : int = 0 , collate_fn : Callable [[ List [ ~ T ]], Any ] = None , pin_memory : bool = False , drop_last : bool = False , timeout : float = 0 , worker_init_fn : Callable [[ int ], NoneType ] = None , multiprocessing_context = None , generator = None , * , prefetch_factor : int = 2 , persistent_workers : bool = False ) Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters: Name Type Description Default dataset Dataset dataset from which to load the data. None shuffle bool, default: False set to True to have the data reshuffled at every epoch. None sampler Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, :attr: shuffle must not be specified. None batch_sampler Sampler or Iterable, default: None: like :attr: sampler , but returns a batch of indices at a time. Mutually exclusive with :attr: batch_size , :attr: shuffle , :attr: sampler , and :attr: drop_last . None num_workers int, default: 0 how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. None collate_fn callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. None pin_memory bool, default: False: If True , the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr: collate_fn returns a batch that is a custom type, see the example below. None drop_last bool, default: False set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. None timeout numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. None worker_init_fn callable, default: None If not None , this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1] ) as input, after seeding and before data loading. None prefetch_factor int, default: 2 Number of samples loaded in advance by each worker. 2 means there will be a total of 2 * num_workers samples prefetched across all workers. None persistent_workers bool, default: False If True , the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. None Returns: Type Description StokeDataLoader wrapped torch.utils.data.DataLoader object ??? example \"View Source\" def DataLoader( self, dataset: Dataset[T_co], shuffle: bool = False, sampler: Optional[Sampler[int]] = None, batch_sampler: Optional[Sampler[Sequence[int]]] = None, num_workers: int = 0, collate_fn: _collate_fn_t = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: _worker_init_fn_t = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False, ): \"\"\"Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs. Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called) and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus object is available which is post init. This could be disconnected from this class but it would require the user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and never handled Parameters ---------- dataset: Dataset dataset from which to load the data. shuffle: bool, default: False set to ``True`` to have the data reshuffled at every epoch. sampler: Sampler or Iterable, default: None defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__`` implemented. If specified, :attr:`shuffle` must not be specified. batch_sampler: Sampler or Iterable, default: None: like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`. num_workers: int, default: 0 how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process. collate_fn: callable, optional: merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory: bool, default: False: If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type, see the example below. drop_last: bool, default: False set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If ``False`` and the size of dataset is not divisible by the batch size, then the last batch will be smaller. timeout: numeric, default: 0 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. worker_init_fn: callable, default: None If not ``None``, this will be called on each worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading. prefetch_factor: int, default: 2 Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers samples prefetched across all workers. persistent_workers: bool, default: False If ``True``, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers `Dataset` instances alive. Returns ------- StokeDataLoader wrapped torch.utils.data.DataLoader object \"\"\" # Check if forkserver is available for horovod and use if ( num_workers > 0 and hasattr(torch.multiprocessing, \"_supports_context\") and torch.multiprocessing._supports_context and \"forkserver\" in torch.multiprocessing.get_all_start_methods() and self.is_horovod ): multiprocessing_context = \"forkserver\" if self._verbose and self.gpu: print(f\"Automatically handling moving model input data to GPU(s)...\") # Forward the already known options from the Stoke status return StokeDataLoader( gpu=self.gpu, fp16=self.fp16, batch_size=self.batch_size, dataset=dataset, shuffle=shuffle, sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers, collate_fn=collate_fn, pin_memory=pin_memory, drop_last=drop_last, timeout=timeout, worker_init_fn=worker_init_fn, multiprocessing_context=multiprocessing_context, generator=generator, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers, )","title":"DataLoader"},{"location":"reference/stoke/stoke/#backward","text":"def backward ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]] ) Wrapped backwards call Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) None Returns: Type Description None None ??? example \"View Source\" def backward( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] ): \"\"\"Wrapped backwards call Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] Callable loss function(s) Returns ------- None \"\"\" # Increment the grad counter self._grad_accum_counter += 1 # Set the context based on the counter dist_cm = ( nullcontext() if self._check_accum() else self._runner.grad_accum_context(self._model) ) with dist_cm: self._runner.backward_call( loss=loss, model=self.model_access, optimizer=self._optimizer ) # Increment the number of total calls to backward (each backward to a loss is only considered 1) self._backward_steps += 1","title":"backward"},{"location":"reference/stoke/stoke/#barrier","text":"def barrier ( self ) Calls the underlying distributed barrier if available ??? example \"View Source\" def barrier(self): \"\"\"Calls the underlying distributed barrier if available\"\"\" self._runner.barrier()","title":"barrier"},{"location":"reference/stoke/stoke/#detach_and_sync_loss","text":"def detach_and_sync_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], device = None ) Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) None device default: None device to sync across None Returns: Type Description loss that is synced across devices and all_reduced w/ SUM None ??? example \"View Source\" def detach_and_sync_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], device=None, ): \"\"\"Shorthand method to detach and sync loss Maps to the runner function of the same name Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) device: default: None device to sync across Returns ------- loss that is synced across devices and all_reduced w/ SUM \"\"\" return self._runner.detach_and_sync_loss(loss=loss, device=device)","title":"detach_and_sync_loss"},{"location":"reference/stoke/stoke/#dump_model_parameter_info","text":"def dump_model_parameter_info ( self ) Dumps all parameter information for named parameters (shape, device, dtype) Returns: Type Description None None ??? example \"View Source\" def dump_model_parameter_info(self): \"\"\"Dumps all parameter information for named parameters (shape, device, dtype) Returns ------- None \"\"\" self.print(\"Dumping all model parameter information to stdout....\") for name, param in self.model_access.named_parameters(): if param.requires_grad: self.print( f\"Name: {name}, Shape: {param.shape}, \" f\"Device: {param.device}, dtype: {param.dtype}\" )","title":"dump_model_parameter_info"},{"location":"reference/stoke/stoke/#load","text":"def load ( self , path : str , tag : str , strict : bool = True ) Loads a model checkpoint using the correct backend interface Parameters: Name Type Description Default path str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) None tag str full tag name the model checkpoint was saved as None strict bool ignore non-matching keys None Returns: Type Description dict, default: None a dictionary of any custom fields the user passed to the save function ??? example \"View Source\" def load(self, path: str, tag: str, strict: bool = True): \"\"\"Loads a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory that the model checkpoint was saved (prefer absolute paths over relative paths) tag: str full tag name the model checkpoint was saved as strict: bool ignore non-matching keys Returns ------- extras: dict, default: None a dictionary of any custom fields the user passed to the save function \"\"\" # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU? backward_step, grad_accum_step, optimizer_step, extras = self._runner.load( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, gpu=self.gpu, path=path, tag=tag, scaler_dict_fn=self._load_fp16_state_dict_fn(), strict=strict, ) # Reset values based on what was in the load dict self._backward_steps = backward_step self._grad_accum_counter = grad_accum_step self._optimizer_steps = optimizer_step self.print(f\"Successfully loaded model checkpoint from {path}/{tag}\") # Return the extras dict return extras","title":"load"},{"location":"reference/stoke/stoke/#loss","text":"def loss ( self , * args , ** kwargs ) Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters: Name Type Description Default *args list or tuple Additional arguments should be passed as keyword arguments None **kwargs dict Extra arguments passed to the loss function call(s) None Returns: Type Description outputs of callable loss function(s) None ??? example \"View Source\" def loss(self, args, *kwargs): \"\"\"Wrapped callable loss function call Handles internal logic of aggregating up the losses for single and multiple losses Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the loss function call(s) Returns ------- outputs of callable loss function(s) \"\"\" # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch with self._runner.loss_context: if isinstance(self._loss, (list, tuple)): loss = type(self._loss)(val(*args, **kwargs) for val in self._loss) sync_loss = [self.detach_and_sync_loss(val) for val in loss] self._last_step_loss = type(self._loss)( val for idx, val in enumerate(sync_loss) ) self._agg_loss = type(self._loss)( self._agg_loss[idx] + val for idx, val in enumerate(sync_loss) ) self._handle_ema_loss(loss=sync_loss) if self.grad_accum > 1 and self.model_access.training: loss = type(loss)(val / self.grad_accum for val in loss) else: loss = self._loss(*args, **kwargs) sync_loss = self.detach_and_sync_loss(loss) self._last_step_loss = sync_loss self._agg_loss += sync_loss self._handle_ema_loss(loss=sync_loss) # Handle grad accumulation by dividing by the accumulation steps if self.grad_accum > 1 and self.model_access.training: loss = loss / self.grad_accum return loss","title":"loss"},{"location":"reference/stoke/stoke/#model","text":"def model ( self , * args , ** kwargs ) Wrapped model forward call Parameters: Name Type Description Default *args list or tuple Additional arguments should be passed as keyword arguments None **kwargs dict Extra arguments passed to the model forward call None Returns: Type Description model forward output None ??? example \"View Source\" def model(self, args, *kwargs): \"\"\"Wrapped model forward call Parameters ---------- *args: list or tuple Additional arguments should be passed as keyword arguments **kwargs: dict, optional Extra arguments passed to the model forward call Returns ------- model forward output \"\"\" with self._runner.model_context: return self._model(*args, **kwargs) # return self.model_access(*args, **kwargs)","title":"model"},{"location":"reference/stoke/stoke/#print","text":"def print ( self , msg : Union [ str , List [ str ]], single_line : bool = False ) Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters: Name Type Description Default msg str message to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print(self, msg: Union[str, List[str]], single_line: bool = False): \"\"\"Wraps the runners print device and forces print on the _info_rank attribute(s) Parameters ---------- msg: str message to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" self._runner.print_device( msg=msg, rank=self._info_rank, single_line=single_line )","title":"print"},{"location":"reference/stoke/stoke/#print_ema_loss","text":"def print_ema_loss ( self , prepend_msg : str = 'Current EMA Loss' , single_line : bool = False ) Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default prepend_msg str, default: \"Current EMA Loss\" message prepend to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_ema_loss( self, prepend_msg: str = \"Current EMA Loss\", single_line: bool = False ): \"\"\"Prints the current ema loss synced across all devices Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Current EMA Loss\" message prepend to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(self._rolling_mean_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val:.3f}\" for idx, val in enumerate(self._rolling_mean_loss) ] self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._rolling_mean_loss:.3f}\")","title":"print_ema_loss"},{"location":"reference/stoke/stoke/#print_mean_accumulated_synced_loss","text":"def print_mean_accumulated_synced_loss ( self , prepend_msg : str = 'Mean Accumulated & Synced Loss' , pre_backwards : bool = True , single_line : bool = False ) Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default prepend_msg str, default: \"Mean Accumulated & Synced Loss\" message prepend to print None pre_backwards bool, default: True if being called pre backward step None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_mean_accumulated_synced_loss( self, prepend_msg: str = \"Mean Accumulated & Synced Loss\", pre_backwards: bool = True, single_line: bool = False, ): \"\"\"Prints the mean accumulated and device synced loss only after the grad accumulation step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- prepend_msg: str, default: \"Mean Accumulated & Synced Loss\" message prepend to print pre_backwards: bool, default: True if being called pre backward step single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" check_fn = self._check_pre_accum if pre_backwards else self._check_accum if check_fn(): if isinstance(self._agg_loss, (list, tuple)): print_vals = self._scale_agg_loss() self.print(print_vals, single_line=single_line) else: self.print(f\"{prepend_msg}: {self._scale_agg_loss():.3f}\")","title":"print_mean_accumulated_synced_loss"},{"location":"reference/stoke/stoke/#print_num_model_parameters","text":"def print_num_model_parameters ( self , normalize : stoke . utils . ParamNormalize = < ParamNormalize . MILLION : 1000000.0 > ) Parameters: Name Type Description Default normalize ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing None Returns: Type Description None None ??? example \"View Source\" def print_num_model_parameters( self, normalize: ParamNormalize = ParamNormalize.MILLION ): \"\"\" Parameters ---------- normalize: ParamNormalize, default: ParamNormalize.MILLION ParamNormalize choice for pretty print normalizing Returns ------- None \"\"\" self.print( f\"Total Trainable Model Parameters: \" f\"{(self.num_model_parameters / normalize.value):.3f} {normalize.name}\" )","title":"print_num_model_parameters"},{"location":"reference/stoke/stoke/#print_on_devices","text":"def print_on_devices ( self , msg : Union [ str , List [ str ]], rank : Union [ int , List [ int ], NoneType ] = 0 ) Wraps runner print interface for shorter semantics Parameters: Name Type Description Default msg str message to print None rank Union[int, List[int]], default: 0 which ranks to print on None Returns: Type Description None None ??? example \"View Source\" def print_on_devices( self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0 ): \"\"\"Wraps runner print interface for shorter semantics Parameters ---------- msg: str message to print rank: Union[int, List[int]], default: 0 which ranks to print on Returns ------- None \"\"\" self._runner.print_device(msg=msg, rank=rank)","title":"print_on_devices"},{"location":"reference/stoke/stoke/#print_synced_loss","text":"def print_synced_loss ( self , loss : Union [ torch . Tensor , List [ torch . Tensor ], Tuple [ torch . Tensor ]], prepend_msg : str = 'Step Synced Loss' , device = None , single_line : bool = False ) Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters: Name Type Description Default loss Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device None prepend_msg str, default: \"Step Synced Loss\" message prepend to print None device default: None specify the device to place the synced loss on (defaults to same device) same single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def print_synced_loss( self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]], prepend_msg: str = \"Step Synced Loss\", device=None, single_line: bool = False, ): \"\"\"Prints a device synced loss at a single step Handles single or multiple losses. Prints only on devices specified by self._info_rank Parameters ---------- loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]] current loss(es) on the device prepend_msg: str, default: \"Step Synced Loss\" message prepend to print device: default: None specify the device to place the synced loss on (defaults to same device) single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" printable_loss = self.detach_and_sync_loss(loss, device) if isinstance(printable_loss, (list, tuple)): print_vals = [ f\"{prepend_msg} {idx}: {val * self.grad_accum:.3f}\" for idx, val in enumerate(printable_loss) ] self.print(print_vals, single_line=single_line) else: self.print(msg=f\"{prepend_msg}: {printable_loss * self.grad_accum:.3f}\")","title":"print_synced_loss"},{"location":"reference/stoke/stoke/#reset","text":"def reset ( self ) Public method for resetting the underlying stoke state Returns: Type Description None None ??? example \"View Source\" def reset(self): \"\"\"Public method for resetting the underlying stoke state Returns ------- None \"\"\" self._reset()","title":"reset"},{"location":"reference/stoke/stoke/#reset_ema","text":"def reset_ema ( self ) Used to reset the current state of the rolling mean loss Returns: Type Description None None ??? example \"View Source\" def reset_ema(self): \"\"\"Used to reset the current state of the rolling mean loss Returns ------- None \"\"\" self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0","title":"reset_ema"},{"location":"reference/stoke/stoke/#reset_tracking","text":"def reset_tracking ( self ) Public method for resetting all underlying stoke tracked variables Returns: Type Description None None ??? example \"View Source\" def reset_tracking(self): \"\"\"Public method for resetting all underlying stoke tracked variables Returns ------- None \"\"\" # Create some tracking vars self._grad_accum_counter = 0 self._optimizer_steps = 0 self._backward_steps = 0 self._last_step_loss = self._set_loss_to_zero() self._agg_loss = self._set_loss_to_zero() self._rolling_mean_loss = self._set_loss_to_zero() self._rolling_loss_steps = 0","title":"reset_tracking"},{"location":"reference/stoke/stoke/#save","text":"def save ( self , path : str , name : str = UUID ( '1bec68f4-7df7-48d2-a526-14685e92f54f' ), extension : str = 'pt' , create_directory : bool = True , extras : Union [ dict , NoneType ] = None ) Saves a model checkpoint using the correct backend interface Parameters: Name Type Description Default path str path to directory to save the model checkpoint (prefer absolute paths over relative paths) None name str, default: uuid4() name used to save checkpoint file None extension str, default: '.pt' extension used to save PyTorch model checkpoint None create_directory bool, default: True flag to create the directory path if it doesn't exist None extras dict, default: None a dictionary of any extra things to save None Returns: Type Description str path to directory that the model checkpoint was saved ??? example \"View Source\" def save( self, path: str, name: str = uuid4(), extension: str = \"pt\", create_directory: bool = True, extras: Optional[dict] = None, ): \"\"\"Saves a model checkpoint using the correct backend interface Parameters ---------- path: str path to directory to save the model checkpoint (prefer absolute paths over relative paths) name: str, default: uuid4() name used to save checkpoint file extension: str, default: '.pt' extension used to save PyTorch model checkpoint create_directory: bool, default: True flag to create the directory path if it doesn't exist extras: dict, default: None a dictionary of any extra things to save Returns ------- path: str path to directory that the model checkpoint was saved tag: str full tag name the model checkpoint was saved as \"\"\" out_path, tag = self._runner.save( model=self._model if self.fully_sharded else self.model_access, optimizer=self.optimizer, path=path, backward_step=self._backward_steps, grad_accum_step=self._grad_accum_counter, optimizer_step=self._optimizer_steps, name=name, scaler_dict=self.fp16_state_dict, extension=extension, create_directory=create_directory, extras=extras, status=self.status.status, ) self.print(f\"Successfully saved model checkpoint to {out_path}/{tag}\") return out_path, tag","title":"save"},{"location":"reference/stoke/stoke/#step","text":"def step ( self ) Wrapped step call Handles grad clipping internally Returns: Type Description None None ??? example \"View Source\" def step(self): \"\"\"Wrapped step call Handles grad clipping internally Returns ------- None \"\"\" # Step the optimizer only if the modulo is zero if self._check_accum(): if self._verbose and self.grad_accum > 0: self.print(f\"Gradient Accumulation Steps: {self.grad_accum}\") # Clip if needed if self.grad_clip is not None: self._runner.clip_grad( self.grad_clip, self._model if self.fully_sharded else self.model_access, self._optimizer, oss=self.oss, horovod=self.is_horovod, deepspeed=self.is_deepspeed, fsdp=self.fully_sharded, ) # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer ) # Reset for the accumulated step self._reset() # Increment the number of step calls to the optimizer self._optimizer_steps += 1 # if deepspeed we need to step everytime as it handles the grad accumulation internally elif self.is_deepspeed: # Handle the optimizer step step_cm = ( self._runner.step_context(self._optimizer) if self.grad_clip is not None else nullcontext() ) with step_cm: self._runner.step_call( model=self.model_access, optimizer=self._optimizer )","title":"step"},{"location":"reference/stoke/stoke/#zero_grads","text":"def zero_grads ( self ) Zeros the optimizer grads depending on the optimizer type Returns: Type Description None None ??? example \"View Source\" def zero_grads(self): \"\"\"Zeros the optimizer grads depending on the optimizer type Returns ------- None \"\"\" zero_optimizer_grads( optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod )","title":"zero_grads"},{"location":"reference/stoke/utils/","text":"Module stoke.utils Stoke utility functions None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Stoke utility functions\"\"\" import os from enum import Enum from typing import Any, Callable, List, Tuple, TypeVar, Union import torch from fairscale.optim.oss import OSS # Taken from torch/utils/data/dataloader T_co = TypeVar(\"T_co\", covariant=True) T = TypeVar(\"T\") # Taken from torch/utils/data/dataloader _worker_init_fn_t = Callable[[int], None] # Ideally we would parameterize `DataLoader` by the return type of `collate_fn`, # but there is currently no way to have that # type parameter set to a default value if the user doesn't pass in a custom 'collate_fn'. # See https://github.com/python/mypy/issues/3737. _collate_fn_t = Callable[[List[T]], Any] class ParamNormalize(Enum): \"\"\"Normalization enum for total number of model parameters used to help with a pretty print\"\"\" THOUSAND = 1e3 MILLION = 1e6 BILLION = 1e9 TRILLION = 1e12 def zero_optimizer_grads( optimizer: Union[torch.optim.Optimizer, OSS], apex: bool = False, horovod: bool = False, ): \"\"\"Zeros grads depending on if it is a base Torch optimizer or a Fused version from APEX Parameters ---------- optimizer: torch.optim.Optimizer current optimizer object apex: bool, default: False if apex is active horovod: bool, default: False if horovod is active Returns ------- None \"\"\" if (optimizer.__class__.__name__.find(\"Fused\") == -1) and not apex and not horovod: optimizer.zero_grad(set_to_none=True) else: optimizer.zero_grad() def unrolled_print(msg: Union[str, List[str], Tuple[str]], single_line: bool = False): \"\"\"Prints the msg if it's a string or iterable of strings Parameters ---------- msg: Union[str, List[str], Tuple[str] string(s) to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(msg, (list, tuple)): if single_line: msg = type(msg)( f\"Stoke -- {val}\" if idx == 0 else f\"{val}\" for idx, val in enumerate(msg) ) else: msg = type(msg)(f\"Stoke -- {val}\" for idx, val in enumerate(msg)) print(*msg, sep=\", \" if single_line else \"\\n\") else: print(f\"Stoke -- {msg}\") def make_folder(path: str): \"\"\" Parameters ---------- path: str path to write Returns ------- \"\"\" # Make the folder if it doesn't exist if not os.path.isdir(path): os.makedirs(path, exist_ok=True) Variables T T_co Functions make_folder def make_folder ( path : str ) Parameters: Name Type Description Default path str path to write None ??? example \"View Source\" def make_folder(path: str): \"\"\" Parameters ---------- path: str path to write Returns ------- \"\"\" # Make the folder if it doesn't exist if not os.path.isdir(path): os.makedirs(path, exist_ok=True) unrolled_print def unrolled_print ( msg : Union [ str , List [ str ], Tuple [ str ]], single_line : bool = False ) Prints the msg if it's a string or iterable of strings Parameters: Name Type Description Default msg Union[str, List[str], Tuple[str] string(s) to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def unrolled_print(msg: Union[str, List[str], Tuple[str]], single_line: bool = False): \"\"\"Prints the msg if it's a string or iterable of strings Parameters ---------- msg: Union[str, List[str], Tuple[str] string(s) to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(msg, (list, tuple)): if single_line: msg = type(msg)( f\"Stoke -- {val}\" if idx == 0 else f\"{val}\" for idx, val in enumerate(msg) ) else: msg = type(msg)(f\"Stoke -- {val}\" for idx, val in enumerate(msg)) print(*msg, sep=\", \" if single_line else \"\\n\") else: print(f\"Stoke -- {msg}\") zero_optimizer_grads def zero_optimizer_grads ( optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], apex : bool = False , horovod : bool = False ) Zeros grads depending on if it is a base Torch optimizer or a Fused version from APEX Parameters: Name Type Description Default optimizer torch.optim.Optimizer current optimizer object None apex bool, default: False if apex is active None horovod bool, default: False if horovod is active None Returns: Type Description None None ??? example \"View Source\" def zero_optimizer_grads( optimizer: Union[torch.optim.Optimizer, OSS], apex: bool = False, horovod: bool = False, ): \"\"\"Zeros grads depending on if it is a base Torch optimizer or a Fused version from APEX Parameters ---------- optimizer: torch.optim.Optimizer current optimizer object apex: bool, default: False if apex is active horovod: bool, default: False if horovod is active Returns ------- None \"\"\" if (optimizer.__class__.__name__.find(\"Fused\") == -1) and not apex and not horovod: optimizer.zero_grad(set_to_none=True) else: optimizer.zero_grad() Classes ParamNormalize class ParamNormalize ( / , * args , ** kwargs ) ??? example \"View Source\" class ParamNormalize(Enum): \"\"\"Normalization enum for total number of model parameters used to help with a pretty print\"\"\" THOUSAND = 1e3 MILLION = 1e6 BILLION = 1e9 TRILLION = 1e12 Ancestors (in MRO) enum.Enum Class variables BILLION MILLION THOUSAND TRILLION name value","title":"Utils"},{"location":"reference/stoke/utils/#module-stokeutils","text":"Stoke utility functions None ??? example \"View Source\" # - - coding: utf-8 - - # Copyright FMR LLC <opensource@fidelity.com> # SPDX-License-Identifier: Apache-2.0 \"\"\"Stoke utility functions\"\"\" import os from enum import Enum from typing import Any, Callable, List, Tuple, TypeVar, Union import torch from fairscale.optim.oss import OSS # Taken from torch/utils/data/dataloader T_co = TypeVar(\"T_co\", covariant=True) T = TypeVar(\"T\") # Taken from torch/utils/data/dataloader _worker_init_fn_t = Callable[[int], None] # Ideally we would parameterize `DataLoader` by the return type of `collate_fn`, # but there is currently no way to have that # type parameter set to a default value if the user doesn't pass in a custom 'collate_fn'. # See https://github.com/python/mypy/issues/3737. _collate_fn_t = Callable[[List[T]], Any] class ParamNormalize(Enum): \"\"\"Normalization enum for total number of model parameters used to help with a pretty print\"\"\" THOUSAND = 1e3 MILLION = 1e6 BILLION = 1e9 TRILLION = 1e12 def zero_optimizer_grads( optimizer: Union[torch.optim.Optimizer, OSS], apex: bool = False, horovod: bool = False, ): \"\"\"Zeros grads depending on if it is a base Torch optimizer or a Fused version from APEX Parameters ---------- optimizer: torch.optim.Optimizer current optimizer object apex: bool, default: False if apex is active horovod: bool, default: False if horovod is active Returns ------- None \"\"\" if (optimizer.__class__.__name__.find(\"Fused\") == -1) and not apex and not horovod: optimizer.zero_grad(set_to_none=True) else: optimizer.zero_grad() def unrolled_print(msg: Union[str, List[str], Tuple[str]], single_line: bool = False): \"\"\"Prints the msg if it's a string or iterable of strings Parameters ---------- msg: Union[str, List[str], Tuple[str] string(s) to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(msg, (list, tuple)): if single_line: msg = type(msg)( f\"Stoke -- {val}\" if idx == 0 else f\"{val}\" for idx, val in enumerate(msg) ) else: msg = type(msg)(f\"Stoke -- {val}\" for idx, val in enumerate(msg)) print(*msg, sep=\", \" if single_line else \"\\n\") else: print(f\"Stoke -- {msg}\") def make_folder(path: str): \"\"\" Parameters ---------- path: str path to write Returns ------- \"\"\" # Make the folder if it doesn't exist if not os.path.isdir(path): os.makedirs(path, exist_ok=True)","title":"Module stoke.utils"},{"location":"reference/stoke/utils/#variables","text":"T T_co","title":"Variables"},{"location":"reference/stoke/utils/#functions","text":"","title":"Functions"},{"location":"reference/stoke/utils/#make_folder","text":"def make_folder ( path : str ) Parameters: Name Type Description Default path str path to write None ??? example \"View Source\" def make_folder(path: str): \"\"\" Parameters ---------- path: str path to write Returns ------- \"\"\" # Make the folder if it doesn't exist if not os.path.isdir(path): os.makedirs(path, exist_ok=True)","title":"make_folder"},{"location":"reference/stoke/utils/#unrolled_print","text":"def unrolled_print ( msg : Union [ str , List [ str ], Tuple [ str ]], single_line : bool = False ) Prints the msg if it's a string or iterable of strings Parameters: Name Type Description Default msg Union[str, List[str], Tuple[str] string(s) to print None single_line bool, default: False if iterable print all on one line space and comma separated None Returns: Type Description None None ??? example \"View Source\" def unrolled_print(msg: Union[str, List[str], Tuple[str]], single_line: bool = False): \"\"\"Prints the msg if it's a string or iterable of strings Parameters ---------- msg: Union[str, List[str], Tuple[str] string(s) to print single_line: bool, default: False if iterable print all on one line space and comma separated Returns ------- None \"\"\" if isinstance(msg, (list, tuple)): if single_line: msg = type(msg)( f\"Stoke -- {val}\" if idx == 0 else f\"{val}\" for idx, val in enumerate(msg) ) else: msg = type(msg)(f\"Stoke -- {val}\" for idx, val in enumerate(msg)) print(*msg, sep=\", \" if single_line else \"\\n\") else: print(f\"Stoke -- {msg}\")","title":"unrolled_print"},{"location":"reference/stoke/utils/#zero_optimizer_grads","text":"def zero_optimizer_grads ( optimizer : Union [ torch . optim . optimizer . Optimizer , fairscale . optim . oss . OSS ], apex : bool = False , horovod : bool = False ) Zeros grads depending on if it is a base Torch optimizer or a Fused version from APEX Parameters: Name Type Description Default optimizer torch.optim.Optimizer current optimizer object None apex bool, default: False if apex is active None horovod bool, default: False if horovod is active None Returns: Type Description None None ??? example \"View Source\" def zero_optimizer_grads( optimizer: Union[torch.optim.Optimizer, OSS], apex: bool = False, horovod: bool = False, ): \"\"\"Zeros grads depending on if it is a base Torch optimizer or a Fused version from APEX Parameters ---------- optimizer: torch.optim.Optimizer current optimizer object apex: bool, default: False if apex is active horovod: bool, default: False if horovod is active Returns ------- None \"\"\" if (optimizer.__class__.__name__.find(\"Fused\") == -1) and not apex and not horovod: optimizer.zero_grad(set_to_none=True) else: optimizer.zero_grad()","title":"zero_optimizer_grads"},{"location":"reference/stoke/utils/#classes","text":"","title":"Classes"},{"location":"reference/stoke/utils/#paramnormalize","text":"class ParamNormalize ( / , * args , ** kwargs ) ??? example \"View Source\" class ParamNormalize(Enum): \"\"\"Normalization enum for total number of model parameters used to help with a pretty print\"\"\" THOUSAND = 1e3 MILLION = 1e6 BILLION = 1e9 TRILLION = 1e12","title":"ParamNormalize"},{"location":"reference/stoke/utils/#ancestors-in-mro","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/stoke/utils/#class-variables","text":"BILLION MILLION THOUSAND TRILLION name value","title":"Class variables"}]}