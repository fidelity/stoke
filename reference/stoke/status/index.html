
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../../resources/images/stoke_small.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Status - stoke</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-stokestatus" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="stoke" class="md-header__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            stoke
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Status
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Home/" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Installation/" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Quick-Start/" class="md-tabs__link">
      Quick Start
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Launchers/" class="md-tabs__link">
      Launchers
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Examples/" class="md-tabs__link">
      Examples
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../CONTRIBUTING/" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../configs/" class="md-tabs__link md-tabs__link--active">
        Reference
      </a>
    </li>
  

  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="stoke" class="md-nav__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    stoke
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Home/" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Quick-Start/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Launchers/" class="md-nav__link">
        Launchers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Examples/" class="md-nav__link">
        Examples
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_1" type="checkbox" id="__nav_7_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_1">
          Stoke
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stoke" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          Stoke
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../configs/" class="md-nav__link">
        Configs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../distributed/" class="md-nav__link">
        Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../extensions/" class="md-nav__link">
        Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../fp16/" class="md-nav__link">
        Fp16
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../io/" class="md-nav__link">
        Io
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Status
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Status
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#distributedoptions" class="md-nav__link">
    DistributedOptions
  </a>
  
    <nav class="md-nav" aria-label="DistributedOptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp16options" class="md-nav__link">
    FP16Options
  </a>
  
    <nav class="md-nav" aria-label="FP16Options">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_1" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stokestatus" class="md-nav__link">
    StokeStatus
  </a>
  
    <nav class="md-nav" aria-label="StokeStatus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_post_init_values" class="md-nav__link">
    set_post_init_values
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../stoke/" class="md-nav__link">
        Stoke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#distributedoptions" class="md-nav__link">
    DistributedOptions
  </a>
  
    <nav class="md-nav" aria-label="DistributedOptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp16options" class="md-nav__link">
    FP16Options
  </a>
  
    <nav class="md-nav" aria-label="FP16Options">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_1" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stokestatus" class="md-nav__link">
    StokeStatus
  </a>
  
    <nav class="md-nav" aria-label="StokeStatus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_post_init_values" class="md-nav__link">
    set_post_init_values
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/fidelity/stoke/edit/main/reference/stoke/status.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="module-stokestatus">Module stoke.status</h1>
<p>Handles setting the status/state of Stoke</p>
<p>None</p>
<p>??? example "View Source"
        # -<em>- coding: utf-8 -</em>-</p>
<div class="highlight"><pre><span></span><code>    # Copyright FMR LLC &lt;opensource@fidelity.com&gt;

    # SPDX-License-Identifier: Apache-2.0



    &quot;&quot;&quot;Handles setting the status/state of Stoke&quot;&quot;&quot;



    import os

    from enum import Enum

    from typing import List, Optional, Union



    import attr

    import torch



    from stoke.configs import (

        AMPConfig,

        ApexConfig,

        ClipGradConfig,

        ClipGradNormConfig,

        DDPConfig,

        DeepspeedConfig,

        DeepspeedFP16Config,

        FairscaleFSDPConfig,

        FairscaleOSSConfig,

        FairscaleSDDPConfig,

        HorovodConfig,

    )

    from stoke.extensions import _FairscaleFSDPConfig





    class DistributedOptions(Enum):

        &quot;&quot;&quot;Enum that defines the options for Distributed backends&quot;&quot;&quot;



        horovod = &quot;horovod&quot;

        ddp = &quot;ddp&quot;

        deepspeed = &quot;deepspeed&quot;





    class FP16Options(Enum):

        &quot;&quot;&quot;Enum that defines the options for FP16 backends&quot;&quot;&quot;



        apex_O1 = &quot;apex_O1&quot;

        apex_O2 = &quot;apex_O2&quot;

        amp = &quot;amp&quot;

        deepspeed = &quot;deepspeed&quot;





    class _MissingLocalRankException(Exception):

        &quot;&quot;&quot;Custom exception for when local rank cannot be found&quot;&quot;&quot;



        pass





    class StokeStatus:

        &quot;&quot;&quot;Low level stoke object that manages and sets the status of the overall run time configuration



        Based on the set flags this object checks for valid combinations (as there are a lot that will not work together)

        and builds a status object whose attributes are forwarded on via property decorators. Handles managing init of

        backend config objects and any post init modifications.



        Attributes

        ----------

        amp_config

        apex_config

        batch_size

        cuda

        ddp_config

        deepspeed_config

        distributed

        effective_batch_size

        fp16

        fsdp_config

        fully_sharded

        gpu

        grad_accum

        grad_clip

        horovod_config

        is_distributed_ddp

        is_distributed_deepspeed

        is_distributed_horovod

        is_fairscale

        is_fp16_apex

        is_fp16_deepspeed

        nccl

        oss

        oss_config

        sddp_config

        sharded

        status

        zero



        _configs: dict

            dictionary of config objects

        _key_list: list

            valid config objects to check against

        _status: dict

            dictionary that is the current requested state of Stoke



        &quot;&quot;&quot;



        def __init__(

            self,

            batch_size_per_device: int,

            grad_accum: Optional[int],

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]],

            gpu: bool,

            fp16: Optional[FP16Options],

            distributed: Optional[DistributedOptions],

            fairscale_oss: bool,

            fairscale_sddp: bool,

            fairscale_fsdp: bool,

            configs: Optional[

                List[

                    Union[

                        AMPConfig,

                        ApexConfig,

                        DDPConfig,

                        DeepspeedConfig,

                        FairscaleOSSConfig,

                        FairscaleSDDPConfig,

                        FairscaleFSDPConfig,

                        HorovodConfig,

                    ]

                ]

            ],

        ):

            &quot;&quot;&quot;Init for StokeStatus class object



            Parameters

            ----------

            batch_size_per_device: int

                Batch size at the single device level

            grad_accum: Optional[int], default: 1

                Number of gradient accumulation steps

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                Gradient clipping configuration

            gpu: bool, default: False

                flag to use GPU device(s)

            fp16: Optional[FP16Options], default: None

                Choice of mixed-precision backend

            distributed: Optional[DistributedOptions], default: None

                Choice of distributed backend

            fairscale_oss: bool, default: False

                Flag to activate optimizer state sharding using Fairscale

            fairscale_sddp: bool, default: False

                Flag to activate sharded DDP using Fairscale

            fairscale_fsdp: bool, default: False

                Flag to activate fully sharded DDP using Fairscale

            configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig]], default: None

                Configuration objects for runtimes

            &quot;&quot;&quot;

            # Allowed keys for configs

            self._key_list = [

                &quot;AMPConfig&quot;,

                &quot;ApexConfig&quot;,

                &quot;DDPConfig&quot;,

                &quot;DeepspeedConfig&quot;,

                &quot;FairscaleOSSConfig&quot;,

                &quot;FairscaleSDDPConfig&quot;,

                &quot;FairscaleFSDPConfig&quot; &quot;HorovodConfig&quot;,

            ]

            # Set the configs first which allows for checking of some config vars later

            self._configs = self._set_configs(configs=configs)

            # Set simple state vars -- post combo check so validity is fine to set

            self._status = {

                &quot;cuda&quot;: torch.cuda.is_available(),

                &quot;nccl&quot;: torch.distributed.is_nccl_available(),

                &quot;batch_size&quot;: batch_size_per_device,

                &quot;grad_accum&quot;: grad_accum if grad_accum is not None else 1,

                &quot;grad_clip&quot;: grad_clip,

                &quot;gpu&quot;: gpu,

                &quot;distributed&quot;: distributed,

                &quot;zero&quot;: self._configs.get(&quot;DeepspeedConfig&quot;).zero_optimization.stage

                if self._configs.get(&quot;DeepspeedConfig&quot;)

                else None,

                &quot;oss&quot;: fairscale_oss,

                &quot;sharded&quot;: fairscale_sddp,

                &quot;fully_sharded&quot;: fairscale_fsdp,

                &quot;world_size&quot;: -1,

            }

            # Check fp16 since it might need APEX imports and update state dict

            self._status.update({&quot;fp16&quot;: self._set_fp16(fp16=fp16)})

            # Check all the invalid combinations

            self._check_all_raised_combinations()



        def _check_all_raised_combinations(self):

            &quot;&quot;&quot;Checks all acceptable/restricted combinations and raises exceptions for any invalid combinations



            README.md should have a table of acceptable combinations



            Returns

            -------

            None



            &quot;&quot;&quot;

            # No gpu if no CUDA

            if self.gpu and not self.cuda:

                raise ValueError(&quot;Stoke -- GPU(s) cannot be used as CUDA is not available&quot;)

            # No fairscale and deepspeed

            if self.is_fairscale and (

                self.is_distributed_deepspeed or self.is_fp16_deepspeed

            ):

                raise ValueError(

                    f&quot;Stoke -- Cannot use both fairscale extensions &quot;

                    f&quot;(currently: oss: {self.oss}, sddp: {self.sharded}) &quot;

                    f&quot;and deepspeed (currently: distributed: {self.is_distributed_deepspeed}, &quot;

                    f&quot;fp16: {self.is_fp16_deepspeed})&quot;

                )

            # No Distributed without gpu, cuda, and nccl

            if (

                not self.cuda or not self.gpu or not self.nccl

            ) and self.distributed is not None:

                raise ValueError(

                    f&quot;Stoke -- Distributed requires CUDA (currently: {self.cuda}), GPU (currently: {self.gpu}), &quot;

                    f&quot;and NCCL (currently: {self.nccl})&quot;

                )

            # No FP16 without CUDA

            if not self.cuda and (self.fp16 is not None):

                raise ValueError(f&quot;Stoke -- FP16 training requires CUDA availability&quot;)

            # No fairscale without gpu, cuda, and nccl and DDP (will catch Horovod)

            if (

                not self.cuda

                or not self.gpu

                or not self.nccl

                or not self.is_distributed_ddp

            ) and self.is_fairscale:

                raise ValueError(

                    f&quot;Stoke -- Fairscale extensions (currently: oss: {self.oss}, sddp: {self.sharded}) &quot;

                    f&quot;requires CUDA (currently: {self.cuda}), &quot;

                    f&quot;GPU (currently: {self.gpu}), &quot;

                    f&quot;DDP (currently: {self.is_distributed_ddp}) and NCCL (currently: {self.nccl})&quot;

                )

            # No SDDP w/o OSS

            if self.sharded and not self.oss:

                raise ValueError(

                    f&quot;Stoke -- Fairscale SDDP requires OSS (currently: oss: {self.oss}, sddp: {self.sharded})&quot;

                )

            # FSDP stands alone

            if (self.sharded or self.oss) and self.fully_sharded:

                raise ValueError(

                    f&quot;Stoke -- Fairscale FSDP does not require SDDP or OSS as it manages OSS itself&quot;

                    f&quot;(currently: oss: {self.oss}, sddp: {self.sharded}. fsdp: {self.fully_sharded})&quot;

                )

            # No fairscale with APEX

            if self.is_fairscale and self.is_fp16_apex:

                raise ValueError(

                    f&quot;Stoke -- Fairscale does not currently support APEX (currently: {self.is_fp16_apex}) &quot;

                    f&quot;for mixed precision&quot;

                )

            # No fairscale oss with grad clip by value

            if (self.oss or self.fully_sharded) and isinstance(

                self.grad_clip, ClipGradConfig

            ):

                raise ValueError(

                    f&quot;Stoke -- Fairscale OSS and FSDP do not currently support torch.nn.utils.clip_grad_value_ &quot;

                    f&quot;(currently: {type(self.grad_clip).__name__})&quot;

                )

            # No deepspeed FP16 without deepspeed distributed

            if self.is_fp16_deepspeed and not self.is_distributed_deepspeed:

                raise ValueError(

                    f&quot;Stoke -- Deepspeed FP16 (currently: {self.is_fp16_deepspeed}) requires the use of &quot;

                    f&quot;Deepspeed distributed (currently: {self.is_distributed_deepspeed})&quot;

                )

            # No other FP16 with deepspeed distributed

            if (

                self.is_distributed_deepspeed

                and self.fp16 is not None

                and not self.is_fp16_deepspeed

            ):

                raise ValueError(

                    f&quot;Stoke -- Deepspeed distributed (currently: {self.is_distributed_deepspeed}) only &quot;

                    f&quot;supports its own internal FP16 implementation (currently: {self.fp16})&quot;

                )

            # No zero &gt; 0 without deepspeed FP16

            if (

                self.is_distributed_deepspeed

                and self.zero &gt; 0

                and not self.is_fp16_deepspeed

            ):

                raise ValueError(

                    f&quot;Stoke -- Deepspeed ZeRO extension (currently: Stage-{self.zero}) requires Deepspeed&quot;

                    f&quot;FP16 extension (currently: {self.is_fp16_deepspeed})&quot;

                )



        def _set_fp16(self, fp16: Optional[FP16Options]):

            &quot;&quot;&quot;Sets the state of the FP16 backend



            Seeing as the APEX install is not packaged currently with Stoke (or if it is requires building some things from

            source it&#39;s liable to fail). Handling it this way allows Stoke not to break if APEX isn&#39;t installed correctly



            Parameters

            ----------

            fp16: FP16Options, optional

                Enum that defines the options for FP16 backends



            Returns

            -------

            FP16Options or None



            &quot;&quot;&quot;

            if self._status.get(&quot;cuda&quot;) and (fp16 is not None):

                if fp16 == &quot;apex_O1&quot; or fp16 == &quot;apex_O2&quot;:

                    # Try/Except the apex import to see if it&#39;s available

                    try:

                        from apex import amp

                    except ImportError as e:

                        print(

                            e,

                            &quot;: Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                        )

                return fp16

            else:

                return None



        def _set_configs(self, configs):

            &quot;&quot;&quot;Determines which configs were set from user input and sets all others to None



            Parameters

            ----------

            configs: list

                List of any user specified run time configs



            Returns

            -------

            config_dict: dict or None

                dictionary of config objects or None



            &quot;&quot;&quot;

            # Set those that are specified within a dict

            if configs is not None:

                config_dict = {type(val).__name__: val for val in configs}

            else:

                config_dict = {}

            # Set those missing within the existing config dict to None so property accessors work correctly

            none_dict = {val: None for val in self._key_list if val not in config_dict}

            config_dict.update(none_dict)

            return config_dict



        def set_post_init_values(self, world_size: int):

            &quot;&quot;&quot;Sets post-init values that cannot be set prior to run-time instantiation



            Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet



            Parameters

            ----------

            world_size: int

                current distributed world size



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._status.update({&quot;world_size&quot;: world_size})



        @property

        def status(self):

            &quot;&quot;&quot;Shortcut to status dict&quot;&quot;&quot;

            return self._status



        @property

        def batch_size(self):

            &quot;&quot;&quot;Shortcut to batch size&quot;&quot;&quot;

            return self._status.get(&quot;batch_size&quot;)



        @property

        def effective_batch_size(self):

            &quot;&quot;&quot;Shortcut to effective batch size&quot;&quot;&quot;

            return self.batch_size * self.grad_accum * self._status.get(&quot;world_size&quot;)



        @property

        def grad_clip(self):

            &quot;&quot;&quot;Shortcut to get grad clip&quot;&quot;&quot;

            return self._status.get(&quot;grad_clip&quot;)



        @property

        def grad_accum(self):

            &quot;&quot;&quot;Shortcut to get grad accumulation&quot;&quot;&quot;

            return self._status.get(&quot;grad_accum&quot;)



        @property

        def gpu(self):

            &quot;&quot;&quot;Shortcut to get GPU status&quot;&quot;&quot;

            return self._status.get(&quot;gpu&quot;)



        @property

        def cuda(self):

            &quot;&quot;&quot;Shortcut to get cuda status&quot;&quot;&quot;

            return self._status.get(&quot;cuda&quot;)



        @property

        def nccl(self):

            &quot;&quot;&quot;Shortcut to get nccl status&quot;&quot;&quot;

            return self._status.get(&quot;nccl&quot;)



        @property

        def fp16(self):

            &quot;&quot;&quot;Shortcut to get FP16 status&quot;&quot;&quot;

            return self._status.get(&quot;fp16&quot;)



        @property

        def is_fp16_apex(self):

            &quot;&quot;&quot;Returns if APEX is activated&quot;&quot;&quot;

            return self.fp16 == &quot;apex_O1&quot; or self.fp16 == &quot;apex_O2&quot;



        @property

        def is_fp16_amp(self):

            &quot;&quot;&quot;Returns if AMP is activated&quot;&quot;&quot;

            return self.fp16 == &quot;amp&quot;



        @property

        def is_fp16_deepspeed(self):

            &quot;&quot;&quot;Returns if Deepspeed FP16 is activated&quot;&quot;&quot;

            return self.fp16 == &quot;deepspeed&quot;



        @property

        def oss(self):

            &quot;&quot;&quot;Returns if Fairscale optimizer state sharding status&quot;&quot;&quot;

            return self._status.get(&quot;oss&quot;)



        @property

        def sharded(self):

            &quot;&quot;&quot;Returns if Fairscale sharded DDP status&quot;&quot;&quot;

            return self._status.get(&quot;sharded&quot;)



        @property

        def fully_sharded(self):

            &quot;&quot;&quot;Returns if Fairscale fully sharded DDP status&quot;&quot;&quot;

            return self._status.get(&quot;fully_sharded&quot;)



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns the current world size&quot;&quot;&quot;

            return self._status.get(&quot;world_size&quot;)



        @property

        def zero(self):

            &quot;&quot;&quot;Returns what stage of ZeRO Deepspeed is using&quot;&quot;&quot;

            return self._status.get(&quot;zero&quot;)



        @property

        def is_fairscale(self):

            &quot;&quot;&quot;Returns if any part of Fairscale is activated&quot;&quot;&quot;

            return self.oss or self.sharded or self.fully_sharded



        @property

        def distributed(self):

            &quot;&quot;&quot;Shortcut to distributed setting&quot;&quot;&quot;

            return self._status.get(&quot;distributed&quot;)



        @property

        def is_distributed_deepspeed(self):

            &quot;&quot;&quot;Returns if Deepspeed is activated&quot;&quot;&quot;

            return self.distributed == &quot;deepspeed&quot;



        @property

        def is_distributed_ddp(self):

            &quot;&quot;&quot;Returns if DDP is activated&quot;&quot;&quot;

            return self.distributed == &quot;ddp&quot;



        @property

        def is_distributed_horovod(self):

            &quot;&quot;&quot;Returns if Horovod is activated&quot;&quot;&quot;

            return self.distributed == &quot;horovod&quot;



        @property

        def apex_config(self):

            &quot;&quot;&quot;Checks for user defined ApexConfig and/or sets a default config object



            Returns

            -------

            ApexConfig

                User set ApexConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;ApexConfig&quot;)

            return config if config is not None else ApexConfig()



        @property

        def amp_config(self):

            &quot;&quot;&quot;Checks for user defined AMPConfig and/or sets a default config object



            Returns

            -------

            AMPConfig

                User set AMPConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;AMPConfig&quot;)

            return config if config is not None else AMPConfig()



        @property

        def ddp_config(self):

            &quot;&quot;&quot;Checks for user defined DDPConfig and/or sets a default config object



            Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it

            https://pytorch.org/docs/stable/distributed.html#launch-utility



            Returns

            -------

            DDPConfig

                User set DDPConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;DDPConfig&quot;)

            # Here need to check if the config passed through defined the local rank or not...

            # Assuming that it&#39;s being caught from the arg parser... if not try and grab it from

            # the env (set from the launcher)

            if config is not None and config.local_rank is None:

                try:

                    local_rank = int(os.environ[&quot;LOCAL_RANK&quot;])

                except _MissingLocalRankException:

                    raise _MissingLocalRankException(

                        f&quot;Stoke -- Device local rank must be defined within the DDPConfig &quot;

                        f&quot; (handled by parsing --local_arg from the torch.distributed.launch &quot;

                        f&quot;command) or defined as env variable LOCAL_RANK (handled by calling &quot;

                        f&quot;torch.distributed.launch with the --use_env flag)&quot;

                    )

                # Evolve the config if grabbing from the env variable

                config = attr.evolve(config, local_rank=local_rank)

            elif config is None:

                try:

                    local_rank = int(os.environ[&quot;LOCAL_RANK&quot;])

                except _MissingLocalRankException:

                    raise _MissingLocalRankException(

                        f&quot;Stoke -- Device local rank must be defined within the DDPConfig &quot;

                        f&quot; (handled by parsing --local_arg from the torch.distributed.launch &quot;

                        f&quot;command) or defined as env variable LOCAL_RANK (handled by calling &quot;

                        f&quot;torch.distributed.launch with the --use_env flag)&quot;

                    )

                # Set a default config with the local rank from the env

                config = DDPConfig(local_rank=local_rank)

            return config



        @property

        def deepspeed_config(self):

            &quot;&quot;&quot;Checks for user defined DeepspeedConfig and/or sets a default config object



            Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object

            like AMP or APEX



            Returns

            -------

            DeepspeedConfig

                User set DeepspeedConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;DeepspeedConfig&quot;)

            # Deepspeed only has a single config so FP16 needs to be handled here based on the status flag if no config

            # is passed through

            # Fall back to basics of both if no config

            if self.fp16 == &quot;deepspeed&quot; and config is None:

                config = DeepspeedConfig(fp16=DeepspeedFP16Config())

            # Fall back to defaults if a config is passed but the FP16 Config wasn&#39;t set

            elif self.fp16 == &quot;deepspeed&quot; and config is not None and config.fp16 is None:

                config = attr.evolve(config, fp16=DeepspeedFP16Config())

            # Fall back to hard defaults if just using distributed

            elif config is None:

                config = DeepspeedConfig()

            else:

                config = config

            return config



        @property

        def oss_config(self):

            &quot;&quot;&quot;Checks for user defined FairscaleOSSConfig and/or sets a default config object



            Returns

            -------

            FairscaleOSSConfig

                User set FairscaleOSSConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;FairscaleOSSConfig&quot;)

            return config if config is not None else FairscaleOSSConfig()



        @property

        def sddp_config(self):

            &quot;&quot;&quot;Checks for user defined FairscaleSDDPConfig and/or sets a default config object



            Returns

            -------

            FairscaleSDDPConfig

                User set FairscaleSDDPConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;FairscaleSDDPConfig&quot;)

            return config if config is not None else FairscaleSDDPConfig()



        @property

        def fsdp_config(self):

            &quot;&quot;&quot;Checks for user defined FairscaleFSDPConfig and/or sets a default config object



            Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings



            Returns

            -------

            FairscaleFSDPConfig mutated with mixed-precision state



            &quot;&quot;&quot;

            config = self._configs.get(&quot;FairscaleFSDPConfig&quot;)

            # Swap in a default config if none

            if config is None:

                config = FairscaleFSDPConfig()

            # Handle FP16 settings if set via constructor -- these need to be morphed at runtime to a new attr class

            config_dict = attr.asdict(config)

            config_dict.update({&quot;mixed_precision&quot;: self.is_fp16_amp})

            return _FairscaleFSDPConfig(**config_dict)



        @property

        def horovod_config(self):

            &quot;&quot;&quot;Checks for user defined HorovodConfig and/or sets a default config object



            Returns

            -------

            HorovodConfig

                User set HorovodConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;HorovodConfig&quot;)

            return config if config is not None else HorovodConfig()



        def __repr__(self):

            &quot;&quot;&quot;Formats the status for pretty printing



            Returns

            -------

            str

                pretty formatted status string



            &quot;&quot;&quot;

            return (

                f&quot;STOKE STATE:\n&quot;

                f&quot;    CUDA AVAILABLE: {self.cuda}\n&quot;

                f&quot;    NCCL AVAILABLE: {self.nccl}\n&quot;

                f&quot;    GPU FLAG: {self.gpu}\n&quot;

                f&quot;    FP16 FLAG: {self.fp16}\n&quot;

                f&quot;    DISTRIBUTED BACKEND: {self.distributed}\n&quot;

                f&quot;    FAIRSCALE OSS: {self.oss}\n&quot;

                f&quot;    FAIRSCALE SDDP: {self.sharded}\n&quot;

                f&quot;    FAIRSCALE FSDP: {self.fully_sharded}\n&quot;

                f&#39;    DEEPSPEED ZeRO: {f&quot;Stage {self.zero}&quot; if self.is_distributed_deepspeed else f&quot;False&quot;}\n&#39;

                f&quot;    WORLD SIZE: {self.world_size}\n&quot;

                f&quot;    GRAD ACCUMULATION STEPS: {self.grad_accum}\n&quot;

                f&quot;    BATCH SIZE (PER DEVICE): {self.batch_size}\n&quot;

                f&quot;    EFFECTIVE BATCH SIZE (ALL DEVICES): {self.effective_batch_size}\n&quot;

                f&#39;    GRAD CLIP: ({&quot;, &quot;.join(f&quot;{k}: {v}&quot; for k, v in attr.asdict(self.grad_clip).items()) if self.grad_clip is not None else &quot;None&quot;})&#39;

            )
</code></pre></div>
<h2 id="classes">Classes</h2>
<h3 id="distributedoptions">DistributedOptions</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DistributedOptions</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class DistributedOptions(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Enum that defines the options for Distributed backends&quot;&quot;&quot;



        horovod = &quot;horovod&quot;

        ddp = &quot;ddp&quot;

        deepspeed = &quot;deepspeed&quot;
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">ddp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">horovod</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="fp16options">FP16Options</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FP16Options</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class FP16Options(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Enum that defines the options for FP16 backends&quot;&quot;&quot;



        apex_O1 = &quot;apex_O1&quot;

        apex_O2 = &quot;apex_O2&quot;

        amp = &quot;amp&quot;

        deepspeed = &quot;deepspeed&quot;
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables_1">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">amp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">apex_O1</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">apex_O2</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="stokestatus">StokeStatus</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">StokeStatus</span><span class="p">(</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">gpu</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fp16</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">status</span><span class="o">.</span><span class="n">FP16Options</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">distributed</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">status</span><span class="o">.</span><span class="n">DistributedOptions</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">fairscale_oss</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fairscale_sddp</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fairscale_fsdp</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">configs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">AMPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ApexConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleOSSConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleSDDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleFSDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">HorovodConfig</span><span class="p">]],</span> <span class="n">NoneType</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>amp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>apex_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>batch_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>cuda</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>ddp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>distributed</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>effective_batch_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fp16</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fsdp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fully_sharded</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>gpu</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>grad_clip</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>horovod_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_distributed_ddp</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_distributed_deepspeed</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_distributed_horovod</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_fairscale</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_fp16_apex</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_fp16_deepspeed</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>nccl</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>oss_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>sddp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>sharded</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>status</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>zero</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_configs</td>
<td>dict</td>
<td>dictionary of config objects</td>
<td>None</td>
</tr>
<tr>
<td>_key_list</td>
<td>list</td>
<td>valid config objects to check against</td>
<td>None</td>
</tr>
<tr>
<td>_status</td>
<td>dict</td>
<td>dictionary that is the current requested state of Stoke</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class StokeStatus:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Low level stoke object that manages and sets the status of the overall run time configuration



        Based on the set flags this object checks for valid combinations (as there are a lot that will not work together)

        and builds a status object whose attributes are forwarded on via property decorators. Handles managing init of

        backend config objects and any post init modifications.



        Attributes

        ----------

        amp_config

        apex_config

        batch_size

        cuda

        ddp_config

        deepspeed_config

        distributed

        effective_batch_size

        fp16

        fsdp_config

        fully_sharded

        gpu

        grad_accum

        grad_clip

        horovod_config

        is_distributed_ddp

        is_distributed_deepspeed

        is_distributed_horovod

        is_fairscale

        is_fp16_apex

        is_fp16_deepspeed

        nccl

        oss

        oss_config

        sddp_config

        sharded

        status

        zero



        _configs: dict

            dictionary of config objects

        _key_list: list

            valid config objects to check against

        _status: dict

            dictionary that is the current requested state of Stoke



        &quot;&quot;&quot;



        def __init__(

            self,

            batch_size_per_device: int,

            grad_accum: Optional[int],

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]],

            gpu: bool,

            fp16: Optional[FP16Options],

            distributed: Optional[DistributedOptions],

            fairscale_oss: bool,

            fairscale_sddp: bool,

            fairscale_fsdp: bool,

            configs: Optional[

                List[

                    Union[

                        AMPConfig,

                        ApexConfig,

                        DDPConfig,

                        DeepspeedConfig,

                        FairscaleOSSConfig,

                        FairscaleSDDPConfig,

                        FairscaleFSDPConfig,

                        HorovodConfig,

                    ]

                ]

            ],

        ):

            &quot;&quot;&quot;Init for StokeStatus class object



            Parameters

            ----------

            batch_size_per_device: int

                Batch size at the single device level

            grad_accum: Optional[int], default: 1

                Number of gradient accumulation steps

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                Gradient clipping configuration

            gpu: bool, default: False

                flag to use GPU device(s)

            fp16: Optional[FP16Options], default: None

                Choice of mixed-precision backend

            distributed: Optional[DistributedOptions], default: None

                Choice of distributed backend

            fairscale_oss: bool, default: False

                Flag to activate optimizer state sharding using Fairscale

            fairscale_sddp: bool, default: False

                Flag to activate sharded DDP using Fairscale

            fairscale_fsdp: bool, default: False

                Flag to activate fully sharded DDP using Fairscale

            configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, HorovodConfig]], default: None

                Configuration objects for runtimes

            &quot;&quot;&quot;

            # Allowed keys for configs

            self._key_list = [

                &quot;AMPConfig&quot;,

                &quot;ApexConfig&quot;,

                &quot;DDPConfig&quot;,

                &quot;DeepspeedConfig&quot;,

                &quot;FairscaleOSSConfig&quot;,

                &quot;FairscaleSDDPConfig&quot;,

                &quot;FairscaleFSDPConfig&quot; &quot;HorovodConfig&quot;,

            ]

            # Set the configs first which allows for checking of some config vars later

            self._configs = self._set_configs(configs=configs)

            # Set simple state vars -- post combo check so validity is fine to set

            self._status = {

                &quot;cuda&quot;: torch.cuda.is_available(),

                &quot;nccl&quot;: torch.distributed.is_nccl_available(),

                &quot;batch_size&quot;: batch_size_per_device,

                &quot;grad_accum&quot;: grad_accum if grad_accum is not None else 1,

                &quot;grad_clip&quot;: grad_clip,

                &quot;gpu&quot;: gpu,

                &quot;distributed&quot;: distributed,

                &quot;zero&quot;: self._configs.get(&quot;DeepspeedConfig&quot;).zero_optimization.stage

                if self._configs.get(&quot;DeepspeedConfig&quot;)

                else None,

                &quot;oss&quot;: fairscale_oss,

                &quot;sharded&quot;: fairscale_sddp,

                &quot;fully_sharded&quot;: fairscale_fsdp,

                &quot;world_size&quot;: -1,

            }

            # Check fp16 since it might need APEX imports and update state dict

            self._status.update({&quot;fp16&quot;: self._set_fp16(fp16=fp16)})

            # Check all the invalid combinations

            self._check_all_raised_combinations()



        def _check_all_raised_combinations(self):

            &quot;&quot;&quot;Checks all acceptable/restricted combinations and raises exceptions for any invalid combinations



            README.md should have a table of acceptable combinations



            Returns

            -------

            None



            &quot;&quot;&quot;

            # No gpu if no CUDA

            if self.gpu and not self.cuda:

                raise ValueError(&quot;Stoke -- GPU(s) cannot be used as CUDA is not available&quot;)

            # No fairscale and deepspeed

            if self.is_fairscale and (

                self.is_distributed_deepspeed or self.is_fp16_deepspeed

            ):

                raise ValueError(

                    f&quot;Stoke -- Cannot use both fairscale extensions &quot;

                    f&quot;(currently: oss: {self.oss}, sddp: {self.sharded}) &quot;

                    f&quot;and deepspeed (currently: distributed: {self.is_distributed_deepspeed}, &quot;

                    f&quot;fp16: {self.is_fp16_deepspeed})&quot;

                )

            # No Distributed without gpu, cuda, and nccl

            if (

                not self.cuda or not self.gpu or not self.nccl

            ) and self.distributed is not None:

                raise ValueError(

                    f&quot;Stoke -- Distributed requires CUDA (currently: {self.cuda}), GPU (currently: {self.gpu}), &quot;

                    f&quot;and NCCL (currently: {self.nccl})&quot;

                )

            # No FP16 without CUDA

            if not self.cuda and (self.fp16 is not None):

                raise ValueError(f&quot;Stoke -- FP16 training requires CUDA availability&quot;)

            # No fairscale without gpu, cuda, and nccl and DDP (will catch Horovod)

            if (

                not self.cuda

                or not self.gpu

                or not self.nccl

                or not self.is_distributed_ddp

            ) and self.is_fairscale:

                raise ValueError(

                    f&quot;Stoke -- Fairscale extensions (currently: oss: {self.oss}, sddp: {self.sharded}) &quot;

                    f&quot;requires CUDA (currently: {self.cuda}), &quot;

                    f&quot;GPU (currently: {self.gpu}), &quot;

                    f&quot;DDP (currently: {self.is_distributed_ddp}) and NCCL (currently: {self.nccl})&quot;

                )

            # No SDDP w/o OSS

            if self.sharded and not self.oss:

                raise ValueError(

                    f&quot;Stoke -- Fairscale SDDP requires OSS (currently: oss: {self.oss}, sddp: {self.sharded})&quot;

                )

            # FSDP stands alone

            if (self.sharded or self.oss) and self.fully_sharded:

                raise ValueError(

                    f&quot;Stoke -- Fairscale FSDP does not require SDDP or OSS as it manages OSS itself&quot;

                    f&quot;(currently: oss: {self.oss}, sddp: {self.sharded}. fsdp: {self.fully_sharded})&quot;

                )

            # No fairscale with APEX

            if self.is_fairscale and self.is_fp16_apex:

                raise ValueError(

                    f&quot;Stoke -- Fairscale does not currently support APEX (currently: {self.is_fp16_apex}) &quot;

                    f&quot;for mixed precision&quot;

                )

            # No fairscale oss with grad clip by value

            if (self.oss or self.fully_sharded) and isinstance(

                self.grad_clip, ClipGradConfig

            ):

                raise ValueError(

                    f&quot;Stoke -- Fairscale OSS and FSDP do not currently support torch.nn.utils.clip_grad_value_ &quot;

                    f&quot;(currently: {type(self.grad_clip).__name__})&quot;

                )

            # No deepspeed FP16 without deepspeed distributed

            if self.is_fp16_deepspeed and not self.is_distributed_deepspeed:

                raise ValueError(

                    f&quot;Stoke -- Deepspeed FP16 (currently: {self.is_fp16_deepspeed}) requires the use of &quot;

                    f&quot;Deepspeed distributed (currently: {self.is_distributed_deepspeed})&quot;

                )

            # No other FP16 with deepspeed distributed

            if (

                self.is_distributed_deepspeed

                and self.fp16 is not None

                and not self.is_fp16_deepspeed

            ):

                raise ValueError(

                    f&quot;Stoke -- Deepspeed distributed (currently: {self.is_distributed_deepspeed}) only &quot;

                    f&quot;supports its own internal FP16 implementation (currently: {self.fp16})&quot;

                )

            # No zero &gt; 0 without deepspeed FP16

            if (

                self.is_distributed_deepspeed

                and self.zero &gt; 0

                and not self.is_fp16_deepspeed

            ):

                raise ValueError(

                    f&quot;Stoke -- Deepspeed ZeRO extension (currently: Stage-{self.zero}) requires Deepspeed&quot;

                    f&quot;FP16 extension (currently: {self.is_fp16_deepspeed})&quot;

                )



        def _set_fp16(self, fp16: Optional[FP16Options]):

            &quot;&quot;&quot;Sets the state of the FP16 backend



            Seeing as the APEX install is not packaged currently with Stoke (or if it is requires building some things from

            source it&#39;s liable to fail). Handling it this way allows Stoke not to break if APEX isn&#39;t installed correctly



            Parameters

            ----------

            fp16: FP16Options, optional

                Enum that defines the options for FP16 backends



            Returns

            -------

            FP16Options or None



            &quot;&quot;&quot;

            if self._status.get(&quot;cuda&quot;) and (fp16 is not None):

                if fp16 == &quot;apex_O1&quot; or fp16 == &quot;apex_O2&quot;:

                    # Try/Except the apex import to see if it&#39;s available

                    try:

                        from apex import amp

                    except ImportError as e:

                        print(

                            e,

                            &quot;: Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                        )

                return fp16

            else:

                return None



        def _set_configs(self, configs):

            &quot;&quot;&quot;Determines which configs were set from user input and sets all others to None



            Parameters

            ----------

            configs: list

                List of any user specified run time configs



            Returns

            -------

            config_dict: dict or None

                dictionary of config objects or None



            &quot;&quot;&quot;

            # Set those that are specified within a dict

            if configs is not None:

                config_dict = {type(val).__name__: val for val in configs}

            else:

                config_dict = {}

            # Set those missing within the existing config dict to None so property accessors work correctly

            none_dict = {val: None for val in self._key_list if val not in config_dict}

            config_dict.update(none_dict)

            return config_dict



        def set_post_init_values(self, world_size: int):

            &quot;&quot;&quot;Sets post-init values that cannot be set prior to run-time instantiation



            Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet



            Parameters

            ----------

            world_size: int

                current distributed world size



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._status.update({&quot;world_size&quot;: world_size})



        @property

        def status(self):

            &quot;&quot;&quot;Shortcut to status dict&quot;&quot;&quot;

            return self._status



        @property

        def batch_size(self):

            &quot;&quot;&quot;Shortcut to batch size&quot;&quot;&quot;

            return self._status.get(&quot;batch_size&quot;)



        @property

        def effective_batch_size(self):

            &quot;&quot;&quot;Shortcut to effective batch size&quot;&quot;&quot;

            return self.batch_size * self.grad_accum * self._status.get(&quot;world_size&quot;)



        @property

        def grad_clip(self):

            &quot;&quot;&quot;Shortcut to get grad clip&quot;&quot;&quot;

            return self._status.get(&quot;grad_clip&quot;)



        @property

        def grad_accum(self):

            &quot;&quot;&quot;Shortcut to get grad accumulation&quot;&quot;&quot;

            return self._status.get(&quot;grad_accum&quot;)



        @property

        def gpu(self):

            &quot;&quot;&quot;Shortcut to get GPU status&quot;&quot;&quot;

            return self._status.get(&quot;gpu&quot;)



        @property

        def cuda(self):

            &quot;&quot;&quot;Shortcut to get cuda status&quot;&quot;&quot;

            return self._status.get(&quot;cuda&quot;)



        @property

        def nccl(self):

            &quot;&quot;&quot;Shortcut to get nccl status&quot;&quot;&quot;

            return self._status.get(&quot;nccl&quot;)



        @property

        def fp16(self):

            &quot;&quot;&quot;Shortcut to get FP16 status&quot;&quot;&quot;

            return self._status.get(&quot;fp16&quot;)



        @property

        def is_fp16_apex(self):

            &quot;&quot;&quot;Returns if APEX is activated&quot;&quot;&quot;

            return self.fp16 == &quot;apex_O1&quot; or self.fp16 == &quot;apex_O2&quot;



        @property

        def is_fp16_amp(self):

            &quot;&quot;&quot;Returns if AMP is activated&quot;&quot;&quot;

            return self.fp16 == &quot;amp&quot;



        @property

        def is_fp16_deepspeed(self):

            &quot;&quot;&quot;Returns if Deepspeed FP16 is activated&quot;&quot;&quot;

            return self.fp16 == &quot;deepspeed&quot;



        @property

        def oss(self):

            &quot;&quot;&quot;Returns if Fairscale optimizer state sharding status&quot;&quot;&quot;

            return self._status.get(&quot;oss&quot;)



        @property

        def sharded(self):

            &quot;&quot;&quot;Returns if Fairscale sharded DDP status&quot;&quot;&quot;

            return self._status.get(&quot;sharded&quot;)



        @property

        def fully_sharded(self):

            &quot;&quot;&quot;Returns if Fairscale fully sharded DDP status&quot;&quot;&quot;

            return self._status.get(&quot;fully_sharded&quot;)



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns the current world size&quot;&quot;&quot;

            return self._status.get(&quot;world_size&quot;)



        @property

        def zero(self):

            &quot;&quot;&quot;Returns what stage of ZeRO Deepspeed is using&quot;&quot;&quot;

            return self._status.get(&quot;zero&quot;)



        @property

        def is_fairscale(self):

            &quot;&quot;&quot;Returns if any part of Fairscale is activated&quot;&quot;&quot;

            return self.oss or self.sharded or self.fully_sharded



        @property

        def distributed(self):

            &quot;&quot;&quot;Shortcut to distributed setting&quot;&quot;&quot;

            return self._status.get(&quot;distributed&quot;)



        @property

        def is_distributed_deepspeed(self):

            &quot;&quot;&quot;Returns if Deepspeed is activated&quot;&quot;&quot;

            return self.distributed == &quot;deepspeed&quot;



        @property

        def is_distributed_ddp(self):

            &quot;&quot;&quot;Returns if DDP is activated&quot;&quot;&quot;

            return self.distributed == &quot;ddp&quot;



        @property

        def is_distributed_horovod(self):

            &quot;&quot;&quot;Returns if Horovod is activated&quot;&quot;&quot;

            return self.distributed == &quot;horovod&quot;



        @property

        def apex_config(self):

            &quot;&quot;&quot;Checks for user defined ApexConfig and/or sets a default config object



            Returns

            -------

            ApexConfig

                User set ApexConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;ApexConfig&quot;)

            return config if config is not None else ApexConfig()



        @property

        def amp_config(self):

            &quot;&quot;&quot;Checks for user defined AMPConfig and/or sets a default config object



            Returns

            -------

            AMPConfig

                User set AMPConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;AMPConfig&quot;)

            return config if config is not None else AMPConfig()



        @property

        def ddp_config(self):

            &quot;&quot;&quot;Checks for user defined DDPConfig and/or sets a default config object



            Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it

            https://pytorch.org/docs/stable/distributed.html#launch-utility



            Returns

            -------

            DDPConfig

                User set DDPConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;DDPConfig&quot;)

            # Here need to check if the config passed through defined the local rank or not...

            # Assuming that it&#39;s being caught from the arg parser... if not try and grab it from

            # the env (set from the launcher)

            if config is not None and config.local_rank is None:

                try:

                    local_rank = int(os.environ[&quot;LOCAL_RANK&quot;])

                except _MissingLocalRankException:

                    raise _MissingLocalRankException(

                        f&quot;Stoke -- Device local rank must be defined within the DDPConfig &quot;

                        f&quot; (handled by parsing --local_arg from the torch.distributed.launch &quot;

                        f&quot;command) or defined as env variable LOCAL_RANK (handled by calling &quot;

                        f&quot;torch.distributed.launch with the --use_env flag)&quot;

                    )

                # Evolve the config if grabbing from the env variable

                config = attr.evolve(config, local_rank=local_rank)

            elif config is None:

                try:

                    local_rank = int(os.environ[&quot;LOCAL_RANK&quot;])

                except _MissingLocalRankException:

                    raise _MissingLocalRankException(

                        f&quot;Stoke -- Device local rank must be defined within the DDPConfig &quot;

                        f&quot; (handled by parsing --local_arg from the torch.distributed.launch &quot;

                        f&quot;command) or defined as env variable LOCAL_RANK (handled by calling &quot;

                        f&quot;torch.distributed.launch with the --use_env flag)&quot;

                    )

                # Set a default config with the local rank from the env

                config = DDPConfig(local_rank=local_rank)

            return config



        @property

        def deepspeed_config(self):

            &quot;&quot;&quot;Checks for user defined DeepspeedConfig and/or sets a default config object



            Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object

            like AMP or APEX



            Returns

            -------

            DeepspeedConfig

                User set DeepspeedConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;DeepspeedConfig&quot;)

            # Deepspeed only has a single config so FP16 needs to be handled here based on the status flag if no config

            # is passed through

            # Fall back to basics of both if no config

            if self.fp16 == &quot;deepspeed&quot; and config is None:

                config = DeepspeedConfig(fp16=DeepspeedFP16Config())

            # Fall back to defaults if a config is passed but the FP16 Config wasn&#39;t set

            elif self.fp16 == &quot;deepspeed&quot; and config is not None and config.fp16 is None:

                config = attr.evolve(config, fp16=DeepspeedFP16Config())

            # Fall back to hard defaults if just using distributed

            elif config is None:

                config = DeepspeedConfig()

            else:

                config = config

            return config



        @property

        def oss_config(self):

            &quot;&quot;&quot;Checks for user defined FairscaleOSSConfig and/or sets a default config object



            Returns

            -------

            FairscaleOSSConfig

                User set FairscaleOSSConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;FairscaleOSSConfig&quot;)

            return config if config is not None else FairscaleOSSConfig()



        @property

        def sddp_config(self):

            &quot;&quot;&quot;Checks for user defined FairscaleSDDPConfig and/or sets a default config object



            Returns

            -------

            FairscaleSDDPConfig

                User set FairscaleSDDPConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;FairscaleSDDPConfig&quot;)

            return config if config is not None else FairscaleSDDPConfig()



        @property

        def fsdp_config(self):

            &quot;&quot;&quot;Checks for user defined FairscaleFSDPConfig and/or sets a default config object



            Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings



            Returns

            -------

            FairscaleFSDPConfig mutated with mixed-precision state



            &quot;&quot;&quot;

            config = self._configs.get(&quot;FairscaleFSDPConfig&quot;)

            # Swap in a default config if none

            if config is None:

                config = FairscaleFSDPConfig()

            # Handle FP16 settings if set via constructor -- these need to be morphed at runtime to a new attr class

            config_dict = attr.asdict(config)

            config_dict.update({&quot;mixed_precision&quot;: self.is_fp16_amp})

            return _FairscaleFSDPConfig(**config_dict)



        @property

        def horovod_config(self):

            &quot;&quot;&quot;Checks for user defined HorovodConfig and/or sets a default config object



            Returns

            -------

            HorovodConfig

                User set HorovodConfig or the defaulted version



            &quot;&quot;&quot;

            config = self._configs.get(&quot;HorovodConfig&quot;)

            return config if config is not None else HorovodConfig()



        def __repr__(self):

            &quot;&quot;&quot;Formats the status for pretty printing



            Returns

            -------

            str

                pretty formatted status string



            &quot;&quot;&quot;

            return (

                f&quot;STOKE STATE:\n&quot;

                f&quot;    CUDA AVAILABLE: {self.cuda}\n&quot;

                f&quot;    NCCL AVAILABLE: {self.nccl}\n&quot;

                f&quot;    GPU FLAG: {self.gpu}\n&quot;

                f&quot;    FP16 FLAG: {self.fp16}\n&quot;

                f&quot;    DISTRIBUTED BACKEND: {self.distributed}\n&quot;

                f&quot;    FAIRSCALE OSS: {self.oss}\n&quot;

                f&quot;    FAIRSCALE SDDP: {self.sharded}\n&quot;

                f&quot;    FAIRSCALE FSDP: {self.fully_sharded}\n&quot;

                f&#39;    DEEPSPEED ZeRO: {f&quot;Stage {self.zero}&quot; if self.is_distributed_deepspeed else f&quot;False&quot;}\n&#39;

                f&quot;    WORLD SIZE: {self.world_size}\n&quot;

                f&quot;    GRAD ACCUMULATION STEPS: {self.grad_accum}\n&quot;

                f&quot;    BATCH SIZE (PER DEVICE): {self.batch_size}\n&quot;

                f&quot;    EFFECTIVE BATCH SIZE (ALL DEVICES): {self.effective_batch_size}\n&quot;

                f&#39;    GRAD CLIP: ({&quot;, &quot;.join(f&quot;{k}: {v}&quot; for k, v in attr.asdict(self.grad_clip).items()) if self.grad_clip is not None else &quot;None&quot;})&#39;

            )
</code></pre></div>
<hr />
<h4 id="instance-variables">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">amp_config</span>
</code></pre></div>
<p>Checks for user defined AMPConfig and/or sets a default config object</p>
<div class="highlight"><pre><span></span><code><span class="n">apex_config</span>
</code></pre></div>
<p>Checks for user defined ApexConfig and/or sets a default config object</p>
<div class="highlight"><pre><span></span><code><span class="n">batch_size</span>
</code></pre></div>
<p>Shortcut to batch size</p>
<div class="highlight"><pre><span></span><code><span class="n">cuda</span>
</code></pre></div>
<p>Shortcut to get cuda status</p>
<div class="highlight"><pre><span></span><code><span class="n">ddp_config</span>
</code></pre></div>
<p>Checks for user defined DDPConfig and/or sets a default config object</p>
<p>Handles some post init logic looking for LOCAL_RANK and raises if it cannot find it
https://pytorch.org/docs/stable/distributed.html#launch-utility</p>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed_config</span>
</code></pre></div>
<p>Checks for user defined DeepspeedConfig and/or sets a default config object</p>
<p>Handles the internal logic of Deepspeed FP16 as it is a status flag in the config and not a class object
like AMP or APEX</p>
<div class="highlight"><pre><span></span><code><span class="n">distributed</span>
</code></pre></div>
<p>Shortcut to distributed setting</p>
<div class="highlight"><pre><span></span><code><span class="n">effective_batch_size</span>
</code></pre></div>
<p>Shortcut to effective batch size</p>
<div class="highlight"><pre><span></span><code><span class="n">fp16</span>
</code></pre></div>
<p>Shortcut to get FP16 status</p>
<div class="highlight"><pre><span></span><code><span class="n">fsdp_config</span>
</code></pre></div>
<p>Checks for user defined FairscaleFSDPConfig and/or sets a default config object</p>
<p>Mutates the default attr class to contain the mixed_precision attribute that is derived from FP16 settings</p>
<div class="highlight"><pre><span></span><code><span class="n">fully_sharded</span>
</code></pre></div>
<p>Returns if Fairscale fully sharded DDP status</p>
<div class="highlight"><pre><span></span><code><span class="n">gpu</span>
</code></pre></div>
<p>Shortcut to get GPU status</p>
<div class="highlight"><pre><span></span><code><span class="n">grad_accum</span>
</code></pre></div>
<p>Shortcut to get grad accumulation</p>
<div class="highlight"><pre><span></span><code><span class="n">grad_clip</span>
</code></pre></div>
<p>Shortcut to get grad clip</p>
<div class="highlight"><pre><span></span><code><span class="n">horovod_config</span>
</code></pre></div>
<p>Checks for user defined HorovodConfig and/or sets a default config object</p>
<div class="highlight"><pre><span></span><code><span class="n">is_distributed_ddp</span>
</code></pre></div>
<p>Returns if DDP is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_distributed_deepspeed</span>
</code></pre></div>
<p>Returns if Deepspeed is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_distributed_horovod</span>
</code></pre></div>
<p>Returns if Horovod is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_fairscale</span>
</code></pre></div>
<p>Returns if any part of Fairscale is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_fp16_amp</span>
</code></pre></div>
<p>Returns if AMP is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_fp16_apex</span>
</code></pre></div>
<p>Returns if APEX is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_fp16_deepspeed</span>
</code></pre></div>
<p>Returns if Deepspeed FP16 is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">nccl</span>
</code></pre></div>
<p>Shortcut to get nccl status</p>
<div class="highlight"><pre><span></span><code><span class="n">oss</span>
</code></pre></div>
<p>Returns if Fairscale optimizer state sharding status</p>
<div class="highlight"><pre><span></span><code><span class="n">oss_config</span>
</code></pre></div>
<p>Checks for user defined FairscaleOSSConfig and/or sets a default config object</p>
<div class="highlight"><pre><span></span><code><span class="n">sddp_config</span>
</code></pre></div>
<p>Checks for user defined FairscaleSDDPConfig and/or sets a default config object</p>
<div class="highlight"><pre><span></span><code><span class="n">sharded</span>
</code></pre></div>
<p>Returns if Fairscale sharded DDP status</p>
<div class="highlight"><pre><span></span><code><span class="n">status</span>
</code></pre></div>
<p>Shortcut to status dict</p>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<p>Returns the current world size</p>
<div class="highlight"><pre><span></span><code><span class="n">zero</span>
</code></pre></div>
<p>Returns what stage of ZeRO Deepspeed is using</p>
<h4 id="methods">Methods</h4>
<h4 id="set_post_init_values">set_post_init_values</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">set_post_init_values</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span>
</code></pre></div>
<p>Sets post-init values that cannot be set prior to run-time instantiation</p>
<p>Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>world_size</td>
<td>int</td>
<td>current distributed world size</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def set_post_init_values(self, world_size: int):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Sets post-init values that cannot be set prior to run-time instantiation



            Some values cannot be accessed until after run-time instantiation as the property accessors are not setup yet



            Parameters

            ----------

            world_size: int

                current distributed world size



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._status.update({&quot;world_size&quot;: world_size})
</code></pre></div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../io/" title="Io" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Io
              </span>
            </div>
          </a>
        
        
          <a href="../stoke/" title="Stoke" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Stoke
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ncilfone" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/ncilfone/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
    
  </body>
</html>