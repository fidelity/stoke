
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../resources/images/stoke_small.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Index - stoke</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-stoke" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="stoke" class="md-header__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../resources/images/stoke_small.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            stoke
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Index
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../docs/Home/" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../docs/Installation/" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../docs/Quick-Start/" class="md-tabs__link">
      Quick Start
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../docs/Launchers/" class="md-tabs__link">
      Launchers
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../docs/Examples/" class="md-tabs__link">
      Examples
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../CONTRIBUTING/" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="configs/" class="md-tabs__link md-tabs__link--active">
        Reference
      </a>
    </li>
  

  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="stoke" class="md-nav__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../resources/images/stoke_small.png" alt="logo">

    </a>
    stoke
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../docs/Home/" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../docs/Installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../docs/Quick-Start/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../docs/Launchers/" class="md-nav__link">
        Launchers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../docs/Examples/" class="md-nav__link">
        Examples
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_1" type="checkbox" id="__nav_7_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_1">
          Stoke
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stoke" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          Stoke
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="configs/" class="md-nav__link">
        Configs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data/" class="md-nav__link">
        Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="distributed/" class="md-nav__link">
        Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="extensions/" class="md-nav__link">
        Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="fp16/" class="md-nav__link">
        Fp16
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Index
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Index
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sub-modules" class="md-nav__link">
    Sub-modules
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ampconfig" class="md-nav__link">
    AMPConfig
  </a>
  
    <nav class="md-nav" aria-label="AMPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apexconfig" class="md-nav__link">
    ApexConfig
  </a>
  
    <nav class="md-nav" aria-label="ApexConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bucketeddistributedsampler" class="md-nav__link">
    BucketedDistributedSampler
  </a>
  
    <nav class="md-nav" aria-label="BucketedDistributedSampler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_epoch" class="md-nav__link">
    set_epoch
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clipgradconfig" class="md-nav__link">
    ClipGradConfig
  </a>
  
    <nav class="md-nav" aria-label="ClipGradConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clipgradnormconfig" class="md-nav__link">
    ClipGradNormConfig
  </a>
  
    <nav class="md-nav" aria-label="ClipGradNormConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddpconfig" class="md-nav__link">
    DDPConfig
  </a>
  
    <nav class="md-nav" aria-label="DDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_5" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedaioconfig" class="md-nav__link">
    DeepspeedAIOConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedAIOConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_6" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedactivationcheckpointingconfig" class="md-nav__link">
    DeepspeedActivationCheckpointingConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedActivationCheckpointingConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_7" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedconfig" class="md-nav__link">
    DeepspeedConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_8" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedfp16config" class="md-nav__link">
    DeepspeedFP16Config
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFP16Config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_9" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedflopsconfig" class="md-nav__link">
    DeepspeedFlopsConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFlopsConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_10" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedoffloadoptimizerconfig" class="md-nav__link">
    DeepspeedOffloadOptimizerConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedOffloadOptimizerConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_11" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedoffloadparamconfig" class="md-nav__link">
    DeepspeedOffloadParamConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedOffloadParamConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_12" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedpldconfig" class="md-nav__link">
    DeepspeedPLDConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedPLDConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_13" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedtensorboardconfig" class="md-nav__link">
    DeepspeedTensorboardConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedTensorboardConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_14" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedzeroconfig" class="md-nav__link">
    DeepspeedZeROConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedZeROConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_15" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedoptions" class="md-nav__link">
    DistributedOptions
  </a>
  
    <nav class="md-nav" aria-label="DistributedOptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp16options" class="md-nav__link">
    FP16Options
  </a>
  
    <nav class="md-nav" aria-label="FP16Options">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_1" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalefsdpconfig" class="md-nav__link">
    FairscaleFSDPConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleFSDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_16" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscaleossconfig" class="md-nav__link">
    FairscaleOSSConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleOSSConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_17" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalesddpconfig" class="md-nav__link">
    FairscaleSDDPConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleSDDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_18" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#horovodconfig" class="md-nav__link">
    HorovodConfig
  </a>
  
    <nav class="md-nav" aria-label="HorovodConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_19" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paramnormalize" class="md-nav__link">
    ParamNormalize
  </a>
  
    <nav class="md-nav" aria-label="ParamNormalize">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_2" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stoke" class="md-nav__link">
    Stoke
  </a>
  
    <nav class="md-nav" aria-label="Stoke">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_20" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataloader" class="md-nav__link">
    DataLoader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward" class="md-nav__link">
    backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dump_model_parameter_info" class="md-nav__link">
    dump_model_parameter_info
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load" class="md-nav__link">
    load
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss" class="md-nav__link">
    loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model" class="md-nav__link">
    model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print" class="md-nav__link">
    print
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_ema_loss" class="md-nav__link">
    print_ema_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_mean_accumulated_synced_loss" class="md-nav__link">
    print_mean_accumulated_synced_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_num_model_parameters" class="md-nav__link">
    print_num_model_parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_on_devices" class="md-nav__link">
    print_on_devices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_synced_loss" class="md-nav__link">
    print_synced_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset" class="md-nav__link">
    reset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset_ema" class="md-nav__link">
    reset_ema
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset_tracking" class="md-nav__link">
    reset_tracking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step" class="md-nav__link">
    step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_grads" class="md-nav__link">
    zero_grads
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stokeoptimizer" class="md-nav__link">
    StokeOptimizer
  </a>
  
    <nav class="md-nav" aria-label="StokeOptimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_21" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clear" class="md-nav__link">
    clear
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#copy" class="md-nav__link">
    copy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fromkeys" class="md-nav__link">
    fromkeys
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get" class="md-nav__link">
    get
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#items" class="md-nav__link">
    items
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#keys" class="md-nav__link">
    keys
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pop" class="md-nav__link">
    pop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#popitem" class="md-nav__link">
    popitem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setdefault" class="md-nav__link">
    setdefault
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#update" class="md-nav__link">
    update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#values" class="md-nav__link">
    values
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="io/" class="md-nav__link">
        Io
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="status/" class="md-nav__link">
        Status
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="stoke/" class="md-nav__link">
        Stoke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#sub-modules" class="md-nav__link">
    Sub-modules
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ampconfig" class="md-nav__link">
    AMPConfig
  </a>
  
    <nav class="md-nav" aria-label="AMPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apexconfig" class="md-nav__link">
    ApexConfig
  </a>
  
    <nav class="md-nav" aria-label="ApexConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bucketeddistributedsampler" class="md-nav__link">
    BucketedDistributedSampler
  </a>
  
    <nav class="md-nav" aria-label="BucketedDistributedSampler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_epoch" class="md-nav__link">
    set_epoch
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clipgradconfig" class="md-nav__link">
    ClipGradConfig
  </a>
  
    <nav class="md-nav" aria-label="ClipGradConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clipgradnormconfig" class="md-nav__link">
    ClipGradNormConfig
  </a>
  
    <nav class="md-nav" aria-label="ClipGradNormConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddpconfig" class="md-nav__link">
    DDPConfig
  </a>
  
    <nav class="md-nav" aria-label="DDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_5" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedaioconfig" class="md-nav__link">
    DeepspeedAIOConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedAIOConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_6" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedactivationcheckpointingconfig" class="md-nav__link">
    DeepspeedActivationCheckpointingConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedActivationCheckpointingConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_7" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedconfig" class="md-nav__link">
    DeepspeedConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_8" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedfp16config" class="md-nav__link">
    DeepspeedFP16Config
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFP16Config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_9" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedflopsconfig" class="md-nav__link">
    DeepspeedFlopsConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFlopsConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_10" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedoffloadoptimizerconfig" class="md-nav__link">
    DeepspeedOffloadOptimizerConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedOffloadOptimizerConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_11" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedoffloadparamconfig" class="md-nav__link">
    DeepspeedOffloadParamConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedOffloadParamConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_12" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedpldconfig" class="md-nav__link">
    DeepspeedPLDConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedPLDConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_13" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedtensorboardconfig" class="md-nav__link">
    DeepspeedTensorboardConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedTensorboardConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_14" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedzeroconfig" class="md-nav__link">
    DeepspeedZeROConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedZeROConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_15" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedoptions" class="md-nav__link">
    DistributedOptions
  </a>
  
    <nav class="md-nav" aria-label="DistributedOptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp16options" class="md-nav__link">
    FP16Options
  </a>
  
    <nav class="md-nav" aria-label="FP16Options">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_1" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalefsdpconfig" class="md-nav__link">
    FairscaleFSDPConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleFSDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_16" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscaleossconfig" class="md-nav__link">
    FairscaleOSSConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleOSSConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_17" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalesddpconfig" class="md-nav__link">
    FairscaleSDDPConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleSDDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_18" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#horovodconfig" class="md-nav__link">
    HorovodConfig
  </a>
  
    <nav class="md-nav" aria-label="HorovodConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_19" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#paramnormalize" class="md-nav__link">
    ParamNormalize
  </a>
  
    <nav class="md-nav" aria-label="ParamNormalize">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_2" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stoke" class="md-nav__link">
    Stoke
  </a>
  
    <nav class="md-nav" aria-label="Stoke">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_20" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataloader" class="md-nav__link">
    DataLoader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward" class="md-nav__link">
    backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dump_model_parameter_info" class="md-nav__link">
    dump_model_parameter_info
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load" class="md-nav__link">
    load
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss" class="md-nav__link">
    loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model" class="md-nav__link">
    model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print" class="md-nav__link">
    print
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_ema_loss" class="md-nav__link">
    print_ema_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_mean_accumulated_synced_loss" class="md-nav__link">
    print_mean_accumulated_synced_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_num_model_parameters" class="md-nav__link">
    print_num_model_parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_on_devices" class="md-nav__link">
    print_on_devices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_synced_loss" class="md-nav__link">
    print_synced_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset" class="md-nav__link">
    reset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset_ema" class="md-nav__link">
    reset_ema
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset_tracking" class="md-nav__link">
    reset_tracking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step" class="md-nav__link">
    step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_grads" class="md-nav__link">
    zero_grads
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stokeoptimizer" class="md-nav__link">
    StokeOptimizer
  </a>
  
    <nav class="md-nav" aria-label="StokeOptimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_21" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clear" class="md-nav__link">
    clear
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#copy" class="md-nav__link">
    copy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fromkeys" class="md-nav__link">
    fromkeys
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get" class="md-nav__link">
    get
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#items" class="md-nav__link">
    items
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#keys" class="md-nav__link">
    keys
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pop" class="md-nav__link">
    pop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#popitem" class="md-nav__link">
    popitem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setdefault" class="md-nav__link">
    setdefault
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#update" class="md-nav__link">
    update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#values" class="md-nav__link">
    values
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/fidelity/stoke/edit/main/reference/stoke/index.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="module-stoke">Module stoke</h1>
<p>Stoke is a lightweight wrapper for PyTorch that provides a simple unified interface for context switching</p>
<p>Please refer to the documentation provided in the README.md</p>
<p>??? example "View Source"
        # -<em>- coding: utf-8 -</em>-</p>
<div class="highlight"><pre><span></span><code>    # Copyright FMR LLC &lt;opensource@fidelity.com&gt;

    # SPDX-License-Identifier: Apache-2.0



    &quot;&quot;&quot;Stoke is a lightweight wrapper for PyTorch that provides a simple unified interface for context switching



    Please refer to the documentation provided in the README.md

    &quot;&quot;&quot;



    from .configs import *

    from .data import BucketedDistributedSampler

    from .status import DistributedOptions, FP16Options

    from .stoke import Stoke

    from .utils import ParamNormalize



    __all__ = [

        &quot;Stoke&quot;,

        &quot;ParamNormalize&quot;,

        &quot;FP16Options&quot;,

        &quot;DistributedOptions&quot;,

        &quot;StokeOptimizer&quot;,

        &quot;ClipGradNormConfig&quot;,

        &quot;ClipGradConfig&quot;,

        &quot;FairscaleOSSConfig&quot;,

        &quot;FairscaleSDDPConfig&quot;,

        &quot;FairscaleFSDPConfig&quot;,

        &quot;HorovodConfig&quot;,

        &quot;ApexConfig&quot;,

        &quot;DeepspeedConfig&quot;,

        &quot;DDPConfig&quot;,

        &quot;AMPConfig&quot;,

        &quot;DeepspeedAIOConfig&quot;,

        &quot;DeepspeedActivationCheckpointingConfig&quot;,

        &quot;DeepspeedFlopsConfig&quot;,

        &quot;DeepspeedFP16Config&quot;,

        &quot;DeepspeedPLDConfig&quot;,

        &quot;DeepspeedOffloadOptimizerConfig&quot;,

        &quot;DeepspeedOffloadParamConfig&quot;,

        &quot;DeepspeedTensorboardConfig&quot;,

        &quot;DeepspeedZeROConfig&quot;,

        &quot;BucketedDistributedSampler&quot;,

    ]



    from ._version import get_versions



    __version__ = get_versions()[&quot;version&quot;]



    del get_versions
</code></pre></div>
<h2 id="sub-modules">Sub-modules</h2>
<ul>
<li><a href="configs/">stoke.configs</a></li>
<li><a href="data/">stoke.data</a></li>
<li><a href="distributed/">stoke.distributed</a></li>
<li><a href="extensions/">stoke.extensions</a></li>
<li><a href="fp16/">stoke.fp16</a></li>
<li><a href="io/">stoke.io</a></li>
<li><a href="status/">stoke.status</a></li>
<li><a href="stoke/">stoke.stoke</a></li>
<li><a href="utils/">stoke.utils</a></li>
</ul>
<h2 id="classes">Classes</h2>
<h3 id="ampconfig">AMPConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">AMPConfig</span><span class="p">(</span>
    <span class="n">backoff_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">growth_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
    <span class="n">growth_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
    <span class="n">init_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">65536.0</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>backoff_factor</td>
<td>float, default: 0.5</td>
<td>Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration</td>
<td>None</td>
</tr>
<tr>
<td>growth_factor</td>
<td>float, default: 2.0</td>
<td>Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations.</td>
<td>None</td>
</tr>
<tr>
<td>growth_interval</td>
<td>int, default: 2000</td>
<td>Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by</td>
<td></td>
</tr>
<tr>
<td>growth_factor</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>init_scale</td>
<td>float, default: 2.**16</td>
<td>Initial scale factor</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class AMPConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;PyTorch AMP configuration class



        Attributes

        ----------

        backoff_factor : float, default: 0.5

            Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration

        growth_factor : float, default: 2.0

            Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations.

        growth_interval : int, default: 2000

            Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by

            growth_factor

        init_scale : float, default: 2.**16

            Initial scale factor



        &quot;&quot;&quot;



        backoff_factor: float = 0.5

        growth_factor: float = 2.0

        growth_interval: int = 2000

        init_scale: float = 2.0 ** 16
</code></pre></div>
<hr />
<h3 id="apexconfig">ApexConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ApexConfig</span><span class="p">(</span>
    <span class="n">cast_model_outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">convert_to_sync_batch_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">max_loss_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">16777216.0</span><span class="p">,</span>
    <span class="n">min_loss_scale</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scaler_per_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">verbosity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_1">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>cast_model_outputs</td>
<td>Optional[torch.dtype], default: None</td>
<td>Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level</td>
<td>None</td>
</tr>
<tr>
<td>convert_to_sync_batch_norm</td>
<td>bool, default: False</td>
<td>Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls</td>
<td></td>
</tr>
<tr>
<td>https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>max_loss_scale</td>
<td>float, default: 2.**24</td>
<td>Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling</td>
<td>None</td>
</tr>
<tr>
<td>min_loss_scale</td>
<td>Optional[float], default: None</td>
<td>Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None</td>
<td></td>
</tr>
<tr>
<td>means that no floor is imposed</td>
<td>value</td>
<td></td>
<td></td>
</tr>
<tr>
<td>scaler_per_loss</td>
<td>bool, default: False</td>
<td>Option to impose a scaler for each loss instead of a global scaler</td>
<td>None</td>
</tr>
<tr>
<td>verbosity</td>
<td>int, default: 0</td>
<td>Set to 0 to suppress Amp-related output</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ApexConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Nvidia APEX configuration class



        Attributes

        ----------

        cast_model_outputs: Optional[torch.dtype], default: None

            Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls

            https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm

        max_loss_scale: float, default: 2.**24

            Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling

        min_loss_scale: Optional[float], default: None

            Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None

            means that no floor is imposed

        scaler_per_loss: bool, default: False

            Option to impose a scaler for each loss instead of a global scaler

        verbosity: int, default: 0

            Set to 0 to suppress Amp-related output



        &quot;&quot;&quot;



        cast_model_outputs: Optional[torch.dtype] = None

        convert_to_sync_batch_norm: bool = False

        max_loss_scale: float = 2.0 ** 24

        min_loss_scale: Optional[float] = None

        scaler_per_loss: bool = False

        verbosity: int = 0
</code></pre></div>
<hr />
<h3 id="bucketeddistributedsampler">BucketedDistributedSampler</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BucketedDistributedSampler</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">Dataset</span><span class="p">,</span>
    <span class="n">buckets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">sorted_idx</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">status</span><span class="o">.</span><span class="n">DistributedOptions</span><span class="p">,</span>
    <span class="n">allow_bucket_overlap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_replicas</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_2">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>num_replicas</td>
<td>int, default: None</td>
<td>number of replicas</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>int, default: None</td>
<td>current device rank</td>
<td>None</td>
</tr>
<tr>
<td>epoch</td>
<td>int</td>
<td>current training epoch</td>
<td>None</td>
</tr>
<tr>
<td>drop_last</td>
<td>bool, default: False</td>
<td>whether to drop last set of samples that don't fit into a batch</td>
<td>None</td>
</tr>
<tr>
<td>shuffle</td>
<td>bool, default: True</td>
<td>flag to shuffle dataset</td>
<td>None</td>
</tr>
<tr>
<td>seed</td>
<td>int, default: 0</td>
<td>seed to use for generators</td>
<td>None</td>
</tr>
<tr>
<td>buckets</td>
<td>int</td>
<td>number of buckets to break the dataset into</td>
<td>None</td>
</tr>
<tr>
<td>sorted_n_samples</td>
<td>list</td>
<td>sorted list of samples by the characteristic to bucket by (e.g. seq len)</td>
<td>None</td>
</tr>
<tr>
<td>batch_size</td>
<td>int</td>
<td>batch size that will be used (needed to make sure slices are correct)</td>
<td>None</td>
</tr>
<tr>
<td>allow_bucket_overlap</td>
<td>bool, default: False</td>
<td>allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into</td>
<td></td>
</tr>
<tr>
<td>an un-bucketed batch</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>slice_size</td>
<td>int</td>
<td>computed from batch size and number of replicas</td>
<td>None</td>
</tr>
<tr>
<td>num_samples_per_bucket</td>
<td>int</td>
<td>computed value that represents the number of samples in a single bucket</td>
<td>None</td>
</tr>
<tr>
<td>num_slices_per_bucket</td>
<td>int</td>
<td>computed value that represents the number of slices available in a bucket</td>
<td>None</td>
</tr>
<tr>
<td>bucket_idx</td>
<td>list</td>
<td>computed value that make a contiguous list of indices in each bucket</td>
<td>None</td>
</tr>
<tr>
<td>rounded_num_samples_per_bucket</td>
<td>int</td>
<td>computed value post round for number of samples in a single bucket</td>
<td>None</td>
</tr>
<tr>
<td>rounded_num_samples_per_replica</td>
<td>int</td>
<td>computed value post round for number of slices available in a bucket</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class BucketedDistributedSampler(Sampler[T_co]):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Sampler that buckets samples by sorted_idx and then randomly samples from a specific bucket to prevent excess

        padding leading to wasted computation



        Borrowing heavily from the base DistributedSampler

        https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler



        Attributes

        ----------

        num_replicas: int, default: None

            number of replicas

        rank: int, default: None

            current device rank

        epoch: int

            current training epoch

        drop_last: bool, default: False

            whether to drop last set of samples that don&#39;t fit into a batch

        shuffle: bool, default: True

            flag to shuffle dataset

        seed: int, default: 0

            seed to use for generators

        buckets: int

            number of buckets to break the dataset into

        sorted_n_samples: list

            sorted list of samples by the characteristic to bucket by (e.g. seq len)

        batch_size: int

            batch size that will be used (needed to make sure slices are correct)

        allow_bucket_overlap: bool, default: False

            allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into

            an un-bucketed batch

        slice_size: int

            computed from batch size and number of replicas

        num_samples_per_bucket: int

            computed value that represents the number of samples in a single bucket

        num_slices_per_bucket: int

            computed value that represents the number of slices available in a bucket

        bucket_idx: list

            computed value that make a contiguous list of indices in each bucket

        rounded_num_samples_per_bucket: int

            computed value post round for number of samples in a single bucket

        rounded_num_samples_per_replica: int

            computed value post round for number of slices available in a bucket



        &quot;&quot;&quot;



        def __init__(

            self,

            dataset: Dataset,

            buckets: int,

            batch_size: int,

            sorted_idx: List,

            backend: DistributedOptions,

            allow_bucket_overlap: bool = False,

            num_replicas: Optional[int] = None,

            rank: Optional[int] = None,

            shuffle: bool = True,

            seed: int = 0,

            drop_last: bool = False,

            info_rank: int = 0,

        ) -&gt; None:

            &quot;&quot;&quot;Init for BucketedDistributedSampler



            Parameters

            ----------

            dataset: Dataset

                dataset from which to load the data.

            buckets: int

                number of buckets to break the dataset into

            batch_size: int

                batch size that will be used (needed to make sure slices are correct)

            sorted_idx: list

                sorted list of samples by the characteristic to bucket by (e.g. seq le

            backend: DistributedOptions

                which backend is being used (as rank, world size, etc. need to be used)

            allow_bucket_overlap: bool, default: False

                allow for the residual samples (those that are not divisible by batch and num_replicas) to be assembled into

                an un-bucketed batch

            num_replicas: int, default: None

                number of replicas

            rank: int, default: None

                current device rank

            shuffle: bool, default: True

                flag to shuffle dataset

            seed: int, default: 0

                seed to use for generators

            drop_last: bool, default: False

                whether to drop last set of samples that don&#39;t fit into a

            info_rank: int, default: 0

                which device to print information on



            &quot;&quot;&quot;

            # If the backend isnt DDP there needs to be an additional import

            num_replicas, rank = self._conditional_distributed(

                backend=backend, num_replicas=num_replicas, rank=rank

            )

            self.num_replicas = num_replicas

            self.rank = rank

            self.epoch = 0

            self.drop_last = drop_last

            self.shuffle = shuffle

            self.seed = seed

            self.buckets = buckets

            self.sorted_n_samples = sorted_idx

            # Batch size is needed here so a contiguous iter of buckets can be formed

            self.batch_size = batch_size

            # This is a flag to batch up the dropped samples (that would be &#39;wasted&#39;) if drop_last is flagged

            self.allow_bucket_overlap = allow_bucket_overlap

            # Calculate the size of each slice that will be indexed across the replicas

            self.slice_size = self.batch_size * self.num_replicas

            # Calculate the size of the buckets (rounded or not based on drop last)

            self.num_samples_per_bucket = self._get_size(

                len(dataset), self.buckets, self.drop_last

            )

            # Calculate the number of slices per bucket

            self.num_slices_per_bucket = self._get_size(

                self.num_samples_per_bucket, self.slice_size, self.drop_last

            )

            if self.num_samples_per_bucket &lt; self.slice_size:

                raise ValueError(

                    f&quot;Stoke -- Resulting number of slices (batch * replicas) per bucket &quot;

                    f&quot;({self.num_samples_per_bucket}) is less than the batch size &quot;

                    f&quot;({self.batch_size})&quot;

                )

            if self.num_slices_per_bucket &lt; 2:

                raise ValueError(

                    f&quot;Stoke -- Number of slices per bucket {self.num_slices_per_bucket} is less than 2 &quot;

                    f&quot;which is not recommended&quot;

                )

            if self.num_samples_per_bucket &lt; 100:

                raise ValueError(

                    f&quot;Stoke -- Number of samples per bucket {self.num_samples_per_bucket} is less than 100 &quot;

                    f&quot;which is not recommended as this might lead to dropping of excessive data&quot;

                )

            # Split into buckets and turn into lists

            self.bucket_idx = [

                list(val) for val in np.array_split(self.sorted_n_samples, self.buckets)

            ]

            # Calculate the post rounded numbers

            self.rounded_num_samples_per_bucket = (

                self.slice_size * self.num_slices_per_bucket

            )

            self.rounded_num_samples_per_replica = (

                self.num_slices_per_bucket * self.batch_size * self.buckets

            )

            # Add the bucket overlap samples

            if self.allow_bucket_overlap:

                self.rounded_num_samples_per_replica += (

                    (len(dataset) - (self.rounded_num_samples_per_bucket * self.buckets))

                    // self.slice_size

                ) * self.batch_size

            if self.rank == info_rank:

                print(

                    f&quot;Stoke -- BucketedDistributedSampler -- # Samples Per Bucket: &quot;

                    f&quot;{self.rounded_num_samples_per_bucket}, # of Samples Per Replica: &quot;

                    f&quot;{self.rounded_num_samples_per_replica}&quot;

                )



        def _conditional_distributed(

            self,

            backend: DistributedOptions,

            num_replicas: Optional[int],

            rank: Optional[int],

        ):

            &quot;&quot;&quot;



            Parameters

            ----------

            backend: DistributedOptions

                which backend is being used

            num_replicas: int, default: None

                total number of replicas

            rank: int, default: None

                current device rank



            Returns

            -------

            Tuple[int, int]

                num_replicas, rank

            &quot;&quot;&quot;

            return self._check_backend(backend, num_replicas, rank)



        def _get_backend_functions(self, backend: DistributedOptions):

            &quot;&quot;&quot;Gets backend functions if needed



            Parameters

            ----------

            backend: DistributedOptions

                which backend is being used



            Returns

            -------

            Tuple[bool, int, int]

                is_init, num_replicas, rank



            &quot;&quot;&quot;

            if backend.value == &quot;ddp&quot; or backend.value == &quot;deepspeed&quot;:

                return (

                    torch.distributed.is_initialized,

                    torch.distributed.get_world_size,

                    torch.distributed.get_rank,

                )

            else:

                return hvd.is_initialized, hvd.size, hvd.rank



        def _check_backend(

            self,

            backend: DistributedOptions,

            num_replicas: Optional[int],

            rank: Optional[int],

        ):

            &quot;&quot;&quot;Checks the backend for correct device info



            Parameters

            ----------

            backend: DistributedOptions

                which backend is being used

            num_replicas: int, default: None

                total number of replicas

            rank: int, default: None

                current device rank



            Returns

            -------

            Tuple[int, int]

                num_replicas, rank



            &quot;&quot;&quot;

            if num_replicas is None or rank is None:

                is_avail, get_world_size, get_rank = self._get_backend_functions(

                    backend=backend

                )

            if num_replicas is None:

                if not is_avail():

                    raise RuntimeError(

                        &quot;Requires distributed package (torch.dist or hvd) to be available&quot;

                    )

                num_replicas = get_world_size()

            if rank is None:

                if not is_avail():

                    raise RuntimeError(

                        &quot;Requires distributed package (torch.dist or hvd) to be available&quot;

                    )

                rank = get_rank()

            return num_replicas, rank



        @staticmethod

        def _get_size(data_len: int, split_var: int, drop_last: bool = False):

            &quot;&quot;&quot;Gets the size of a split



            Parameters

            ----------

            data_len: int

                current dataset length

            split_var: int

                how many to split into

            drop_last: bool, default: False

                drop last hanging samples if not batch_size



            Returns

            -------

            num_samples: int



            &quot;&quot;&quot;

            if drop_last:

                num_samples = data_len // split_var

            else:

                num_samples = ceil(data_len / split_var)

            return num_samples



        def __iter__(self) -&gt; Iterator[T_co]:

            &quot;&quot;&quot;Handles assembling the batches from a bucketed perspective



            Shuffle bucket order-&gt;Pad if necessary-&gt;Slice across replicas-&gt;Possibly batch up residuals-&gt;shuffle bucketed

            batches-&gt;Unroll into list-&gt;Make iter



            Returns

            -------

            Iterator[T_co]



            &quot;&quot;&quot;

            # Shuffle the bucketed idx

            if self.shuffle:

                # deterministically shuffle based on epoch and seed

                g = torch.Generator()

                g.manual_seed(self.seed + self.epoch)

                # Permute each bucket

                indices = [

                    [val[idx] for idx in torch.randperm(len(val), generator=g).tolist()]

                    for val in self.bucket_idx

                ]

            else:

                indices = self.bucket_idx

            # Iterate over the buckets

            for idx, val in enumerate(indices):

                # If this is true we need to handle padding

                if (self.num_slices_per_bucket * self.slice_size) &gt; len(val):

                    split_val = self._handle_padding(val)

                    indices[idx] = list(itertools.chain(*split_val))

                    assert len(indices[idx]) == self.rounded_num_samples_per_bucket

            # Now slice across replicas

            final_indices = []

            for val in indices:

                for idx in range(self.num_slices_per_bucket):

                    replica_slice = val[

                        (idx * self.slice_size) : ((idx + 1) * self.slice_size)

                    ][self.rank : self.slice_size : self.num_replicas]

                    final_indices.append(replica_slice)

            # If bucket overlap is allowed then we just batch up the residual indices

            if self.drop_last and self.allow_bucket_overlap:

                residual_idx = list(

                    itertools.chain(

                        *[val[self.rounded_num_samples_per_bucket :] for val in indices]

                    )

                )

                if len(residual_idx) &gt; self.slice_size:

                    # Cut by slices then by replicas

                    residual_idx = [

                        residual_idx[

                            (idx * self.slice_size) : ((idx + 1) * self.slice_size)

                        ][self.rank : self.slice_size : self.num_replicas]

                        for idx in range(len(residual_idx) // self.slice_size)

                    ]

                    # Append to the final indices

                    final_indices.extend(residual_idx)

            # Shuffle the bucketed batches

            if self.shuffle:

                # deterministically shuffle based on epoch and seed

                g = torch.Generator()

                g.manual_seed(self.seed + self.epoch)

                # Permute the bucket order

                final_indices = [

                    final_indices[val]

                    for val in torch.randperm(len(final_indices), generator=g)

                ]

            # Unroll into a single list

            final_indices = list(itertools.chain(*final_indices))

            assert len(final_indices) == self.rounded_num_samples_per_replica

            return iter(final_indices)



        def _handle_padding(self, idx_list: List):

            &quot;&quot;&quot;Handles padding out if a batch is short



            Parameters

            ----------

            idx_list: List

                list of indices



            Returns

            -------

            split_val: List

                list with correctly padded sizes



            &quot;&quot;&quot;

            split_val = []

            for idx in range(self.num_slices_per_bucket):

                if idx == (self.num_slices_per_bucket - 1):

                    # Get the short batch

                    short_batch = idx_list[(idx * self.slice_size) :]

                    # Short batch replica slice sizes

                    short_len = [

                        self.batch_size - len(list(val))

                        for val in np.array_split(short_batch, self.num_replicas)

                    ]

                    # Pop the necessary values from the entire bucket

                    pad_values = [

                        idx_list[s_idx : (self.num_replicas * s_len) : self.num_replicas]

                        for s_idx, s_len in enumerate(short_len)

                    ]

                    # If not a consistent list then we need to reorder so that the step size alignment slicing

                    # of the replicas works

                    if len(set(short_len)) != 1:

                        # here we need to find the first larger idx and reorder

                        first_idx = short_len.index(max(set(short_len)))

                        # Reorder

                        pad_values = pad_values[first_idx:] + pad_values[0:first_idx]

                    extended_batch = short_batch + [

                        pad

                        for pad in list(

                            itertools.chain(*itertools.zip_longest(*pad_values))

                        )

                        if pad is not None

                    ]

                    split_val.append(extended_batch)

                else:

                    split_val.append(

                        idx_list[(idx * self.slice_size) : ((idx + 1) * self.slice_size)]

                    )

            return split_val



        def __len__(self) -&gt; int:

            return self.rounded_num_samples_per_replica



        def set_epoch(self, epoch: int) -&gt; None:

            &quot;&quot;&quot;Sets the epoch for this sampler.



            When :attr:`shuffle=True`, this ensures all replicas

            use a different random ordering for each epoch. Otherwise, the next iteration of this

            sampler will yield the same ordering.



            Parameters

            ----------

            epoch: int

                Epoch number



            &quot;&quot;&quot;

            self.epoch = epoch
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>torch.utils.data.sampler.Sampler</li>
<li>typing.Generic</li>
</ul>
<h4 id="methods">Methods</h4>
<h4 id="set_epoch">set_epoch</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">set_epoch</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span>
</code></pre></div>
<p>Sets the epoch for this sampler.</p>
<p>When :attr:<code>shuffle=True</code>, this ensures all replicas
use a different random ordering for each epoch. Otherwise, the next iteration of this
sampler will yield the same ordering.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>epoch</td>
<td>int</td>
<td>Epoch number</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def set_epoch(self, epoch: int) -&gt; None:</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Sets the epoch for this sampler.



            When :attr:`shuffle=True`, this ensures all replicas

            use a different random ordering for each epoch. Otherwise, the next iteration of this

            sampler will yield the same ordering.



            Parameters

            ----------

            epoch: int

                Epoch number



            &quot;&quot;&quot;

            self.epoch = epoch
</code></pre></div>
<h3 id="clipgradconfig">ClipGradConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ClipGradConfig</span><span class="p">(</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_3">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>clip_value</td>
<td>float</td>
<td>maximum allowed absolute value of the gradients [-clip_value, clip_value]</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ClipGradConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Gradient clipping by value configuration class



        Attributes

        ----------

        clip_value: float

            maximum allowed absolute value of the gradients [-clip_value, clip_value]



        &quot;&quot;&quot;



        clip_value: float
</code></pre></div>
<hr />
<h3 id="clipgradnormconfig">ClipGradNormConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ClipGradNormConfig</span><span class="p">(</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_4">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>max_norm</td>
<td>float</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>float</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ClipGradNormConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Gradient clipping by p-norm configuration class



        Attributes

        ----------

        max_norm: float

            max norm of the gradients

        norm_type: float

            type of the used p-norm



        &quot;&quot;&quot;



        max_norm: float

        norm_type: float
</code></pre></div>
<hr />
<h3 id="ddpconfig">DDPConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DDPConfig</span><span class="p">(</span>
    <span class="n">local_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">auto_mpi_discovery</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">convert_to_sync_batch_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">BackendOptions</span> <span class="o">=</span> <span class="s1">&#39;nccl&#39;</span><span class="p">,</span>
    <span class="n">broadcast_buffers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">bucket_cap_mb</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span>
    <span class="n">find_unused_parameters</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gradient_as_bucket_view</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;env://&#39;</span><span class="p">,</span>
    <span class="n">no_sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_5">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>local_rank</td>
<td>Optional[int]</td>
<td>Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg)</td>
<td>None</td>
</tr>
<tr>
<td>auto_mpi_discovery</td>
<td>bool, default: False</td>
<td>if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed</td>
<td></td>
</tr>
<tr>
<td>function call)</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>convert_to_sync_batch_norm</td>
<td>bool, default: False</td>
<td>Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls</td>
<td></td>
</tr>
<tr>
<td>https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>backend</td>
<td>BackendOptions, default: 'nccl'</td>
<td>Which communication backend to use</td>
<td>None</td>
</tr>
<tr>
<td>broadcast_buffers</td>
<td>bool, default: True</td>
<td>Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function</td>
<td>None</td>
</tr>
<tr>
<td>bucket_cap_mb</td>
<td>int, default: 25</td>
<td>DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket</td>
<td></td>
</tr>
<tr>
<td>can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB)</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>find_unused_parameters</td>
<td>bool, default: False</td>
<td>Traverse the autograd graph from all tensors contained in the return value of the wrapped modules forward</td>
<td></td>
</tr>
<tr>
<td>function. Parameters that dont receive gradients as part of this graph are preemptively marked as being ready</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>to be reduced. Note that all forward outputs that are derived from module parameters must participate in</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>calculating loss and later the gradient computation. If they dont, this wrapper will hang waiting for autograd</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>can be detached from the autograd graph using torch.Tensor.detach</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradient_as_bucket_view</td>
<td>bool, default: False</td>
<td>When set to True, gradients will be views pointing to different offsets of allreduce communication</td>
<td></td>
</tr>
<tr>
<td>buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>referring to the zero_grad() function in torch/optim/optimizer.py as a solution.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>init_method</td>
<td>str, default: 'env://'</td>
<td>URL specifying how to initialize the process group</td>
<td>None</td>
</tr>
<tr>
<td>no_sync</td>
<td>bool, default: True</td>
<td>for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on</td>
<td></td>
</tr>
<tr>
<td>module variables, which will later be synchronized in the first forward-backward pass after exiting the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>context. no sync might lead to higher memory usage but lower communication overhead</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DDPConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;PyTorch DistributedDataParallel configuration class



        Attributes

        ----------

        local_rank: Optional[int]

            Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg)

        auto_mpi_discovery: bool, default: False

            if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed

            function call)

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls

            https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html

        backend: BackendOptions, default: &#39;nccl&#39;

            Which communication backend to use

        broadcast_buffers: bool, default: True

            Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function

        bucket_cap_mb: int, default: 25

            DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket

            can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB)

        find_unused_parameters: bool, default: False

            Traverse the autograd graph from all tensors contained in the return value of the wrapped modules forward

            function. Parameters that dont receive gradients as part of this graph are preemptively marked as being ready

            to be reduced. Note that all forward outputs that are derived from module parameters must participate in

            calculating loss and later the gradient computation. If they dont, this wrapper will hang waiting for autograd

            to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused

            can be detached from the autograd graph using torch.Tensor.detach

        gradient_as_bucket_view: bool, default: False

            When set to True, gradients will be views pointing to different offsets of allreduce communication

            buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients

            size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When

            gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by

            referring to the zero_grad() function in torch/optim/optimizer.py as a solution.

        init_method: str, default: &#39;env://&#39;

            URL specifying how to initialize the process group

        no_sync: bool, default: True

            for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on

            module variables, which will later be synchronized in the first forward-backward pass after exiting the

            context. no sync might lead to higher memory usage but lower communication overhead



        &quot;&quot;&quot;



        local_rank: Optional[int]

        auto_mpi_discovery: bool = False

        convert_to_sync_batch_norm: bool = False

        backend: BackendOptions = &quot;nccl&quot;

        broadcast_buffers: bool = True

        bucket_cap_mb: int = 25

        find_unused_parameters: bool = False

        gradient_as_bucket_view: bool = False

        init_method: str = &quot;env://&quot;

        no_sync: bool = True
</code></pre></div>
<hr />
<h3 id="deepspeedaioconfig">DeepspeedAIOConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedAIOConfig</span><span class="p">(</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1048576</span><span class="p">,</span>
    <span class="n">ignore_unused_parameters</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">overlap_events</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">queue_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">single_submit</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">thread_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_6">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>block_size</td>
<td>int, default: 1048576</td>
<td>I/O block size in bytes</td>
<td>None</td>
</tr>
<tr>
<td>ignore_unused_parameters</td>
<td>bool, default: True</td>
<td>Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks.</td>
<td></td>
</tr>
<tr>
<td>This controls whether or not training should terminate with an error message when unused parameters are</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>detected.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>overlap_events</td>
<td>bool, default: True</td>
<td>Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests.</td>
<td>None</td>
</tr>
<tr>
<td>queue_depth</td>
<td>int, default: 8</td>
<td>I/O queue depth</td>
<td>None</td>
</tr>
<tr>
<td>single_submit</td>
<td>bool, default: False</td>
<td>Submit requests to storage device as multiple individual requests as opposed to one block of requests.</td>
<td>None</td>
</tr>
<tr>
<td>thread_count</td>
<td>int, default: 1</td>
<td>Intra-request parallelism for each read/write submitted by a user thread.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedAIOConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed asynchronous I/O configuration class



        Attributes

        ----------

        block_size: int, default: 1048576

            I/O block size in bytes

        ignore_unused_parameters: bool, default: True

            Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks.

            This controls whether or not training should terminate with an error message when unused parameters are

            detected.

        overlap_events: bool, default: True

            Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests.

        queue_depth: int, default: 8

            I/O queue depth

        single_submit: bool, default: False

            Submit requests to storage device as multiple individual requests as opposed to one block of requests.

        thread_count: int, default: 1

            Intra-request parallelism for each read/write submitted by a user thread.



        &quot;&quot;&quot;



        block_size: int = 1048576

        ignore_unused_parameters: bool = True

        overlap_events: bool = True

        queue_depth: int = 8

        single_submit: bool = False

        thread_count: int = 1
</code></pre></div>
<hr />
<h3 id="deepspeedactivationcheckpointingconfig">DeepspeedActivationCheckpointingConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedActivationCheckpointingConfig</span><span class="p">(</span>
    <span class="n">contiguous_memory_optimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cpu_checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">number_checkpoints</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_activations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">profile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">synchronize_checkpoint_boundary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_7">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>contiguous_memory_optimization</td>
<td>bool, default: False</td>
<td>Copies partitioned activations so that they are contiguous in memory</td>
<td>None</td>
</tr>
<tr>
<td>cpu_checkpointing</td>
<td>bool, default: False</td>
<td>Offloads partitioned activations to CPU if partition_activations is enabled</td>
<td>None</td>
</tr>
<tr>
<td>number_checkpoints</td>
<td>Optional[int], default: None</td>
<td>Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization</td>
<td>None</td>
</tr>
<tr>
<td>partition_activations</td>
<td>bool, default: False</td>
<td>Enables partition activation when used with model parallelism</td>
<td>None</td>
</tr>
<tr>
<td>profile</td>
<td>bool, default: False</td>
<td>Logs the forward and backward time for each checkpoint function</td>
<td>None</td>
</tr>
<tr>
<td>synchronize_checkpoint_boundary</td>
<td>bool, default: False</td>
<td>Inserts torch.cuda.synchronize() at each checkpoint boundary</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedActivationCheckpointingConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed activation checkpointing configuration class



        Attributes

        ----------

        contiguous_memory_optimization: bool, default: False

            Copies partitioned activations so that they are contiguous in memory

        cpu_checkpointing: bool, default: False

            Offloads partitioned activations to CPU if partition_activations is enabled

        number_checkpoints: Optional[int], default: None

            Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization

        partition_activations: bool, default: False

            Enables partition activation when used with model parallelism

        profile: bool, default: False

            Logs the forward and backward time for each checkpoint function

        synchronize_checkpoint_boundary: bool, default: False

            Inserts torch.cuda.synchronize() at each checkpoint boundary



        &quot;&quot;&quot;



        contiguous_memory_optimization: bool = False

        cpu_checkpointing: bool = False

        number_checkpoints: Optional[int] = None

        partition_activations: bool = False

        profile: bool = False

        synchronize_checkpoint_boundary: bool = False
</code></pre></div>
<hr />
<h3 id="deepspeedconfig">DeepspeedConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedConfig</span><span class="p">(</span>
    <span class="n">activation_checkpointing</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedActivationCheckpointingConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="n">DeepspeedActivationCheckpointingConfig</span><span class="p">(</span><span class="n">contiguous_memory_optimization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cpu_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">number_checkpoints</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">partition_activations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">profile</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">synchronize_checkpoint_boundary</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">aio</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedAIOConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="n">DeepspeedAIOConfig</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">1048576</span><span class="p">,</span> <span class="n">ignore_unused_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">overlap_events</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">queue_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">single_submit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">thread_count</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">auto_mpi_discovery</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">disable_allgather</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dist_backend</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">BackendOptions</span> <span class="o">=</span> <span class="s1">&#39;nccl&#39;</span><span class="p">,</span>
    <span class="n">distributed_port</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">29500</span><span class="p">,</span>
    <span class="n">dump_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">flops_profiler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedFlopsConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fp16</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedFP16Config</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fp32_allreduce</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gradient_predivide_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;env://&#39;</span><span class="p">,</span>
    <span class="n">prescale_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">progressive_layer_drop</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedPLDConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sparse_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">steps_per_print</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">tensorboard</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedTensorboardConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">wall_clock_breakdown</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">zero_optimization</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedZeROConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="n">DeepspeedZeROConfig</span><span class="p">(</span><span class="n">allgather_bucket_size</span><span class="o">=</span><span class="mi">500000000</span><span class="p">,</span> <span class="n">allgather_partitions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">contiguous_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_unused_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">legacy_stage1</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offload_param</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">overlap_comm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduce_bucket_size</span><span class="o">=</span><span class="mi">500000000</span><span class="p">,</span> <span class="n">reduce_scatter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stage3_max_live_parameters</span><span class="o">=</span><span class="mi">1000000000</span><span class="p">,</span> <span class="n">stage3_max_reuse_distance</span><span class="o">=</span><span class="mi">1000000000</span><span class="p">,</span> <span class="n">stage3_prefetch_bucket_size</span><span class="o">=</span><span class="mi">500000000</span><span class="p">,</span> <span class="n">stage3_param_persistence_threshold</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">stage3_gather_fp16_weights_on_model_save</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sub_group_size</span><span class="o">=</span><span class="mi">1000000000000</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_8">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>activation_checkpointing</td>
<td>Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig()</td>
<td>Enables and configures activation checkpointing</td>
<td>None</td>
</tr>
<tr>
<td>aio</td>
<td>Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig()</td>
<td>Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent</td>
<td></td>
</tr>
<tr>
<td>(NVMe) storage</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>auto_mpi_discovery</td>
<td>bool, default: True</td>
<td>if distributed environment variables are not set, attempt to discover them from MPI</td>
<td>None</td>
</tr>
<tr>
<td>disable_allgather</td>
<td>bool, default: False</td>
<td>Disables allgather</td>
<td>None</td>
</tr>
<tr>
<td>dist_backend</td>
<td>BackendOptions, default: 'nccl'</td>
<td>Which communication backend to use</td>
<td>None</td>
</tr>
<tr>
<td>distributed_port</td>
<td>int, default: 29500</td>
<td>torch distributed backend port</td>
<td>None</td>
</tr>
<tr>
<td>dump_state</td>
<td>bool, default: False</td>
<td>Print out state information of DeepSpeed object after initialization</td>
<td>None</td>
</tr>
<tr>
<td>flops_profiler</td>
<td>Optional[DeepspeedFlopsConfig], default: None</td>
<td>Enables and configures the flops profiler. This would also enable wall_clock_breakdown</td>
<td>None</td>
</tr>
<tr>
<td>fp16</td>
<td>Optional[DeepspeedFP16Config], default: None</td>
<td>Enables and configures mixed precision/FP16 training that leverages NVIDIAs Apex package</td>
<td>None</td>
</tr>
<tr>
<td>fp32_allreduce</td>
<td>bool, default: False</td>
<td>During gradient averaging perform allreduce with 32 bit values</td>
<td>None</td>
</tr>
<tr>
<td>gradient_predivide_factor</td>
<td>float, default: 1.0</td>
<td>Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability</td>
<td></td>
</tr>
<tr>
<td>when scaling to large numbers of GPUs</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>init_method</td>
<td>str, default: 'env://'</td>
<td>URL specifying how to initialize the process group</td>
<td>None</td>
</tr>
<tr>
<td>prescale_gradients</td>
<td>float, default: 1.0</td>
<td>Scale gradients before doing allreduce</td>
<td>None</td>
</tr>
<tr>
<td>progressive_layer_drop</td>
<td>Optional[DeepspeedPLDConfig], default: None</td>
<td>Enables and configures progressive layer dropping</td>
<td>None</td>
</tr>
<tr>
<td>sparse_gradients</td>
<td>bool, default: False</td>
<td>Enable sparse compression of torch.nn.Embedding gradients</td>
<td>None</td>
</tr>
<tr>
<td>steps_per_print</td>
<td>int, default: 10</td>
<td>Print train loss every N steps</td>
<td>None</td>
</tr>
<tr>
<td>tensorboard</td>
<td>Optional[DeepspeedTensorboardConfig], default: None</td>
<td>Enables and configures tensorboard support</td>
<td>None</td>
</tr>
<tr>
<td>verbose</td>
<td>bool, default: True</td>
<td>flag to make deepspeed engine verbose with information</td>
<td>None</td>
</tr>
<tr>
<td>wall_clock_breakdown</td>
<td>bool, default: False</td>
<td>Enable timing of the latency of forward/backward/update training phases</td>
<td>None</td>
</tr>
<tr>
<td>zero_optimization</td>
<td>Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig()</td>
<td>Enables and configures ZeRO memory optimizations</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed configuration class



        Composed of other configuration classes related to specific functionality



        Attributes

        ----------

        activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig()

            Enables and configures activation checkpointing

        aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig()

            Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent

            (NVMe) storage

        auto_mpi_discovery: bool, default: True

            if distributed environment variables are not set, attempt to discover them from MPI

        disable_allgather: bool, default: False

            Disables allgather

        dist_backend: BackendOptions, default: &#39;nccl&#39;

            Which communication backend to use

        distributed_port: int, default: 29500

            torch distributed backend port

        dump_state: bool, default: False

            Print out state information of DeepSpeed object after initialization

        flops_profiler: Optional[DeepspeedFlopsConfig], default: None

            Enables and configures the flops profiler. This would also enable wall_clock_breakdown

        fp16: Optional[DeepspeedFP16Config], default: None

            Enables and configures mixed precision/FP16 training that leverages NVIDIAs Apex package

        fp32_allreduce: bool, default: False

            During gradient averaging perform allreduce with 32 bit values

        gradient_predivide_factor: float, default: 1.0

            Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability

            when scaling to large numbers of GPUs

        init_method: str, default: &#39;env://&#39;

            URL specifying how to initialize the process group

        prescale_gradients: float, default: 1.0

            Scale gradients before doing allreduce

        progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None

            Enables and configures progressive layer dropping

        sparse_gradients: bool, default: False

            Enable sparse compression of torch.nn.Embedding gradients

        steps_per_print: int, default: 10

            Print train loss every N steps

        tensorboard: Optional[DeepspeedTensorboardConfig], default: None

            Enables and configures tensorboard support

        verbose: bool, default: True

            flag to make deepspeed engine verbose with information

        wall_clock_breakdown: bool, default: False

            Enable timing of the latency of forward/backward/update training phases

        zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig()

            Enables and configures ZeRO memory optimizations



        Notes

        -----

        Deepspeed does not use Apexs AMP mode whihc allows for more flexibility in mixed precision training modes. FP16

        here is similar to AMPs O2 mode



        &quot;&quot;&quot;



        activation_checkpointing: Optional[

            DeepspeedActivationCheckpointingConfig

        ] = DeepspeedActivationCheckpointingConfig()

        aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig()

        auto_mpi_discovery: bool = True

        disable_allgather: bool = False

        dist_backend: BackendOptions = &quot;nccl&quot;

        distributed_port: int = 29500

        dump_state: bool = False

        flops_profiler: Optional[DeepspeedFlopsConfig] = None

        fp16: Optional[DeepspeedFP16Config] = None

        fp32_allreduce: bool = False

        gradient_predivide_factor: float = 1.0

        init_method: str = &quot;env://&quot;

        prescale_gradients: bool = False

        progressive_layer_drop: Optional[DeepspeedPLDConfig] = None

        sparse_gradients: bool = False

        steps_per_print: int = 10

        tensorboard: Optional[DeepspeedTensorboardConfig] = None

        verbose: bool = True

        wall_clock_breakdown: bool = False

        zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig()
</code></pre></div>
<hr />
<h3 id="deepspeedfp16config">DeepspeedFP16Config</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedFP16Config</span><span class="p">(</span>
    <span class="n">hysteresis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">initial_scale_power</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">loss_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">loss_scale_window</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">min_loss_scale</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_9">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>hysteresis</td>
<td>int, default: 2</td>
<td>represents the delay shift in dynamic loss scaling</td>
<td>None</td>
</tr>
<tr>
<td>initial_scale_power</td>
<td>int, default: 32</td>
<td>power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power</td>
<td>None</td>
</tr>
<tr>
<td>loss_scale</td>
<td>float, default: 0.0</td>
<td>loss scaling value for FP16 training (0.0 --&gt; dynamic scaling)</td>
<td>None</td>
</tr>
<tr>
<td>loss_scale_window</td>
<td>int, default: 1000</td>
<td>the window over which to raise/lower the dynamic loss scale value</td>
<td>None</td>
</tr>
<tr>
<td>min_loss_scale</td>
<td>int, default: 1000</td>
<td>minimum dynamic loss scale value</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedFP16Config:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed FP16 configuration class



        Attributes

        ----------

        hysteresis: int, default: 2

            represents the delay shift in dynamic loss scaling

        initial_scale_power: int, default: 32

            power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power

        loss_scale: float, default: 0.0

            loss scaling value for FP16 training (0.0 --&gt; dynamic scaling)

        loss_scale_window: int, default: 1000

            the window over which to raise/lower the dynamic loss scale value

        min_loss_scale: int, default: 1000

            minimum dynamic loss scale value



        &quot;&quot;&quot;



        hysteresis: int = 2

        initial_scale_power: int = 32

        loss_scale: float = 0.0

        loss_scale_window: int = 1000

        min_loss_scale: int = 1000
</code></pre></div>
<hr />
<h3 id="deepspeedflopsconfig">DeepspeedFlopsConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedFlopsConfig</span><span class="p">(</span>
    <span class="n">detailed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">module_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">output_file</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">profile_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">top_modules</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_10">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>detailed</td>
<td>bool, default: True</td>
<td>Whether to print the detailed model profile</td>
<td>None</td>
</tr>
<tr>
<td>module_depth</td>
<td>int, default: -1</td>
<td>The depth of the model at which to print the aggregated module information. When set to -1, it prints</td>
<td></td>
</tr>
<tr>
<td>information from the top module to the innermost modules (the maximum depth).</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>output_file</td>
<td>Optional[str], default: None</td>
<td>Path to the output file. If None, the profiler prints to stdout</td>
<td>None</td>
</tr>
<tr>
<td>profile_step</td>
<td>int, default: 1</td>
<td>The global training step at which to profile.</td>
<td>None</td>
</tr>
<tr>
<td>top_modules</td>
<td>int, default: 1</td>
<td>Limits the aggregated profile output to the number of top modules specified.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedFlopsConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed flops profiler configuration class



        Attributes

        ----------

        detailed: bool, default: True

            Whether to print the detailed model profile

        module_depth: int, default: -1

            The depth of the model at which to print the aggregated module information. When set to -1, it prints

            information from the top module to the innermost modules (the maximum depth).

        output_file: Optional[str], default: None

            Path to the output file. If None, the profiler prints to stdout

        profile_step: int, default: 1

            The global training step at which to profile.

        top_modules: int, default: 1

            Limits the aggregated profile output to the number of top modules specified.



        Notes

        -----

        Warm up steps are needed for accurate time measurement



        &quot;&quot;&quot;



        detailed: bool = True

        module_depth: int = -1

        output_file: Optional[str] = None

        profile_step: int = 1

        top_modules: int = 1
</code></pre></div>
<hr />
<h3 id="deepspeedoffloadoptimizerconfig">DeepspeedOffloadOptimizerConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedOffloadOptimizerConfig</span><span class="p">(</span>
    <span class="n">buffer_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">OffloadDevice</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span>
    <span class="n">fast_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">nvme_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;/local_nvme&#39;</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">pipeline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">pipeline_read</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">pipeline_write</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_11">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>buffer_count</td>
<td>int, default: 4</td>
<td>Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number</td>
<td></td>
</tr>
<tr>
<td>of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter,</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradient, momentum, and variance).</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>device</td>
<td>OffloadDevice, default: 'cpu'</td>
<td>Device memory to offload optimizer state</td>
<td>None</td>
</tr>
<tr>
<td>fast_init</td>
<td>bool, default: False</td>
<td>Enable fast optimizer initialization when offloading to NVMe</td>
<td>None</td>
</tr>
<tr>
<td>nvme_path</td>
<td>str, default: '/local_nvme'</td>
<td>Filesystem path for NVMe device for optimizer state offloading</td>
<td>None</td>
</tr>
<tr>
<td>pin_memory</td>
<td>bool, default: False</td>
<td>Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.</td>
<td>None</td>
</tr>
<tr>
<td>pipeline</td>
<td>bool, default: False</td>
<td>pipeline activated (will default to True if either pipeline_read or pipeline_write is set</td>
<td>to</td>
</tr>
<tr>
<td>pipeline_read</td>
<td>bool, default: False</td>
<td>activate pipeline read (deepspeed has limited docs for what this does)</td>
<td>None</td>
</tr>
<tr>
<td>pipeline_write</td>
<td>bool, default: False</td>
<td>activate pipeline write(deepspeed has limited docs for what this does)</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedOffloadOptimizerConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed optimizer offloading configuration class



        Attributes

        ----------

        buffer_count: int, default: 4

            Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number

            of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter,

            gradient, momentum, and variance).

        device: OffloadDevice, default: &#39;cpu&#39;

            Device memory to offload optimizer state

        fast_init: bool, default: False

            Enable fast optimizer initialization when offloading to NVMe

        nvme_path: str, default: &#39;/local_nvme&#39;

            Filesystem path for NVMe device for optimizer state offloading

        pin_memory: bool, default: False

            Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.

        pipeline: bool, default: False

            pipeline activated (will default to True if either pipeline_read or pipeline_write is set

        pipeline_read: bool, default: False

            activate pipeline read (deepspeed has limited docs for what this does)

        pipeline_write: bool, default: False

            activate pipeline write(deepspeed has limited docs for what this does)



        &quot;&quot;&quot;



        buffer_count: int = 4

        device: OffloadDevice = &quot;cpu&quot;

        fast_init: bool = False

        nvme_path: str = &quot;/local_nvme&quot;

        pin_memory: bool = False

        pipeline: bool = False

        pipeline_read: bool = False

        pipeline_write: bool = False
</code></pre></div>
<hr />
<h3 id="deepspeedoffloadparamconfig">DeepspeedOffloadParamConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedOffloadParamConfig</span><span class="p">(</span>
    <span class="n">buffer_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000000</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">OffloadDevice</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span>
    <span class="n">max_in_cpu</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000000</span><span class="p">,</span>
    <span class="n">nvme_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;/local_nvme&#39;</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_12">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>buffer_count</td>
<td>int, default: 5</td>
<td>Number of buffers in buffer pool for parameter offloading to NVMe</td>
<td>None</td>
</tr>
<tr>
<td>buffer_size</td>
<td>int, default: int(1E8)</td>
<td>Size of buffers in buffer pool for parameter offloading to NVMe</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>OffloadDevice, default: 'cpu'</td>
<td>Device memory to offload model parameters</td>
<td>None</td>
</tr>
<tr>
<td>max_in_cpu</td>
<td>int, default: int(1E9)</td>
<td>Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.</td>
<td>None</td>
</tr>
<tr>
<td>nvme_path</td>
<td>str, default: '/local_nvme'</td>
<td>Filesystem path for NVMe device for parameter offloading</td>
<td>None</td>
</tr>
<tr>
<td>pin_memory</td>
<td>bool, default: False</td>
<td>Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedOffloadParamConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed parameter offloading configuration class



        Attributes

        ----------

        buffer_count: int, default: 5

            Number of buffers in buffer pool for parameter offloading to NVMe

        buffer_size: int, default: int(1E8)

            Size of buffers in buffer pool for parameter offloading to NVMe

        device: OffloadDevice, default: &#39;cpu&#39;

            Device memory to offload model parameters

        max_in_cpu: int, default: int(1E9)

            Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.

        nvme_path: str, default: &#39;/local_nvme&#39;

            Filesystem path for NVMe device for parameter offloading

        pin_memory: bool, default: False

            Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.



        &quot;&quot;&quot;



        buffer_count: int = 5

        buffer_size: int = int(1e8)

        device: OffloadDevice = &quot;cpu&quot;

        max_in_cpu: int = int(1e9)

        nvme_path: str = &quot;/local_nvme&quot;

        pin_memory: bool = False
</code></pre></div>
<hr />
<h3 id="deepspeedpldconfig">DeepspeedPLDConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedPLDConfig</span><span class="p">(</span>
    <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_13">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>theta</td>
<td>float, default: 1.0</td>
<td>Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value,</td>
<td></td>
</tr>
<tr>
<td>the faster the training speed</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>gamma</td>
<td>float, default: 0.001</td>
<td>Hyper-parameter that controls how fast the drop ratio increases</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedPLDConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;

        Attributes

        ----------

        theta: float, default: 1.0

            Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value,

            the faster the training speed

        gamma: float, default: 0.001

            Hyper-parameter that controls how fast the drop ratio increases



        &quot;&quot;&quot;



        theta: float = 1.0

        gamma: float = 0.001
</code></pre></div>
<hr />
<h3 id="deepspeedtensorboardconfig">DeepspeedTensorboardConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedTensorboardConfig</span><span class="p">(</span>
    <span class="n">output_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
    <span class="n">job_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;DeepSpeedJobName&#39;</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_14">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>output_path</td>
<td>str, default: ''</td>
<td>Tensorboard output path</td>
<td>None</td>
</tr>
<tr>
<td>job_name</td>
<td>str, default: 'DeepSpeedJobName'</td>
<td>Tensorboard job name</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedTensorboardConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed Tensorboard configuration class



        Attributes

        ----------

        output_path: str, default: &#39;&#39;

            Tensorboard output path

        job_name: str, default: &#39;DeepSpeedJobName&#39;

            Tensorboard job name



        &quot;&quot;&quot;



        output_path: str = &quot;&quot;

        job_name: str = &quot;DeepSpeedJobName&quot;
</code></pre></div>
<hr />
<h3 id="deepspeedzeroconfig">DeepspeedZeROConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedZeROConfig</span><span class="p">(</span>
    <span class="n">allgather_bucket_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500000000</span><span class="p">,</span>
    <span class="n">allgather_partitions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">contiguous_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">ignore_unused_parameters</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">legacy_stage1</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">offload_optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedOffloadOptimizerConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">offload_param</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedOffloadParamConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">overlap_comm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">reduce_bucket_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500000000</span><span class="p">,</span>
    <span class="n">reduce_scatter</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">stage</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">stage3_max_live_parameters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000000</span><span class="p">,</span>
    <span class="n">stage3_max_reuse_distance</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000000</span><span class="p">,</span>
    <span class="n">stage3_prefetch_bucket_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500000000</span><span class="p">,</span>
    <span class="n">stage3_param_persistence_threshold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000</span><span class="p">,</span>
    <span class="n">stage3_gather_fp16_weights_on_model_save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sub_group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000000000</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_15">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>allgather_bucket_size</td>
<td>int, default: int(5E8)</td>
<td>Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes</td>
<td>None</td>
</tr>
<tr>
<td>allgather_partitions</td>
<td>bool, default: True</td>
<td>Chooses between allgather collective or a series of broadcast collectives to gather updated parameters</td>
<td></td>
</tr>
<tr>
<td>from all the GPUs at the end of each step</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>contiguous_gradients</td>
<td>bool, default: False</td>
<td>Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward</td>
<td></td>
</tr>
<tr>
<td>pass. Only useful when running very large models.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ignore_unused_parameters</td>
<td>bool, default: True</td>
<td>Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload</td>
<td></td>
</tr>
<tr>
<td>Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>legacy_stage1</td>
<td>bool, default: False</td>
<td>Use deepspeed &lt; v0.3.17 zero stage 1, kept for backwards compatability reasons</td>
<td>None</td>
</tr>
<tr>
<td>offload_optimizer</td>
<td>Optional[DeepspeedOffloadOptimizerConfig], default: None</td>
<td>Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU</td>
<td></td>
</tr>
<tr>
<td>memory for larger models or batch sizes. Valid only with stage 3</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>offload_param</td>
<td>Optional[DeepspeedOffloadParamConfig], default: None</td>
<td>Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch</td>
<td></td>
</tr>
<tr>
<td>sizes. Valid only with stage 3.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>overlap_comm</td>
<td>bool, default: False</td>
<td>Attempts to overlap the reduction of the gradients with backward computation</td>
<td>None</td>
</tr>
<tr>
<td>reduce_bucket_size</td>
<td>int, default: int(5E8)</td>
<td>Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large</td>
<td></td>
</tr>
<tr>
<td>model sizes</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>reduce_scatter</td>
<td>bool, default: True</td>
<td>Uses reduce or reduce scatter instead of allreduce to average gradients</td>
<td>None</td>
</tr>
<tr>
<td>stage</td>
<td>int, default: 0</td>
<td>Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state</td>
<td></td>
</tr>
<tr>
<td>partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning,</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>respectively</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_max_live_parameters</td>
<td>int, default: int(1E9)</td>
<td>The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but</td>
<td></td>
</tr>
<tr>
<td>perform more communication.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_max_reuse_distance</td>
<td>int, default: int(1E9)</td>
<td>Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less</td>
<td></td>
</tr>
<tr>
<td>memory, but perform more communication.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_prefetch_bucket_size</td>
<td>int, default: int(5E8)</td>
<td>The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase</td>
<td></td>
</tr>
<tr>
<td>stalls due to communication.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_param_persistence_threshold</td>
<td>int, default: int(1E6)</td>
<td>Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly</td>
<td></td>
</tr>
<tr>
<td>increase communication (especially latency-bound messages).</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_gather_fp16_weights_on_model_save</td>
<td>bool, default: False</td>
<td>Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned</td>
<td></td>
</tr>
<tr>
<td>across GPUs, they arent part of state_dict, so this function automatically gather the weights when this</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>option is enabled and then saves the fp16 model weights.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>sub_group_size</td>
<td>int, default: int(1E12)</td>
<td>sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are</td>
<td></td>
</tr>
<tr>
<td>grouped into buckets of sub_group_size and each buckets is updated one at a time.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedZeROConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed ZeRO configuration class



        Attributes

        ----------

        allgather_bucket_size: int, default: int(5E8)

            Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes

        allgather_partitions: bool, default: True

            Chooses between allgather collective or a series of broadcast collectives to gather updated parameters

            from all the GPUs at the end of each step

        contiguous_gradients: bool, default: False

            Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward

            pass. Only useful when running very large models.

        ignore_unused_parameters: bool, default: True

            Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload

            Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707

        legacy_stage1: bool, default: False

            Use deepspeed &lt; v0.3.17 zero stage 1, kept for backwards compatability reasons

        offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None

            Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU

            memory for larger models or batch sizes. Valid only with stage 3

        offload_param: Optional[DeepspeedOffloadParamConfig], default: None

            Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch

            sizes. Valid only with stage 3.

        overlap_comm: bool, default: False

            Attempts to overlap the reduction of the gradients with backward computation

        reduce_bucket_size: int, default: int(5E8)

            Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large

            model sizes

        reduce_scatter: bool, default: True

            Uses reduce or reduce scatter instead of allreduce to average gradients

        stage: int, default: 0

            Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state

            partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning,

            respectively

        stage3_max_live_parameters: int, default: int(1E9)

            The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but

            perform more communication.

        stage3_max_reuse_distance: int, default: int(1E9)

            Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less

            memory, but perform more communication.

        stage3_prefetch_bucket_size: int, default: int(5E8)

            The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase

            stalls due to communication.

        stage3_param_persistence_threshold: int, default: int(1E6)

            Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly

            increase communication (especially latency-bound messages).

        stage3_gather_fp16_weights_on_model_save: bool, default: False

            Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned

            across GPUs, they arent part of state_dict, so this function automatically gather the weights when this

            option is enabled and then saves the fp16 model weights.

        sub_group_size: int, default: int(1E12)

            sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are

            grouped into buckets of sub_group_size and each buckets is updated one at a time.



        &quot;&quot;&quot;



        allgather_bucket_size: int = int(5e8)

        allgather_partitions: bool = True

        contiguous_gradients: bool = False

        ignore_unused_parameters: bool = True

        legacy_stage1: bool = False

        offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None

        offload_param: Optional[DeepspeedOffloadParamConfig] = None

        overlap_comm: bool = False

        reduce_bucket_size: int = int(5e8)

        reduce_scatter: bool = True

        stage: int = 0

        stage3_max_live_parameters: int = int(1e9)

        stage3_max_reuse_distance: int = int(1e9)

        stage3_prefetch_bucket_size: int = int(5e8)

        stage3_param_persistence_threshold: int = int(1e6)

        stage3_gather_fp16_weights_on_model_save: bool = False

        sub_group_size: int = int(1e12)
</code></pre></div>
<hr />
<h3 id="distributedoptions">DistributedOptions</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DistributedOptions</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class DistributedOptions(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Enum that defines the options for Distributed backends&quot;&quot;&quot;



        horovod = &quot;horovod&quot;

        ddp = &quot;ddp&quot;

        deepspeed = &quot;deepspeed&quot;
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">ddp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">horovod</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="fp16options">FP16Options</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FP16Options</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class FP16Options(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Enum that defines the options for FP16 backends&quot;&quot;&quot;



        apex_O1 = &quot;apex_O1&quot;

        apex_O2 = &quot;apex_O2&quot;

        amp = &quot;amp&quot;

        deepspeed = &quot;deepspeed&quot;
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_2">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables_1">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">amp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">apex_O1</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">apex_O2</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="fairscalefsdpconfig">FairscaleFSDPConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleFSDPConfig</span><span class="p">(</span>
    <span class="n">bucket_cap_mb</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span>
    <span class="n">buffer_dtype</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clear_autocast_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">compute_dtype</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">flatten_parameters</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">force_input_to_fp32</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fp32_reduce_scatter</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gradient_predivide_factor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gradient_postdivide_factor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">move_grads_to_cpu</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">move_params_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">no_broadcast_optim_state</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">reshard_after_forward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_16">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>bucket_cap_mb</td>
<td>int, default: 25</td>
<td>FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters.</td>
<td></td>
</tr>
<tr>
<td>bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>the backward pass and freed at the end of the backward pass to save more memory for other phases of the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>backward. In FSDP, the buffer size does not change with model size (it changes based on number of</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><dtype, device, process_group> tuples) and gradient ready order matters little since FSDP has a final flush</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>compute is done differently too. Values &lt;= 0 disable bucketing</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>buffer_dtype</td>
<td>Optional[torch.dtype], default: None</td>
<td>dtype for buffers for computation. defaults to value of compute_dtype</td>
<td>value</td>
</tr>
<tr>
<td>clear_autocast_cache</td>
<td>bool, default: False</td>
<td>When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast</td>
<td></td>
</tr>
<tr>
<td>maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPU memory</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>compute_dtype</td>
<td>Optional[torch.dtype], default: None</td>
<td>dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set,</td>
<td></td>
</tr>
<tr>
<td>in which case it defaults to torch.float16.</td>
<td>torch.float32</td>
<td></td>
<td></td>
</tr>
<tr>
<td>flatten_parameters</td>
<td>bool, default: True</td>
<td>flatten parameters into a single contiguous tensor, which improves training speed</td>
<td>None</td>
</tr>
<tr>
<td>force_input_to_fp32</td>
<td>bool, default: False:</td>
<td>force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision</td>
<td></td>
</tr>
<tr>
<td>mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>fp32_reduce_scatter</td>
<td>bool, default: False</td>
<td>reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used</td>
<td>None</td>
</tr>
<tr>
<td>gradient_predivide_factor</td>
<td>Optional[float], default: None</td>
<td>divide factor before the reduction</td>
<td>None</td>
</tr>
<tr>
<td>gradient_postdivide_factor</td>
<td>Optional[float], default: None</td>
<td>divide factor after the reduction</td>
<td>None</td>
</tr>
<tr>
<td>move_grads_to_cpu</td>
<td>Optional[bool], default: None</td>
<td>move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used</td>
<td>None</td>
</tr>
<tr>
<td>move_params_to_cpu</td>
<td>bool, default: False</td>
<td>offload FP32 params to CPU. This is only relevant when FP16 AMP is used</td>
<td>None</td>
</tr>
<tr>
<td>no_broadcast_optim_state</td>
<td>Optional[bool], default: False</td>
<td>do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this</td>
<td></td>
</tr>
<tr>
<td>true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>parameters can fit on one node</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>reshard_after_forward</td>
<td>bool, default: True</td>
<td>reshard parameters after the forward pass. This saves memory but slows training. This is only relevant</td>
<td></td>
</tr>
<tr>
<td>when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html)</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>verbose</td>
<td>bool, default: True</td>
<td>turn on verbose output for models string representation</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleFSDPConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Fairscale Fully Sharded Data Parallel configuration class



        Attributes

        ----------

        bucket_cap_mb: int, default: 25

            FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters.

            bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the

            max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple

            bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without

            using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with

            computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory

            overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during

            the backward pass and freed at the end of the backward pass to save more memory for other phases of the

            training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP

            engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The

            order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the

            backward. In FSDP, the buffer size does not change with model size (it changes based on number of

            &lt;dtype, device, process_group&gt; tuples) and gradient ready order matters little since FSDP has a final flush

            call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with

            compute is done differently too. Values &lt;= 0 disable bucketing

        buffer_dtype: Optional[torch.dtype], default: None

            dtype for buffers for computation. defaults to value of compute_dtype

        clear_autocast_cache: bool, default: False

            When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast

            maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this

            flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save

            GPU memory

        compute_dtype: Optional[torch.dtype], default: None

            dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set,

            in which case it defaults to torch.float16.

        flatten_parameters: bool, default: True

            flatten parameters into a single contiguous tensor, which improves training speed

        force_input_to_fp32: bool, default: False:

            force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision

            mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper.

        fp32_reduce_scatter: bool, default: False

            reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used

        gradient_predivide_factor: Optional[float], default: None

            divide factor before the reduction

        gradient_postdivide_factor: Optional[float], default: None

            divide factor after the reduction

        move_grads_to_cpu: Optional[bool], default: None

            move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used

        move_params_to_cpu: bool, default: False

            offload FP32 params to CPU. This is only relevant when FP16 AMP is used

        no_broadcast_optim_state: Optional[bool], default: False

            do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this

            true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the

            proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few

            parameters can fit on one node

        reshard_after_forward: bool, default: True

            reshard parameters after the forward pass. This saves memory but slows training. This is only relevant

            when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html)

        verbose: bool, default: True

            turn on verbose output for models string representation



        Notes

        -----

        mixed_precision: bool

            This value will automatically be set from the Stoke FP16 selected option (AMP only)

        state_dict_device: torch.device

            this is not exposed as it should be managed internally from the DDP backend setup

        compute_device: torch.device

            this is not exposed as it should be managed internally from the DDP backend setup



        &quot;&quot;&quot;



        bucket_cap_mb: int = 25

        buffer_dtype: Optional[torch.dtype] = None

        clear_autocast_cache: bool = False

        compute_dtype: Optional[torch.dtype] = None

        flatten_parameters: bool = True

        force_input_to_fp32: bool = False

        fp32_reduce_scatter: bool = False

        gradient_predivide_factor: Optional[float] = None

        gradient_postdivide_factor: Optional[float] = None

        move_grads_to_cpu: Optional[bool] = None

        move_params_to_cpu: bool = False

        no_broadcast_optim_state: Optional[bool] = False

        reshard_after_forward: bool = True

        verbose: bool = False
</code></pre></div>
<hr />
<h4 id="descendants">Descendants</h4>
<ul>
<li>stoke.extensions._FairscaleFSDPConfig</li>
</ul>
<h3 id="fairscaleossconfig">FairscaleOSSConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleOSSConfig</span><span class="p">(</span>
    <span class="n">broadcast_fp16</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_17">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>broadcast_fp16</td>
<td>bool, default: False</td>
<td>Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP</td>
<td></td>
</tr>
<tr>
<td>is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleOSSConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Fairscale optimizer state sharding configuration class



        Attributes

        ----------

        broadcast_fp16: bool, default: False

            Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP

            is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy.



        &quot;&quot;&quot;



        broadcast_fp16: bool = False
</code></pre></div>
<hr />
<h3 id="fairscalesddpconfig">FairscaleSDDPConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleSDDPConfig</span><span class="p">(</span>
    <span class="n">auto_refresh_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">broadcast_buffers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">reduce_buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8388608</span><span class="p">,</span>
    <span class="n">reduce_fp16</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sync_models_at_startup</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_18">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>auto_refresh_trainable</td>
<td>bool, default: True</td>
<td>Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS</td>
<td></td>
</tr>
<tr>
<td>automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>parameter is frozen or unfrozen</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>broadcast_buffers</td>
<td>bool, default: True</td>
<td>Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same</td>
<td></td>
</tr>
<tr>
<td>setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>reduce_buffer_size</td>
<td>int, default: 2 ** 23</td>
<td>he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact</td>
<td></td>
</tr>
<tr>
<td>the long term memory consumption, because these buckets correspond to parameters which will not be sharded.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Set to 0 to remove all bucketing, 1M to 8M is usually reasonable.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>reduce_fp16</td>
<td>bool, default: False</td>
<td>cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve</td>
<td></td>
</tr>
<tr>
<td>performance for multi node jobs using PyTorch AMP. The effect is similar to DDPs fp16_compress_hook and</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>will also save some memory.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>sync_models_at_startup</td>
<td>bool, default: True</td>
<td>Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or</td>
<td></td>
</tr>
<tr>
<td>the training restarts from a saved state</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleSDDPConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Fairscale sharded data parallel (SDDP) configuration class



        Attributes

        ----------

        auto_refresh_trainable: bool, default: True

            Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS

            automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a

            parameter is frozen or unfrozen

        broadcast_buffers: bool, default: True

            Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same

            setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters.

        reduce_buffer_size: int, default: 2 ** 23

            he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact

            the long term memory consumption, because these buckets correspond to parameters which will not be sharded.

            Set to 0 to remove all bucketing, 1M to 8M is usually reasonable.

        reduce_fp16: bool, default: False

            cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve

            performance for multi node jobs using PyTorch AMP. The effect is similar to DDPs fp16_compress_hook and

            will also save some memory.

        sync_models_at_startup: bool, default: True

            Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or

            the training restarts from a saved state



        &quot;&quot;&quot;



        auto_refresh_trainable: bool = True

        broadcast_buffers: bool = True

        reduce_buffer_size: int = 2 ** 23

        reduce_fp16: bool = False

        sync_models_at_startup: bool = True
</code></pre></div>
<hr />
<h3 id="horovodconfig">HorovodConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">HorovodConfig</span><span class="p">(</span>
    <span class="n">compression</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">convert_to_sync_batch_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gradient_predivide_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">op</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">HorovodOps</span> <span class="o">=</span> <span class="s1">&#39;Average&#39;</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_19">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>compression</td>
<td>bool, default: False</td>
<td>Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter</td>
<td></td>
</tr>
<tr>
<td>update step.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>convert_to_sync_batch_norm</td>
<td>bool, default: False</td>
<td>Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls</td>
<td></td>
</tr>
<tr>
<td>https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradient_predivide_factor</td>
<td>float, default: 1.0</td>
<td>If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled</td>
<td></td>
</tr>
<tr>
<td>by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>op</td>
<td>HorovodOps, default: 'Average'</td>
<td>The reduction operation to use when combining gradients across different ranks.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class HorovodConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Horovod configuration class



        Attributes

        ----------

        compression: bool, default: False

            Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter

            update step.

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls

            https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm

        gradient_predivide_factor: float, default: 1.0

            If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled

            by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum.

        op: HorovodOps, default: &#39;Average&#39;

            The reduction operation to use when combining gradients across different ranks.



        &quot;&quot;&quot;



        compression: bool = False

        convert_to_sync_batch_norm: bool = False

        gradient_predivide_factor: float = 1.0

        op: HorovodOps = &quot;Average&quot;
</code></pre></div>
<hr />
<h3 id="paramnormalize">ParamNormalize</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ParamNormalize</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class ParamNormalize(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Normalization enum for total number of model parameters used to help with a pretty print&quot;&quot;&quot;



        THOUSAND = 1e3

        MILLION = 1e6

        BILLION = 1e9

        TRILLION = 1e12
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_3">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables_2">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">BILLION</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">MILLION</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">THOUSAND</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">TRILLION</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="stoke">Stoke</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Stoke</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">StokeOptimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">]],</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">grad_accum_steps</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fp16</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">status</span><span class="o">.</span><span class="n">FP16Options</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">distributed</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">status</span><span class="o">.</span><span class="n">DistributedOptions</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fairscale_oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fairscale_sddp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fairscale_fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">configs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">AMPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ApexConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleOSSConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleSDDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleFSDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">HorovodConfig</span><span class="p">]],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">ema_weight</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_20">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>amp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>apex_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>batch_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>cuda</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>ddp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>distributed</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>effective_batch_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>ema_loss</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fp16</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fsdp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fully_sharded</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>gpu</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>grad_clip</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>horovod_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_amp</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_apex</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_ddp</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_deepspeed</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_horovod</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>loss_access</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_access</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>nccl</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>num_model_parameters</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>oss_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>sddp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>sharded</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>status</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>world_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_agg_loss</td>
<td>Union[float, List[float], Tuple[float]]</td>
<td>aggregated loss for grad accumulation (single or multiple losses)</td>
<td>None</td>
</tr>
<tr>
<td>_backward_steps</td>
<td>int</td>
<td>Number of times gradients have been calculated on a batch of samples (calls to backward)</td>
<td>None</td>
</tr>
<tr>
<td>_grad_accum_counter</td>
<td>int</td>
<td>counter for grad accumulation steps</td>
<td>None</td>
</tr>
<tr>
<td>_loss</td>
<td>Union[Callable, List[Callable], Tuple[Callable]]</td>
<td>callable function that calculates a loss from the model outputs</td>
<td>None</td>
</tr>
<tr>
<td>_last_step_loss</td>
<td>list, tuple, or float</td>
<td>last loss step calculation aggregated over device(s)</td>
<td>None</td>
</tr>
<tr>
<td>_model</td>
<td>torch.nn.Module</td>
<td>instance of torch.nn.Module for Stoke to handle</td>
<td>None</td>
</tr>
<tr>
<td>_optimizer</td>
<td>StokeOptimizer</td>
<td>StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs</td>
<td>None</td>
</tr>
<tr>
<td>_optimizer_steps</td>
<td>int</td>
<td>Number of times step has been called on the optimizer</td>
<td>None</td>
</tr>
<tr>
<td>_runner</td>
<td>StokeRunner</td>
<td>the dynamically created runtime object that handles all ops</td>
<td>None</td>
</tr>
<tr>
<td>_status</td>
<td>StokeStatus</td>
<td>StokeStatus object that sets and maintains the current configuration</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool</td>
<td>print verbosity</td>
<td>None</td>
</tr>
<tr>
<td>_rolling_loss_steps</td>
<td>int</td>
<td>number of steps that have been called for the rolling loss</td>
<td>None</td>
</tr>
<tr>
<td>_rolling_mean_loss</td>
<td>list, tuple, or float</td>
<td>current ema loss</td>
<td>None</td>
</tr>
<tr>
<td>_ema_weight</td>
<td>float</td>
<td>weight used for any ema calculation on metrics</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class Stoke:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;High level stoke object that manages all necessary configs and provides a unified interface to ops



        This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model,

        loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the

        combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale),

        mixed-precision (AMP or APEX) and devices (CPU or GPU)



        Attributes

        ----------

        amp_config

        apex_config

        batch_size

        cuda

        ddp_config

        deepspeed_config

        distributed

        effective_batch_size

        ema_loss

        fp16

        fsdp_config

        fully_sharded

        gpu

        grad_accum

        grad_clip

        horovod_config

        is_amp

        is_apex

        is_ddp

        is_deepspeed

        is_horovod

        loss_access

        model_access

        nccl

        num_model_parameters

        optimizer

        oss

        oss_config

        rank

        scaler

        sddp_config

        sharded

        status

        world_size

        _agg_loss: Union[float, List[float], Tuple[float]]

            aggregated loss for grad accumulation (single or multiple losses)

        _backward_steps: int

            Number of times gradients have been calculated on a batch of samples (calls to backward)

        _grad_accum_counter: int

            counter for grad accumulation steps

        _loss: Union[Callable, List[Callable], Tuple[Callable]]

            callable function that calculates a loss from the model outputs

        _last_step_loss: list, tuple, or float

            last loss step calculation aggregated over device(s)

        _model: torch.nn.Module

            instance of torch.nn.Module for Stoke to handle

        _optimizer: StokeOptimizer

            StokeOptimizer config object that describes the torch.optim.Optimizer and it&#39;s kwargs

        _optimizer_steps: int

            Number of times step has been called on the optimizer

        _runner: StokeRunner

            the dynamically created runtime object that handles all ops

        _status: StokeStatus

            StokeStatus object that sets and maintains the current configuration

        _verbose: bool

            print verbosity

        _rolling_loss_steps: int

            number of steps that have been called for the rolling loss

        _rolling_mean_loss: list, tuple, or float

            current ema loss

        _ema_weight: float

            weight used for any ema calculation on metrics



        &quot;&quot;&quot;



        def __init__(

            self,

            model: torch.nn.Module,

            optimizer: StokeOptimizer,

            loss: Union[Callable, List[Callable], Tuple[Callable]],

            batch_size_per_device: int,

            grad_accum_steps: Optional[int] = 1,

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None,

            gpu: bool = False,

            fp16: Optional[FP16Options] = None,

            distributed: Optional[DistributedOptions] = None,

            fairscale_oss: bool = False,

            fairscale_sddp: bool = False,

            fairscale_fsdp: bool = False,

            configs: Optional[

                List[

                    Union[

                        AMPConfig,

                        ApexConfig,

                        DDPConfig,

                        DeepspeedConfig,

                        FairscaleOSSConfig,

                        FairscaleSDDPConfig,

                        FairscaleFSDPConfig,

                        HorovodConfig,

                    ]

                ]

            ] = None,

            info_rank: Optional[Union[int, List[int]]] = 0,

            verbose: bool = True,

            ema_weight: float = 0.1,

        ):

            &quot;&quot;&quot;Init for Stoke class object



            Parameters

            ----------

            model: torch.nn.Module

                PyTorch model

            optimizer: StokeOptimizer

                Optimizer configuration

            loss: Union[Callable, List[Callable], Tuple[Callable]]

                Callable loss function or functions

            batch_size_per_device: int

                Batch size at the single device level

            grad_accum_steps: Optional[int], default: 1

                Number of gradient accumulation steps

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                Gradient clipping configuration

            gpu: bool, default: False

                flag to use GPU device(s)

            fp16: Optional[FP16Options], default: None

                Choice of mixed-precision backend

            distributed: Optional[DistributedOptions], default: None

                Choice of distributed backend

            fairscale_oss: bool, default: False

                Flag to activate optimizer state sharding using Fairscale

            fairscale_sddp: bool, default: False

                Flag to activate sharded DDP using Fairscale

            fairscale_fsdp: bool, default: False

                Flag to activate fully sharded DDP using Fairscale

            configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None

                Configuration objects for runtimes

            info_rank: Optional[Union[int, List[int]]], default = 0

                Constrain prints to specific devices

            verbose: bool, default: True

                Flag for verbosity

            ema_weight: float, default: 0.5

                weight used for any ema calculation on metrics



            &quot;&quot;&quot;

            # Verbosity

            self._verbose = verbose

            # Info rank

            self._info_rank = info_rank

            # EMA

            self._ema_weight = ema_weight

            # Setup the StokeState

            self._status = StokeStatus(

                batch_size_per_device=batch_size_per_device,

                grad_accum=grad_accum_steps,

                grad_clip=grad_clip,

                gpu=gpu,

                fp16=fp16,

                distributed=distributed,

                fairscale_oss=fairscale_oss,

                fairscale_sddp=fairscale_sddp,

                fairscale_fsdp=fairscale_fsdp,

                configs=configs,

            )

            # Run some checks

            self._model = self._check_model(model)

            self._optimizer = self._check_optimizer(optimizer)

            self._loss = self._check_loss(loss)

            # Dynamically construct the StokeRunner from the StokeStatus

            self._runner, class_info = self._build_runner()

            # Setup distributed backend

            self._runner.setup_distributed()

            # Post here the runner will have the print_device function that is mapped to the self.print here

            # as it needs rank to be accessible before working

            if self._verbose:

                dev_id = (

                    self.rank

                    if (self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;)

                    else self._info_rank

                )

                self.print(f&quot;Printing verbose information on rank(s): {dev_id}&quot;)

                # Print the runner class info from the mixins

                self.print(class_info)

            # Possibly place model on GPU depending on StokeStatus -- before wrap calls

            self._place_model_on_gpu()

            # Handle the wrap ops in the correct order

            self._handle_ordered_wrap_ops(optimizer=optimizer)

            # Create some tracking vars

            self._grad_accum_counter = 0

            self._optimizer_steps = 0

            self._backward_steps = 0

            self._last_step_loss = self._set_loss_to_zero()

            self._agg_loss = self._set_loss_to_zero()

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0

            # Set post-init status variables

            self._status.set_post_init_values(world_size=self.world_size)

            # Print the final configuration

            if self._verbose:

                self.print(msg=self._status)



        def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping of optimizer then the model



            This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped

            methods are called



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Build the optimizer

            self._optimizer = self._runner.build_optimizer(

                optimizer=optimizer[&quot;optimizer&quot;],

                optimizer_kwargs=optimizer[&quot;optimizer_kwargs&quot;],

                model=self._model,

            )

            # Setup/Initialize FP16 backend -- in this case the optimizer is passed through

            self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer)

            # Wrap with distributed backend -- in this case the optimizer is passed through

            self._model, self._optimizer = self._runner.wrap_distributed(

                model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer

            )



        def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping of model then optimizer



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn&#39;t exist yet

            # don&#39;t use the return for the optimizer in this case

            self._model, _ = self._runner.wrap_distributed(

                model=self._model, grad_accum=self.grad_accum, optimizer=None

            )

            # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn&#39;t exist yet

            self._runner.wrap_fp16(model=self._model, optimizer=None)

            # Build the optimizer

            self._optimizer = self._runner.build_optimizer(

                optimizer=optimizer[&quot;optimizer&quot;],

                optimizer_kwargs=optimizer[&quot;optimizer_kwargs&quot;],

                model=self._model,

            )



        def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model

            # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed)

            if (self.sharded and self.oss) or self.is_apex or self.is_horovod:

                self._wrap_optimizer_then_model(optimizer=optimizer)

            else:

                self._wrap_model_then_optimizer(optimizer=optimizer)



        def _check_accum(self):

            &quot;&quot;&quot;Checks if the current step is the last accumulation step



            Returns

            -------

            bool



            &quot;&quot;&quot;

            return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0



        def _check_pre_accum(self):

            &quot;&quot;&quot;Checks if we are at the pre-accumulate step



            Returns

            -------

            bool



            &quot;&quot;&quot;

            return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum



        def _set_loss_to_zero(self):

            &quot;&quot;&quot;Used to set a loss tracker to zero depending on the type



            Returns

            -------

            float or list or tuple of reset loss



            &quot;&quot;&quot;

            return (

                type(self._loss)([0.0] * len(self._loss))

                if isinstance(self._loss, (list, tuple))

                else 0.0

            )



        def reset_ema(self):

            &quot;&quot;&quot;Used to reset the current state of the rolling mean loss



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0



        def print_ema_loss(

            self, prepend_msg: str = &quot;Current EMA Loss&quot;, single_line: bool = False

        ):

            &quot;&quot;&quot;Prints the current ema loss synced across all devices



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Current EMA Loss&quot;

                message prepend to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(self._rolling_mean_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val:.3f}&quot;

                    for idx, val in enumerate(self._rolling_mean_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(f&quot;{prepend_msg}: {self._rolling_mean_loss:.3f}&quot;)



        def print_mean_accumulated_synced_loss(

            self,

            prepend_msg: str = &quot;Mean Accumulated &amp; Synced Loss&quot;,

            pre_backwards: bool = True,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints the mean accumulated and device synced loss only after the grad accumulation step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Mean Accumulated &amp; Synced Loss&quot;

                message prepend to print

            pre_backwards: bool, default: True

                if being called pre backward step

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            check_fn = self._check_pre_accum if pre_backwards else self._check_accum

            if check_fn():

                if isinstance(self._agg_loss, (list, tuple)):

                    print_vals = self._scale_agg_loss()

                    self.print(print_vals, single_line=single_line)

                else:

                    self.print(f&quot;{prepend_msg}: {self._scale_agg_loss():.3f}&quot;)



        def _scale_agg_loss(self):

            &quot;&quot;&quot;Scales the mean aggregated loss by  grad accum



            Returns

            -------

            scale_vals: list or float of mean aggregated loss



            &quot;&quot;&quot;

            if isinstance(self._agg_loss, (list, tuple)):

                scale_vals = [

                    val / self.grad_accum for idx, val in enumerate(self._agg_loss)

                ]

            else:

                scale_vals = self._agg_loss / self.grad_accum

            return scale_vals



        def print_synced_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            prepend_msg: str = &quot;Step Synced Loss&quot;,

            device=None,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints a device synced loss at a single step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            prepend_msg: str, default: &quot;Step Synced Loss&quot;

                message prepend to print

            device: default: None

                specify the device to place the synced loss on (defaults to same device)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            printable_loss = self.detach_and_sync_loss(loss, device)

            if isinstance(printable_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val * self.grad_accum:.3f}&quot;

                    for idx, val in enumerate(printable_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(msg=f&quot;{prepend_msg}: {printable_loss * self.grad_accum:.3f}&quot;)



        def print_on_devices(

            self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0

        ):

            &quot;&quot;&quot;Wraps runner print interface for shorter semantics



            Parameters

            ----------

            msg: str

                message to print

            rank: Union[int, List[int]], default: 0

                which ranks to print on



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(msg=msg, rank=rank)



        def print(self, msg: Union[str, List[str]], single_line: bool = False):

            &quot;&quot;&quot;Wraps the runners print device and forces print on the _info_rank attribute(s)



            Parameters

            ----------

            msg: str

                message to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(

                msg=msg, rank=self._info_rank, single_line=single_line

            )



        @staticmethod

        def _check_model(model: torch.nn.Module):

            &quot;&quot;&quot;Verifies the type of the model



            Parameters

            ----------

            model: torch.nn.Module

                current torch model



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Check if the model is an nn.Module such that it has a forward method

            if not isinstance(model, torch.nn.Module):

                raise TypeError(

                    f&quot;Stoke -- Model is not of type torch.nn.Module, currently {type(model)}&quot;

                )

            return model



        @staticmethod

        def _check_optimizer(optimizer: StokeOptimizer):

            &quot;&quot;&quot;Verifies the type of the optimizer



            Parameters

            ----------

            optimizer: StokeOptimizer

                Current optimizer configuration TypedDict (aka dict)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if not isinstance(optimizer, dict):

                raise TypeError(

                    f&quot;Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}&quot;

                )

            return optimizer



        def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]):

            &quot;&quot;&quot;Checks to make sure the loss function(s) is/are callable



            Parameters

            ----------

            loss: Union[Callable, List[Callable], Tuple[Callable]]

                Current callable loss(es)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                loss = [self._check_loss(val) for val in loss]

                return loss

            elif isinstance(loss, Callable):

                return loss

            else:

                raise TypeError(

                    f&quot;Stoke -- Loss is not of type Callable, currently {type(loss)}&quot;

                )



        def _place_model_on_gpu(self):

            &quot;&quot;&quot;Automatically moves the model to GPU device(s)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.gpu and not self.is_deepspeed:

                if self._verbose:

                    self.print(f&quot;Automatically handling moving model to GPU(s)...&quot;)

                self._model.cuda()



        def _build_runner(self):

            &quot;&quot;&quot;Builds the runtime object from the mixin style classes



            Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called

            from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a

            a single interface. Might prevent some IDE type-hinting as it&#39;s dynamic



            Returns

            -------

            StokeRunner

                runtime runner object



            &quot;&quot;&quot;

            # Get the classes

            dist_class = self._get_distributed_mixin()

            fp16_class = self._get_fp16_mixin()

            optimizer_class = self._get_optimizer_mixin()

            io_class = self._get_io_mixin()



            # Python MRO hack to make sure the inits of all the Mixin classes get called

            def __multiple_mixin_init__(*args, **kwargs):

                dist_class.__init__(*args, **kwargs)

                fp16_class.__init__(*args, **kwargs)

                optimizer_class.__init__(*args, **kwargs)

                io_class.__init__(*args, **kwargs)



            # Configs pass through

            kwargs_dict = {

                &quot;amp_config&quot;: self.amp_config,

                &quot;apex_config&quot;: self.apex_config,

                &quot;ddp_config&quot;: self.ddp_config,

                &quot;deepspeed_config&quot;: self.deepspeed_config,

                &quot;horovod_config&quot;: self.horovod_config,

                &quot;oss_config&quot;: self.oss_config,

                &quot;sharded_config&quot;: self.sddp_config,

                &quot;fully_sharded_config&quot;: self.fsdp_config,

            }

            # Generate the runner class from the mixins based on the StokeStatus

            runner_class = type(

                &quot;StokeRunner&quot;,

                (dist_class, fp16_class, optimizer_class, io_class),

                {&quot;__init__&quot;: __multiple_mixin_init__},

            )(

                verbose=self._verbose,

                batch_size_per_device=self.batch_size,

                grad_accum_steps=self.grad_accum,

                grad_clip=self.grad_clip,

                info_rank=self._info_rank,

                loss=self._loss,

                **kwargs_dict,

            )

            # Make a list of class info for print later

            class_info = [

                f&quot;Distributed Mixin: {dist_class.__name__}&quot;,

                f&quot;Optimizer Mixin: {dist_class.__name__}&quot;,

                f&quot;FP16 Mixin: {fp16_class.__name__}&quot;,

                f&quot;IO Mixin: {io_class.__name__}&quot;,

            ]

            return runner_class, class_info



        def _get_io_mixin(self):

            &quot;&quot;&quot;Determines which IO class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated ioclass



            &quot;&quot;&quot;

            if self.is_deepspeed:

                return_class = RunnerIOEnum.deepspeed.value

            elif self.is_horovod:

                return_class = RunnerIOEnum.horovod.value

            elif self.is_ddp:

                return_class = RunnerIOEnum.ddp.value

            else:

                return_class = RunnerIOEnum.base.value

            return return_class



        def _get_optimizer_mixin(self):

            &quot;&quot;&quot;Determines which optimizer class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated optimizer class



            &quot;&quot;&quot;

            if self.oss:

                return_class = RunnerOptimizerEnum.oss.value

            else:

                return_class = RunnerOptimizerEnum.base.value

            return return_class



        def _get_distributed_mixin(self):

            &quot;&quot;&quot;Determines which distributed class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated distributed class



            &quot;&quot;&quot;

            # if not gpu then fall to cpu single

            if not self.gpu:

                return_class = RunnerDistEnum.cpu.value

            # if gpu but no distributed then fall to single gpu

            elif self.gpu and (self.distributed is None):

                return_class = RunnerDistEnum.gpu.value

            elif self.gpu and (self.distributed is not None):

                return_class = RunnerDistEnum[self.distributed].value

            else:

                raise ValueError(&quot;Stoke -- Cannot map to a valid distributed class&quot;)

            return return_class



        def _get_fp16_mixin(self):

            &quot;&quot;&quot;Determines which fp16 class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated fp16 class



            &quot;&quot;&quot;

            if self.fp16 is not None:

                return_class = RunnerFP16Enum[self.fp16].value

            else:

                return_class = RunnerFP16Enum.full.value

            return return_class



        def DataLoader(

            self,

            dataset: Dataset[T_co],

            shuffle: bool = False,

            sampler: Optional[Sampler[int]] = None,

            batch_sampler: Optional[Sampler[Sequence[int]]] = None,

            num_workers: int = 0,

            collate_fn: _collate_fn_t = None,

            pin_memory: bool = False,

            drop_last: bool = False,

            timeout: float = 0,

            worker_init_fn: _worker_init_fn_t = None,

            multiprocessing_context=None,

            generator=None,

            *,

            prefetch_factor: int = 2,

            persistent_workers: bool = False,

        ):

            &quot;&quot;&quot;Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs.



            Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called)

            and to automatically handle device placement since the gpu/fp16 flags can&#39;t be determined until the StokeStatus

            object is available which is post init. This could be disconnected from this class but it would require the

            user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and

            never handled



            Parameters

            ----------

            dataset: Dataset

                dataset from which to load the data.

            shuffle: bool, default: False

                set to ``True`` to have the data reshuffled at every epoch.

            sampler: Sampler or Iterable, default: None

                defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__``

                implemented. If specified, :attr:`shuffle` must not be specified.

            batch_sampler: Sampler or Iterable, default: None:

                like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with

                :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.

            num_workers: int, default: 0

                how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process.

            collate_fn: callable, optional:

                merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a

                map-style dataset.

            pin_memory: bool, default: False:

                If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your

                data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,

                see the example below.

            drop_last: bool, default: False

                set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size.

                If ``False`` and the size of dataset is not divisible by the batch size, then the last batch

                will be smaller.

            timeout: numeric, default: 0

                if positive, the timeout value for collecting a batch from workers. Should always be non-negative.

            worker_init_fn: callable, default: None

                If not ``None``, this will be called on each worker subprocess with the worker id

                (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading.

            prefetch_factor: int, default: 2

                Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers

                samples prefetched across all workers.

            persistent_workers: bool, default: False

                If ``True``, the data loader will not shutdown the worker processes after a dataset has been

                consumed once. This allows to maintain the workers `Dataset` instances alive.



            Returns

            -------

            StokeDataLoader

                wrapped torch.utils.data.DataLoader object



            &quot;&quot;&quot;

            # Check if forkserver is available for horovod and use

            if (

                num_workers &gt; 0

                and hasattr(torch.multiprocessing, &quot;_supports_context&quot;)

                and torch.multiprocessing._supports_context

                and &quot;forkserver&quot; in torch.multiprocessing.get_all_start_methods()

                and self.is_horovod

            ):

                multiprocessing_context = &quot;forkserver&quot;



            if self._verbose and self.gpu:

                print(f&quot;Automatically handling moving model input data to GPU(s)...&quot;)

            # Forward the already known options from the Stoke status

            return StokeDataLoader(

                gpu=self.gpu,

                fp16=self.fp16,

                batch_size=self.batch_size,

                dataset=dataset,

                shuffle=shuffle,

                sampler=sampler,

                batch_sampler=batch_sampler,

                num_workers=num_workers,

                collate_fn=collate_fn,

                pin_memory=pin_memory,

                drop_last=drop_last,

                timeout=timeout,

                worker_init_fn=worker_init_fn,

                multiprocessing_context=multiprocessing_context,

                generator=generator,

                prefetch_factor=prefetch_factor,

                persistent_workers=persistent_workers,

            )



        def model(self, *args, **kwargs):

            &quot;&quot;&quot;Wrapped model forward call



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the model forward call



            Returns

            -------

            model forward output



            &quot;&quot;&quot;

            with self._runner.model_context:

                return self._model(*args, **kwargs)

                # return self.model_access(*args, **kwargs)



        def loss(self, *args, **kwargs):

            &quot;&quot;&quot;Wrapped callable loss function call



            Handles internal logic of aggregating up the losses for single and multiple losses



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the loss function call(s)



            Returns

            -------

            outputs of callable loss function(s)



            &quot;&quot;&quot;

            # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch

            with self._runner.loss_context:

                if isinstance(self._loss, (list, tuple)):

                    loss = type(self._loss)(val(*args, **kwargs) for val in self._loss)

                    sync_loss = [self.detach_and_sync_loss(val) for val in loss]

                    self._last_step_loss = type(self._loss)(

                        val for idx, val in enumerate(sync_loss)

                    )

                    self._agg_loss = type(self._loss)(

                        self._agg_loss[idx] + val for idx, val in enumerate(sync_loss)

                    )

                    self._handle_ema_loss(loss=sync_loss)

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = type(loss)(val / self.grad_accum for val in loss)

                else:

                    loss = self._loss(*args, **kwargs)

                    sync_loss = self.detach_and_sync_loss(loss)

                    self._last_step_loss = sync_loss

                    self._agg_loss += sync_loss

                    self._handle_ema_loss(loss=sync_loss)

                    # Handle grad accumulation by dividing by the accumulation steps

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = loss / self.grad_accum

                return loss



        def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]):

            &quot;&quot;&quot;Handles calculating the ema loss



            Parameters

            ----------

            loss: Union[float, List[float], Tuple[float]]

                current calculated loss list, tuple or float



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._rolling_loss_steps += 1

            if isinstance(loss, (list, tuple)):

                self._rolling_mean_loss = type(self._rolling_mean_loss)(

                    self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx])

                    for idx, val in enumerate(loss)

                )

            else:

                self._rolling_mean_loss = self._ema_loss(

                    value=loss, current_mean=self._rolling_mean_loss

                )



        def _ema_loss(self, value: float, current_mean: float):

            &quot;&quot;&quot;Calculate the ema of the loss



            Parameters

            ----------

            value: float

                current loss value

            current_mean: float

                current mean value



            Returns

            -------

            current ema value: float



            &quot;&quot;&quot;

            if self._rolling_loss_steps == 1:

                return value

            else:

                return (self._ema_weight * value) + (

                    (1.0 - self._ema_weight) * current_mean

                )



        def backward(

            self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

        ):

            &quot;&quot;&quot;Wrapped backwards call



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                Callable loss function(s)



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Increment the grad counter

            self._grad_accum_counter += 1

            # Set the context based on the counter

            dist_cm = (

                nullcontext()

                if self._check_accum()

                else self._runner.grad_accum_context(self._model)

            )

            with dist_cm:

                self._runner.backward_call(

                    loss=loss, model=self.model_access, optimizer=self._optimizer

                )

            # Increment the number of total calls to backward (each backward to a loss is only considered 1)

            self._backward_steps += 1



        def step(self):

            &quot;&quot;&quot;Wrapped step call



            Handles grad clipping internally



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer only if the modulo is zero

            if self._check_accum():

                if self._verbose and self.grad_accum &gt; 0:

                    self.print(f&quot;Gradient Accumulation Steps: {self.grad_accum}&quot;)

                # Clip if needed

                if self.grad_clip is not None:

                    self._runner.clip_grad(

                        self.grad_clip,

                        self._model if self.fully_sharded else self.model_access,

                        self._optimizer,

                        oss=self.oss,

                        horovod=self.is_horovod,

                        deepspeed=self.is_deepspeed,

                        fsdp=self.fully_sharded,

                    )

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )

                # Reset for the accumulated step

                self._reset()

                # Increment the number of step calls to the optimizer

                self._optimizer_steps += 1

            # if deepspeed we need to step everytime as it handles the grad accumulation internally

            elif self.is_deepspeed:

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )



        def _reset(self):

            &quot;&quot;&quot;Resets the state post optimizer step call



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self.print(&quot;Resetting all grad/variables for next optimizer step&quot;)

            # Zero the grads if not deepspeed

            if not self.is_deepspeed:

                self.zero_grads()

            # Reset counter

            self._grad_accum_counter = 0

            # Reset agg loss -- single or mutiple losses

            self._agg_loss = self._set_loss_to_zero()



        def save(

            self,

            path: str,

            name: str = uuid4(),

            extension: str = &quot;pt&quot;,

            create_directory: bool = True,

            extras: Optional[dict] = None,

        ):

            &quot;&quot;&quot;Saves a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory to save the model checkpoint (prefer absolute paths over relative paths)

            name: str, default: uuid4()

                name used to save checkpoint file

            extension: str, default: &#39;.pt&#39;

                extension used to save PyTorch model checkpoint

            create_directory: bool, default: True

                flag to create the directory path if it doesn&#39;t exist

            extras: dict, default: None

                a dictionary of any extra things to save



            Returns

            -------

            path: str

                path to directory that the model checkpoint was saved

            tag: str

                full tag name the model checkpoint was saved as



            &quot;&quot;&quot;

            out_path, tag = self._runner.save(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                path=path,

                backward_step=self._backward_steps,

                grad_accum_step=self._grad_accum_counter,

                optimizer_step=self._optimizer_steps,

                name=name,

                scaler_dict=self.fp16_state_dict,

                extension=extension,

                create_directory=create_directory,

                extras=extras,

                status=self.status.status,

            )

            self.print(f&quot;Successfully saved model checkpoint to {out_path}/{tag}&quot;)

            return out_path, tag



        def load(self, path: str, tag: str, strict: bool = True):

            &quot;&quot;&quot;Loads a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory that the model checkpoint was saved (prefer absolute paths over relative paths)

            tag: str

                full tag name the model checkpoint was saved as

            strict: bool

                ignore non-matching keys



            Returns

            -------

            extras: dict, default: None

                a dictionary of any custom fields the user passed to the save function



            &quot;&quot;&quot;

            # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU?

            backward_step, grad_accum_step, optimizer_step, extras = self._runner.load(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                gpu=self.gpu,

                path=path,

                tag=tag,

                scaler_dict_fn=self._load_fp16_state_dict_fn(),

                strict=strict,

            )

            # Reset values based on what was in the load dict

            self._backward_steps = backward_step

            self._grad_accum_counter = grad_accum_step

            self._optimizer_steps = optimizer_step

            self.print(f&quot;Successfully loaded model checkpoint from {path}/{tag}&quot;)

            # Return the extras dict

            return extras



        def print_num_model_parameters(

            self, normalize: ParamNormalize = ParamNormalize.MILLION

        ):

            &quot;&quot;&quot;



            Parameters

            ----------

            normalize: ParamNormalize, default: ParamNormalize.MILLION

                ParamNormalize choice for pretty print normalizing



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(

                f&quot;Total Trainable Model Parameters: &quot;

                f&quot;{(self.num_model_parameters / normalize.value):.3f} {normalize.name}&quot;

            )



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Shorthand method to detach and sync loss



            Maps to the runner function of the same name



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es)

            device: default: None

                device to sync across



            Returns

            -------

            loss that is synced across devices and all_reduced w/ SUM



            &quot;&quot;&quot;

            return self._runner.detach_and_sync_loss(loss=loss, device=device)



        def zero_grads(self):

            &quot;&quot;&quot;Zeros the optimizer grads depending on the optimizer type



            Returns

            -------

            None



            &quot;&quot;&quot;

            zero_optimizer_grads(

                optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod

            )



        def reset(self):

            &quot;&quot;&quot;Public method for resetting the underlying stoke state



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._reset()



        def reset_tracking(self):

            &quot;&quot;&quot;Public method for resetting all underlying stoke tracked variables



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Create some tracking vars

            self._grad_accum_counter = 0

            self._optimizer_steps = 0

            self._backward_steps = 0

            self._last_step_loss = self._set_loss_to_zero()

            self._agg_loss = self._set_loss_to_zero()

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0



        def dump_model_parameter_info(self):

            &quot;&quot;&quot;Dumps all parameter information for named parameters (shape, device, dtype)



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(&quot;Dumping all model parameter information to stdout....&quot;)

            for name, param in self.model_access.named_parameters():

                if param.requires_grad:

                    self.print(

                        f&quot;Name: {name}, Shape: {param.shape}, &quot;

                        f&quot;Device: {param.device}, dtype: {param.dtype}&quot;

                    )



        def _load_fp16_state_dict_fn(self):

            &quot;&quot;&quot;Returns the function to load the sacler state dict



            Returns

            -------

            mp_state_dict_fn: Callable, default: None

                callable function to load the scaler state dict



            &quot;&quot;&quot;

            mp_state_dict_fn = None

            if self.scaler is not None:

                if self.is_apex:

                    try:

                        from apex import amp



                        mp_state_dict_fn = amp.load_state_dict

                    except ImportError as e:

                        print(

                            e,

                            &quot;: Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                        )

                else:

                    mp_state_dict_fn = self.scaler.load_state_dict

            return mp_state_dict_fn



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            self._runner.barrier()



        @property

        def step_loss(self):

            &quot;&quot;&quot;Gets the last step loss synced across device(s) (unscaled)&quot;&quot;&quot;

            return self._last_step_loss



        @property

        def model_access(self):

            &quot;&quot;&quot;Interface for model access due to the different types between the DP, DDP, and SDDP implementations&quot;&quot;&quot;

            if isinstance(self._model, (DDP, DP, SDDP, FSDP)):

                return self._model.module

            else:

                return self._model



        @property

        def loss_access(self):

            &quot;&quot;&quot;Gets loss tensor(s)&quot;&quot;&quot;

            return self._loss



        @property

        def optimizer(self):

            &quot;&quot;&quot;Gets the optimizer&quot;&quot;&quot;

            return self._optimizer



        @property

        def scaler(self):

            &quot;&quot;&quot;Gets the current scaler object&quot;&quot;&quot;

            return self._runner.scaler



        @property

        def fp16_state_dict(self):

            &quot;&quot;&quot;Gets the fp16 state dict from various methods&quot;&quot;&quot;

            mp_state_dict = None

            if self.scaler is not None:

                if self.is_apex:

                    try:

                        from apex import amp



                        mp_state_dict = amp.state_dict()

                    except ImportError as e:

                        print(

                            e,

                            &quot;: Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                        )

                elif self.is_amp:

                    mp_state_dict = self.scaler.state_dict()

            return mp_state_dict



        @property

        def status(self):

            &quot;&quot;&quot;Gets the StokeStatus object&quot;&quot;&quot;

            return self._status



        @property

        def batch_size(self):

            &quot;&quot;&quot;Shortcut to batch size&quot;&quot;&quot;

            return self._status.batch_size



        @property

        def effective_batch_size(self):

            &quot;&quot;&quot;Shortcut to effective batch size&quot;&quot;&quot;

            return self._status.effective_batch_size



        @property

        def grad_clip(self):

            &quot;&quot;&quot;Shortcut to get grad clip&quot;&quot;&quot;

            return self._status.grad_clip



        @property

        def grad_accum(self):

            &quot;&quot;&quot;Shortcut to get grad accumulation&quot;&quot;&quot;

            return self._status.grad_accum



        @property

        def gpu(self):

            &quot;&quot;&quot;Shortcut to get GPU status&quot;&quot;&quot;

            return self._status.gpu



        @property

        def cuda(self):

            &quot;&quot;&quot;Shortcut to get cuda status&quot;&quot;&quot;

            return self._status.cuda



        @property

        def nccl(self):

            &quot;&quot;&quot;Shortcut to get nccl status&quot;&quot;&quot;

            return self._status.nccl



        @property

        def fp16(self):

            &quot;&quot;&quot;Shortcut to get FP16 status&quot;&quot;&quot;

            return self._status.fp16



        @property

        def is_apex(self):

            &quot;&quot;&quot;Returns if APEX is activated&quot;&quot;&quot;

            return self._status.is_fp16_apex



        @property

        def is_amp(self):

            &quot;&quot;&quot;Returns if AMP is activated&quot;&quot;&quot;

            return self._status.is_fp16_amp



        @property

        def distributed(self):

            &quot;&quot;&quot;Shortcut to distributed status&quot;&quot;&quot;

            return self._status.distributed



        @property

        def is_ddp(self):

            &quot;&quot;&quot;Returns if DDP is activated&quot;&quot;&quot;

            return self._status.is_distributed_ddp



        @property

        def is_horovod(self):

            &quot;&quot;&quot;Returns if Horovod is activated&quot;&quot;&quot;

            return self._status.is_distributed_horovod



        @property

        def is_deepspeed(self):

            &quot;&quot;&quot;Returns if Deepspeed is acticated&quot;&quot;&quot;

            return self._status.is_distributed_deepspeed



        @property

        def oss(self):

            &quot;&quot;&quot;Returns if Fairscale optimizer state sharding status&quot;&quot;&quot;

            return self._status.oss



        @property

        def sharded(self):

            &quot;&quot;&quot;Returns if Fairscale sharded DDP status&quot;&quot;&quot;

            return self._status.sharded



        @property

        def fully_sharded(self):

            &quot;&quot;&quot;Returns if Fairscale fully sharded DDP status&quot;&quot;&quot;

            return self._status.fully_sharded



        @property

        def world_size(self):

            &quot;&quot;&quot;Shortcut to get world size&quot;&quot;&quot;

            return self._runner.world_size



        @property

        def rank(self):

            &quot;&quot;&quot;Shortcut to get rank&quot;&quot;&quot;

            return self._runner.rank



        @property

        def amp_config(self):

            &quot;&quot;&quot;Returns amp config or None based on amp state&quot;&quot;&quot;

            return self._status.amp_config if self.is_amp else None



        @property

        def apex_config(self):

            &quot;&quot;&quot;Returns apex config or None based on apex state&quot;&quot;&quot;

            return self._status.apex_config if self.is_apex else None



        @property

        def ddp_config(self):

            &quot;&quot;&quot;Returns ddp config or None based on ddp state&quot;&quot;&quot;

            return self._status.ddp_config if self.is_ddp else None



        @property

        def deepspeed_config(self):

            &quot;&quot;&quot;Returns deepspeed config or None based on deepspeed state&quot;&quot;&quot;

            return self._status.deepspeed_config if self.is_deepspeed else None



        @property

        def oss_config(self):

            &quot;&quot;&quot;Returns oss config or None based on ossstate&quot;&quot;&quot;

            return self._status.oss_config if self.oss else None



        @property

        def sddp_config(self):

            &quot;&quot;&quot;Returns sddp config or None based on sddp state&quot;&quot;&quot;

            return self._status.sddp_config if self.sharded else None



        @property

        def fsdp_config(self):

            &quot;&quot;&quot;Returns fsdp config or None based on fsdp state&quot;&quot;&quot;

            return self._status.fsdp_config if self.fully_sharded else None



        @property

        def horovod_config(self):

            &quot;&quot;&quot;Returns horovod config or None based on horovod state&quot;&quot;&quot;

            return self._status.horovod_config if self.is_horovod else None



        @property

        def num_model_parameters(self):

            &quot;&quot;&quot;Returns number of parameters that require gradients&quot;&quot;&quot;

            return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad)



        @property

        def ema_loss(self):

            &quot;&quot;&quot;Returns the current rolling mean loss&quot;&quot;&quot;

            return self._rolling_mean_loss
</code></pre></div>
<hr />
<h4 id="instance-variables">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">amp_config</span>
</code></pre></div>
<p>Returns amp config or None based on amp state</p>
<div class="highlight"><pre><span></span><code><span class="n">apex_config</span>
</code></pre></div>
<p>Returns apex config or None based on apex state</p>
<div class="highlight"><pre><span></span><code><span class="n">batch_size</span>
</code></pre></div>
<p>Shortcut to batch size</p>
<div class="highlight"><pre><span></span><code><span class="n">cuda</span>
</code></pre></div>
<p>Shortcut to get cuda status</p>
<div class="highlight"><pre><span></span><code><span class="n">ddp_config</span>
</code></pre></div>
<p>Returns ddp config or None based on ddp state</p>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed_config</span>
</code></pre></div>
<p>Returns deepspeed config or None based on deepspeed state</p>
<div class="highlight"><pre><span></span><code><span class="n">distributed</span>
</code></pre></div>
<p>Shortcut to distributed status</p>
<div class="highlight"><pre><span></span><code><span class="n">effective_batch_size</span>
</code></pre></div>
<p>Shortcut to effective batch size</p>
<div class="highlight"><pre><span></span><code><span class="n">ema_loss</span>
</code></pre></div>
<p>Returns the current rolling mean loss</p>
<div class="highlight"><pre><span></span><code><span class="n">fp16</span>
</code></pre></div>
<p>Shortcut to get FP16 status</p>
<div class="highlight"><pre><span></span><code><span class="n">fp16_state_dict</span>
</code></pre></div>
<p>Gets the fp16 state dict from various methods</p>
<div class="highlight"><pre><span></span><code><span class="n">fsdp_config</span>
</code></pre></div>
<p>Returns fsdp config or None based on fsdp state</p>
<div class="highlight"><pre><span></span><code><span class="n">fully_sharded</span>
</code></pre></div>
<p>Returns if Fairscale fully sharded DDP status</p>
<div class="highlight"><pre><span></span><code><span class="n">gpu</span>
</code></pre></div>
<p>Shortcut to get GPU status</p>
<div class="highlight"><pre><span></span><code><span class="n">grad_accum</span>
</code></pre></div>
<p>Shortcut to get grad accumulation</p>
<div class="highlight"><pre><span></span><code><span class="n">grad_clip</span>
</code></pre></div>
<p>Shortcut to get grad clip</p>
<div class="highlight"><pre><span></span><code><span class="n">horovod_config</span>
</code></pre></div>
<p>Returns horovod config or None based on horovod state</p>
<div class="highlight"><pre><span></span><code><span class="n">is_amp</span>
</code></pre></div>
<p>Returns if AMP is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_apex</span>
</code></pre></div>
<p>Returns if APEX is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_ddp</span>
</code></pre></div>
<p>Returns if DDP is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_deepspeed</span>
</code></pre></div>
<p>Returns if Deepspeed is acticated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_horovod</span>
</code></pre></div>
<p>Returns if Horovod is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">loss_access</span>
</code></pre></div>
<p>Gets loss tensor(s)</p>
<div class="highlight"><pre><span></span><code><span class="n">model_access</span>
</code></pre></div>
<p>Interface for model access due to the different types between the DP, DDP, and SDDP implementations</p>
<div class="highlight"><pre><span></span><code><span class="n">nccl</span>
</code></pre></div>
<p>Shortcut to get nccl status</p>
<div class="highlight"><pre><span></span><code><span class="n">num_model_parameters</span>
</code></pre></div>
<p>Returns number of parameters that require gradients</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span>
</code></pre></div>
<p>Gets the optimizer</p>
<div class="highlight"><pre><span></span><code><span class="n">oss</span>
</code></pre></div>
<p>Returns if Fairscale optimizer state sharding status</p>
<div class="highlight"><pre><span></span><code><span class="n">oss_config</span>
</code></pre></div>
<p>Returns oss config or None based on ossstate</p>
<div class="highlight"><pre><span></span><code><span class="n">rank</span>
</code></pre></div>
<p>Shortcut to get rank</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Gets the current scaler object</p>
<div class="highlight"><pre><span></span><code><span class="n">sddp_config</span>
</code></pre></div>
<p>Returns sddp config or None based on sddp state</p>
<div class="highlight"><pre><span></span><code><span class="n">sharded</span>
</code></pre></div>
<p>Returns if Fairscale sharded DDP status</p>
<div class="highlight"><pre><span></span><code><span class="n">status</span>
</code></pre></div>
<p>Gets the StokeStatus object</p>
<div class="highlight"><pre><span></span><code><span class="n">step_loss</span>
</code></pre></div>
<p>Gets the last step loss synced across device(s) (unscaled)</p>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<p>Shortcut to get world size</p>
<h4 id="methods_1">Methods</h4>
<h4 id="dataloader">DataLoader</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">DataLoader</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">Dataset</span><span class="p">[</span><span class="o">+</span><span class="n">T_co</span><span class="p">],</span>
    <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Sampler</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Sampler</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">List</span><span class="p">[</span><span class="o">~</span><span class="n">T</span><span class="p">]],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">worker_init_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">multiprocessing_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">generator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">prefetch_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">persistent_workers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs.</p>
<p>Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called)
and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus
object is available which is post init. This could be disconnected from this class but it would require the
user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and
never handled</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>dataset</td>
<td>Dataset</td>
<td>dataset from which to load the data.</td>
<td>None</td>
</tr>
<tr>
<td>shuffle</td>
<td>bool, default: False</td>
<td>set to <code>True</code> to have the data reshuffled at every epoch.</td>
<td>None</td>
</tr>
<tr>
<td>sampler</td>
<td>Sampler or Iterable, default: None</td>
<td>defines the strategy to draw samples from the dataset. Can be any <code>Iterable</code> with <code>__len__</code></td>
<td></td>
</tr>
<tr>
<td>implemented. If specified, :attr:<code>shuffle</code> must not be specified.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>batch_sampler</td>
<td>Sampler or Iterable, default: None:</td>
<td>like :attr:<code>sampler</code>, but returns a batch of indices at a time. Mutually exclusive with</td>
<td></td>
</tr>
<tr>
<td>:attr:<code>batch_size</code>, :attr:<code>shuffle</code>, :attr:<code>sampler</code>, and :attr:<code>drop_last</code>.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>num_workers</td>
<td>int, default: 0</td>
<td>how many subprocesses to use for data loading. <code>0</code> means that the data will be loaded in the main process.</td>
<td>None</td>
</tr>
<tr>
<td>collate_fn</td>
<td>callable, optional:</td>
<td>merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a</td>
<td></td>
</tr>
<tr>
<td>map-style dataset.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>pin_memory</td>
<td>bool, default: False:</td>
<td>If <code>True</code>, the data loader will copy Tensors into CUDA pinned memory before returning them. If your</td>
<td></td>
</tr>
<tr>
<td>data elements are a custom type, or your :attr:<code>collate_fn</code> returns a batch that is a custom type,</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>see the example below.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>drop_last</td>
<td>bool, default: False</td>
<td>set to <code>True</code> to drop the last incomplete batch, if the dataset size is not divisible by the batch size.</td>
<td></td>
</tr>
<tr>
<td>If <code>False</code> and the size of dataset is not divisible by the batch size, then the last batch</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>will be smaller.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>timeout</td>
<td>numeric, default: 0</td>
<td>if positive, the timeout value for collecting a batch from workers. Should always be non-negative.</td>
<td>None</td>
</tr>
<tr>
<td>worker_init_fn</td>
<td>callable, default: None</td>
<td>If not <code>None</code>, this will be called on each worker subprocess with the worker id</td>
<td></td>
</tr>
<tr>
<td>(an int in <code>[0, num_workers - 1]</code>) as input, after seeding and before data loading.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>prefetch_factor</td>
<td>int, default: 2</td>
<td>Number of samples loaded in advance by each worker. <code>2</code> means there will be a total of 2 * num_workers</td>
<td></td>
</tr>
<tr>
<td>samples prefetched across all workers.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>persistent_workers</td>
<td>bool, default: False</td>
<td>If <code>True</code>, the data loader will not shutdown the worker processes after a dataset has been</td>
<td></td>
</tr>
<tr>
<td>consumed once. This allows to maintain the workers <code>Dataset</code> instances alive.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>StokeDataLoader</td>
<td>wrapped torch.utils.data.DataLoader object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def DataLoader(</p>
<div class="highlight"><pre><span></span><code>            self,

            dataset: Dataset[T_co],

            shuffle: bool = False,

            sampler: Optional[Sampler[int]] = None,

            batch_sampler: Optional[Sampler[Sequence[int]]] = None,

            num_workers: int = 0,

            collate_fn: _collate_fn_t = None,

            pin_memory: bool = False,

            drop_last: bool = False,

            timeout: float = 0,

            worker_init_fn: _worker_init_fn_t = None,

            multiprocessing_context=None,

            generator=None,

            *,

            prefetch_factor: int = 2,

            persistent_workers: bool = False,

        ):

            &quot;&quot;&quot;Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs.



            Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called)

            and to automatically handle device placement since the gpu/fp16 flags can&#39;t be determined until the StokeStatus

            object is available which is post init. This could be disconnected from this class but it would require the

            user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and

            never handled



            Parameters

            ----------

            dataset: Dataset

                dataset from which to load the data.

            shuffle: bool, default: False

                set to ``True`` to have the data reshuffled at every epoch.

            sampler: Sampler or Iterable, default: None

                defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__``

                implemented. If specified, :attr:`shuffle` must not be specified.

            batch_sampler: Sampler or Iterable, default: None:

                like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with

                :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.

            num_workers: int, default: 0

                how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process.

            collate_fn: callable, optional:

                merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a

                map-style dataset.

            pin_memory: bool, default: False:

                If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your

                data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,

                see the example below.

            drop_last: bool, default: False

                set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size.

                If ``False`` and the size of dataset is not divisible by the batch size, then the last batch

                will be smaller.

            timeout: numeric, default: 0

                if positive, the timeout value for collecting a batch from workers. Should always be non-negative.

            worker_init_fn: callable, default: None

                If not ``None``, this will be called on each worker subprocess with the worker id

                (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading.

            prefetch_factor: int, default: 2

                Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers

                samples prefetched across all workers.

            persistent_workers: bool, default: False

                If ``True``, the data loader will not shutdown the worker processes after a dataset has been

                consumed once. This allows to maintain the workers `Dataset` instances alive.



            Returns

            -------

            StokeDataLoader

                wrapped torch.utils.data.DataLoader object



            &quot;&quot;&quot;

            # Check if forkserver is available for horovod and use

            if (

                num_workers &gt; 0

                and hasattr(torch.multiprocessing, &quot;_supports_context&quot;)

                and torch.multiprocessing._supports_context

                and &quot;forkserver&quot; in torch.multiprocessing.get_all_start_methods()

                and self.is_horovod

            ):

                multiprocessing_context = &quot;forkserver&quot;



            if self._verbose and self.gpu:

                print(f&quot;Automatically handling moving model input data to GPU(s)...&quot;)

            # Forward the already known options from the Stoke status

            return StokeDataLoader(

                gpu=self.gpu,

                fp16=self.fp16,

                batch_size=self.batch_size,

                dataset=dataset,

                shuffle=shuffle,

                sampler=sampler,

                batch_sampler=batch_sampler,

                num_workers=num_workers,

                collate_fn=collate_fn,

                pin_memory=pin_memory,

                drop_last=drop_last,

                timeout=timeout,

                worker_init_fn=worker_init_fn,

                multiprocessing_context=multiprocessing_context,

                generator=generator,

                prefetch_factor=prefetch_factor,

                persistent_workers=persistent_workers,

            )
</code></pre></div>
<h4 id="backward">backward</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span>
</code></pre></div>
<p>Wrapped backwards call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>Callable loss function(s)</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward(</p>
<div class="highlight"><pre><span></span><code>            self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

        ):

            &quot;&quot;&quot;Wrapped backwards call



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                Callable loss function(s)



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Increment the grad counter

            self._grad_accum_counter += 1

            # Set the context based on the counter

            dist_cm = (

                nullcontext()

                if self._check_accum()

                else self._runner.grad_accum_context(self._model)

            )

            with dist_cm:

                self._runner.backward_call(

                    loss=loss, model=self.model_access, optimizer=self._optimizer

                )

            # Increment the number of total calls to backward (each backward to a loss is only considered 1)

            self._backward_steps += 1
</code></pre></div>
<h4 id="barrier">barrier</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Calls the underlying distributed barrier if available</p>
<p>??? example "View Source"
            def barrier(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            self._runner.barrier()
</code></pre></div>
<h4 id="detach_and_sync_loss">detach_and_sync_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detach_and_sync_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Shorthand method to detach and sync loss</p>
<p>Maps to the runner function of the same name</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es)</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>device to sync across</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss that is synced across devices and all_reduced w/ SUM</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def detach_and_sync_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Shorthand method to detach and sync loss



            Maps to the runner function of the same name



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es)

            device: default: None

                device to sync across



            Returns

            -------

            loss that is synced across devices and all_reduced w/ SUM



            &quot;&quot;&quot;

            return self._runner.detach_and_sync_loss(loss=loss, device=device)
</code></pre></div>
<h4 id="dump_model_parameter_info">dump_model_parameter_info</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">dump_model_parameter_info</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Dumps all parameter information for named parameters (shape, device, dtype)</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def dump_model_parameter_info(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Dumps all parameter information for named parameters (shape, device, dtype)



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(&quot;Dumping all model parameter information to stdout....&quot;)

            for name, param in self.model_access.named_parameters():

                if param.requires_grad:

                    self.print(

                        f&quot;Name: {name}, Shape: {param.shape}, &quot;

                        f&quot;Device: {param.device}, dtype: {param.dtype}&quot;

                    )
</code></pre></div>
<h4 id="load">load</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">load</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">tag</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<p>Loads a model checkpoint using the correct backend interface</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>str</td>
<td>path to directory that the model checkpoint was saved (prefer absolute paths over relative paths)</td>
<td>None</td>
</tr>
<tr>
<td>tag</td>
<td>str</td>
<td>full tag name the model checkpoint was saved as</td>
<td>None</td>
</tr>
<tr>
<td>strict</td>
<td>bool</td>
<td>ignore non-matching keys</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>dict, default: None</td>
<td>a dictionary of any custom fields the user passed to the save function</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def load(self, path: str, tag: str, strict: bool = True):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Loads a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory that the model checkpoint was saved (prefer absolute paths over relative paths)

            tag: str

                full tag name the model checkpoint was saved as

            strict: bool

                ignore non-matching keys



            Returns

            -------

            extras: dict, default: None

                a dictionary of any custom fields the user passed to the save function



            &quot;&quot;&quot;

            # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU?

            backward_step, grad_accum_step, optimizer_step, extras = self._runner.load(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                gpu=self.gpu,

                path=path,

                tag=tag,

                scaler_dict_fn=self._load_fp16_state_dict_fn(),

                strict=strict,

            )

            # Reset values based on what was in the load dict

            self._backward_steps = backward_step

            self._grad_accum_counter = grad_accum_step

            self._optimizer_steps = optimizer_step

            self.print(f&quot;Successfully loaded model checkpoint from {path}/{tag}&quot;)

            # Return the extras dict

            return extras
</code></pre></div>
<h4 id="loss">loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>Wrapped callable loss function call</p>
<p>Handles internal logic of aggregating up the losses for single and multiple losses</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>*args</td>
<td>list or tuple</td>
<td>Additional arguments should be passed as keyword arguments</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>dict</td>
<td>Extra arguments passed to the loss function call(s)</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>outputs of callable loss function(s)</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def loss(self, <em>args, </em>*kwargs):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Wrapped callable loss function call



            Handles internal logic of aggregating up the losses for single and multiple losses



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the loss function call(s)



            Returns

            -------

            outputs of callable loss function(s)



            &quot;&quot;&quot;

            # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch

            with self._runner.loss_context:

                if isinstance(self._loss, (list, tuple)):

                    loss = type(self._loss)(val(*args, **kwargs) for val in self._loss)

                    sync_loss = [self.detach_and_sync_loss(val) for val in loss]

                    self._last_step_loss = type(self._loss)(

                        val for idx, val in enumerate(sync_loss)

                    )

                    self._agg_loss = type(self._loss)(

                        self._agg_loss[idx] + val for idx, val in enumerate(sync_loss)

                    )

                    self._handle_ema_loss(loss=sync_loss)

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = type(loss)(val / self.grad_accum for val in loss)

                else:

                    loss = self._loss(*args, **kwargs)

                    sync_loss = self.detach_and_sync_loss(loss)

                    self._last_step_loss = sync_loss

                    self._agg_loss += sync_loss

                    self._handle_ema_loss(loss=sync_loss)

                    # Handle grad accumulation by dividing by the accumulation steps

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = loss / self.grad_accum

                return loss
</code></pre></div>
<h4 id="model">model</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>Wrapped model forward call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>*args</td>
<td>list or tuple</td>
<td>Additional arguments should be passed as keyword arguments</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>dict</td>
<td>Extra arguments passed to the model forward call</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>model forward output</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def model(self, <em>args, </em>*kwargs):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Wrapped model forward call



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the model forward call



            Returns

            -------

            model forward output



            &quot;&quot;&quot;

            with self._runner.model_context:

                return self._model(*args, **kwargs)

                # return self.model_access(*args, **kwargs)
</code></pre></div>
<h4 id="print">print</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Wraps the runners print device and forces print on the _info_rank attribute(s)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>str</td>
<td>message to print</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print(self, msg: Union[str, List[str]], single_line: bool = False):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Wraps the runners print device and forces print on the _info_rank attribute(s)



            Parameters

            ----------

            msg: str

                message to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(

                msg=msg, rank=self._info_rank, single_line=single_line

            )
</code></pre></div>
<h4 id="print_ema_loss">print_ema_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_ema_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prepend_msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Current EMA Loss&#39;</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Prints the current ema loss synced across all devices</p>
<p>Handles single or multiple losses. Prints only on devices specified by self._info_rank</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>prepend_msg</td>
<td>str, default: "Current EMA Loss"</td>
<td>message prepend to print</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_ema_loss(</p>
<div class="highlight"><pre><span></span><code>            self, prepend_msg: str = &quot;Current EMA Loss&quot;, single_line: bool = False

        ):

            &quot;&quot;&quot;Prints the current ema loss synced across all devices



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Current EMA Loss&quot;

                message prepend to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(self._rolling_mean_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val:.3f}&quot;

                    for idx, val in enumerate(self._rolling_mean_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(f&quot;{prepend_msg}: {self._rolling_mean_loss:.3f}&quot;)
</code></pre></div>
<h4 id="print_mean_accumulated_synced_loss">print_mean_accumulated_synced_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_mean_accumulated_synced_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prepend_msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Mean Accumulated &amp; Synced Loss&#39;</span><span class="p">,</span>
    <span class="n">pre_backwards</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Prints the mean accumulated and device synced loss only after the grad accumulation step</p>
<p>Handles single or multiple losses. Prints only on devices specified by self._info_rank</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>prepend_msg</td>
<td>str, default: "Mean Accumulated &amp; Synced Loss"</td>
<td>message prepend to print</td>
<td>None</td>
</tr>
<tr>
<td>pre_backwards</td>
<td>bool, default: True</td>
<td>if being called pre backward step</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_mean_accumulated_synced_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            prepend_msg: str = &quot;Mean Accumulated &amp; Synced Loss&quot;,

            pre_backwards: bool = True,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints the mean accumulated and device synced loss only after the grad accumulation step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Mean Accumulated &amp; Synced Loss&quot;

                message prepend to print

            pre_backwards: bool, default: True

                if being called pre backward step

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            check_fn = self._check_pre_accum if pre_backwards else self._check_accum

            if check_fn():

                if isinstance(self._agg_loss, (list, tuple)):

                    print_vals = self._scale_agg_loss()

                    self.print(print_vals, single_line=single_line)

                else:

                    self.print(f&quot;{prepend_msg}: {self._scale_agg_loss():.3f}&quot;)
</code></pre></div>
<h4 id="print_num_model_parameters">print_num_model_parameters</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_num_model_parameters</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">normalize</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">ParamNormalize</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">ParamNormalize</span><span class="o">.</span><span class="n">MILLION</span><span class="p">:</span> <span class="mf">1000000.0</span><span class="o">&gt;</span>
<span class="p">)</span>
</code></pre></div>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>normalize</td>
<td>ParamNormalize, default: ParamNormalize.MILLION</td>
<td>ParamNormalize choice for pretty print normalizing</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_num_model_parameters(</p>
<div class="highlight"><pre><span></span><code>            self, normalize: ParamNormalize = ParamNormalize.MILLION

        ):

            &quot;&quot;&quot;



            Parameters

            ----------

            normalize: ParamNormalize, default: ParamNormalize.MILLION

                ParamNormalize choice for pretty print normalizing



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(

                f&quot;Total Trainable Model Parameters: &quot;

                f&quot;{(self.num_model_parameters / normalize.value):.3f} {normalize.name}&quot;

            )
</code></pre></div>
<h4 id="print_on_devices">print_on_devices</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_on_devices</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div>
<p>Wraps runner print interface for shorter semantics</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>str</td>
<td>message to print</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>Union[int, List[int]], default: 0</td>
<td>which ranks to print on</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_on_devices(</p>
<div class="highlight"><pre><span></span><code>            self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0

        ):

            &quot;&quot;&quot;Wraps runner print interface for shorter semantics



            Parameters

            ----------

            msg: str

                message to print

            rank: Union[int, List[int]], default: 0

                which ranks to print on



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(msg=msg, rank=rank)
</code></pre></div>
<h4 id="print_synced_loss">print_synced_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_synced_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">prepend_msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Step Synced Loss&#39;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Prints a device synced loss at a single step</p>
<p>Handles single or multiple losses. Prints only on devices specified by self._info_rank</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es) on the device</td>
<td>None</td>
</tr>
<tr>
<td>prepend_msg</td>
<td>str, default: "Step Synced Loss"</td>
<td>message prepend to print</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>specify the device to place the synced loss on (defaults to same device)</td>
<td>same</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_synced_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            prepend_msg: str = &quot;Step Synced Loss&quot;,

            device=None,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints a device synced loss at a single step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            prepend_msg: str, default: &quot;Step Synced Loss&quot;

                message prepend to print

            device: default: None

                specify the device to place the synced loss on (defaults to same device)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            printable_loss = self.detach_and_sync_loss(loss, device)

            if isinstance(printable_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val * self.grad_accum:.3f}&quot;

                    for idx, val in enumerate(printable_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(msg=f&quot;{prepend_msg}: {printable_loss * self.grad_accum:.3f}&quot;)
</code></pre></div>
<h4 id="reset">reset</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Public method for resetting the underlying stoke state</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def reset(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Public method for resetting the underlying stoke state



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._reset()
</code></pre></div>
<h4 id="reset_ema">reset_ema</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset_ema</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Used to reset the current state of the rolling mean loss</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def reset_ema(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Used to reset the current state of the rolling mean loss



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0
</code></pre></div>
<h4 id="reset_tracking">reset_tracking</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset_tracking</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Public method for resetting all underlying stoke tracked variables</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def reset_tracking(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Public method for resetting all underlying stoke tracked variables



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Create some tracking vars

            self._grad_accum_counter = 0

            self._optimizer_steps = 0

            self._backward_steps = 0

            self._last_step_loss = self._set_loss_to_zero()

            self._agg_loss = self._set_loss_to_zero()

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0
</code></pre></div>
<h4 id="save">save</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">UUID</span><span class="p">(</span><span class="s1">&#39;1bec68f4-7df7-48d2-a526-14685e92f54f&#39;</span><span class="p">),</span>
    <span class="n">extension</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;pt&#39;</span><span class="p">,</span>
    <span class="n">create_directory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">extras</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Saves a model checkpoint using the correct backend interface</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>str</td>
<td>path to directory to save the model checkpoint (prefer absolute paths over relative paths)</td>
<td>None</td>
</tr>
<tr>
<td>name</td>
<td>str, default: uuid4()</td>
<td>name used to save checkpoint file</td>
<td>None</td>
</tr>
<tr>
<td>extension</td>
<td>str, default: '.pt'</td>
<td>extension used to save PyTorch model checkpoint</td>
<td>None</td>
</tr>
<tr>
<td>create_directory</td>
<td>bool, default: True</td>
<td>flag to create the directory path if it doesn't exist</td>
<td>None</td>
</tr>
<tr>
<td>extras</td>
<td>dict, default: None</td>
<td>a dictionary of any extra things to save</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>str</td>
<td>path to directory that the model checkpoint was saved</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def save(</p>
<div class="highlight"><pre><span></span><code>            self,

            path: str,

            name: str = uuid4(),

            extension: str = &quot;pt&quot;,

            create_directory: bool = True,

            extras: Optional[dict] = None,

        ):

            &quot;&quot;&quot;Saves a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory to save the model checkpoint (prefer absolute paths over relative paths)

            name: str, default: uuid4()

                name used to save checkpoint file

            extension: str, default: &#39;.pt&#39;

                extension used to save PyTorch model checkpoint

            create_directory: bool, default: True

                flag to create the directory path if it doesn&#39;t exist

            extras: dict, default: None

                a dictionary of any extra things to save



            Returns

            -------

            path: str

                path to directory that the model checkpoint was saved

            tag: str

                full tag name the model checkpoint was saved as



            &quot;&quot;&quot;

            out_path, tag = self._runner.save(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                path=path,

                backward_step=self._backward_steps,

                grad_accum_step=self._grad_accum_counter,

                optimizer_step=self._optimizer_steps,

                name=name,

                scaler_dict=self.fp16_state_dict,

                extension=extension,

                create_directory=create_directory,

                extras=extras,

                status=self.status.status,

            )

            self.print(f&quot;Successfully saved model checkpoint to {out_path}/{tag}&quot;)

            return out_path, tag
</code></pre></div>
<h4 id="step">step</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Wrapped step call</p>
<p>Handles grad clipping internally</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Wrapped step call



            Handles grad clipping internally



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer only if the modulo is zero

            if self._check_accum():

                if self._verbose and self.grad_accum &gt; 0:

                    self.print(f&quot;Gradient Accumulation Steps: {self.grad_accum}&quot;)

                # Clip if needed

                if self.grad_clip is not None:

                    self._runner.clip_grad(

                        self.grad_clip,

                        self._model if self.fully_sharded else self.model_access,

                        self._optimizer,

                        oss=self.oss,

                        horovod=self.is_horovod,

                        deepspeed=self.is_deepspeed,

                        fsdp=self.fully_sharded,

                    )

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )

                # Reset for the accumulated step

                self._reset()

                # Increment the number of step calls to the optimizer

                self._optimizer_steps += 1

            # if deepspeed we need to step everytime as it handles the grad accumulation internally

            elif self.is_deepspeed:

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )
</code></pre></div>
<h4 id="zero_grads">zero_grads</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">zero_grads</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Zeros the optimizer grads depending on the optimizer type</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def zero_grads(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Zeros the optimizer grads depending on the optimizer type



            Returns

            -------

            None



            &quot;&quot;&quot;

            zero_optimizer_grads(

                optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod

            )
</code></pre></div>
<h3 id="stokeoptimizer">StokeOptimizer</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">StokeOptimizer</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_21">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Type[torch.optim.Optimizer]</td>
<td>un-instantiated torch.optim.Optimizer class</td>
<td>None</td>
</tr>
<tr>
<td>optimizer_kwargs</td>
<td>Dict</td>
<td>any keyword args to be unrolled into the optimizer at instantiation time</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class StokeOptimizer(TypedDict):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Stoke optimizer wrapper class



        Given all the different backends and extensions the optimizer might need to be instantiated in a different way

        thus this typed dict holds the configuration without instantiation



        Attributes

        ----------

        optimizer: Type[torch.optim.Optimizer]

            un-instantiated torch.optim.Optimizer class

        optimizer_kwargs: Dict

            any keyword args to be unrolled into the optimizer at instantiation time



        &quot;&quot;&quot;



        optimizer: Type[torch.optim.Optimizer]

        optimizer_kwargs: Dict
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_4">Ancestors (in MRO)</h4>
<ul>
<li>builtins.dict</li>
</ul>
<h4 id="methods_2">Methods</h4>
<h4 id="clear">clear</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clear</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.clear() -&gt; None.  Remove all items from D.</p>
<h4 id="copy">copy</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">copy</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.copy() -&gt; a shallow copy of D</p>
<h4 id="fromkeys">fromkeys</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fromkeys</span><span class="p">(</span>
    <span class="n">iterable</span><span class="p">,</span>
    <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">/</span>
<span class="p">)</span>
</code></pre></div>
<p>Create a new dictionary with keys from iterable and values set to value.</p>
<h4 id="get">get</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">key</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">/</span>
<span class="p">)</span>
</code></pre></div>
<p>Return the value for key if key is in the dictionary, else default.</p>
<h4 id="items">items</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">items</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.items() -&gt; a set-like object providing a view on D's items</p>
<h4 id="keys">keys</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">keys</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.keys() -&gt; a set-like object providing a view on D's keys</p>
<h4 id="pop">pop</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">pop</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.</p>
<p>If key is not found, d is returned if given, otherwise KeyError is raised</p>
<h4 id="popitem">popitem</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">popitem</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">/</span>
<span class="p">)</span>
</code></pre></div>
<p>Remove and return a (key, value) pair as a 2-tuple.</p>
<p>Pairs are returned in LIFO (last-in, first-out) order.
Raises KeyError if the dict is empty.</p>
<h4 id="setdefault">setdefault</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setdefault</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">key</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">/</span>
<span class="p">)</span>
</code></pre></div>
<p>Insert key with a value of default if key is not in the dictionary.</p>
<p>Return the value for key if key is in the dictionary, else default.</p>
<h4 id="update">update</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.update([E, ]**F) -&gt; None.  Update D from dict/iterable E and F.</p>
<p>If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
In either case, this is followed by: for k in F:  D[k] = F[k]</p>
<h4 id="values">values</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">values</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.values() -&gt; an object providing a view on D's values</p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="fp16/" title="Fp16" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Fp16
              </span>
            </div>
          </a>
        
        
          <a href="io/" title="Io" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Io
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ncilfone" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/ncilfone/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
    
  </body>
</html>