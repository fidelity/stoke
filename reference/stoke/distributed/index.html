
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../../resources/images/stoke_small.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Distributed - stoke</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-stokedistributed" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="stoke" class="md-header__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            stoke
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Distributed
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Home/" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Installation/" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Quick-Start/" class="md-tabs__link">
      Quick Start
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Launchers/" class="md-tabs__link">
      Launchers
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Examples/" class="md-tabs__link">
      Examples
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../CONTRIBUTING/" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../configs/" class="md-tabs__link md-tabs__link--active">
        Reference
      </a>
    </li>
  

  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="stoke" class="md-nav__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    stoke
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Home/" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Quick-Start/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Launchers/" class="md-nav__link">
        Launchers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Examples/" class="md-nav__link">
        Examples
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_1" type="checkbox" id="__nav_7_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_1">
          Stoke
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stoke" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          Stoke
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../configs/" class="md-nav__link">
        Configs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Distributed
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Distributed
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basedistributed" class="md-nav__link">
    BaseDistributed
  </a>
  
    <nav class="md-nav" aria-label="BaseDistributed">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedddp" class="md-nav__link">
    DistributedDDP
  </a>
  
    <nav class="md-nav" aria-label="DistributedDDP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_1" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_1" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_1" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_1" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_1" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_1" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_1" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_1" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_1" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributeddeepspeed" class="md-nav__link">
    DistributedDeepspeed
  </a>
  
    <nav class="md-nav" aria-label="DistributedDeepspeed">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_2" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_2" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_2" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_2" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_2" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_2" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_2" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_2" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_2" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedhorovod" class="md-nav__link">
    DistributedHorovod
  </a>
  
    <nav class="md-nav" aria-label="DistributedHorovod">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_3" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_3" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_3" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_3" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_3" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_3" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_3" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_3" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_3" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributednullcpu" class="md-nav__link">
    DistributedNullCPU
  </a>
  
    <nav class="md-nav" aria-label="DistributedNullCPU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_4" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_4" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_4" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_4" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_4" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_4" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_4" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_4" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_4" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributednullgpu" class="md-nav__link">
    DistributedNullGPU
  </a>
  
    <nav class="md-nav" aria-label="DistributedNullGPU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_5" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_5" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_5" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_5" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_5" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_5" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_5" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_5" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_5" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_5" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_5" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_5" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runnerdistenum" class="md-nav__link">
    RunnerDistEnum
  </a>
  
    <nav class="md-nav" aria-label="RunnerDistEnum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_6" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../extensions/" class="md-nav__link">
        Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../fp16/" class="md-nav__link">
        Fp16
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../io/" class="md-nav__link">
        Io
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../status/" class="md-nav__link">
        Status
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../stoke/" class="md-nav__link">
        Stoke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basedistributed" class="md-nav__link">
    BaseDistributed
  </a>
  
    <nav class="md-nav" aria-label="BaseDistributed">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedddp" class="md-nav__link">
    DistributedDDP
  </a>
  
    <nav class="md-nav" aria-label="DistributedDDP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_1" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_1" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_1" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_1" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_1" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_1" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_1" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_1" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_1" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributeddeepspeed" class="md-nav__link">
    DistributedDeepspeed
  </a>
  
    <nav class="md-nav" aria-label="DistributedDeepspeed">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_2" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_2" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_2" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_2" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_2" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_2" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_2" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_2" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_2" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedhorovod" class="md-nav__link">
    DistributedHorovod
  </a>
  
    <nav class="md-nav" aria-label="DistributedHorovod">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_3" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_3" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_3" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_3" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_3" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_3" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_3" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_3" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_3" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributednullcpu" class="md-nav__link">
    DistributedNullCPU
  </a>
  
    <nav class="md-nav" aria-label="DistributedNullCPU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_4" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_4" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_4" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_4" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_4" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_4" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_4" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_4" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_4" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributednullgpu" class="md-nav__link">
    DistributedNullGPU
  </a>
  
    <nav class="md-nav" aria-label="DistributedNullGPU">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_5" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_5" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_5" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_5" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier_5" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clean_5" class="md-nav__link">
    clean
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss_5" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grad_accum_context_5" class="md-nav__link">
    grad_accum_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_device_5" class="md-nav__link">
    print_device
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup_distributed_5" class="md-nav__link">
    setup_distributed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_context_5" class="md-nav__link">
    step_context
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_distributed_5" class="md-nav__link">
    wrap_distributed
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runnerdistenum" class="md-nav__link">
    RunnerDistEnum
  </a>
  
    <nav class="md-nav" aria-label="RunnerDistEnum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_6" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/fidelity/stoke/edit/main/reference/stoke/distributed.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="module-stokedistributed">Module stoke.distributed</h1>
<p>Handles distributed related classes -- mixin style</p>
<p>None</p>
<p>??? example "View Source"
        # -<em>- coding: utf-8 -</em>-</p>
<div class="highlight"><pre><span></span><code>    # Copyright FMR LLC &lt;opensource@fidelity.com&gt;

    # SPDX-License-Identifier: Apache-2.0



    &quot;&quot;&quot;Handles distributed related classes -- mixin style&quot;&quot;&quot;



    import os

    from abc import ABC, abstractmethod

    from contextlib import nullcontext

    from enum import Enum

    from typing import List, Optional, Tuple, Union



    import deepspeed as ds

    import horovod.torch as hvd

    import torch

    from deepspeed.utils.distributed import mpi_discovery

    from fairscale.optim.oss import OSS



    from stoke.configs import ClipGradConfig, ClipGradNormConfig

    from stoke.extensions import (

        DistributedHandlerEnum,

        FairscaleFSDPExtension,

        FairscaleSDDPExtension,

    )

    from stoke.utils import unrolled_print





    class BaseDistributed(ABC):

        &quot;&quot;&quot;Base class for distributed backends



        This class handles common functionality for all of the different distributed backends including setup, loss sync,

        gradient accumulation context, step context and various properties/attributes related to distributed frameworks



        Attributes

        ----------

        device_id

        initialized

        rank

        world_size

        _batch_size_per_device: int

            batch size per device or for non-distributed the overall batch size

        _device_id: int, default: None

            Current device id

        _info_rank: Union[int, List[int]]

            Which device(s) to print information

        _name: str

            name of current backend

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self,

            device_id: Optional[Union[int, str]],

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            name: str,

            verbose: bool = True,

        ):

            &quot;&quot;&quot;Init for BaseDistributed class



            Parameters

            ----------

            device_id: int, default: None

                Current device id

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            name: str

                name of current backend

            verbose: bool, default: True

                flag for Stoke print verbosity



            &quot;&quot;&quot;

            self._batch_size_per_device = batch_size_per_device

            self._device_id = device_id

            self._info_rank = info_rank

            self._name = name

            self._verbose = verbose



        def _print_info(self):

            &quot;&quot;&quot;Basic print of backend initialization status



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._print_device(f&quot;{self._name} Initialized: {self.initialized}&quot;)



        def setup_distributed(self):

            &quot;&quot;&quot;Base setup distributed



            Does nothing as nothing needs to be wrapped



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass



        def wrap_distributed(

            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Base wrapper for distributed backends



            Does nothing but print as nothing needs to be wrapped



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                same as input model

            optimizer: Union[torch.optim.Optimizer, OSS]]

                same as input optimizer



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

            return model, optimizer



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(val.item() for val in loss)

            else:

                return loss.item()



        def grad_accum_context(self, model: torch.nn.Module):

            &quot;&quot;&quot;Returns base context for gradient accumulation



            By default no context is used



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()



        def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):

            &quot;&quot;&quot;Returns base context for the step call



            By default no context is used



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()



        def clean(self):

            &quot;&quot;&quot;Base clean call



            Nothing to do here...



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass



        def _call_init(self):

            &quot;&quot;&quot;Base init call



            Nothing to do here...



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass



        def _print_device(self, msg: Union[str, List[str]]):

            &quot;&quot;&quot;Prints a str of list of strs on the currently set _info_rank



            Internal version of public print_device that always points to the set _info_rank



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print_device(msg=msg, rank=self._info_rank)



        def print_device(

            self,

            msg: Union[str, List[str]],

            rank: Optional[Union[int, List[int]]] = 0,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Public facing method to print on specific device ranks



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print

            rank: Optional[Union[int, List[int]]], default: 0

                device rank to print to (prevents printing on multiple devices in distributed mode)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Ignore the rank check if the current rank is a non-distributed version

            if self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;:

                unrolled_print(msg, single_line=single_line)

            # if it&#39;s a list then check the rank against the list

            elif isinstance(rank, list) and self.rank in rank:

                unrolled_print(msg, single_line=single_line)

            # If its an int then check the equality

            elif isinstance(rank, int) and rank == self.rank:

                unrolled_print(msg, single_line=single_line)

            # the else is essentially skip print

            else:

                pass



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            pass



        @property

        def device_id(self):

            &quot;&quot;&quot;Returns the current device id&quot;&quot;&quot;

            return self._device_id



        @property

        @abstractmethod

        def rank(self):

            pass



        @property

        @abstractmethod

        def world_size(self):

            pass



        @property

        @abstractmethod

        def initialized(self):

            pass





    class DistributedNullCPU(BaseDistributed):

        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;Init for DistributedNullCPU



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            Notes

            -----

            Device ID set to None as it is not needed for non distributed CPU



            &quot;&quot;&quot;

            super(DistributedNullCPU, self).__init__(

                device_id=&quot;cpu&quot;,

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;PyTorch CPU&quot;,

                verbose=verbose,

            )



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank



            No rank so return string of cpu

            &quot;&quot;&quot;

            return &quot;cpu&quot;



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return 1



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return True





    class DistributedNullGPU(BaseDistributed):

        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;Init for DistributedNullCPU



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            Notes

            -----

            Device ID set to the current CUDA device as there is only a single GPU being used



            &quot;&quot;&quot;

            super(DistributedNullGPU, self).__init__(

                device_id=torch.cuda.current_device(),

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;PyTorch GPU&quot;,

                verbose=verbose,

            )



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank



            No rank so return string of gpu

            &quot;&quot;&quot;

            return &quot;gpu&quot;



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return 1



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return True





    class DistributedDDP(BaseDistributed):

        &quot;&quot;&quot;Class for using DDP as the distributed backend



        This class handles common functionality for the DDP backend including setup, loss sync,

        gradient accumulation context, step context and various properties/attributes



        Attributes

        ----------

        device_id

        initialized

        rank

        world_size

        _batch_size_per_device: int

            batch size per device or for non-distributed the overall batch size

        _ddp_config: DDPConfig

            Configuration object for DDP backend

        _ddp_handler

            wrapper method that will modify the DDP instance to use SDDP if flagged

        _device_id: int, default: None

            Current device id

        _info_rank: Union[int, List[int]]

            Which device(s) to print information

        _name: str

            name of current backend

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;Init call for DistributedDDP



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here ddp_config, sharded_config, or fully_sharded_config might be passed in



            &quot;&quot;&quot;

            self._ddp_config = kwargs[&quot;ddp_config&quot;]

            super(DistributedDDP, self).__init__(

                device_id=self._ddp_config.local_rank,

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;PyTorch DDP&quot;,

                verbose=verbose,

            )

            # This creates the wrapper method depending on DDP or SDDP

            self._ddp_handler = self._create_ddp_handler(kwargs)(

                verbose=self._verbose,

                sddp_config=kwargs[&quot;sharded_config&quot;],

                fsdp_config=kwargs[&quot;fully_sharded_config&quot;],

                ddp_config=self._ddp_config,

            )



        @staticmethod

        def _create_ddp_handler(kwargs: dict):

            &quot;&quot;&quot;Determines which DDP related class to use based on the kwarg config passed through



            Parameters

            ----------

            kwargs: dict

                Extra arguments from the __init__ call



            Returns

            -------

            FairscaleSDDPExtension or BaseDDP



            &quot;&quot;&quot;

            if kwargs[&quot;sharded_config&quot;] is not None:

                return DistributedHandlerEnum.sddp.value

            elif kwargs[&quot;fully_sharded_config&quot;] is not None:

                return DistributedHandlerEnum.fsdp.value

            else:

                return DistributedHandlerEnum.base.value



        def _call_init(self):

            &quot;&quot;&quot;Does any backend initialization work related to DDP setup



            Borrows code from DeepSpeed to setup DDP via openMPI

            https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/utils/distributed.py



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Borrowing a bit of code from deepspeed

            required_env = [

                &quot;RANK&quot;,

                &quot;WORLD_SIZE&quot;,

                &quot;MASTER_ADDR&quot;,

                &quot;MASTER_PORT&quot;,

                &quot;LOCAL_RANK&quot;,

            ]

            if self._ddp_config.auto_mpi_discovery and not all(

                map(lambda v: v in os.environ, required_env)

            ):

                try:

                    from mpi4py import MPI



                    mpi_discovery(verbose=True)

                except ImportError as e:

                    print(

                        e,

                        &quot;: mpi4py cannot be imported -- please install Stoke with the MPI option (pip install stoke[mpi])&quot;,

                    )

            # Initialize call for DDP

            torch.distributed.init_process_group(

                backend=self._ddp_config.backend, init_method=self._ddp_config.init_method

            )



        def setup_distributed(self):

            &quot;&quot;&quot;Handles any underlying DDP setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)

            # Call the init fnc here after device id is set

            self._call_init()



        def wrap_distributed(

            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            self._print_device(f&quot;{self._name} Class: {type(self._ddp_handler).__name__}&quot;)

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    [

                        f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;,

                        f&quot;{self._name} -- Rank: {self.rank}&quot;,

                    ]

                )

            if self._ddp_config.convert_to_sync_batch_norm:

                self.print_device(

                    f&quot;Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...&quot;

                )

                torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model)

            if self._verbose and isinstance(

                self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension)

            ):

                self._print_device(

                    f&quot;Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}&quot;

                )

            # Pass through to the handler for DDP wrappers

            model, optimizer = self._ddp_handler.handle_ddp(

                model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank

            )

            return model, optimizer



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)



        def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None):

            &quot;&quot;&quot;Take a single loss and detach it from the compute graph and sync across devices if needed



            Parameters

            ----------

            loss: torch.Tensor

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            float

                detached, synced, and mean calculated across devices



            &quot;&quot;&quot;

            # map to the same device the loss is on pre detach if not set

            if device is None:

                device = loss.device

            detached_loss = loss.item()

            with torch.no_grad():

                loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype)

                # Make sure everyone is synced before calling all reduce

                torch.distributed.barrier()

                # Loss tensor is worker specific so all_reduce (and SUM)

                torch.distributed.all_reduce(loss_tensor)

                # Detach and divide by the world size to get the mean on each device

                return loss_tensor.item() / self.world_size



        def grad_accum_context(self, model: torch.nn.Module):

            &quot;&quot;&quot;Return the context to wrap the gradient accumulation steps



            DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s))

            SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient

            accumulation else nullcontext



            &quot;&quot;&quot;

            if self._verbose and self._ddp_config.no_sync:

                self._print_device(&quot;DDP Using no sync context&quot;)

            context = model.no_sync() if self._ddp_config.no_sync else nullcontext()

            return context



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            torch.distributed.barrier()



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank&quot;&quot;&quot;

            return torch.distributed.get_rank()



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return torch.distributed.get_world_size()



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return torch.distributed.is_initialized()



        def clean(self):

            &quot;&quot;&quot;Cleans up at the end of a DDP run&quot;&quot;&quot;

            torch.distributed.destroy_process_group()





    class DistributedDeepspeed(BaseDistributed):

        &quot;&quot;&quot;Class for using Deepspeed as the distributed backend



        This class handles common functionality for the deepspeed backend including setup, loss sync,

        gradient accumulation context, step context and various properties/attributes



        Attributes

        ----------

        device_id

        initialized

        rank

        world_size

        _batch_size_per_device: int

            batch size per device or for non-distributed the overall batch size

        _deepspeed_config: DeepspeedConfig

            Configuration object for Deepspeed backend

        _device_id: int, default: None

            Current device id

        _info_rank: Union[int, List[int]]

            Which device(s) to print information

        _name: str

            name of current backend

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;Init call for DistributedDeepspeed



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here deepspeed_config, grad_accum_steps or grad_clip

                might be passed in



            &quot;&quot;&quot;

            self._deepspeed_config = kwargs[&quot;deepspeed_config&quot;]

            # Call init first to pass local rank to super

            self._call_init()

            # Forward device to super -- should be set from MPI lookup that is called

            super(DistributedDeepspeed, self).__init__(

                device_id=int(os.environ[&quot;LOCAL_RANK&quot;]),

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;Deepspeed&quot;,

                verbose=verbose,

            )

            self._deepspeed_init_config = self._handle_deepspeed_configs(

                grad_accum_steps=kwargs[&quot;grad_accum_steps&quot;], grad_clip=kwargs[&quot;grad_clip&quot;]

            )



        def _call_init(self):

            &quot;&quot;&quot;Does any backend initialization work related to deepspeed setup



            Returns

            -------

            None



            &quot;&quot;&quot;

            ds.init_distributed(

                dist_backend=self._deepspeed_config.dist_backend,

                auto_mpi_discovery=self._deepspeed_config.auto_mpi_discovery,

                distributed_port=self._deepspeed_config.distributed_port,

                verbose=self._deepspeed_config.verbose,

                init_method=self._deepspeed_config.init_method,

            )



        def setup_distributed(self):

            &quot;&quot;&quot;Handles any underlying deepspeed setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)



        def wrap_distributed(

            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with Deepspeed



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;

                )

                self._print_device(f&quot;{self._name} -- Rank: {self.rank}&quot;)



            model, optimizer, _, _ = ds.initialize(

                model=model,

                optimizer=optimizer,

                model_parameters=filter(lambda p: p.requires_grad, model.parameters()),

                config_params=self._deepspeed_init_config,

            )

            return model, optimizer



        def _handle_deepspeed_configs(

            self,

            grad_accum_steps: int,

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]],

        ):

            &quot;&quot;&quot;Handles building the dictionary of configs that the deepspeed initialize call expects



            https://www.deepspeed.ai/docs/config-json/



            Parameters

            ----------

            grad_accum_steps: int

                number of gradient accumulation steps

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                gradient clipping config objects



            Returns

            -------

            dict

                All deepspeed parameters merged together from individual pieces



            &quot;&quot;&quot;

            # empty dict to start

            ds_config = {}

            # Map batch size stuff -- need to define 2/3

            ds_config.update(self._map_ds_batch_configs(grad_accum_steps=grad_accum_steps))

            # Skip optimizer &amp; skip scheduler

            # Map communication

            ds_config.update(self._map_ds_communication_configs())

            # Map FP16 and add enabled flag if selected

            ds_config.update(self._map_ds_fp16_configs())

            # Map grad clipping

            ds_config.update(self._map_ds_grad_clip_configs(grad_clip=grad_clip))

            # Map zero -- internally map param offloading and optimizer offloading to zero

            ds_config.update(self._map_ds_zero_configs())

            # Map aio

            ds_config.update(self._map_ds_aio_configs())

            # Map logging

            ds_config.update(self._map_ds_logging_configs())

            # Map flops -- enabled

            ds_config.update(self._map_ds_flops_configs())

            # Map activation checkpointing

            ds_config.update(self._map_ds_activation_checkpointing_configs())

            # Map tensorboard

            ds_config.update(self._map_ds_tensorboard_config())

            # Map PLD

            ds_config.update(self._map_ds_pld_config())

            return ds_config



        def _map_ds_pld_config(self):

            &quot;&quot;&quot;Maps progressive layer drop parameters



            https://www.deepspeed.ai/tutorials/progressive_layer_dropping/

            https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L293



            Returns

            -------

            dict

                pld parameters or enabled false dict



            &quot;&quot;&quot;

            if self._deepspeed_config.progressive_layer_drop is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.progressive_layer_drop, v.name)

                    for v in self._deepspeed_config.progressive_layer_drop.__attrs_attrs__

                }

                map_dict.update({&quot;enabled&quot;: True})

                return {&quot;progressive_layer_drop&quot;: map_dict}

            else:

                return {&quot;progressive_layer_drop&quot;: {&quot;enabled&quot;: False}}



        def _map_ds_tensorboard_config(self):

            &quot;&quot;&quot;Maps tensorboard related parameters



            https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L268



            Returns

            -------

            dict

                tensorboard parameters or enabled false dict



            &quot;&quot;&quot;

            if self._deepspeed_config.tensorboard is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.tensorboard, v.name)

                    for v in self._deepspeed_config.tensorboard.__attrs_attrs__

                }

                map_dict.update({&quot;enabled&quot;: True})

                return {&quot;tensorboard&quot;: map_dict}

            else:

                return {&quot;tensorboard&quot;: {&quot;enabled&quot;: False}}



        def _map_ds_grad_clip_configs(

            self, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]]

        ):

            &quot;&quot;&quot;Maps grad clipping related parameters



            https://www.deepspeed.ai/docs/config-json/#gradient-clipping



            Parameters

            ----------

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                gradient clipping config objects



            Returns

            -------

            dict

                gradient clipping parameters or empty dict



            &quot;&quot;&quot;

            if grad_clip is not None:

                if isinstance(grad_clip, ClipGradNormConfig):

                    return {&quot;gradient_clipping&quot;: grad_clip.max_norm}

                else:

                    raise ValueError(

                        f&quot;Deepspeed does not currently support &quot;

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)}&#39;

                    )

            else:

                return {}



        def _map_ds_logging_configs(self):

            &quot;&quot;&quot;Maps logging related parameters



            https://www.deepspeed.ai/docs/config-json/#logging



            Returns

            -------

            dict

                logging parameters or empty dict



            &quot;&quot;&quot;

            return {

                &quot;steps_per_print&quot;: self._deepspeed_config.steps_per_print,

                &quot;dump_state&quot;: self._deepspeed_config.dump_state,

                &quot;wall_clock_breakdown&quot;: self._deepspeed_config.wall_clock_breakdown,

            }



        def _map_ds_activation_checkpointing_configs(self):

            &quot;&quot;&quot;Maps activation checkpointing related parameters



            https://www.deepspeed.ai/docs/config-json/#activation-checkpointing



            Returns

            -------

            dict

                activation checkpointing parameters or empty dict



            &quot;&quot;&quot;

            if self._deepspeed_config.activation_checkpointing is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.activation_checkpointing, v.name)

                    for v in self._deepspeed_config.activation_checkpointing.__attrs_attrs__

                }

                return {&quot;activation_checkpointing&quot;: map_dict}

            else:

                return {}



        def _map_ds_flops_configs(self):

            &quot;&quot;&quot;Maps flops related parameters



            https://www.deepspeed.ai/docs/config-json/#flops-profiler



            Returns

            -------

            dict

                flops parameters or enabled false dict



            &quot;&quot;&quot;

            if self._deepspeed_config.flops_profiler is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.flops_profiler, v.name)

                    for v in self._deepspeed_config.flops_profiler.__attrs_attrs__

                }

                map_dict.update({&quot;enabled&quot;: True})

                return {&quot;flops_profiler&quot;: map_dict}

            else:

                return {&quot;flops_profiler&quot;: {&quot;enabled&quot;: False}}



        def _map_ds_aio_configs(self):

            &quot;&quot;&quot;Maps async i/o related parameters



            https://www.deepspeed.ai/docs/config-json/#asynchronous-io



            Returns

            -------

            dict

                async i/o parameters or empty dict



            &quot;&quot;&quot;

            if self._deepspeed_config.aio is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.aio, v.name)

                    for v in self._deepspeed_config.aio.__attrs_attrs__

                }

                return {&quot;aio&quot;: map_dict}

            else:

                return {}



        def _map_ds_zero_configs(self):

            &quot;&quot;&quot;Maps ZeRO related parameters



            https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training



            Returns

            -------

            dict

                ZeRO related parameters



            &quot;&quot;&quot;

            map_dict = {}

            for v in self._deepspeed_config.zero_optimization.__attrs_attrs__:

                if v.name == &quot;offload_optimizer&quot;:

                    map_dict.update(self._map_ds_offload_optimizer_configs())

                elif v.name == &quot;offload_param&quot;:

                    map_dict.update(self._map_ds_offload_param_configs())

                # Just map the rest since the name:value is correct

                else:

                    map_dict.update(

                        {v.name: getattr(self._deepspeed_config.zero_optimization, v.name)}

                    )

            # Default overlap com to True for ZeRO stage 3

            map_dict[&quot;overlap_comm&quot;] = (

                True if map_dict[&quot;stage&quot;] == 3 else map_dict[&quot;overlap_comm&quot;]

            )

            return {&quot;zero_optimization&quot;: map_dict}



        def _map_ds_offload_param_configs(self):

            &quot;&quot;&quot;Maps ZeRO parameter offload parameters



            https://www.deepspeed.ai/docs/config-json/#parameter-offloading



            Returns

            -------

            dict

                ZeRO offload parameter parameters



            &quot;&quot;&quot;

            # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct

            if self._deepspeed_config.zero_optimization.offload_param is not None:

                map_dict = {

                    v.name: getattr(

                        self._deepspeed_config.zero_optimization.offload_param, v.name

                    )

                    for v in self._deepspeed_config.zero_optimization.offload_param.__attrs_attrs__

                }

                return {&quot;offload_param&quot;: map_dict}

            else:

                return {&quot;offload_param&quot;: None}



        def _map_ds_offload_optimizer_configs(self):

            &quot;&quot;&quot;Maps ZeRO optimizer offload parameters



            https://www.deepspeed.ai/docs/config-json/#optimizer-offloading



            Returns

            -------

            dict

                ZeRO offload optimizer parameters



            &quot;&quot;&quot;

            # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct

            if self._deepspeed_config.zero_optimization.offload_optimizer is not None:

                map_dict = {

                    v.name: getattr(

                        self._deepspeed_config.zero_optimization.offload_optimizer, v.name

                    )

                    for v in self._deepspeed_config.zero_optimization.offload_optimizer.__attrs_attrs__

                }

                # Set some post init values

                map_dict[&quot;pipeline&quot;] = (

                    map_dict[&quot;pipeline_read&quot;] or map_dict[&quot;pipeline_write&quot;]

                )

                return {&quot;offload_optimizer&quot;: map_dict}

            else:

                return {&quot;offload_optimizer&quot;: None}



        def _map_ds_fp16_configs(self):

            &quot;&quot;&quot;Maps FP16 related parameters



            https://www.deepspeed.ai/docs/config-json/#fp16-training-options



            Returns

            -------

            dict

                fp16 related parameters or enabled false dict



            &quot;&quot;&quot;

            if self._deepspeed_config.fp16 is not None:

                # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct

                map_dict = {

                    v.name: getattr(self._deepspeed_config.fp16, v.name)

                    for v in self._deepspeed_config.fp16.__attrs_attrs__

                }

                # Add the enabled flag

                map_dict.update({&quot;enabled&quot;: True})

                return {&quot;fp16&quot;: map_dict}

            else:

                return {&quot;fp16&quot;: {&quot;enabled&quot;: False}}



        def _map_ds_batch_configs(self, grad_accum_steps: int):

            &quot;&quot;&quot;Maps batch size related parameters



            https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters



            Parameters

            ----------

            grad_accum_steps: int

                number of gradient accumulation steps



            Returns

            -------

            dict

                batch size related parameters



            &quot;&quot;&quot;

            # Need to define 2/3

            return {

                &quot;train_micro_batch_size_per_gpu&quot;: self._batch_size_per_device,

                &quot;gradient_accumulation_steps&quot;: grad_accum_steps,

            }



        def _map_ds_communication_configs(self):

            &quot;&quot;&quot;Maps communication related parameters



            https://www.deepspeed.ai/docs/config-json/#communication-options



            Returns

            -------

            dict

                communication related parameters



            &quot;&quot;&quot;

            return {

                &quot;fp32_allreduce&quot;: self._deepspeed_config.fp32_allreduce,

                &quot;gradient_predivide_factor&quot;: self._deepspeed_config.gradient_predivide_factor,

                &quot;prescale_gradients:&quot;: self._deepspeed_config.prescale_gradients,

                &quot;sparse_gradients&quot;: self._deepspeed_config.sparse_gradients,

            }



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)



        def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None):

            &quot;&quot;&quot;Take a single loss and detach it from the compute graph and sync across devices if needed



            Parameters

            ----------

            loss: torch.Tensor

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            float

                detached, synced, and mean calculated across devices



            &quot;&quot;&quot;

            # map to the same device the loss is on pre detach if not set

            if device is None:

                device = loss.device

            detached_loss = loss.item()

            with torch.no_grad():

                loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype)

                # Loss tensor is worker specific so all_reduce (and SUM)

                torch.distributed.all_reduce(loss_tensor)

                # Detach and divide by the world size to get the mean on each device

                return loss_tensor.item() / self.world_size



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            torch.distributed.barrier()



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank&quot;&quot;&quot;

            return torch.distributed.get_rank()



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return torch.distributed.get_world_size()



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return torch.distributed.is_initialized()



        def clean(self):

            &quot;&quot;&quot;Cleans up at the end of a DDP run&quot;&quot;&quot;

            torch.distributed.destroy_process_group()





    class DistributedHorovod(BaseDistributed):

        &quot;&quot;&quot;Class for using Horovod as the distributed backend



        This class handles common functionality for the horovod backend including setup, loss sync,

        gradient accumulation context, step context and various properties/attributes



        Attributes

        ----------

        device_id

        initialized

        rank

        world_size

        _batch_size_per_device: int

            batch size per device or for non-distributed the overall batch size

        _device_id: int, default: None

            Current device id

        _horovod_config: HorovodConfig

            Configuration object for Horovod backend

        _info_rank: Union[int, List[int]]

            Which device(s) to print information

        _name: str

            name of current backend

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here horovod_config might be passed in



            &quot;&quot;&quot;

            # Grab the config

            self._horovod_config = kwargs[&quot;horovod_config&quot;]

            # Initialize first so the local rank call cal be forwarded to super

            self._call_init()

            super(DistributedHorovod, self).__init__(

                device_id=hvd.local_rank(),

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;Horovod&quot;,

                verbose=verbose,

            )

            self._multi_loss = (

                len(kwargs[&quot;loss&quot;]) if isinstance(kwargs[&quot;loss&quot;], (list, tuple)) else 1

            )



        def _call_init(self):

            &quot;&quot;&quot;Does any backend initialization work related to horovod setup



            Returns

            -------

            None



            &quot;&quot;&quot;

            hvd.init()



        def _hvd_convert_to_sync_batch_norm(

            self, module: torch.nn.Module, process_group=None

        ):

            &quot;&quot;&quot;Replaces all BatchNorm*D layers with horovod.torch.SyncBatchNorm layers



            https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm

            https://nvidia.github.io/apex/_modules/apex/parallel.html#convert_syncbn_model



            Parameters

            ----------

            module: torch.nn.Module

                current model object

            process_group: default: None

                process group to scope synchronization, default is the whole world



            Returns

            -------

            module_output: torch.nn.Module

                modified version of model with all BatchNorm*D layers replaced with horovod.torch.SyncBatchNorm layers



            Notes

            -----

            Borrows heavily from the current torch convert_sync_batchnorm and apex convert_syncbn_model implementations

            only changing the underlying layer type to use the hvd implementation



            &quot;&quot;&quot;

            module_output = module

            if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):

                module_output = hvd.SyncBatchNorm(

                    num_features=module.num_features,

                    eps=module.eps,

                    momentum=module.momentum,

                    affine=module.affine,

                    track_running_stats=module.track_running_stats,

                )

                # Handle the copy of affine vars if affine

                if module.affine:

                    with torch.no_grad():

                        module_output.weight = module.weight

                        module_output.bias = module.bias

                # Handle the swap of running stats

                module_output.running_mean = module.running_mean

                module_output.running_var = module.running_var

            # Iterate recursively and replace

            for name, child in module.named_children():

                module_output.add_module(

                    name=name,

                    module=self._hvd_convert_to_sync_batch_norm(

                        module=child, process_group=process_group

                    ),

                )

            # delete and return

            del module

            return module_output



        def setup_distributed(self):

            &quot;&quot;&quot;Handles any underlying horovod setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)



        def wrap_distributed(

            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with Horovod



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;

                )

                self._print_device(f&quot;{self._name} -- Rank: {self.rank}&quot;)

            op_dict = {&quot;Average&quot;: hvd.Average, &quot;Sum&quot;: hvd.Sum, &quot;Adasum&quot;: hvd.Adasum}

            optimizer = hvd.DistributedOptimizer(

                optimizer=optimizer,

                named_parameters=model.named_parameters(),

                backward_passes_per_step=grad_accum * self._multi_loss

                if grad_accum is not None

                else self._multi_loss,

                compression=hvd.Compression.fp16

                if self._horovod_config.compression

                else hvd.Compression.none,

                gradient_predivide_factor=self._horovod_config.gradient_predivide_factor,

                op=op_dict.get(self._horovod_config.op),

            )

            # Broadcast the initial variable states from rank 0 to all other processes

            hvd.broadcast_parameters(model.state_dict(), root_rank=0)

            hvd.broadcast_optimizer_state(optimizer, root_rank=0)

            return model, optimizer



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)



        def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None):

            &quot;&quot;&quot;Take a single loss and detach it from the compute graph and sync across devices if needed



            Parameters

            ----------

            loss: torch.Tensor

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            float

                detached, synced, and mean calculated across devices



            &quot;&quot;&quot;

            # map to the same device the loss is on pre detach if not set

            if device is None:

                device = loss.device

            detached_loss = loss.item()

            with torch.no_grad():

                loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype)

                # Make sure everyone is synced before the all-reduce

                # Horovod doesn&#39;t have a native barrier so lean on join to take care of it

                # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join

                hvd.join()

                # Loss tensor is worker specific so allreduce -- force SUM from Horovod

                sum_tensor = hvd.allreduce(loss_tensor, op=hvd.Sum)

                # Detach and divide by the world size to get the mean on each device

                return sum_tensor.item() / self.world_size



        def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):

            &quot;&quot;&quot;Return the context to wrap the step call



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation



            &quot;&quot;&quot;

            # Hidden here -- Horovod docs are terrible

            # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

            if self._verbose:

                self._print_device(

                    &quot;Horovod skipping synchronize as it was triggered pre grad-clip&quot;

                )

            return optimizer.skip_synchronize()



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            # Horovod doesn&#39;t have a native barrier so lean on join to take care of it

            # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join

            hvd.join()



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank&quot;&quot;&quot;

            return hvd.rank()



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return hvd.size()



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return hvd.is_initialized()





    class RunnerDistEnum(Enum):

        &quot;&quot;&quot;Enum for building the runtime object with distributed functionality&quot;&quot;&quot;



        cpu = DistributedNullCPU

        gpu = DistributedNullGPU

        ddp = DistributedDDP

        horovod = DistributedHorovod

        deepspeed = DistributedDeepspeed
</code></pre></div>
<h2 id="classes">Classes</h2>
<h3 id="basedistributed">BaseDistributed</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BaseDistributed</span><span class="p">(</span>
    <span class="n">device_id</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>device_id</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>initialized</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>world_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_batch_size_per_device</td>
<td>int</td>
<td>batch size per device or for non-distributed the overall batch size</td>
<td>None</td>
</tr>
<tr>
<td>_device_id</td>
<td>int, default: None</td>
<td>Current device id</td>
<td>None</td>
</tr>
<tr>
<td>_info_rank</td>
<td>Union[int, List[int]]</td>
<td>Which device(s) to print information</td>
<td>None</td>
</tr>
<tr>
<td>_name</td>
<td>str</td>
<td>name of current backend</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class BaseDistributed(ABC):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Base class for distributed backends



        This class handles common functionality for all of the different distributed backends including setup, loss sync,

        gradient accumulation context, step context and various properties/attributes related to distributed frameworks



        Attributes

        ----------

        device_id

        initialized

        rank

        world_size

        _batch_size_per_device: int

            batch size per device or for non-distributed the overall batch size

        _device_id: int, default: None

            Current device id

        _info_rank: Union[int, List[int]]

            Which device(s) to print information

        _name: str

            name of current backend

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self,

            device_id: Optional[Union[int, str]],

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            name: str,

            verbose: bool = True,

        ):

            &quot;&quot;&quot;Init for BaseDistributed class



            Parameters

            ----------

            device_id: int, default: None

                Current device id

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            name: str

                name of current backend

            verbose: bool, default: True

                flag for Stoke print verbosity



            &quot;&quot;&quot;

            self._batch_size_per_device = batch_size_per_device

            self._device_id = device_id

            self._info_rank = info_rank

            self._name = name

            self._verbose = verbose



        def _print_info(self):

            &quot;&quot;&quot;Basic print of backend initialization status



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._print_device(f&quot;{self._name} Initialized: {self.initialized}&quot;)



        def setup_distributed(self):

            &quot;&quot;&quot;Base setup distributed



            Does nothing as nothing needs to be wrapped



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass



        def wrap_distributed(

            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Base wrapper for distributed backends



            Does nothing but print as nothing needs to be wrapped



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                same as input model

            optimizer: Union[torch.optim.Optimizer, OSS]]

                same as input optimizer



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

            return model, optimizer



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(val.item() for val in loss)

            else:

                return loss.item()



        def grad_accum_context(self, model: torch.nn.Module):

            &quot;&quot;&quot;Returns base context for gradient accumulation



            By default no context is used



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()



        def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):

            &quot;&quot;&quot;Returns base context for the step call



            By default no context is used



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()



        def clean(self):

            &quot;&quot;&quot;Base clean call



            Nothing to do here...



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass



        def _call_init(self):

            &quot;&quot;&quot;Base init call



            Nothing to do here...



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass



        def _print_device(self, msg: Union[str, List[str]]):

            &quot;&quot;&quot;Prints a str of list of strs on the currently set _info_rank



            Internal version of public print_device that always points to the set _info_rank



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print_device(msg=msg, rank=self._info_rank)



        def print_device(

            self,

            msg: Union[str, List[str]],

            rank: Optional[Union[int, List[int]]] = 0,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Public facing method to print on specific device ranks



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print

            rank: Optional[Union[int, List[int]]], default: 0

                device rank to print to (prevents printing on multiple devices in distributed mode)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Ignore the rank check if the current rank is a non-distributed version

            if self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;:

                unrolled_print(msg, single_line=single_line)

            # if it&#39;s a list then check the rank against the list

            elif isinstance(rank, list) and self.rank in rank:

                unrolled_print(msg, single_line=single_line)

            # If its an int then check the equality

            elif isinstance(rank, int) and rank == self.rank:

                unrolled_print(msg, single_line=single_line)

            # the else is essentially skip print

            else:

                pass



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            pass



        @property

        def device_id(self):

            &quot;&quot;&quot;Returns the current device id&quot;&quot;&quot;

            return self._device_id



        @property

        @abstractmethod

        def rank(self):

            pass



        @property

        @abstractmethod

        def world_size(self):

            pass



        @property

        @abstractmethod

        def initialized(self):

            pass
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>abc.ABC</li>
</ul>
<h4 id="descendants">Descendants</h4>
<ul>
<li>stoke.distributed.DistributedNullCPU</li>
<li>stoke.distributed.DistributedNullGPU</li>
<li>stoke.distributed.DistributedDDP</li>
<li>stoke.distributed.DistributedDeepspeed</li>
<li>stoke.distributed.DistributedHorovod</li>
</ul>
<h4 id="instance-variables">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">device_id</span>
</code></pre></div>
<p>Returns the current device id</p>
<div class="highlight"><pre><span></span><code><span class="n">initialized</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">rank</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<h4 id="methods">Methods</h4>
<h4 id="barrier">barrier</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Calls the underlying distributed barrier if available</p>
<p>??? example "View Source"
            def barrier(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="clean">clean</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clean</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Base clean call</p>
<p>Nothing to do here...</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clean(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Base clean call



            Nothing to do here...



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="detach_and_sync_loss">detach_and_sync_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detach_and_sync_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es) on the device</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>output device of the sync call</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Union[float, List[float], Tuple[float]]</td>
<td>loss(es) that has(have) been detached from the graph</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def detach_and_sync_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(val.item() for val in loss)

            else:

                return loss.item()
</code></pre></div>
<h4 id="grad_accum_context">grad_accum_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">grad_accum_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for gradient accumulation</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def grad_accum_context(self, model: torch.nn.Module):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for gradient accumulation



            By default no context is used



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="print_device">print_device</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_device</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Public facing method to print on specific device ranks</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>Union[str, List[str]]</td>
<td>message(s) to print</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>Optional[Union[int, List[int]]], default: 0</td>
<td>device rank to print to (prevents printing on multiple devices in distributed mode)</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_device(</p>
<div class="highlight"><pre><span></span><code>            self,

            msg: Union[str, List[str]],

            rank: Optional[Union[int, List[int]]] = 0,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Public facing method to print on specific device ranks



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print

            rank: Optional[Union[int, List[int]]], default: 0

                device rank to print to (prevents printing on multiple devices in distributed mode)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Ignore the rank check if the current rank is a non-distributed version

            if self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;:

                unrolled_print(msg, single_line=single_line)

            # if it&#39;s a list then check the rank against the list

            elif isinstance(rank, list) and self.rank in rank:

                unrolled_print(msg, single_line=single_line)

            # If its an int then check the equality

            elif isinstance(rank, int) and rank == self.rank:

                unrolled_print(msg, single_line=single_line)

            # the else is essentially skip print

            else:

                pass
</code></pre></div>
<h4 id="setup_distributed">setup_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setup_distributed</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Base setup distributed</p>
<p>Does nothing as nothing needs to be wrapped</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def setup_distributed(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Base setup distributed



            Does nothing as nothing needs to be wrapped



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="step_context">step_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for the step call</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for the step call



            By default no context is used



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="wrap_distributed">wrap_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_distributed</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Base wrapper for distributed backends</p>
<p>Does nothing but print as nothing needs to be wrapped</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Optional[Union[torch.optim.Optimizer, OSS]], default: None</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>same as input model</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_distributed(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Base wrapper for distributed backends



            Does nothing but print as nothing needs to be wrapped



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                same as input model

            optimizer: Union[torch.optim.Optimizer, OSS]]

                same as input optimizer



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

            return model, optimizer
</code></pre></div>
<h3 id="distributedddp">DistributedDDP</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DistributedDDP</span><span class="p">(</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_1">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>device_id</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>initialized</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>world_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_batch_size_per_device</td>
<td>int</td>
<td>batch size per device or for non-distributed the overall batch size</td>
<td>None</td>
</tr>
<tr>
<td>_ddp_config</td>
<td>DDPConfig</td>
<td>Configuration object for DDP backend</td>
<td>None</td>
</tr>
<tr>
<td>_ddp_handler</td>
<td>None</td>
<td>wrapper method that will modify the DDP instance to use SDDP if flagged</td>
<td>None</td>
</tr>
<tr>
<td>_device_id</td>
<td>int, default: None</td>
<td>Current device id</td>
<td>None</td>
</tr>
<tr>
<td>_info_rank</td>
<td>Union[int, List[int]]</td>
<td>Which device(s) to print information</td>
<td>None</td>
</tr>
<tr>
<td>_name</td>
<td>str</td>
<td>name of current backend</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DistributedDDP(BaseDistributed):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Class for using DDP as the distributed backend



        This class handles common functionality for the DDP backend including setup, loss sync,

        gradient accumulation context, step context and various properties/attributes



        Attributes

        ----------

        device_id

        initialized

        rank

        world_size

        _batch_size_per_device: int

            batch size per device or for non-distributed the overall batch size

        _ddp_config: DDPConfig

            Configuration object for DDP backend

        _ddp_handler

            wrapper method that will modify the DDP instance to use SDDP if flagged

        _device_id: int, default: None

            Current device id

        _info_rank: Union[int, List[int]]

            Which device(s) to print information

        _name: str

            name of current backend

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;Init call for DistributedDDP



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here ddp_config, sharded_config, or fully_sharded_config might be passed in



            &quot;&quot;&quot;

            self._ddp_config = kwargs[&quot;ddp_config&quot;]

            super(DistributedDDP, self).__init__(

                device_id=self._ddp_config.local_rank,

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;PyTorch DDP&quot;,

                verbose=verbose,

            )

            # This creates the wrapper method depending on DDP or SDDP

            self._ddp_handler = self._create_ddp_handler(kwargs)(

                verbose=self._verbose,

                sddp_config=kwargs[&quot;sharded_config&quot;],

                fsdp_config=kwargs[&quot;fully_sharded_config&quot;],

                ddp_config=self._ddp_config,

            )



        @staticmethod

        def _create_ddp_handler(kwargs: dict):

            &quot;&quot;&quot;Determines which DDP related class to use based on the kwarg config passed through



            Parameters

            ----------

            kwargs: dict

                Extra arguments from the __init__ call



            Returns

            -------

            FairscaleSDDPExtension or BaseDDP



            &quot;&quot;&quot;

            if kwargs[&quot;sharded_config&quot;] is not None:

                return DistributedHandlerEnum.sddp.value

            elif kwargs[&quot;fully_sharded_config&quot;] is not None:

                return DistributedHandlerEnum.fsdp.value

            else:

                return DistributedHandlerEnum.base.value



        def _call_init(self):

            &quot;&quot;&quot;Does any backend initialization work related to DDP setup



            Borrows code from DeepSpeed to setup DDP via openMPI

            https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/utils/distributed.py



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Borrowing a bit of code from deepspeed

            required_env = [

                &quot;RANK&quot;,

                &quot;WORLD_SIZE&quot;,

                &quot;MASTER_ADDR&quot;,

                &quot;MASTER_PORT&quot;,

                &quot;LOCAL_RANK&quot;,

            ]

            if self._ddp_config.auto_mpi_discovery and not all(

                map(lambda v: v in os.environ, required_env)

            ):

                try:

                    from mpi4py import MPI



                    mpi_discovery(verbose=True)

                except ImportError as e:

                    print(

                        e,

                        &quot;: mpi4py cannot be imported -- please install Stoke with the MPI option (pip install stoke[mpi])&quot;,

                    )

            # Initialize call for DDP

            torch.distributed.init_process_group(

                backend=self._ddp_config.backend, init_method=self._ddp_config.init_method

            )



        def setup_distributed(self):

            &quot;&quot;&quot;Handles any underlying DDP setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)

            # Call the init fnc here after device id is set

            self._call_init()



        def wrap_distributed(

            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            self._print_device(f&quot;{self._name} Class: {type(self._ddp_handler).__name__}&quot;)

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    [

                        f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;,

                        f&quot;{self._name} -- Rank: {self.rank}&quot;,

                    ]

                )

            if self._ddp_config.convert_to_sync_batch_norm:

                self.print_device(

                    f&quot;Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...&quot;

                )

                torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model)

            if self._verbose and isinstance(

                self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension)

            ):

                self._print_device(

                    f&quot;Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}&quot;

                )

            # Pass through to the handler for DDP wrappers

            model, optimizer = self._ddp_handler.handle_ddp(

                model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank

            )

            return model, optimizer



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)



        def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None):

            &quot;&quot;&quot;Take a single loss and detach it from the compute graph and sync across devices if needed



            Parameters

            ----------

            loss: torch.Tensor

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            float

                detached, synced, and mean calculated across devices



            &quot;&quot;&quot;

            # map to the same device the loss is on pre detach if not set

            if device is None:

                device = loss.device

            detached_loss = loss.item()

            with torch.no_grad():

                loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype)

                # Make sure everyone is synced before calling all reduce

                torch.distributed.barrier()

                # Loss tensor is worker specific so all_reduce (and SUM)

                torch.distributed.all_reduce(loss_tensor)

                # Detach and divide by the world size to get the mean on each device

                return loss_tensor.item() / self.world_size



        def grad_accum_context(self, model: torch.nn.Module):

            &quot;&quot;&quot;Return the context to wrap the gradient accumulation steps



            DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s))

            SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient

            accumulation else nullcontext



            &quot;&quot;&quot;

            if self._verbose and self._ddp_config.no_sync:

                self._print_device(&quot;DDP Using no sync context&quot;)

            context = model.no_sync() if self._ddp_config.no_sync else nullcontext()

            return context



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            torch.distributed.barrier()



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank&quot;&quot;&quot;

            return torch.distributed.get_rank()



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return torch.distributed.get_world_size()



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return torch.distributed.is_initialized()



        def clean(self):

            &quot;&quot;&quot;Cleans up at the end of a DDP run&quot;&quot;&quot;

            torch.distributed.destroy_process_group()
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>stoke.distributed.BaseDistributed</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_1">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">device_id</span>
</code></pre></div>
<p>Returns the current device id</p>
<div class="highlight"><pre><span></span><code><span class="n">initialized</span>
</code></pre></div>
<p>Returns if distributed backend is initialized correctly</p>
<div class="highlight"><pre><span></span><code><span class="n">rank</span>
</code></pre></div>
<p>Returns current distributed rank</p>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<p>Returns current world size</p>
<h4 id="methods_1">Methods</h4>
<h4 id="barrier_1">barrier</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Calls the underlying distributed barrier if available</p>
<p>??? example "View Source"
            def barrier(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            torch.distributed.barrier()
</code></pre></div>
<h4 id="clean_1">clean</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clean</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Cleans up at the end of a DDP run</p>
<p>??? example "View Source"
            def clean(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Cleans up at the end of a DDP run&quot;&quot;&quot;

            torch.distributed.destroy_process_group()
</code></pre></div>
<h4 id="detach_and_sync_loss_1">detach_and_sync_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detach_and_sync_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es) on the device</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>output device of the sync call</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Union[float, List[float], Tuple[float]]</td>
<td>loss(es) that has(have) been synced across multiple devices and detached from the graph</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def detach_and_sync_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)
</code></pre></div>
<h4 id="grad_accum_context_1">grad_accum_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">grad_accum_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span>
</code></pre></div>
<p>Return the context to wrap the gradient accumulation steps</p>
<p>DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s))
SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html
FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def grad_accum_context(self, model: torch.nn.Module):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Return the context to wrap the gradient accumulation steps



            DDP: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html (Skip unnecessary all-reduce(s))

            SDDP: https://fairscale.readthedocs.io/en/latest/api/nn/sharded_ddp.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            no_sync() context if no_sync flag in config to prevent un-needed communication overhead when using gradient

            accumulation else nullcontext



            &quot;&quot;&quot;

            if self._verbose and self._ddp_config.no_sync:

                self._print_device(&quot;DDP Using no sync context&quot;)

            context = model.no_sync() if self._ddp_config.no_sync else nullcontext()

            return context
</code></pre></div>
<h4 id="print_device_1">print_device</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_device</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Public facing method to print on specific device ranks</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>Union[str, List[str]]</td>
<td>message(s) to print</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>Optional[Union[int, List[int]]], default: 0</td>
<td>device rank to print to (prevents printing on multiple devices in distributed mode)</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_device(</p>
<div class="highlight"><pre><span></span><code>            self,

            msg: Union[str, List[str]],

            rank: Optional[Union[int, List[int]]] = 0,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Public facing method to print on specific device ranks



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print

            rank: Optional[Union[int, List[int]]], default: 0

                device rank to print to (prevents printing on multiple devices in distributed mode)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Ignore the rank check if the current rank is a non-distributed version

            if self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;:

                unrolled_print(msg, single_line=single_line)

            # if it&#39;s a list then check the rank against the list

            elif isinstance(rank, list) and self.rank in rank:

                unrolled_print(msg, single_line=single_line)

            # If its an int then check the equality

            elif isinstance(rank, int) and rank == self.rank:

                unrolled_print(msg, single_line=single_line)

            # the else is essentially skip print

            else:

                pass
</code></pre></div>
<h4 id="setup_distributed_1">setup_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setup_distributed</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Handles any underlying DDP setup post init</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def setup_distributed(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Handles any underlying DDP setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)

            # Call the init fnc here after device id is set

            self._call_init()
</code></pre></div>
<h4 id="step_context_1">step_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for the step call</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for the step call



            By default no context is used



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="wrap_distributed_1">wrap_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_distributed</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Optional[Union[torch.optim.Optimizer, OSS]], default: None</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>Wrapped model object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_distributed(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with either DDP, Fairscale SDDP or Fairscale FSDP



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            self._print_device(f&quot;{self._name} Class: {type(self._ddp_handler).__name__}&quot;)

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    [

                        f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;,

                        f&quot;{self._name} -- Rank: {self.rank}&quot;,

                    ]

                )

            if self._ddp_config.convert_to_sync_batch_norm:

                self.print_device(

                    f&quot;Converting all BatchNorm*D layers to torch.nn.SyncBatchNorm layers...&quot;

                )

                torch.nn.SyncBatchNorm.convert_sync_batchnorm(module=model)

            if self._verbose and isinstance(

                self._ddp_handler, (FairscaleSDDPExtension, FairscaleFSDPExtension)

            ):

                self._print_device(

                    f&quot;Wrapped PyTorch DDP with {type(self._ddp_handler).__name__}&quot;

                )

            # Pass through to the handler for DDP wrappers

            model, optimizer = self._ddp_handler.handle_ddp(

                model=model, optimizer=optimizer, grad_accum=grad_accum, rank=self.rank

            )

            return model, optimizer
</code></pre></div>
<h3 id="distributeddeepspeed">DistributedDeepspeed</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DistributedDeepspeed</span><span class="p">(</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_2">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>device_id</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>initialized</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>world_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_batch_size_per_device</td>
<td>int</td>
<td>batch size per device or for non-distributed the overall batch size</td>
<td>None</td>
</tr>
<tr>
<td>_deepspeed_config</td>
<td>DeepspeedConfig</td>
<td>Configuration object for Deepspeed backend</td>
<td>None</td>
</tr>
<tr>
<td>_device_id</td>
<td>int, default: None</td>
<td>Current device id</td>
<td>None</td>
</tr>
<tr>
<td>_info_rank</td>
<td>Union[int, List[int]]</td>
<td>Which device(s) to print information</td>
<td>None</td>
</tr>
<tr>
<td>_name</td>
<td>str</td>
<td>name of current backend</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DistributedDeepspeed(BaseDistributed):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Class for using Deepspeed as the distributed backend



        This class handles common functionality for the deepspeed backend including setup, loss sync,

        gradient accumulation context, step context and various properties/attributes



        Attributes

        ----------

        device_id

        initialized

        rank

        world_size

        _batch_size_per_device: int

            batch size per device or for non-distributed the overall batch size

        _deepspeed_config: DeepspeedConfig

            Configuration object for Deepspeed backend

        _device_id: int, default: None

            Current device id

        _info_rank: Union[int, List[int]]

            Which device(s) to print information

        _name: str

            name of current backend

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;Init call for DistributedDeepspeed



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here deepspeed_config, grad_accum_steps or grad_clip

                might be passed in



            &quot;&quot;&quot;

            self._deepspeed_config = kwargs[&quot;deepspeed_config&quot;]

            # Call init first to pass local rank to super

            self._call_init()

            # Forward device to super -- should be set from MPI lookup that is called

            super(DistributedDeepspeed, self).__init__(

                device_id=int(os.environ[&quot;LOCAL_RANK&quot;]),

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;Deepspeed&quot;,

                verbose=verbose,

            )

            self._deepspeed_init_config = self._handle_deepspeed_configs(

                grad_accum_steps=kwargs[&quot;grad_accum_steps&quot;], grad_clip=kwargs[&quot;grad_clip&quot;]

            )



        def _call_init(self):

            &quot;&quot;&quot;Does any backend initialization work related to deepspeed setup



            Returns

            -------

            None



            &quot;&quot;&quot;

            ds.init_distributed(

                dist_backend=self._deepspeed_config.dist_backend,

                auto_mpi_discovery=self._deepspeed_config.auto_mpi_discovery,

                distributed_port=self._deepspeed_config.distributed_port,

                verbose=self._deepspeed_config.verbose,

                init_method=self._deepspeed_config.init_method,

            )



        def setup_distributed(self):

            &quot;&quot;&quot;Handles any underlying deepspeed setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)



        def wrap_distributed(

            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with Deepspeed



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;

                )

                self._print_device(f&quot;{self._name} -- Rank: {self.rank}&quot;)



            model, optimizer, _, _ = ds.initialize(

                model=model,

                optimizer=optimizer,

                model_parameters=filter(lambda p: p.requires_grad, model.parameters()),

                config_params=self._deepspeed_init_config,

            )

            return model, optimizer



        def _handle_deepspeed_configs(

            self,

            grad_accum_steps: int,

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]],

        ):

            &quot;&quot;&quot;Handles building the dictionary of configs that the deepspeed initialize call expects



            https://www.deepspeed.ai/docs/config-json/



            Parameters

            ----------

            grad_accum_steps: int

                number of gradient accumulation steps

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                gradient clipping config objects



            Returns

            -------

            dict

                All deepspeed parameters merged together from individual pieces



            &quot;&quot;&quot;

            # empty dict to start

            ds_config = {}

            # Map batch size stuff -- need to define 2/3

            ds_config.update(self._map_ds_batch_configs(grad_accum_steps=grad_accum_steps))

            # Skip optimizer &amp; skip scheduler

            # Map communication

            ds_config.update(self._map_ds_communication_configs())

            # Map FP16 and add enabled flag if selected

            ds_config.update(self._map_ds_fp16_configs())

            # Map grad clipping

            ds_config.update(self._map_ds_grad_clip_configs(grad_clip=grad_clip))

            # Map zero -- internally map param offloading and optimizer offloading to zero

            ds_config.update(self._map_ds_zero_configs())

            # Map aio

            ds_config.update(self._map_ds_aio_configs())

            # Map logging

            ds_config.update(self._map_ds_logging_configs())

            # Map flops -- enabled

            ds_config.update(self._map_ds_flops_configs())

            # Map activation checkpointing

            ds_config.update(self._map_ds_activation_checkpointing_configs())

            # Map tensorboard

            ds_config.update(self._map_ds_tensorboard_config())

            # Map PLD

            ds_config.update(self._map_ds_pld_config())

            return ds_config



        def _map_ds_pld_config(self):

            &quot;&quot;&quot;Maps progressive layer drop parameters



            https://www.deepspeed.ai/tutorials/progressive_layer_dropping/

            https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L293



            Returns

            -------

            dict

                pld parameters or enabled false dict



            &quot;&quot;&quot;

            if self._deepspeed_config.progressive_layer_drop is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.progressive_layer_drop, v.name)

                    for v in self._deepspeed_config.progressive_layer_drop.__attrs_attrs__

                }

                map_dict.update({&quot;enabled&quot;: True})

                return {&quot;progressive_layer_drop&quot;: map_dict}

            else:

                return {&quot;progressive_layer_drop&quot;: {&quot;enabled&quot;: False}}



        def _map_ds_tensorboard_config(self):

            &quot;&quot;&quot;Maps tensorboard related parameters



            https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/runtime/constants.py#L268



            Returns

            -------

            dict

                tensorboard parameters or enabled false dict



            &quot;&quot;&quot;

            if self._deepspeed_config.tensorboard is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.tensorboard, v.name)

                    for v in self._deepspeed_config.tensorboard.__attrs_attrs__

                }

                map_dict.update({&quot;enabled&quot;: True})

                return {&quot;tensorboard&quot;: map_dict}

            else:

                return {&quot;tensorboard&quot;: {&quot;enabled&quot;: False}}



        def _map_ds_grad_clip_configs(

            self, grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]]

        ):

            &quot;&quot;&quot;Maps grad clipping related parameters



            https://www.deepspeed.ai/docs/config-json/#gradient-clipping



            Parameters

            ----------

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                gradient clipping config objects



            Returns

            -------

            dict

                gradient clipping parameters or empty dict



            &quot;&quot;&quot;

            if grad_clip is not None:

                if isinstance(grad_clip, ClipGradNormConfig):

                    return {&quot;gradient_clipping&quot;: grad_clip.max_norm}

                else:

                    raise ValueError(

                        f&quot;Deepspeed does not currently support &quot;

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)}&#39;

                    )

            else:

                return {}



        def _map_ds_logging_configs(self):

            &quot;&quot;&quot;Maps logging related parameters



            https://www.deepspeed.ai/docs/config-json/#logging



            Returns

            -------

            dict

                logging parameters or empty dict



            &quot;&quot;&quot;

            return {

                &quot;steps_per_print&quot;: self._deepspeed_config.steps_per_print,

                &quot;dump_state&quot;: self._deepspeed_config.dump_state,

                &quot;wall_clock_breakdown&quot;: self._deepspeed_config.wall_clock_breakdown,

            }



        def _map_ds_activation_checkpointing_configs(self):

            &quot;&quot;&quot;Maps activation checkpointing related parameters



            https://www.deepspeed.ai/docs/config-json/#activation-checkpointing



            Returns

            -------

            dict

                activation checkpointing parameters or empty dict



            &quot;&quot;&quot;

            if self._deepspeed_config.activation_checkpointing is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.activation_checkpointing, v.name)

                    for v in self._deepspeed_config.activation_checkpointing.__attrs_attrs__

                }

                return {&quot;activation_checkpointing&quot;: map_dict}

            else:

                return {}



        def _map_ds_flops_configs(self):

            &quot;&quot;&quot;Maps flops related parameters



            https://www.deepspeed.ai/docs/config-json/#flops-profiler



            Returns

            -------

            dict

                flops parameters or enabled false dict



            &quot;&quot;&quot;

            if self._deepspeed_config.flops_profiler is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.flops_profiler, v.name)

                    for v in self._deepspeed_config.flops_profiler.__attrs_attrs__

                }

                map_dict.update({&quot;enabled&quot;: True})

                return {&quot;flops_profiler&quot;: map_dict}

            else:

                return {&quot;flops_profiler&quot;: {&quot;enabled&quot;: False}}



        def _map_ds_aio_configs(self):

            &quot;&quot;&quot;Maps async i/o related parameters



            https://www.deepspeed.ai/docs/config-json/#asynchronous-io



            Returns

            -------

            dict

                async i/o parameters or empty dict



            &quot;&quot;&quot;

            if self._deepspeed_config.aio is not None:

                map_dict = {

                    v.name: getattr(self._deepspeed_config.aio, v.name)

                    for v in self._deepspeed_config.aio.__attrs_attrs__

                }

                return {&quot;aio&quot;: map_dict}

            else:

                return {}



        def _map_ds_zero_configs(self):

            &quot;&quot;&quot;Maps ZeRO related parameters



            https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training



            Returns

            -------

            dict

                ZeRO related parameters



            &quot;&quot;&quot;

            map_dict = {}

            for v in self._deepspeed_config.zero_optimization.__attrs_attrs__:

                if v.name == &quot;offload_optimizer&quot;:

                    map_dict.update(self._map_ds_offload_optimizer_configs())

                elif v.name == &quot;offload_param&quot;:

                    map_dict.update(self._map_ds_offload_param_configs())

                # Just map the rest since the name:value is correct

                else:

                    map_dict.update(

                        {v.name: getattr(self._deepspeed_config.zero_optimization, v.name)}

                    )

            # Default overlap com to True for ZeRO stage 3

            map_dict[&quot;overlap_comm&quot;] = (

                True if map_dict[&quot;stage&quot;] == 3 else map_dict[&quot;overlap_comm&quot;]

            )

            return {&quot;zero_optimization&quot;: map_dict}



        def _map_ds_offload_param_configs(self):

            &quot;&quot;&quot;Maps ZeRO parameter offload parameters



            https://www.deepspeed.ai/docs/config-json/#parameter-offloading



            Returns

            -------

            dict

                ZeRO offload parameter parameters



            &quot;&quot;&quot;

            # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct

            if self._deepspeed_config.zero_optimization.offload_param is not None:

                map_dict = {

                    v.name: getattr(

                        self._deepspeed_config.zero_optimization.offload_param, v.name

                    )

                    for v in self._deepspeed_config.zero_optimization.offload_param.__attrs_attrs__

                }

                return {&quot;offload_param&quot;: map_dict}

            else:

                return {&quot;offload_param&quot;: None}



        def _map_ds_offload_optimizer_configs(self):

            &quot;&quot;&quot;Maps ZeRO optimizer offload parameters



            https://www.deepspeed.ai/docs/config-json/#optimizer-offloading



            Returns

            -------

            dict

                ZeRO offload optimizer parameters



            &quot;&quot;&quot;

            # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct

            if self._deepspeed_config.zero_optimization.offload_optimizer is not None:

                map_dict = {

                    v.name: getattr(

                        self._deepspeed_config.zero_optimization.offload_optimizer, v.name

                    )

                    for v in self._deepspeed_config.zero_optimization.offload_optimizer.__attrs_attrs__

                }

                # Set some post init values

                map_dict[&quot;pipeline&quot;] = (

                    map_dict[&quot;pipeline_read&quot;] or map_dict[&quot;pipeline_write&quot;]

                )

                return {&quot;offload_optimizer&quot;: map_dict}

            else:

                return {&quot;offload_optimizer&quot;: None}



        def _map_ds_fp16_configs(self):

            &quot;&quot;&quot;Maps FP16 related parameters



            https://www.deepspeed.ai/docs/config-json/#fp16-training-options



            Returns

            -------

            dict

                fp16 related parameters or enabled false dict



            &quot;&quot;&quot;

            if self._deepspeed_config.fp16 is not None:

                # Use a bit of introspection to pull out the attrs stuff systematically as the name mapping is correct

                map_dict = {

                    v.name: getattr(self._deepspeed_config.fp16, v.name)

                    for v in self._deepspeed_config.fp16.__attrs_attrs__

                }

                # Add the enabled flag

                map_dict.update({&quot;enabled&quot;: True})

                return {&quot;fp16&quot;: map_dict}

            else:

                return {&quot;fp16&quot;: {&quot;enabled&quot;: False}}



        def _map_ds_batch_configs(self, grad_accum_steps: int):

            &quot;&quot;&quot;Maps batch size related parameters



            https://www.deepspeed.ai/docs/config-json/#batch-size-related-parameters



            Parameters

            ----------

            grad_accum_steps: int

                number of gradient accumulation steps



            Returns

            -------

            dict

                batch size related parameters



            &quot;&quot;&quot;

            # Need to define 2/3

            return {

                &quot;train_micro_batch_size_per_gpu&quot;: self._batch_size_per_device,

                &quot;gradient_accumulation_steps&quot;: grad_accum_steps,

            }



        def _map_ds_communication_configs(self):

            &quot;&quot;&quot;Maps communication related parameters



            https://www.deepspeed.ai/docs/config-json/#communication-options



            Returns

            -------

            dict

                communication related parameters



            &quot;&quot;&quot;

            return {

                &quot;fp32_allreduce&quot;: self._deepspeed_config.fp32_allreduce,

                &quot;gradient_predivide_factor&quot;: self._deepspeed_config.gradient_predivide_factor,

                &quot;prescale_gradients:&quot;: self._deepspeed_config.prescale_gradients,

                &quot;sparse_gradients&quot;: self._deepspeed_config.sparse_gradients,

            }



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)



        def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None):

            &quot;&quot;&quot;Take a single loss and detach it from the compute graph and sync across devices if needed



            Parameters

            ----------

            loss: torch.Tensor

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            float

                detached, synced, and mean calculated across devices



            &quot;&quot;&quot;

            # map to the same device the loss is on pre detach if not set

            if device is None:

                device = loss.device

            detached_loss = loss.item()

            with torch.no_grad():

                loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype)

                # Loss tensor is worker specific so all_reduce (and SUM)

                torch.distributed.all_reduce(loss_tensor)

                # Detach and divide by the world size to get the mean on each device

                return loss_tensor.item() / self.world_size



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            torch.distributed.barrier()



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank&quot;&quot;&quot;

            return torch.distributed.get_rank()



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return torch.distributed.get_world_size()



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return torch.distributed.is_initialized()



        def clean(self):

            &quot;&quot;&quot;Cleans up at the end of a DDP run&quot;&quot;&quot;

            torch.distributed.destroy_process_group()
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_2">Ancestors (in MRO)</h4>
<ul>
<li>stoke.distributed.BaseDistributed</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_2">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">device_id</span>
</code></pre></div>
<p>Returns the current device id</p>
<div class="highlight"><pre><span></span><code><span class="n">initialized</span>
</code></pre></div>
<p>Returns if distributed backend is initialized correctly</p>
<div class="highlight"><pre><span></span><code><span class="n">rank</span>
</code></pre></div>
<p>Returns current distributed rank</p>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<p>Returns current world size</p>
<h4 id="methods_2">Methods</h4>
<h4 id="barrier_2">barrier</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Calls the underlying distributed barrier if available</p>
<p>??? example "View Source"
            def barrier(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            torch.distributed.barrier()
</code></pre></div>
<h4 id="clean_2">clean</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clean</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Cleans up at the end of a DDP run</p>
<p>??? example "View Source"
            def clean(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Cleans up at the end of a DDP run&quot;&quot;&quot;

            torch.distributed.destroy_process_group()
</code></pre></div>
<h4 id="detach_and_sync_loss_2">detach_and_sync_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detach_and_sync_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es) on the device</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>output device of the sync call</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Union[float, List[float], Tuple[float]]</td>
<td>loss(es) that has(have) been synced across multiple devices and detached from the graph</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def detach_and_sync_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)
</code></pre></div>
<h4 id="grad_accum_context_2">grad_accum_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">grad_accum_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for gradient accumulation</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def grad_accum_context(self, model: torch.nn.Module):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for gradient accumulation



            By default no context is used



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="print_device_2">print_device</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_device</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Public facing method to print on specific device ranks</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>Union[str, List[str]]</td>
<td>message(s) to print</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>Optional[Union[int, List[int]]], default: 0</td>
<td>device rank to print to (prevents printing on multiple devices in distributed mode)</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_device(</p>
<div class="highlight"><pre><span></span><code>            self,

            msg: Union[str, List[str]],

            rank: Optional[Union[int, List[int]]] = 0,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Public facing method to print on specific device ranks



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print

            rank: Optional[Union[int, List[int]]], default: 0

                device rank to print to (prevents printing on multiple devices in distributed mode)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Ignore the rank check if the current rank is a non-distributed version

            if self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;:

                unrolled_print(msg, single_line=single_line)

            # if it&#39;s a list then check the rank against the list

            elif isinstance(rank, list) and self.rank in rank:

                unrolled_print(msg, single_line=single_line)

            # If its an int then check the equality

            elif isinstance(rank, int) and rank == self.rank:

                unrolled_print(msg, single_line=single_line)

            # the else is essentially skip print

            else:

                pass
</code></pre></div>
<h4 id="setup_distributed_2">setup_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setup_distributed</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Handles any underlying deepspeed setup post init</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def setup_distributed(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Handles any underlying deepspeed setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)
</code></pre></div>
<h4 id="step_context_2">step_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for the step call</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for the step call



            By default no context is used



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="wrap_distributed_2">wrap_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_distributed</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Overrides base implementation for wrapping with Deepspeed</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Optional[Union[torch.optim.Optimizer, OSS]], default: None</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>Wrapped model object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_distributed(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with Deepspeed



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;

                )

                self._print_device(f&quot;{self._name} -- Rank: {self.rank}&quot;)



            model, optimizer, _, _ = ds.initialize(

                model=model,

                optimizer=optimizer,

                model_parameters=filter(lambda p: p.requires_grad, model.parameters()),

                config_params=self._deepspeed_init_config,

            )

            return model, optimizer
</code></pre></div>
<h3 id="distributedhorovod">DistributedHorovod</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DistributedHorovod</span><span class="p">(</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_3">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>device_id</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>initialized</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>world_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_batch_size_per_device</td>
<td>int</td>
<td>batch size per device or for non-distributed the overall batch size</td>
<td>None</td>
</tr>
<tr>
<td>_device_id</td>
<td>int, default: None</td>
<td>Current device id</td>
<td>None</td>
</tr>
<tr>
<td>_horovod_config</td>
<td>HorovodConfig</td>
<td>Configuration object for Horovod backend</td>
<td>None</td>
</tr>
<tr>
<td>_info_rank</td>
<td>Union[int, List[int]]</td>
<td>Which device(s) to print information</td>
<td>None</td>
</tr>
<tr>
<td>_name</td>
<td>str</td>
<td>name of current backend</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DistributedHorovod(BaseDistributed):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Class for using Horovod as the distributed backend



        This class handles common functionality for the horovod backend including setup, loss sync,

        gradient accumulation context, step context and various properties/attributes



        Attributes

        ----------

        device_id

        initialized

        rank

        world_size

        _batch_size_per_device: int

            batch size per device or for non-distributed the overall batch size

        _device_id: int, default: None

            Current device id

        _horovod_config: HorovodConfig

            Configuration object for Horovod backend

        _info_rank: Union[int, List[int]]

            Which device(s) to print information

        _name: str

            name of current backend

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here horovod_config might be passed in



            &quot;&quot;&quot;

            # Grab the config

            self._horovod_config = kwargs[&quot;horovod_config&quot;]

            # Initialize first so the local rank call cal be forwarded to super

            self._call_init()

            super(DistributedHorovod, self).__init__(

                device_id=hvd.local_rank(),

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;Horovod&quot;,

                verbose=verbose,

            )

            self._multi_loss = (

                len(kwargs[&quot;loss&quot;]) if isinstance(kwargs[&quot;loss&quot;], (list, tuple)) else 1

            )



        def _call_init(self):

            &quot;&quot;&quot;Does any backend initialization work related to horovod setup



            Returns

            -------

            None



            &quot;&quot;&quot;

            hvd.init()



        def _hvd_convert_to_sync_batch_norm(

            self, module: torch.nn.Module, process_group=None

        ):

            &quot;&quot;&quot;Replaces all BatchNorm*D layers with horovod.torch.SyncBatchNorm layers



            https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm

            https://nvidia.github.io/apex/_modules/apex/parallel.html#convert_syncbn_model



            Parameters

            ----------

            module: torch.nn.Module

                current model object

            process_group: default: None

                process group to scope synchronization, default is the whole world



            Returns

            -------

            module_output: torch.nn.Module

                modified version of model with all BatchNorm*D layers replaced with horovod.torch.SyncBatchNorm layers



            Notes

            -----

            Borrows heavily from the current torch convert_sync_batchnorm and apex convert_syncbn_model implementations

            only changing the underlying layer type to use the hvd implementation



            &quot;&quot;&quot;

            module_output = module

            if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):

                module_output = hvd.SyncBatchNorm(

                    num_features=module.num_features,

                    eps=module.eps,

                    momentum=module.momentum,

                    affine=module.affine,

                    track_running_stats=module.track_running_stats,

                )

                # Handle the copy of affine vars if affine

                if module.affine:

                    with torch.no_grad():

                        module_output.weight = module.weight

                        module_output.bias = module.bias

                # Handle the swap of running stats

                module_output.running_mean = module.running_mean

                module_output.running_var = module.running_var

            # Iterate recursively and replace

            for name, child in module.named_children():

                module_output.add_module(

                    name=name,

                    module=self._hvd_convert_to_sync_batch_norm(

                        module=child, process_group=process_group

                    ),

                )

            # delete and return

            del module

            return module_output



        def setup_distributed(self):

            &quot;&quot;&quot;Handles any underlying horovod setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)



        def wrap_distributed(

            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with Horovod



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;

                )

                self._print_device(f&quot;{self._name} -- Rank: {self.rank}&quot;)

            op_dict = {&quot;Average&quot;: hvd.Average, &quot;Sum&quot;: hvd.Sum, &quot;Adasum&quot;: hvd.Adasum}

            optimizer = hvd.DistributedOptimizer(

                optimizer=optimizer,

                named_parameters=model.named_parameters(),

                backward_passes_per_step=grad_accum * self._multi_loss

                if grad_accum is not None

                else self._multi_loss,

                compression=hvd.Compression.fp16

                if self._horovod_config.compression

                else hvd.Compression.none,

                gradient_predivide_factor=self._horovod_config.gradient_predivide_factor,

                op=op_dict.get(self._horovod_config.op),

            )

            # Broadcast the initial variable states from rank 0 to all other processes

            hvd.broadcast_parameters(model.state_dict(), root_rank=0)

            hvd.broadcast_optimizer_state(optimizer, root_rank=0)

            return model, optimizer



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)



        def _single_detach_and_sync_loss(self, loss: torch.Tensor, device=None):

            &quot;&quot;&quot;Take a single loss and detach it from the compute graph and sync across devices if needed



            Parameters

            ----------

            loss: torch.Tensor

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            float

                detached, synced, and mean calculated across devices



            &quot;&quot;&quot;

            # map to the same device the loss is on pre detach if not set

            if device is None:

                device = loss.device

            detached_loss = loss.item()

            with torch.no_grad():

                loss_tensor = torch.tensor(detached_loss, device=device, dtype=loss.dtype)

                # Make sure everyone is synced before the all-reduce

                # Horovod doesn&#39;t have a native barrier so lean on join to take care of it

                # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join

                hvd.join()

                # Loss tensor is worker specific so allreduce -- force SUM from Horovod

                sum_tensor = hvd.allreduce(loss_tensor, op=hvd.Sum)

                # Detach and divide by the world size to get the mean on each device

                return sum_tensor.item() / self.world_size



        def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):

            &quot;&quot;&quot;Return the context to wrap the step call



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation



            &quot;&quot;&quot;

            # Hidden here -- Horovod docs are terrible

            # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

            if self._verbose:

                self._print_device(

                    &quot;Horovod skipping synchronize as it was triggered pre grad-clip&quot;

                )

            return optimizer.skip_synchronize()



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            # Horovod doesn&#39;t have a native barrier so lean on join to take care of it

            # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join

            hvd.join()



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank&quot;&quot;&quot;

            return hvd.rank()



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return hvd.size()



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return hvd.is_initialized()
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_3">Ancestors (in MRO)</h4>
<ul>
<li>stoke.distributed.BaseDistributed</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_3">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">device_id</span>
</code></pre></div>
<p>Returns the current device id</p>
<div class="highlight"><pre><span></span><code><span class="n">initialized</span>
</code></pre></div>
<p>Returns if distributed backend is initialized correctly</p>
<div class="highlight"><pre><span></span><code><span class="n">rank</span>
</code></pre></div>
<p>Returns current distributed rank</p>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<p>Returns current world size</p>
<h4 id="methods_3">Methods</h4>
<h4 id="barrier_3">barrier</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Calls the underlying distributed barrier if available</p>
<p>??? example "View Source"
            def barrier(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            # Horovod doesn&#39;t have a native barrier so lean on join to take care of it

            # https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.join

            hvd.join()
</code></pre></div>
<h4 id="clean_3">clean</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clean</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Base clean call</p>
<p>Nothing to do here...</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clean(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Base clean call



            Nothing to do here...



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="detach_and_sync_loss_3">detach_and_sync_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detach_and_sync_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es) on the device</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>output device of the sync call</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Union[float, List[float], Tuple[float]]</td>
<td>loss(es) that has(have) been synced across multiple devices and detached from the graph</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def detach_and_sync_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been synced across multiple devices and detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(

                    self._single_detach_and_sync_loss(val, device) for val in loss

                )

            else:

                return self._single_detach_and_sync_loss(loss, device)
</code></pre></div>
<h4 id="grad_accum_context_3">grad_accum_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">grad_accum_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for gradient accumulation</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def grad_accum_context(self, model: torch.nn.Module):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for gradient accumulation



            By default no context is used



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="print_device_3">print_device</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_device</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Public facing method to print on specific device ranks</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>Union[str, List[str]]</td>
<td>message(s) to print</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>Optional[Union[int, List[int]]], default: 0</td>
<td>device rank to print to (prevents printing on multiple devices in distributed mode)</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_device(</p>
<div class="highlight"><pre><span></span><code>            self,

            msg: Union[str, List[str]],

            rank: Optional[Union[int, List[int]]] = 0,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Public facing method to print on specific device ranks



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print

            rank: Optional[Union[int, List[int]]], default: 0

                device rank to print to (prevents printing on multiple devices in distributed mode)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Ignore the rank check if the current rank is a non-distributed version

            if self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;:

                unrolled_print(msg, single_line=single_line)

            # if it&#39;s a list then check the rank against the list

            elif isinstance(rank, list) and self.rank in rank:

                unrolled_print(msg, single_line=single_line)

            # If its an int then check the equality

            elif isinstance(rank, int) and rank == self.rank:

                unrolled_print(msg, single_line=single_line)

            # the else is essentially skip print

            else:

                pass
</code></pre></div>
<h4 id="setup_distributed_3">setup_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setup_distributed</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Handles any underlying horovod setup post init</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def setup_distributed(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Handles any underlying horovod setup post init



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Set the device rank

            torch.cuda.set_device(self._device_id)
</code></pre></div>
<h4 id="step_context_3">step_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Return the context to wrap the step call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Return the context to wrap the step call



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            skip_synchronize() context to prevent un-needed communication overhead when using gradient accumulation



            &quot;&quot;&quot;

            # Hidden here -- Horovod docs are terrible

            # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

            if self._verbose:

                self._print_device(

                    &quot;Horovod skipping synchronize as it was triggered pre grad-clip&quot;

                )

            return optimizer.skip_synchronize()
</code></pre></div>
<h4 id="wrap_distributed_3">wrap_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_distributed</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Overrides base implementation for wrapping with Horovod</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Optional[Union[torch.optim.Optimizer, OSS]], default: None</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>Wrapped model object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_distributed(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Overrides base implementation for wrapping with Horovod



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Wrapped optimizer object



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

                self._print_device(

                    f&quot;{self._name} -- Device ID: {torch.cuda.current_device()}&quot;

                )

                self._print_device(f&quot;{self._name} -- Rank: {self.rank}&quot;)

            op_dict = {&quot;Average&quot;: hvd.Average, &quot;Sum&quot;: hvd.Sum, &quot;Adasum&quot;: hvd.Adasum}

            optimizer = hvd.DistributedOptimizer(

                optimizer=optimizer,

                named_parameters=model.named_parameters(),

                backward_passes_per_step=grad_accum * self._multi_loss

                if grad_accum is not None

                else self._multi_loss,

                compression=hvd.Compression.fp16

                if self._horovod_config.compression

                else hvd.Compression.none,

                gradient_predivide_factor=self._horovod_config.gradient_predivide_factor,

                op=op_dict.get(self._horovod_config.op),

            )

            # Broadcast the initial variable states from rank 0 to all other processes

            hvd.broadcast_parameters(model.state_dict(), root_rank=0)

            hvd.broadcast_optimizer_state(optimizer, root_rank=0)

            return model, optimizer
</code></pre></div>
<h3 id="distributednullcpu">DistributedNullCPU</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DistributedNullCPU</span><span class="p">(</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_4">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>device_id</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>initialized</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>world_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_batch_size_per_device</td>
<td>int</td>
<td>batch size per device or for non-distributed the overall batch size</td>
<td>None</td>
</tr>
<tr>
<td>_device_id</td>
<td>int, default: None</td>
<td>Current device id</td>
<td>None</td>
</tr>
<tr>
<td>_info_rank</td>
<td>Union[int, List[int]]</td>
<td>Which device(s) to print information</td>
<td>None</td>
</tr>
<tr>
<td>_name</td>
<td>str</td>
<td>name of current backend</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DistributedNullCPU(BaseDistributed):</p>
<div class="highlight"><pre><span></span><code>        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;Init for DistributedNullCPU



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            Notes

            -----

            Device ID set to None as it is not needed for non distributed CPU



            &quot;&quot;&quot;

            super(DistributedNullCPU, self).__init__(

                device_id=&quot;cpu&quot;,

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;PyTorch CPU&quot;,

                verbose=verbose,

            )



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank



            No rank so return string of cpu

            &quot;&quot;&quot;

            return &quot;cpu&quot;



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return 1



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return True
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_4">Ancestors (in MRO)</h4>
<ul>
<li>stoke.distributed.BaseDistributed</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_4">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">device_id</span>
</code></pre></div>
<p>Returns the current device id</p>
<div class="highlight"><pre><span></span><code><span class="n">initialized</span>
</code></pre></div>
<p>Returns if distributed backend is initialized correctly</p>
<div class="highlight"><pre><span></span><code><span class="n">rank</span>
</code></pre></div>
<p>Returns current distributed rank</p>
<p>No rank so return string of cpu</p>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<p>Returns current world size</p>
<h4 id="methods_4">Methods</h4>
<h4 id="barrier_4">barrier</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Calls the underlying distributed barrier if available</p>
<p>??? example "View Source"
            def barrier(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="clean_4">clean</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clean</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Base clean call</p>
<p>Nothing to do here...</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clean(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Base clean call



            Nothing to do here...



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="detach_and_sync_loss_4">detach_and_sync_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detach_and_sync_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es) on the device</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>output device of the sync call</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Union[float, List[float], Tuple[float]]</td>
<td>loss(es) that has(have) been detached from the graph</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def detach_and_sync_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(val.item() for val in loss)

            else:

                return loss.item()
</code></pre></div>
<h4 id="grad_accum_context_4">grad_accum_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">grad_accum_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for gradient accumulation</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def grad_accum_context(self, model: torch.nn.Module):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for gradient accumulation



            By default no context is used



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="print_device_4">print_device</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_device</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Public facing method to print on specific device ranks</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>Union[str, List[str]]</td>
<td>message(s) to print</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>Optional[Union[int, List[int]]], default: 0</td>
<td>device rank to print to (prevents printing on multiple devices in distributed mode)</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_device(</p>
<div class="highlight"><pre><span></span><code>            self,

            msg: Union[str, List[str]],

            rank: Optional[Union[int, List[int]]] = 0,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Public facing method to print on specific device ranks



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print

            rank: Optional[Union[int, List[int]]], default: 0

                device rank to print to (prevents printing on multiple devices in distributed mode)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Ignore the rank check if the current rank is a non-distributed version

            if self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;:

                unrolled_print(msg, single_line=single_line)

            # if it&#39;s a list then check the rank against the list

            elif isinstance(rank, list) and self.rank in rank:

                unrolled_print(msg, single_line=single_line)

            # If its an int then check the equality

            elif isinstance(rank, int) and rank == self.rank:

                unrolled_print(msg, single_line=single_line)

            # the else is essentially skip print

            else:

                pass
</code></pre></div>
<h4 id="setup_distributed_4">setup_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setup_distributed</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Base setup distributed</p>
<p>Does nothing as nothing needs to be wrapped</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def setup_distributed(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Base setup distributed



            Does nothing as nothing needs to be wrapped



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="step_context_4">step_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for the step call</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for the step call



            By default no context is used



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="wrap_distributed_4">wrap_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_distributed</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Base wrapper for distributed backends</p>
<p>Does nothing but print as nothing needs to be wrapped</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Optional[Union[torch.optim.Optimizer, OSS]], default: None</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>same as input model</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_distributed(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Base wrapper for distributed backends



            Does nothing but print as nothing needs to be wrapped



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                same as input model

            optimizer: Union[torch.optim.Optimizer, OSS]]

                same as input optimizer



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

            return model, optimizer
</code></pre></div>
<h3 id="distributednullgpu">DistributedNullGPU</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DistributedNullGPU</span><span class="p">(</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_5">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>device_id</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>initialized</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>world_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_batch_size_per_device</td>
<td>int</td>
<td>batch size per device or for non-distributed the overall batch size</td>
<td>None</td>
</tr>
<tr>
<td>_device_id</td>
<td>int, default: None</td>
<td>Current device id</td>
<td>None</td>
</tr>
<tr>
<td>_info_rank</td>
<td>Union[int, List[int]]</td>
<td>Which device(s) to print information</td>
<td>None</td>
</tr>
<tr>
<td>_name</td>
<td>str</td>
<td>name of current backend</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DistributedNullGPU(BaseDistributed):</p>
<div class="highlight"><pre><span></span><code>        def __init__(

            self,

            batch_size_per_device: int,

            info_rank: Union[int, List[int]],

            verbose: bool = True,

            **kwargs,

        ):

            &quot;&quot;&quot;Init for DistributedNullCPU



            Parameters

            ----------

            batch_size_per_device: int

                batch size per device or for non-distributed the overall batch size

            info_rank: Union[int, List[int]]

                Which device(s) to print information

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            Notes

            -----

            Device ID set to the current CUDA device as there is only a single GPU being used



            &quot;&quot;&quot;

            super(DistributedNullGPU, self).__init__(

                device_id=torch.cuda.current_device(),

                batch_size_per_device=batch_size_per_device,

                info_rank=info_rank,

                name=&quot;PyTorch GPU&quot;,

                verbose=verbose,

            )



        @property

        def rank(self):

            &quot;&quot;&quot;Returns current distributed rank



            No rank so return string of gpu

            &quot;&quot;&quot;

            return &quot;gpu&quot;



        @property

        def world_size(self):

            &quot;&quot;&quot;Returns current world size&quot;&quot;&quot;

            return 1



        @property

        def initialized(self):

            &quot;&quot;&quot;Returns if distributed backend is initialized correctly&quot;&quot;&quot;

            return True
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_5">Ancestors (in MRO)</h4>
<ul>
<li>stoke.distributed.BaseDistributed</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_5">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">device_id</span>
</code></pre></div>
<p>Returns the current device id</p>
<div class="highlight"><pre><span></span><code><span class="n">initialized</span>
</code></pre></div>
<p>Returns if distributed backend is initialized correctly</p>
<div class="highlight"><pre><span></span><code><span class="n">rank</span>
</code></pre></div>
<p>Returns current distributed rank</p>
<p>No rank so return string of gpu</p>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<p>Returns current world size</p>
<h4 id="methods_5">Methods</h4>
<h4 id="barrier_5">barrier</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Calls the underlying distributed barrier if available</p>
<p>??? example "View Source"
            def barrier(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="clean_5">clean</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clean</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Base clean call</p>
<p>Nothing to do here...</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clean(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Base clean call



            Nothing to do here...



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="detach_and_sync_loss_5">detach_and_sync_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detach_and_sync_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es) on the device</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>output device of the sync call</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Union[float, List[float], Tuple[float]]</td>
<td>loss(es) that has(have) been detached from the graph</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def detach_and_sync_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Takes loss(es) and detaches from the compute graph and syncs across devices if needed (via an all-reduce)



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            device: default: None

                output device of the sync call



            Returns

            -------

            Union[float, List[float], Tuple[float]]

                loss(es) that has(have) been detached from the graph



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                return type(loss)(val.item() for val in loss)

            else:

                return loss.item()
</code></pre></div>
<h4 id="grad_accum_context_5">grad_accum_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">grad_accum_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for gradient accumulation</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def grad_accum_context(self, model: torch.nn.Module):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for gradient accumulation



            By default no context is used



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="print_device_5">print_device</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_device</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Public facing method to print on specific device ranks</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>Union[str, List[str]]</td>
<td>message(s) to print</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>Optional[Union[int, List[int]]], default: 0</td>
<td>device rank to print to (prevents printing on multiple devices in distributed mode)</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_device(</p>
<div class="highlight"><pre><span></span><code>            self,

            msg: Union[str, List[str]],

            rank: Optional[Union[int, List[int]]] = 0,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Public facing method to print on specific device ranks



            Parameters

            ----------

            msg: Union[str, List[str]]

                message(s) to print

            rank: Optional[Union[int, List[int]]], default: 0

                device rank to print to (prevents printing on multiple devices in distributed mode)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Ignore the rank check if the current rank is a non-distributed version

            if self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;:

                unrolled_print(msg, single_line=single_line)

            # if it&#39;s a list then check the rank against the list

            elif isinstance(rank, list) and self.rank in rank:

                unrolled_print(msg, single_line=single_line)

            # If its an int then check the equality

            elif isinstance(rank, int) and rank == self.rank:

                unrolled_print(msg, single_line=single_line)

            # the else is essentially skip print

            else:

                pass
</code></pre></div>
<h4 id="setup_distributed_5">setup_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setup_distributed</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Base setup distributed</p>
<p>Does nothing as nothing needs to be wrapped</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def setup_distributed(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Base setup distributed



            Does nothing as nothing needs to be wrapped



            Returns

            -------

            None



            &quot;&quot;&quot;

            pass
</code></pre></div>
<h4 id="step_context_5">step_context</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_context</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Returns base context for the step call</p>
<p>By default no context is used</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nullcontext()</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_context(self, optimizer: Union[torch.optim.Optimizer, OSS]):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Returns base context for the step call



            By default no context is used



            Parameters

            ----------

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            nullcontext()



            &quot;&quot;&quot;

            return nullcontext()
</code></pre></div>
<h4 id="wrap_distributed_5">wrap_distributed</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_distributed</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Base wrapper for distributed backends</p>
<p>Does nothing but print as nothing needs to be wrapped</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Optional[Union[torch.optim.Optimizer, OSS]], default: None</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>same as input model</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_distributed(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            grad_accum: Optional[int],

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Base wrapper for distributed backends



            Does nothing but print as nothing needs to be wrapped



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]], default: None

                current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps



            Returns

            -------

            model: torch.nn.Module

                same as input model

            optimizer: Union[torch.optim.Optimizer, OSS]]

                same as input optimizer



            &quot;&quot;&quot;

            # Print info if verbose

            if self._verbose:

                self._print_info()

            return model, optimizer
</code></pre></div>
<h3 id="runnerdistenum">RunnerDistEnum</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">RunnerDistEnum</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class RunnerDistEnum(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Enum for building the runtime object with distributed functionality&quot;&quot;&quot;



        cpu = DistributedNullCPU

        gpu = DistributedNullGPU

        ddp = DistributedDDP

        horovod = DistributedHorovod

        deepspeed = DistributedDeepspeed
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_6">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">cpu</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">ddp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">gpu</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">horovod</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../data/" title="Data" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Data
              </span>
            </div>
          </a>
        
        
          <a href="../extensions/" title="Extensions" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Extensions
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ncilfone" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/ncilfone/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
    
  </body>
</html>