
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../../resources/images/stoke_small.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Stoke - stoke</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-stokestoke" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="stoke" class="md-header__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            stoke
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Stoke
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Home/" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Installation/" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Quick-Start/" class="md-tabs__link">
      Quick Start
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Launchers/" class="md-tabs__link">
      Launchers
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Examples/" class="md-tabs__link">
      Examples
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../CONTRIBUTING/" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../configs/" class="md-tabs__link md-tabs__link--active">
        Reference
      </a>
    </li>
  

  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="stoke" class="md-nav__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    stoke
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Home/" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Quick-Start/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Launchers/" class="md-nav__link">
        Launchers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Examples/" class="md-nav__link">
        Examples
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_1" type="checkbox" id="__nav_7_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_1">
          Stoke
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stoke" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          Stoke
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../configs/" class="md-nav__link">
        Configs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../distributed/" class="md-nav__link">
        Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../extensions/" class="md-nav__link">
        Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../fp16/" class="md-nav__link">
        Fp16
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../io/" class="md-nav__link">
        Io
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../status/" class="md-nav__link">
        Status
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Stoke
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Stoke
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stoke" class="md-nav__link">
    Stoke
  </a>
  
    <nav class="md-nav" aria-label="Stoke">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataloader" class="md-nav__link">
    DataLoader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward" class="md-nav__link">
    backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dump_model_parameter_info" class="md-nav__link">
    dump_model_parameter_info
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load" class="md-nav__link">
    load
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss" class="md-nav__link">
    loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model" class="md-nav__link">
    model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print" class="md-nav__link">
    print
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_ema_loss" class="md-nav__link">
    print_ema_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_mean_accumulated_synced_loss" class="md-nav__link">
    print_mean_accumulated_synced_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_num_model_parameters" class="md-nav__link">
    print_num_model_parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_on_devices" class="md-nav__link">
    print_on_devices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_synced_loss" class="md-nav__link">
    print_synced_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset" class="md-nav__link">
    reset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset_ema" class="md-nav__link">
    reset_ema
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset_tracking" class="md-nav__link">
    reset_tracking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step" class="md-nav__link">
    step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_grads" class="md-nav__link">
    zero_grads
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stoke" class="md-nav__link">
    Stoke
  </a>
  
    <nav class="md-nav" aria-label="Stoke">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dataloader" class="md-nav__link">
    DataLoader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward" class="md-nav__link">
    backward
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#barrier" class="md-nav__link">
    barrier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#detach_and_sync_loss" class="md-nav__link">
    detach_and_sync_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dump_model_parameter_info" class="md-nav__link">
    dump_model_parameter_info
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#load" class="md-nav__link">
    load
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss" class="md-nav__link">
    loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model" class="md-nav__link">
    model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print" class="md-nav__link">
    print
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_ema_loss" class="md-nav__link">
    print_ema_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_mean_accumulated_synced_loss" class="md-nav__link">
    print_mean_accumulated_synced_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_num_model_parameters" class="md-nav__link">
    print_num_model_parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_on_devices" class="md-nav__link">
    print_on_devices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#print_synced_loss" class="md-nav__link">
    print_synced_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset" class="md-nav__link">
    reset
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset_ema" class="md-nav__link">
    reset_ema
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reset_tracking" class="md-nav__link">
    reset_tracking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#save" class="md-nav__link">
    save
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step" class="md-nav__link">
    step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero_grads" class="md-nav__link">
    zero_grads
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/fidelity/stoke/edit/main/reference/stoke/stoke.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="module-stokestoke">Module stoke.stoke</h1>
<p>API interface to Stoke that handles any necessary config, context, setup etc.</p>
<p>None</p>
<p>??? example "View Source"
        # -<em>- coding: utf-8 -</em>-</p>
<div class="highlight"><pre><span></span><code>    # Copyright FMR LLC &lt;opensource@fidelity.com&gt;

    # SPDX-License-Identifier: Apache-2.0



    &quot;&quot;&quot;API interface to Stoke that handles any necessary config, context, setup etc.&quot;&quot;&quot;



    from contextlib import nullcontext

    from typing import Callable, Dict, List, Optional, Sequence, Tuple, Type, Union

    from uuid import uuid4



    import torch

    from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP

    from fairscale.nn.data_parallel import ShardedDataParallel as SDDP

    from torch.nn.parallel import DataParallel as DP

    from torch.nn.parallel import DistributedDataParallel as DDP

    from torch.utils.data import Dataset

    from torch.utils.data.distributed import Sampler



    from stoke.configs import (

        AMPConfig,

        ApexConfig,

        ClipGradConfig,

        ClipGradNormConfig,

        DDPConfig,

        DeepspeedConfig,

        FairscaleFSDPConfig,

        FairscaleOSSConfig,

        FairscaleSDDPConfig,

        HorovodConfig,

        StokeOptimizer,

    )

    from stoke.data import StokeDataLoader

    from stoke.distributed import RunnerDistEnum

    from stoke.extensions import RunnerOptimizerEnum

    from stoke.fp16 import RunnerFP16Enum

    from stoke.io import RunnerIOEnum

    from stoke.status import DistributedOptions, FP16Options, StokeStatus

    from stoke.utils import (

        ParamNormalize,

        T_co,

        _collate_fn_t,

        _worker_init_fn_t,

        zero_optimizer_grads,

    )





    class Stoke:

        &quot;&quot;&quot;High level stoke object that manages all necessary configs and provides a unified interface to ops



        This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model,

        loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the

        combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale),

        mixed-precision (AMP or APEX) and devices (CPU or GPU)



        Attributes

        ----------

        amp_config

        apex_config

        batch_size

        cuda

        ddp_config

        deepspeed_config

        distributed

        effective_batch_size

        ema_loss

        fp16

        fsdp_config

        fully_sharded

        gpu

        grad_accum

        grad_clip

        horovod_config

        is_amp

        is_apex

        is_ddp

        is_deepspeed

        is_horovod

        loss_access

        model_access

        nccl

        num_model_parameters

        optimizer

        oss

        oss_config

        rank

        scaler

        sddp_config

        sharded

        status

        world_size

        _agg_loss: Union[float, List[float], Tuple[float]]

            aggregated loss for grad accumulation (single or multiple losses)

        _backward_steps: int

            Number of times gradients have been calculated on a batch of samples (calls to backward)

        _grad_accum_counter: int

            counter for grad accumulation steps

        _loss: Union[Callable, List[Callable], Tuple[Callable]]

            callable function that calculates a loss from the model outputs

        _last_step_loss: list, tuple, or float

            last loss step calculation aggregated over device(s)

        _model: torch.nn.Module

            instance of torch.nn.Module for Stoke to handle

        _optimizer: StokeOptimizer

            StokeOptimizer config object that describes the torch.optim.Optimizer and it&#39;s kwargs

        _optimizer_steps: int

            Number of times step has been called on the optimizer

        _runner: StokeRunner

            the dynamically created runtime object that handles all ops

        _status: StokeStatus

            StokeStatus object that sets and maintains the current configuration

        _verbose: bool

            print verbosity

        _rolling_loss_steps: int

            number of steps that have been called for the rolling loss

        _rolling_mean_loss: list, tuple, or float

            current ema loss

        _ema_weight: float

            weight used for any ema calculation on metrics



        &quot;&quot;&quot;



        def __init__(

            self,

            model: torch.nn.Module,

            optimizer: StokeOptimizer,

            loss: Union[Callable, List[Callable], Tuple[Callable]],

            batch_size_per_device: int,

            grad_accum_steps: Optional[int] = 1,

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None,

            gpu: bool = False,

            fp16: Optional[FP16Options] = None,

            distributed: Optional[DistributedOptions] = None,

            fairscale_oss: bool = False,

            fairscale_sddp: bool = False,

            fairscale_fsdp: bool = False,

            configs: Optional[

                List[

                    Union[

                        AMPConfig,

                        ApexConfig,

                        DDPConfig,

                        DeepspeedConfig,

                        FairscaleOSSConfig,

                        FairscaleSDDPConfig,

                        FairscaleFSDPConfig,

                        HorovodConfig,

                    ]

                ]

            ] = None,

            info_rank: Optional[Union[int, List[int]]] = 0,

            verbose: bool = True,

            ema_weight: float = 0.1,

        ):

            &quot;&quot;&quot;Init for Stoke class object



            Parameters

            ----------

            model: torch.nn.Module

                PyTorch model

            optimizer: StokeOptimizer

                Optimizer configuration

            loss: Union[Callable, List[Callable], Tuple[Callable]]

                Callable loss function or functions

            batch_size_per_device: int

                Batch size at the single device level

            grad_accum_steps: Optional[int], default: 1

                Number of gradient accumulation steps

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                Gradient clipping configuration

            gpu: bool, default: False

                flag to use GPU device(s)

            fp16: Optional[FP16Options], default: None

                Choice of mixed-precision backend

            distributed: Optional[DistributedOptions], default: None

                Choice of distributed backend

            fairscale_oss: bool, default: False

                Flag to activate optimizer state sharding using Fairscale

            fairscale_sddp: bool, default: False

                Flag to activate sharded DDP using Fairscale

            fairscale_fsdp: bool, default: False

                Flag to activate fully sharded DDP using Fairscale

            configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None

                Configuration objects for runtimes

            info_rank: Optional[Union[int, List[int]]], default = 0

                Constrain prints to specific devices

            verbose: bool, default: True

                Flag for verbosity

            ema_weight: float, default: 0.5

                weight used for any ema calculation on metrics



            &quot;&quot;&quot;

            # Verbosity

            self._verbose = verbose

            # Info rank

            self._info_rank = info_rank

            # EMA

            self._ema_weight = ema_weight

            # Setup the StokeState

            self._status = StokeStatus(

                batch_size_per_device=batch_size_per_device,

                grad_accum=grad_accum_steps,

                grad_clip=grad_clip,

                gpu=gpu,

                fp16=fp16,

                distributed=distributed,

                fairscale_oss=fairscale_oss,

                fairscale_sddp=fairscale_sddp,

                fairscale_fsdp=fairscale_fsdp,

                configs=configs,

            )

            # Run some checks

            self._model = self._check_model(model)

            self._optimizer = self._check_optimizer(optimizer)

            self._loss = self._check_loss(loss)

            # Dynamically construct the StokeRunner from the StokeStatus

            self._runner, class_info = self._build_runner()

            # Setup distributed backend

            self._runner.setup_distributed()

            # Post here the runner will have the print_device function that is mapped to the self.print here

            # as it needs rank to be accessible before working

            if self._verbose:

                dev_id = (

                    self.rank

                    if (self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;)

                    else self._info_rank

                )

                self.print(f&quot;Printing verbose information on rank(s): {dev_id}&quot;)

                # Print the runner class info from the mixins

                self.print(class_info)

            # Possibly place model on GPU depending on StokeStatus -- before wrap calls

            self._place_model_on_gpu()

            # Handle the wrap ops in the correct order

            self._handle_ordered_wrap_ops(optimizer=optimizer)

            # Create some tracking vars

            self._grad_accum_counter = 0

            self._optimizer_steps = 0

            self._backward_steps = 0

            self._last_step_loss = self._set_loss_to_zero()

            self._agg_loss = self._set_loss_to_zero()

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0

            # Set post-init status variables

            self._status.set_post_init_values(world_size=self.world_size)

            # Print the final configuration

            if self._verbose:

                self.print(msg=self._status)



        def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping of optimizer then the model



            This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped

            methods are called



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Build the optimizer

            self._optimizer = self._runner.build_optimizer(

                optimizer=optimizer[&quot;optimizer&quot;],

                optimizer_kwargs=optimizer[&quot;optimizer_kwargs&quot;],

                model=self._model,

            )

            # Setup/Initialize FP16 backend -- in this case the optimizer is passed through

            self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer)

            # Wrap with distributed backend -- in this case the optimizer is passed through

            self._model, self._optimizer = self._runner.wrap_distributed(

                model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer

            )



        def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping of model then optimizer



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn&#39;t exist yet

            # don&#39;t use the return for the optimizer in this case

            self._model, _ = self._runner.wrap_distributed(

                model=self._model, grad_accum=self.grad_accum, optimizer=None

            )

            # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn&#39;t exist yet

            self._runner.wrap_fp16(model=self._model, optimizer=None)

            # Build the optimizer

            self._optimizer = self._runner.build_optimizer(

                optimizer=optimizer[&quot;optimizer&quot;],

                optimizer_kwargs=optimizer[&quot;optimizer_kwargs&quot;],

                model=self._model,

            )



        def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model

            # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed)

            if (self.sharded and self.oss) or self.is_apex or self.is_horovod:

                self._wrap_optimizer_then_model(optimizer=optimizer)

            else:

                self._wrap_model_then_optimizer(optimizer=optimizer)



        def _check_accum(self):

            &quot;&quot;&quot;Checks if the current step is the last accumulation step



            Returns

            -------

            bool



            &quot;&quot;&quot;

            return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0



        def _check_pre_accum(self):

            &quot;&quot;&quot;Checks if we are at the pre-accumulate step



            Returns

            -------

            bool



            &quot;&quot;&quot;

            return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum



        def _set_loss_to_zero(self):

            &quot;&quot;&quot;Used to set a loss tracker to zero depending on the type



            Returns

            -------

            float or list or tuple of reset loss



            &quot;&quot;&quot;

            return (

                type(self._loss)([0.0] * len(self._loss))

                if isinstance(self._loss, (list, tuple))

                else 0.0

            )



        def reset_ema(self):

            &quot;&quot;&quot;Used to reset the current state of the rolling mean loss



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0



        def print_ema_loss(

            self, prepend_msg: str = &quot;Current EMA Loss&quot;, single_line: bool = False

        ):

            &quot;&quot;&quot;Prints the current ema loss synced across all devices



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Current EMA Loss&quot;

                message prepend to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(self._rolling_mean_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val:.3f}&quot;

                    for idx, val in enumerate(self._rolling_mean_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(f&quot;{prepend_msg}: {self._rolling_mean_loss:.3f}&quot;)



        def print_mean_accumulated_synced_loss(

            self,

            prepend_msg: str = &quot;Mean Accumulated &amp; Synced Loss&quot;,

            pre_backwards: bool = True,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints the mean accumulated and device synced loss only after the grad accumulation step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Mean Accumulated &amp; Synced Loss&quot;

                message prepend to print

            pre_backwards: bool, default: True

                if being called pre backward step

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            check_fn = self._check_pre_accum if pre_backwards else self._check_accum

            if check_fn():

                if isinstance(self._agg_loss, (list, tuple)):

                    print_vals = self._scale_agg_loss()

                    self.print(print_vals, single_line=single_line)

                else:

                    self.print(f&quot;{prepend_msg}: {self._scale_agg_loss():.3f}&quot;)



        def _scale_agg_loss(self):

            &quot;&quot;&quot;Scales the mean aggregated loss by  grad accum



            Returns

            -------

            scale_vals: list or float of mean aggregated loss



            &quot;&quot;&quot;

            if isinstance(self._agg_loss, (list, tuple)):

                scale_vals = [

                    val / self.grad_accum for idx, val in enumerate(self._agg_loss)

                ]

            else:

                scale_vals = self._agg_loss / self.grad_accum

            return scale_vals



        def print_synced_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            prepend_msg: str = &quot;Step Synced Loss&quot;,

            device=None,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints a device synced loss at a single step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            prepend_msg: str, default: &quot;Step Synced Loss&quot;

                message prepend to print

            device: default: None

                specify the device to place the synced loss on (defaults to same device)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            printable_loss = self.detach_and_sync_loss(loss, device)

            if isinstance(printable_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val * self.grad_accum:.3f}&quot;

                    for idx, val in enumerate(printable_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(msg=f&quot;{prepend_msg}: {printable_loss * self.grad_accum:.3f}&quot;)



        def print_on_devices(

            self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0

        ):

            &quot;&quot;&quot;Wraps runner print interface for shorter semantics



            Parameters

            ----------

            msg: str

                message to print

            rank: Union[int, List[int]], default: 0

                which ranks to print on



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(msg=msg, rank=rank)



        def print(self, msg: Union[str, List[str]], single_line: bool = False):

            &quot;&quot;&quot;Wraps the runners print device and forces print on the _info_rank attribute(s)



            Parameters

            ----------

            msg: str

                message to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(

                msg=msg, rank=self._info_rank, single_line=single_line

            )



        @staticmethod

        def _check_model(model: torch.nn.Module):

            &quot;&quot;&quot;Verifies the type of the model



            Parameters

            ----------

            model: torch.nn.Module

                current torch model



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Check if the model is an nn.Module such that it has a forward method

            if not isinstance(model, torch.nn.Module):

                raise TypeError(

                    f&quot;Stoke -- Model is not of type torch.nn.Module, currently {type(model)}&quot;

                )

            return model



        @staticmethod

        def _check_optimizer(optimizer: StokeOptimizer):

            &quot;&quot;&quot;Verifies the type of the optimizer



            Parameters

            ----------

            optimizer: StokeOptimizer

                Current optimizer configuration TypedDict (aka dict)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if not isinstance(optimizer, dict):

                raise TypeError(

                    f&quot;Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}&quot;

                )

            return optimizer



        def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]):

            &quot;&quot;&quot;Checks to make sure the loss function(s) is/are callable



            Parameters

            ----------

            loss: Union[Callable, List[Callable], Tuple[Callable]]

                Current callable loss(es)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                loss = [self._check_loss(val) for val in loss]

                return loss

            elif isinstance(loss, Callable):

                return loss

            else:

                raise TypeError(

                    f&quot;Stoke -- Loss is not of type Callable, currently {type(loss)}&quot;

                )



        def _place_model_on_gpu(self):

            &quot;&quot;&quot;Automatically moves the model to GPU device(s)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.gpu and not self.is_deepspeed:

                if self._verbose:

                    self.print(f&quot;Automatically handling moving model to GPU(s)...&quot;)

                self._model.cuda()



        def _build_runner(self):

            &quot;&quot;&quot;Builds the runtime object from the mixin style classes



            Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called

            from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a

            a single interface. Might prevent some IDE type-hinting as it&#39;s dynamic



            Returns

            -------

            StokeRunner

                runtime runner object



            &quot;&quot;&quot;

            # Get the classes

            dist_class = self._get_distributed_mixin()

            fp16_class = self._get_fp16_mixin()

            optimizer_class = self._get_optimizer_mixin()

            io_class = self._get_io_mixin()



            # Python MRO hack to make sure the inits of all the Mixin classes get called

            def __multiple_mixin_init__(*args, **kwargs):

                dist_class.__init__(*args, **kwargs)

                fp16_class.__init__(*args, **kwargs)

                optimizer_class.__init__(*args, **kwargs)

                io_class.__init__(*args, **kwargs)



            # Configs pass through

            kwargs_dict = {

                &quot;amp_config&quot;: self.amp_config,

                &quot;apex_config&quot;: self.apex_config,

                &quot;ddp_config&quot;: self.ddp_config,

                &quot;deepspeed_config&quot;: self.deepspeed_config,

                &quot;horovod_config&quot;: self.horovod_config,

                &quot;oss_config&quot;: self.oss_config,

                &quot;sharded_config&quot;: self.sddp_config,

                &quot;fully_sharded_config&quot;: self.fsdp_config,

            }

            # Generate the runner class from the mixins based on the StokeStatus

            runner_class = type(

                &quot;StokeRunner&quot;,

                (dist_class, fp16_class, optimizer_class, io_class),

                {&quot;__init__&quot;: __multiple_mixin_init__},

            )(

                verbose=self._verbose,

                batch_size_per_device=self.batch_size,

                grad_accum_steps=self.grad_accum,

                grad_clip=self.grad_clip,

                info_rank=self._info_rank,

                loss=self._loss,

                **kwargs_dict,

            )

            # Make a list of class info for print later

            class_info = [

                f&quot;Distributed Mixin: {dist_class.__name__}&quot;,

                f&quot;Optimizer Mixin: {dist_class.__name__}&quot;,

                f&quot;FP16 Mixin: {fp16_class.__name__}&quot;,

                f&quot;IO Mixin: {io_class.__name__}&quot;,

            ]

            return runner_class, class_info



        def _get_io_mixin(self):

            &quot;&quot;&quot;Determines which IO class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated ioclass



            &quot;&quot;&quot;

            if self.is_deepspeed:

                return_class = RunnerIOEnum.deepspeed.value

            elif self.is_horovod:

                return_class = RunnerIOEnum.horovod.value

            elif self.is_ddp:

                return_class = RunnerIOEnum.ddp.value

            else:

                return_class = RunnerIOEnum.base.value

            return return_class



        def _get_optimizer_mixin(self):

            &quot;&quot;&quot;Determines which optimizer class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated optimizer class



            &quot;&quot;&quot;

            if self.oss:

                return_class = RunnerOptimizerEnum.oss.value

            else:

                return_class = RunnerOptimizerEnum.base.value

            return return_class



        def _get_distributed_mixin(self):

            &quot;&quot;&quot;Determines which distributed class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated distributed class



            &quot;&quot;&quot;

            # if not gpu then fall to cpu single

            if not self.gpu:

                return_class = RunnerDistEnum.cpu.value

            # if gpu but no distributed then fall to single gpu

            elif self.gpu and (self.distributed is None):

                return_class = RunnerDistEnum.gpu.value

            elif self.gpu and (self.distributed is not None):

                return_class = RunnerDistEnum[self.distributed].value

            else:

                raise ValueError(&quot;Stoke -- Cannot map to a valid distributed class&quot;)

            return return_class



        def _get_fp16_mixin(self):

            &quot;&quot;&quot;Determines which fp16 class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated fp16 class



            &quot;&quot;&quot;

            if self.fp16 is not None:

                return_class = RunnerFP16Enum[self.fp16].value

            else:

                return_class = RunnerFP16Enum.full.value

            return return_class



        def DataLoader(

            self,

            dataset: Dataset[T_co],

            shuffle: bool = False,

            sampler: Optional[Sampler[int]] = None,

            batch_sampler: Optional[Sampler[Sequence[int]]] = None,

            num_workers: int = 0,

            collate_fn: _collate_fn_t = None,

            pin_memory: bool = False,

            drop_last: bool = False,

            timeout: float = 0,

            worker_init_fn: _worker_init_fn_t = None,

            multiprocessing_context=None,

            generator=None,

            *,

            prefetch_factor: int = 2,

            persistent_workers: bool = False,

        ):

            &quot;&quot;&quot;Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs.



            Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called)

            and to automatically handle device placement since the gpu/fp16 flags can&#39;t be determined until the StokeStatus

            object is available which is post init. This could be disconnected from this class but it would require the

            user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and

            never handled



            Parameters

            ----------

            dataset: Dataset

                dataset from which to load the data.

            shuffle: bool, default: False

                set to ``True`` to have the data reshuffled at every epoch.

            sampler: Sampler or Iterable, default: None

                defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__``

                implemented. If specified, :attr:`shuffle` must not be specified.

            batch_sampler: Sampler or Iterable, default: None:

                like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with

                :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.

            num_workers: int, default: 0

                how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process.

            collate_fn: callable, optional:

                merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a

                map-style dataset.

            pin_memory: bool, default: False:

                If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your

                data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,

                see the example below.

            drop_last: bool, default: False

                set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size.

                If ``False`` and the size of dataset is not divisible by the batch size, then the last batch

                will be smaller.

            timeout: numeric, default: 0

                if positive, the timeout value for collecting a batch from workers. Should always be non-negative.

            worker_init_fn: callable, default: None

                If not ``None``, this will be called on each worker subprocess with the worker id

                (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading.

            prefetch_factor: int, default: 2

                Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers

                samples prefetched across all workers.

            persistent_workers: bool, default: False

                If ``True``, the data loader will not shutdown the worker processes after a dataset has been

                consumed once. This allows to maintain the workers `Dataset` instances alive.



            Returns

            -------

            StokeDataLoader

                wrapped torch.utils.data.DataLoader object



            &quot;&quot;&quot;

            # Check if forkserver is available for horovod and use

            if (

                num_workers &gt; 0

                and hasattr(torch.multiprocessing, &quot;_supports_context&quot;)

                and torch.multiprocessing._supports_context

                and &quot;forkserver&quot; in torch.multiprocessing.get_all_start_methods()

                and self.is_horovod

            ):

                multiprocessing_context = &quot;forkserver&quot;



            if self._verbose and self.gpu:

                print(f&quot;Automatically handling moving model input data to GPU(s)...&quot;)

            # Forward the already known options from the Stoke status

            return StokeDataLoader(

                gpu=self.gpu,

                fp16=self.fp16,

                batch_size=self.batch_size,

                dataset=dataset,

                shuffle=shuffle,

                sampler=sampler,

                batch_sampler=batch_sampler,

                num_workers=num_workers,

                collate_fn=collate_fn,

                pin_memory=pin_memory,

                drop_last=drop_last,

                timeout=timeout,

                worker_init_fn=worker_init_fn,

                multiprocessing_context=multiprocessing_context,

                generator=generator,

                prefetch_factor=prefetch_factor,

                persistent_workers=persistent_workers,

            )



        def model(self, *args, **kwargs):

            &quot;&quot;&quot;Wrapped model forward call



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the model forward call



            Returns

            -------

            model forward output



            &quot;&quot;&quot;

            with self._runner.model_context:

                return self._model(*args, **kwargs)

                # return self.model_access(*args, **kwargs)



        def loss(self, *args, **kwargs):

            &quot;&quot;&quot;Wrapped callable loss function call



            Handles internal logic of aggregating up the losses for single and multiple losses



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the loss function call(s)



            Returns

            -------

            outputs of callable loss function(s)



            &quot;&quot;&quot;

            # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch

            with self._runner.loss_context:

                if isinstance(self._loss, (list, tuple)):

                    loss = type(self._loss)(val(*args, **kwargs) for val in self._loss)

                    sync_loss = [self.detach_and_sync_loss(val) for val in loss]

                    self._last_step_loss = type(self._loss)(

                        val for idx, val in enumerate(sync_loss)

                    )

                    self._agg_loss = type(self._loss)(

                        self._agg_loss[idx] + val for idx, val in enumerate(sync_loss)

                    )

                    self._handle_ema_loss(loss=sync_loss)

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = type(loss)(val / self.grad_accum for val in loss)

                else:

                    loss = self._loss(*args, **kwargs)

                    sync_loss = self.detach_and_sync_loss(loss)

                    self._last_step_loss = sync_loss

                    self._agg_loss += sync_loss

                    self._handle_ema_loss(loss=sync_loss)

                    # Handle grad accumulation by dividing by the accumulation steps

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = loss / self.grad_accum

                return loss



        def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]):

            &quot;&quot;&quot;Handles calculating the ema loss



            Parameters

            ----------

            loss: Union[float, List[float], Tuple[float]]

                current calculated loss list, tuple or float



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._rolling_loss_steps += 1

            if isinstance(loss, (list, tuple)):

                self._rolling_mean_loss = type(self._rolling_mean_loss)(

                    self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx])

                    for idx, val in enumerate(loss)

                )

            else:

                self._rolling_mean_loss = self._ema_loss(

                    value=loss, current_mean=self._rolling_mean_loss

                )



        def _ema_loss(self, value: float, current_mean: float):

            &quot;&quot;&quot;Calculate the ema of the loss



            Parameters

            ----------

            value: float

                current loss value

            current_mean: float

                current mean value



            Returns

            -------

            current ema value: float



            &quot;&quot;&quot;

            if self._rolling_loss_steps == 1:

                return value

            else:

                return (self._ema_weight * value) + (

                    (1.0 - self._ema_weight) * current_mean

                )



        def backward(

            self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

        ):

            &quot;&quot;&quot;Wrapped backwards call



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                Callable loss function(s)



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Increment the grad counter

            self._grad_accum_counter += 1

            # Set the context based on the counter

            dist_cm = (

                nullcontext()

                if self._check_accum()

                else self._runner.grad_accum_context(self._model)

            )

            with dist_cm:

                self._runner.backward_call(

                    loss=loss, model=self.model_access, optimizer=self._optimizer

                )

            # Increment the number of total calls to backward (each backward to a loss is only considered 1)

            self._backward_steps += 1



        def step(self):

            &quot;&quot;&quot;Wrapped step call



            Handles grad clipping internally



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer only if the modulo is zero

            if self._check_accum():

                if self._verbose and self.grad_accum &gt; 0:

                    self.print(f&quot;Gradient Accumulation Steps: {self.grad_accum}&quot;)

                # Clip if needed

                if self.grad_clip is not None:

                    self._runner.clip_grad(

                        self.grad_clip,

                        self._model if self.fully_sharded else self.model_access,

                        self._optimizer,

                        oss=self.oss,

                        horovod=self.is_horovod,

                        deepspeed=self.is_deepspeed,

                        fsdp=self.fully_sharded,

                    )

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )

                # Reset for the accumulated step

                self._reset()

                # Increment the number of step calls to the optimizer

                self._optimizer_steps += 1

            # if deepspeed we need to step everytime as it handles the grad accumulation internally

            elif self.is_deepspeed:

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )



        def _reset(self):

            &quot;&quot;&quot;Resets the state post optimizer step call



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self.print(&quot;Resetting all grad/variables for next optimizer step&quot;)

            # Zero the grads if not deepspeed

            if not self.is_deepspeed:

                self.zero_grads()

            # Reset counter

            self._grad_accum_counter = 0

            # Reset agg loss -- single or mutiple losses

            self._agg_loss = self._set_loss_to_zero()



        def save(

            self,

            path: str,

            name: str = uuid4(),

            extension: str = &quot;pt&quot;,

            create_directory: bool = True,

            extras: Optional[dict] = None,

        ):

            &quot;&quot;&quot;Saves a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory to save the model checkpoint (prefer absolute paths over relative paths)

            name: str, default: uuid4()

                name used to save checkpoint file

            extension: str, default: &#39;.pt&#39;

                extension used to save PyTorch model checkpoint

            create_directory: bool, default: True

                flag to create the directory path if it doesn&#39;t exist

            extras: dict, default: None

                a dictionary of any extra things to save



            Returns

            -------

            path: str

                path to directory that the model checkpoint was saved

            tag: str

                full tag name the model checkpoint was saved as



            &quot;&quot;&quot;

            out_path, tag = self._runner.save(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                path=path,

                backward_step=self._backward_steps,

                grad_accum_step=self._grad_accum_counter,

                optimizer_step=self._optimizer_steps,

                name=name,

                scaler_dict=self.fp16_state_dict,

                extension=extension,

                create_directory=create_directory,

                extras=extras,

                status=self.status.status,

            )

            self.print(f&quot;Successfully saved model checkpoint to {out_path}/{tag}&quot;)

            return out_path, tag



        def load(self, path: str, tag: str, strict: bool = True):

            &quot;&quot;&quot;Loads a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory that the model checkpoint was saved (prefer absolute paths over relative paths)

            tag: str

                full tag name the model checkpoint was saved as

            strict: bool

                ignore non-matching keys



            Returns

            -------

            extras: dict, default: None

                a dictionary of any custom fields the user passed to the save function



            &quot;&quot;&quot;

            # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU?

            backward_step, grad_accum_step, optimizer_step, extras = self._runner.load(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                gpu=self.gpu,

                path=path,

                tag=tag,

                scaler_dict_fn=self._load_fp16_state_dict_fn(),

                strict=strict,

            )

            # Reset values based on what was in the load dict

            self._backward_steps = backward_step

            self._grad_accum_counter = grad_accum_step

            self._optimizer_steps = optimizer_step

            self.print(f&quot;Successfully loaded model checkpoint from {path}/{tag}&quot;)

            # Return the extras dict

            return extras



        def print_num_model_parameters(

            self, normalize: ParamNormalize = ParamNormalize.MILLION

        ):

            &quot;&quot;&quot;



            Parameters

            ----------

            normalize: ParamNormalize, default: ParamNormalize.MILLION

                ParamNormalize choice for pretty print normalizing



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(

                f&quot;Total Trainable Model Parameters: &quot;

                f&quot;{(self.num_model_parameters / normalize.value):.3f} {normalize.name}&quot;

            )



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Shorthand method to detach and sync loss



            Maps to the runner function of the same name



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es)

            device: default: None

                device to sync across



            Returns

            -------

            loss that is synced across devices and all_reduced w/ SUM



            &quot;&quot;&quot;

            return self._runner.detach_and_sync_loss(loss=loss, device=device)



        def zero_grads(self):

            &quot;&quot;&quot;Zeros the optimizer grads depending on the optimizer type



            Returns

            -------

            None



            &quot;&quot;&quot;

            zero_optimizer_grads(

                optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod

            )



        def reset(self):

            &quot;&quot;&quot;Public method for resetting the underlying stoke state



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._reset()



        def reset_tracking(self):

            &quot;&quot;&quot;Public method for resetting all underlying stoke tracked variables



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Create some tracking vars

            self._grad_accum_counter = 0

            self._optimizer_steps = 0

            self._backward_steps = 0

            self._last_step_loss = self._set_loss_to_zero()

            self._agg_loss = self._set_loss_to_zero()

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0



        def dump_model_parameter_info(self):

            &quot;&quot;&quot;Dumps all parameter information for named parameters (shape, device, dtype)



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(&quot;Dumping all model parameter information to stdout....&quot;)

            for name, param in self.model_access.named_parameters():

                if param.requires_grad:

                    self.print(

                        f&quot;Name: {name}, Shape: {param.shape}, &quot;

                        f&quot;Device: {param.device}, dtype: {param.dtype}&quot;

                    )



        def _load_fp16_state_dict_fn(self):

            &quot;&quot;&quot;Returns the function to load the sacler state dict



            Returns

            -------

            mp_state_dict_fn: Callable, default: None

                callable function to load the scaler state dict



            &quot;&quot;&quot;

            mp_state_dict_fn = None

            if self.scaler is not None:

                if self.is_apex:

                    try:

                        from apex import amp



                        mp_state_dict_fn = amp.load_state_dict

                    except ImportError as e:

                        print(

                            e,

                            &quot;: Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                        )

                else:

                    mp_state_dict_fn = self.scaler.load_state_dict

            return mp_state_dict_fn



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            self._runner.barrier()



        @property

        def step_loss(self):

            &quot;&quot;&quot;Gets the last step loss synced across device(s) (unscaled)&quot;&quot;&quot;

            return self._last_step_loss



        @property

        def model_access(self):

            &quot;&quot;&quot;Interface for model access due to the different types between the DP, DDP, and SDDP implementations&quot;&quot;&quot;

            if isinstance(self._model, (DDP, DP, SDDP, FSDP)):

                return self._model.module

            else:

                return self._model



        @property

        def loss_access(self):

            &quot;&quot;&quot;Gets loss tensor(s)&quot;&quot;&quot;

            return self._loss



        @property

        def optimizer(self):

            &quot;&quot;&quot;Gets the optimizer&quot;&quot;&quot;

            return self._optimizer



        @property

        def scaler(self):

            &quot;&quot;&quot;Gets the current scaler object&quot;&quot;&quot;

            return self._runner.scaler



        @property

        def fp16_state_dict(self):

            &quot;&quot;&quot;Gets the fp16 state dict from various methods&quot;&quot;&quot;

            mp_state_dict = None

            if self.scaler is not None:

                if self.is_apex:

                    try:

                        from apex import amp



                        mp_state_dict = amp.state_dict()

                    except ImportError as e:

                        print(

                            e,

                            &quot;: Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                        )

                elif self.is_amp:

                    mp_state_dict = self.scaler.state_dict()

            return mp_state_dict



        @property

        def status(self):

            &quot;&quot;&quot;Gets the StokeStatus object&quot;&quot;&quot;

            return self._status



        @property

        def batch_size(self):

            &quot;&quot;&quot;Shortcut to batch size&quot;&quot;&quot;

            return self._status.batch_size



        @property

        def effective_batch_size(self):

            &quot;&quot;&quot;Shortcut to effective batch size&quot;&quot;&quot;

            return self._status.effective_batch_size



        @property

        def grad_clip(self):

            &quot;&quot;&quot;Shortcut to get grad clip&quot;&quot;&quot;

            return self._status.grad_clip



        @property

        def grad_accum(self):

            &quot;&quot;&quot;Shortcut to get grad accumulation&quot;&quot;&quot;

            return self._status.grad_accum



        @property

        def gpu(self):

            &quot;&quot;&quot;Shortcut to get GPU status&quot;&quot;&quot;

            return self._status.gpu



        @property

        def cuda(self):

            &quot;&quot;&quot;Shortcut to get cuda status&quot;&quot;&quot;

            return self._status.cuda



        @property

        def nccl(self):

            &quot;&quot;&quot;Shortcut to get nccl status&quot;&quot;&quot;

            return self._status.nccl



        @property

        def fp16(self):

            &quot;&quot;&quot;Shortcut to get FP16 status&quot;&quot;&quot;

            return self._status.fp16



        @property

        def is_apex(self):

            &quot;&quot;&quot;Returns if APEX is activated&quot;&quot;&quot;

            return self._status.is_fp16_apex



        @property

        def is_amp(self):

            &quot;&quot;&quot;Returns if AMP is activated&quot;&quot;&quot;

            return self._status.is_fp16_amp



        @property

        def distributed(self):

            &quot;&quot;&quot;Shortcut to distributed status&quot;&quot;&quot;

            return self._status.distributed



        @property

        def is_ddp(self):

            &quot;&quot;&quot;Returns if DDP is activated&quot;&quot;&quot;

            return self._status.is_distributed_ddp



        @property

        def is_horovod(self):

            &quot;&quot;&quot;Returns if Horovod is activated&quot;&quot;&quot;

            return self._status.is_distributed_horovod



        @property

        def is_deepspeed(self):

            &quot;&quot;&quot;Returns if Deepspeed is acticated&quot;&quot;&quot;

            return self._status.is_distributed_deepspeed



        @property

        def oss(self):

            &quot;&quot;&quot;Returns if Fairscale optimizer state sharding status&quot;&quot;&quot;

            return self._status.oss



        @property

        def sharded(self):

            &quot;&quot;&quot;Returns if Fairscale sharded DDP status&quot;&quot;&quot;

            return self._status.sharded



        @property

        def fully_sharded(self):

            &quot;&quot;&quot;Returns if Fairscale fully sharded DDP status&quot;&quot;&quot;

            return self._status.fully_sharded



        @property

        def world_size(self):

            &quot;&quot;&quot;Shortcut to get world size&quot;&quot;&quot;

            return self._runner.world_size



        @property

        def rank(self):

            &quot;&quot;&quot;Shortcut to get rank&quot;&quot;&quot;

            return self._runner.rank



        @property

        def amp_config(self):

            &quot;&quot;&quot;Returns amp config or None based on amp state&quot;&quot;&quot;

            return self._status.amp_config if self.is_amp else None



        @property

        def apex_config(self):

            &quot;&quot;&quot;Returns apex config or None based on apex state&quot;&quot;&quot;

            return self._status.apex_config if self.is_apex else None



        @property

        def ddp_config(self):

            &quot;&quot;&quot;Returns ddp config or None based on ddp state&quot;&quot;&quot;

            return self._status.ddp_config if self.is_ddp else None



        @property

        def deepspeed_config(self):

            &quot;&quot;&quot;Returns deepspeed config or None based on deepspeed state&quot;&quot;&quot;

            return self._status.deepspeed_config if self.is_deepspeed else None



        @property

        def oss_config(self):

            &quot;&quot;&quot;Returns oss config or None based on ossstate&quot;&quot;&quot;

            return self._status.oss_config if self.oss else None



        @property

        def sddp_config(self):

            &quot;&quot;&quot;Returns sddp config or None based on sddp state&quot;&quot;&quot;

            return self._status.sddp_config if self.sharded else None



        @property

        def fsdp_config(self):

            &quot;&quot;&quot;Returns fsdp config or None based on fsdp state&quot;&quot;&quot;

            return self._status.fsdp_config if self.fully_sharded else None



        @property

        def horovod_config(self):

            &quot;&quot;&quot;Returns horovod config or None based on horovod state&quot;&quot;&quot;

            return self._status.horovod_config if self.is_horovod else None



        @property

        def num_model_parameters(self):

            &quot;&quot;&quot;Returns number of parameters that require gradients&quot;&quot;&quot;

            return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad)



        @property

        def ema_loss(self):

            &quot;&quot;&quot;Returns the current rolling mean loss&quot;&quot;&quot;

            return self._rolling_mean_loss
</code></pre></div>
<h2 id="classes">Classes</h2>
<h3 id="stoke">Stoke</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">Stoke</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">StokeOptimizer</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Callable</span><span class="p">]],</span>
    <span class="n">batch_size_per_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">grad_accum_steps</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fp16</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">status</span><span class="o">.</span><span class="n">FP16Options</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">distributed</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">status</span><span class="o">.</span><span class="n">DistributedOptions</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fairscale_oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fairscale_sddp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fairscale_fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">configs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">AMPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ApexConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleOSSConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleSDDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleFSDPConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">HorovodConfig</span><span class="p">]],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">info_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">ema_weight</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>amp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>apex_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>batch_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>cuda</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>ddp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>distributed</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>effective_batch_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>ema_loss</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fp16</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fsdp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>fully_sharded</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>gpu</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>grad_clip</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>horovod_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_amp</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_apex</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_ddp</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_deepspeed</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>is_horovod</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>loss_access</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_access</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>nccl</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>num_model_parameters</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>oss_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>sddp_config</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>sharded</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>status</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>world_size</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_agg_loss</td>
<td>Union[float, List[float], Tuple[float]]</td>
<td>aggregated loss for grad accumulation (single or multiple losses)</td>
<td>None</td>
</tr>
<tr>
<td>_backward_steps</td>
<td>int</td>
<td>Number of times gradients have been calculated on a batch of samples (calls to backward)</td>
<td>None</td>
</tr>
<tr>
<td>_grad_accum_counter</td>
<td>int</td>
<td>counter for grad accumulation steps</td>
<td>None</td>
</tr>
<tr>
<td>_loss</td>
<td>Union[Callable, List[Callable], Tuple[Callable]]</td>
<td>callable function that calculates a loss from the model outputs</td>
<td>None</td>
</tr>
<tr>
<td>_last_step_loss</td>
<td>list, tuple, or float</td>
<td>last loss step calculation aggregated over device(s)</td>
<td>None</td>
</tr>
<tr>
<td>_model</td>
<td>torch.nn.Module</td>
<td>instance of torch.nn.Module for Stoke to handle</td>
<td>None</td>
</tr>
<tr>
<td>_optimizer</td>
<td>StokeOptimizer</td>
<td>StokeOptimizer config object that describes the torch.optim.Optimizer and it's kwargs</td>
<td>None</td>
</tr>
<tr>
<td>_optimizer_steps</td>
<td>int</td>
<td>Number of times step has been called on the optimizer</td>
<td>None</td>
</tr>
<tr>
<td>_runner</td>
<td>StokeRunner</td>
<td>the dynamically created runtime object that handles all ops</td>
<td>None</td>
</tr>
<tr>
<td>_status</td>
<td>StokeStatus</td>
<td>StokeStatus object that sets and maintains the current configuration</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool</td>
<td>print verbosity</td>
<td>None</td>
</tr>
<tr>
<td>_rolling_loss_steps</td>
<td>int</td>
<td>number of steps that have been called for the rolling loss</td>
<td>None</td>
</tr>
<tr>
<td>_rolling_mean_loss</td>
<td>list, tuple, or float</td>
<td>current ema loss</td>
<td>None</td>
</tr>
<tr>
<td>_ema_weight</td>
<td>float</td>
<td>weight used for any ema calculation on metrics</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class Stoke:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;High level stoke object that manages all necessary configs and provides a unified interface to ops



        This is the main class within Stoke. Functionally it manages all interfaces to the necessary wrapped ops (model,

        loss, backward, step), provides helper functions, and dynamically constructs the runtime that handles the

        combinatorics problem of underlying frameworks (DDP, Horovod, Deepspeed, Fairscale),

        mixed-precision (AMP or APEX) and devices (CPU or GPU)



        Attributes

        ----------

        amp_config

        apex_config

        batch_size

        cuda

        ddp_config

        deepspeed_config

        distributed

        effective_batch_size

        ema_loss

        fp16

        fsdp_config

        fully_sharded

        gpu

        grad_accum

        grad_clip

        horovod_config

        is_amp

        is_apex

        is_ddp

        is_deepspeed

        is_horovod

        loss_access

        model_access

        nccl

        num_model_parameters

        optimizer

        oss

        oss_config

        rank

        scaler

        sddp_config

        sharded

        status

        world_size

        _agg_loss: Union[float, List[float], Tuple[float]]

            aggregated loss for grad accumulation (single or multiple losses)

        _backward_steps: int

            Number of times gradients have been calculated on a batch of samples (calls to backward)

        _grad_accum_counter: int

            counter for grad accumulation steps

        _loss: Union[Callable, List[Callable], Tuple[Callable]]

            callable function that calculates a loss from the model outputs

        _last_step_loss: list, tuple, or float

            last loss step calculation aggregated over device(s)

        _model: torch.nn.Module

            instance of torch.nn.Module for Stoke to handle

        _optimizer: StokeOptimizer

            StokeOptimizer config object that describes the torch.optim.Optimizer and it&#39;s kwargs

        _optimizer_steps: int

            Number of times step has been called on the optimizer

        _runner: StokeRunner

            the dynamically created runtime object that handles all ops

        _status: StokeStatus

            StokeStatus object that sets and maintains the current configuration

        _verbose: bool

            print verbosity

        _rolling_loss_steps: int

            number of steps that have been called for the rolling loss

        _rolling_mean_loss: list, tuple, or float

            current ema loss

        _ema_weight: float

            weight used for any ema calculation on metrics



        &quot;&quot;&quot;



        def __init__(

            self,

            model: torch.nn.Module,

            optimizer: StokeOptimizer,

            loss: Union[Callable, List[Callable], Tuple[Callable]],

            batch_size_per_device: int,

            grad_accum_steps: Optional[int] = 1,

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]] = None,

            gpu: bool = False,

            fp16: Optional[FP16Options] = None,

            distributed: Optional[DistributedOptions] = None,

            fairscale_oss: bool = False,

            fairscale_sddp: bool = False,

            fairscale_fsdp: bool = False,

            configs: Optional[

                List[

                    Union[

                        AMPConfig,

                        ApexConfig,

                        DDPConfig,

                        DeepspeedConfig,

                        FairscaleOSSConfig,

                        FairscaleSDDPConfig,

                        FairscaleFSDPConfig,

                        HorovodConfig,

                    ]

                ]

            ] = None,

            info_rank: Optional[Union[int, List[int]]] = 0,

            verbose: bool = True,

            ema_weight: float = 0.1,

        ):

            &quot;&quot;&quot;Init for Stoke class object



            Parameters

            ----------

            model: torch.nn.Module

                PyTorch model

            optimizer: StokeOptimizer

                Optimizer configuration

            loss: Union[Callable, List[Callable], Tuple[Callable]]

                Callable loss function or functions

            batch_size_per_device: int

                Batch size at the single device level

            grad_accum_steps: Optional[int], default: 1

                Number of gradient accumulation steps

            grad_clip: Optional[Union[ClipGradConfig, ClipGradNormConfig]], default: None

                Gradient clipping configuration

            gpu: bool, default: False

                flag to use GPU device(s)

            fp16: Optional[FP16Options], default: None

                Choice of mixed-precision backend

            distributed: Optional[DistributedOptions], default: None

                Choice of distributed backend

            fairscale_oss: bool, default: False

                Flag to activate optimizer state sharding using Fairscale

            fairscale_sddp: bool, default: False

                Flag to activate sharded DDP using Fairscale

            fairscale_fsdp: bool, default: False

                Flag to activate fully sharded DDP using Fairscale

            configs: Optional[List[Union[AMPConfig, ApexConfig, DDPConfig, DeepspeedConfig, FairscaleOSSConfig, FairscaleSDDPConfig, FairscaleFSDPConfig, HorovodConfig]], default: None

                Configuration objects for runtimes

            info_rank: Optional[Union[int, List[int]]], default = 0

                Constrain prints to specific devices

            verbose: bool, default: True

                Flag for verbosity

            ema_weight: float, default: 0.5

                weight used for any ema calculation on metrics



            &quot;&quot;&quot;

            # Verbosity

            self._verbose = verbose

            # Info rank

            self._info_rank = info_rank

            # EMA

            self._ema_weight = ema_weight

            # Setup the StokeState

            self._status = StokeStatus(

                batch_size_per_device=batch_size_per_device,

                grad_accum=grad_accum_steps,

                grad_clip=grad_clip,

                gpu=gpu,

                fp16=fp16,

                distributed=distributed,

                fairscale_oss=fairscale_oss,

                fairscale_sddp=fairscale_sddp,

                fairscale_fsdp=fairscale_fsdp,

                configs=configs,

            )

            # Run some checks

            self._model = self._check_model(model)

            self._optimizer = self._check_optimizer(optimizer)

            self._loss = self._check_loss(loss)

            # Dynamically construct the StokeRunner from the StokeStatus

            self._runner, class_info = self._build_runner()

            # Setup distributed backend

            self._runner.setup_distributed()

            # Post here the runner will have the print_device function that is mapped to the self.print here

            # as it needs rank to be accessible before working

            if self._verbose:

                dev_id = (

                    self.rank

                    if (self.rank == &quot;cpu&quot; or self.rank == &quot;gpu&quot;)

                    else self._info_rank

                )

                self.print(f&quot;Printing verbose information on rank(s): {dev_id}&quot;)

                # Print the runner class info from the mixins

                self.print(class_info)

            # Possibly place model on GPU depending on StokeStatus -- before wrap calls

            self._place_model_on_gpu()

            # Handle the wrap ops in the correct order

            self._handle_ordered_wrap_ops(optimizer=optimizer)

            # Create some tracking vars

            self._grad_accum_counter = 0

            self._optimizer_steps = 0

            self._backward_steps = 0

            self._last_step_loss = self._set_loss_to_zero()

            self._agg_loss = self._set_loss_to_zero()

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0

            # Set post-init status variables

            self._status.set_post_init_values(world_size=self.world_size)

            # Print the final configuration

            if self._verbose:

                self.print(msg=self._status)



        def _wrap_optimizer_then_model(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping of optimizer then the model



            This holds only for SDDP, Horovod, and APEX as these need to use an instantiated optimizer before wrapped

            methods are called



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Build the optimizer

            self._optimizer = self._runner.build_optimizer(

                optimizer=optimizer[&quot;optimizer&quot;],

                optimizer_kwargs=optimizer[&quot;optimizer_kwargs&quot;],

                model=self._model,

            )

            # Setup/Initialize FP16 backend -- in this case the optimizer is passed through

            self._runner.wrap_fp16(model=self._model, optimizer=self._optimizer)

            # Wrap with distributed backend -- in this case the optimizer is passed through

            self._model, self._optimizer = self._runner.wrap_distributed(

                model=self._model, grad_accum=self.grad_accum, optimizer=self._optimizer

            )



        def _wrap_model_then_optimizer(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping of model then optimizer



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Wrap with distributed backend -- in this case the optimizer is passed as None since it doesn&#39;t exist yet

            # don&#39;t use the return for the optimizer in this case

            self._model, _ = self._runner.wrap_distributed(

                model=self._model, grad_accum=self.grad_accum, optimizer=None

            )

            # Setup/Initialize FP16 backend -- in this case the optimizer is passed as None since it doesn&#39;t exist yet

            self._runner.wrap_fp16(model=self._model, optimizer=None)

            # Build the optimizer

            self._optimizer = self._runner.build_optimizer(

                optimizer=optimizer[&quot;optimizer&quot;],

                optimizer_kwargs=optimizer[&quot;optimizer_kwargs&quot;],

                model=self._model,

            )



        def _handle_ordered_wrap_ops(self, optimizer: StokeOptimizer):

            &quot;&quot;&quot;Handles wrapping model, using FP16, and wrapping optimizer in the correct order depending on Stoke Status



            Parameters

            ----------

            optimizer: StokeOptimizer

                Optimizer configuration



            Returns

            -------

            None



            &quot;&quot;&quot;

            # if SDDP + OSS, Horovod, and APEX then we need to make sure that the optimizer gets wrapped before the model

            # gets wrapped, all other models follow standard DDP paradigm (or their own DeepSpeed)

            if (self.sharded and self.oss) or self.is_apex or self.is_horovod:

                self._wrap_optimizer_then_model(optimizer=optimizer)

            else:

                self._wrap_model_then_optimizer(optimizer=optimizer)



        def _check_accum(self):

            &quot;&quot;&quot;Checks if the current step is the last accumulation step



            Returns

            -------

            bool



            &quot;&quot;&quot;

            return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == 0



        def _check_pre_accum(self):

            &quot;&quot;&quot;Checks if we are at the pre-accumulate step



            Returns

            -------

            bool



            &quot;&quot;&quot;

            return (self._grad_accum_counter + 1) % (self.grad_accum + 1) == self.grad_accum



        def _set_loss_to_zero(self):

            &quot;&quot;&quot;Used to set a loss tracker to zero depending on the type



            Returns

            -------

            float or list or tuple of reset loss



            &quot;&quot;&quot;

            return (

                type(self._loss)([0.0] * len(self._loss))

                if isinstance(self._loss, (list, tuple))

                else 0.0

            )



        def reset_ema(self):

            &quot;&quot;&quot;Used to reset the current state of the rolling mean loss



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0



        def print_ema_loss(

            self, prepend_msg: str = &quot;Current EMA Loss&quot;, single_line: bool = False

        ):

            &quot;&quot;&quot;Prints the current ema loss synced across all devices



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Current EMA Loss&quot;

                message prepend to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(self._rolling_mean_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val:.3f}&quot;

                    for idx, val in enumerate(self._rolling_mean_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(f&quot;{prepend_msg}: {self._rolling_mean_loss:.3f}&quot;)



        def print_mean_accumulated_synced_loss(

            self,

            prepend_msg: str = &quot;Mean Accumulated &amp; Synced Loss&quot;,

            pre_backwards: bool = True,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints the mean accumulated and device synced loss only after the grad accumulation step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Mean Accumulated &amp; Synced Loss&quot;

                message prepend to print

            pre_backwards: bool, default: True

                if being called pre backward step

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            check_fn = self._check_pre_accum if pre_backwards else self._check_accum

            if check_fn():

                if isinstance(self._agg_loss, (list, tuple)):

                    print_vals = self._scale_agg_loss()

                    self.print(print_vals, single_line=single_line)

                else:

                    self.print(f&quot;{prepend_msg}: {self._scale_agg_loss():.3f}&quot;)



        def _scale_agg_loss(self):

            &quot;&quot;&quot;Scales the mean aggregated loss by  grad accum



            Returns

            -------

            scale_vals: list or float of mean aggregated loss



            &quot;&quot;&quot;

            if isinstance(self._agg_loss, (list, tuple)):

                scale_vals = [

                    val / self.grad_accum for idx, val in enumerate(self._agg_loss)

                ]

            else:

                scale_vals = self._agg_loss / self.grad_accum

            return scale_vals



        def print_synced_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            prepend_msg: str = &quot;Step Synced Loss&quot;,

            device=None,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints a device synced loss at a single step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            prepend_msg: str, default: &quot;Step Synced Loss&quot;

                message prepend to print

            device: default: None

                specify the device to place the synced loss on (defaults to same device)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            printable_loss = self.detach_and_sync_loss(loss, device)

            if isinstance(printable_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val * self.grad_accum:.3f}&quot;

                    for idx, val in enumerate(printable_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(msg=f&quot;{prepend_msg}: {printable_loss * self.grad_accum:.3f}&quot;)



        def print_on_devices(

            self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0

        ):

            &quot;&quot;&quot;Wraps runner print interface for shorter semantics



            Parameters

            ----------

            msg: str

                message to print

            rank: Union[int, List[int]], default: 0

                which ranks to print on



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(msg=msg, rank=rank)



        def print(self, msg: Union[str, List[str]], single_line: bool = False):

            &quot;&quot;&quot;Wraps the runners print device and forces print on the _info_rank attribute(s)



            Parameters

            ----------

            msg: str

                message to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(

                msg=msg, rank=self._info_rank, single_line=single_line

            )



        @staticmethod

        def _check_model(model: torch.nn.Module):

            &quot;&quot;&quot;Verifies the type of the model



            Parameters

            ----------

            model: torch.nn.Module

                current torch model



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Check if the model is an nn.Module such that it has a forward method

            if not isinstance(model, torch.nn.Module):

                raise TypeError(

                    f&quot;Stoke -- Model is not of type torch.nn.Module, currently {type(model)}&quot;

                )

            return model



        @staticmethod

        def _check_optimizer(optimizer: StokeOptimizer):

            &quot;&quot;&quot;Verifies the type of the optimizer



            Parameters

            ----------

            optimizer: StokeOptimizer

                Current optimizer configuration TypedDict (aka dict)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if not isinstance(optimizer, dict):

                raise TypeError(

                    f&quot;Stoke -- Optimizer is not of type torch.optim.Optimizer, currently {type(optimizer)}&quot;

                )

            return optimizer



        def _check_loss(self, loss: Union[Callable, List[Callable], Tuple[Callable]]):

            &quot;&quot;&quot;Checks to make sure the loss function(s) is/are callable



            Parameters

            ----------

            loss: Union[Callable, List[Callable], Tuple[Callable]]

                Current callable loss(es)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                loss = [self._check_loss(val) for val in loss]

                return loss

            elif isinstance(loss, Callable):

                return loss

            else:

                raise TypeError(

                    f&quot;Stoke -- Loss is not of type Callable, currently {type(loss)}&quot;

                )



        def _place_model_on_gpu(self):

            &quot;&quot;&quot;Automatically moves the model to GPU device(s)



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.gpu and not self.is_deepspeed:

                if self._verbose:

                    self.print(f&quot;Automatically handling moving model to GPU(s)...&quot;)

                self._model.cuda()



        def _build_runner(self):

            &quot;&quot;&quot;Builds the runtime object from the mixin style classes



            Mixes the distributed class, fp16 class, and optimizer class into a single object such that all can be called

            from the same interface. Prevents verbose calls to multiple objects and unifies all functionality under a

            a single interface. Might prevent some IDE type-hinting as it&#39;s dynamic



            Returns

            -------

            StokeRunner

                runtime runner object



            &quot;&quot;&quot;

            # Get the classes

            dist_class = self._get_distributed_mixin()

            fp16_class = self._get_fp16_mixin()

            optimizer_class = self._get_optimizer_mixin()

            io_class = self._get_io_mixin()



            # Python MRO hack to make sure the inits of all the Mixin classes get called

            def __multiple_mixin_init__(*args, **kwargs):

                dist_class.__init__(*args, **kwargs)

                fp16_class.__init__(*args, **kwargs)

                optimizer_class.__init__(*args, **kwargs)

                io_class.__init__(*args, **kwargs)



            # Configs pass through

            kwargs_dict = {

                &quot;amp_config&quot;: self.amp_config,

                &quot;apex_config&quot;: self.apex_config,

                &quot;ddp_config&quot;: self.ddp_config,

                &quot;deepspeed_config&quot;: self.deepspeed_config,

                &quot;horovod_config&quot;: self.horovod_config,

                &quot;oss_config&quot;: self.oss_config,

                &quot;sharded_config&quot;: self.sddp_config,

                &quot;fully_sharded_config&quot;: self.fsdp_config,

            }

            # Generate the runner class from the mixins based on the StokeStatus

            runner_class = type(

                &quot;StokeRunner&quot;,

                (dist_class, fp16_class, optimizer_class, io_class),

                {&quot;__init__&quot;: __multiple_mixin_init__},

            )(

                verbose=self._verbose,

                batch_size_per_device=self.batch_size,

                grad_accum_steps=self.grad_accum,

                grad_clip=self.grad_clip,

                info_rank=self._info_rank,

                loss=self._loss,

                **kwargs_dict,

            )

            # Make a list of class info for print later

            class_info = [

                f&quot;Distributed Mixin: {dist_class.__name__}&quot;,

                f&quot;Optimizer Mixin: {dist_class.__name__}&quot;,

                f&quot;FP16 Mixin: {fp16_class.__name__}&quot;,

                f&quot;IO Mixin: {io_class.__name__}&quot;,

            ]

            return runner_class, class_info



        def _get_io_mixin(self):

            &quot;&quot;&quot;Determines which IO class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated ioclass



            &quot;&quot;&quot;

            if self.is_deepspeed:

                return_class = RunnerIOEnum.deepspeed.value

            elif self.is_horovod:

                return_class = RunnerIOEnum.horovod.value

            elif self.is_ddp:

                return_class = RunnerIOEnum.ddp.value

            else:

                return_class = RunnerIOEnum.base.value

            return return_class



        def _get_optimizer_mixin(self):

            &quot;&quot;&quot;Determines which optimizer class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated optimizer class



            &quot;&quot;&quot;

            if self.oss:

                return_class = RunnerOptimizerEnum.oss.value

            else:

                return_class = RunnerOptimizerEnum.base.value

            return return_class



        def _get_distributed_mixin(self):

            &quot;&quot;&quot;Determines which distributed class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated distributed class



            &quot;&quot;&quot;

            # if not gpu then fall to cpu single

            if not self.gpu:

                return_class = RunnerDistEnum.cpu.value

            # if gpu but no distributed then fall to single gpu

            elif self.gpu and (self.distributed is None):

                return_class = RunnerDistEnum.gpu.value

            elif self.gpu and (self.distributed is not None):

                return_class = RunnerDistEnum[self.distributed].value

            else:

                raise ValueError(&quot;Stoke -- Cannot map to a valid distributed class&quot;)

            return return_class



        def _get_fp16_mixin(self):

            &quot;&quot;&quot;Determines which fp16 class to use



            Embedded logic based on the enum class



            Returns

            -------

            ABCMeta

                un-instantiated fp16 class



            &quot;&quot;&quot;

            if self.fp16 is not None:

                return_class = RunnerFP16Enum[self.fp16].value

            else:

                return_class = RunnerFP16Enum.full.value

            return return_class



        def DataLoader(

            self,

            dataset: Dataset[T_co],

            shuffle: bool = False,

            sampler: Optional[Sampler[int]] = None,

            batch_sampler: Optional[Sampler[Sequence[int]]] = None,

            num_workers: int = 0,

            collate_fn: _collate_fn_t = None,

            pin_memory: bool = False,

            drop_last: bool = False,

            timeout: float = 0,

            worker_init_fn: _worker_init_fn_t = None,

            multiprocessing_context=None,

            generator=None,

            *,

            prefetch_factor: int = 2,

            persistent_workers: bool = False,

        ):

            &quot;&quot;&quot;Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs.



            Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called)

            and to automatically handle device placement since the gpu/fp16 flags can&#39;t be determined until the StokeStatus

            object is available which is post init. This could be disconnected from this class but it would require the

            user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and

            never handled



            Parameters

            ----------

            dataset: Dataset

                dataset from which to load the data.

            shuffle: bool, default: False

                set to ``True`` to have the data reshuffled at every epoch.

            sampler: Sampler or Iterable, default: None

                defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__``

                implemented. If specified, :attr:`shuffle` must not be specified.

            batch_sampler: Sampler or Iterable, default: None:

                like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with

                :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.

            num_workers: int, default: 0

                how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process.

            collate_fn: callable, optional:

                merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a

                map-style dataset.

            pin_memory: bool, default: False:

                If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your

                data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,

                see the example below.

            drop_last: bool, default: False

                set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size.

                If ``False`` and the size of dataset is not divisible by the batch size, then the last batch

                will be smaller.

            timeout: numeric, default: 0

                if positive, the timeout value for collecting a batch from workers. Should always be non-negative.

            worker_init_fn: callable, default: None

                If not ``None``, this will be called on each worker subprocess with the worker id

                (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading.

            prefetch_factor: int, default: 2

                Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers

                samples prefetched across all workers.

            persistent_workers: bool, default: False

                If ``True``, the data loader will not shutdown the worker processes after a dataset has been

                consumed once. This allows to maintain the workers `Dataset` instances alive.



            Returns

            -------

            StokeDataLoader

                wrapped torch.utils.data.DataLoader object



            &quot;&quot;&quot;

            # Check if forkserver is available for horovod and use

            if (

                num_workers &gt; 0

                and hasattr(torch.multiprocessing, &quot;_supports_context&quot;)

                and torch.multiprocessing._supports_context

                and &quot;forkserver&quot; in torch.multiprocessing.get_all_start_methods()

                and self.is_horovod

            ):

                multiprocessing_context = &quot;forkserver&quot;



            if self._verbose and self.gpu:

                print(f&quot;Automatically handling moving model input data to GPU(s)...&quot;)

            # Forward the already known options from the Stoke status

            return StokeDataLoader(

                gpu=self.gpu,

                fp16=self.fp16,

                batch_size=self.batch_size,

                dataset=dataset,

                shuffle=shuffle,

                sampler=sampler,

                batch_sampler=batch_sampler,

                num_workers=num_workers,

                collate_fn=collate_fn,

                pin_memory=pin_memory,

                drop_last=drop_last,

                timeout=timeout,

                worker_init_fn=worker_init_fn,

                multiprocessing_context=multiprocessing_context,

                generator=generator,

                prefetch_factor=prefetch_factor,

                persistent_workers=persistent_workers,

            )



        def model(self, *args, **kwargs):

            &quot;&quot;&quot;Wrapped model forward call



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the model forward call



            Returns

            -------

            model forward output



            &quot;&quot;&quot;

            with self._runner.model_context:

                return self._model(*args, **kwargs)

                # return self.model_access(*args, **kwargs)



        def loss(self, *args, **kwargs):

            &quot;&quot;&quot;Wrapped callable loss function call



            Handles internal logic of aggregating up the losses for single and multiple losses



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the loss function call(s)



            Returns

            -------

            outputs of callable loss function(s)



            &quot;&quot;&quot;

            # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch

            with self._runner.loss_context:

                if isinstance(self._loss, (list, tuple)):

                    loss = type(self._loss)(val(*args, **kwargs) for val in self._loss)

                    sync_loss = [self.detach_and_sync_loss(val) for val in loss]

                    self._last_step_loss = type(self._loss)(

                        val for idx, val in enumerate(sync_loss)

                    )

                    self._agg_loss = type(self._loss)(

                        self._agg_loss[idx] + val for idx, val in enumerate(sync_loss)

                    )

                    self._handle_ema_loss(loss=sync_loss)

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = type(loss)(val / self.grad_accum for val in loss)

                else:

                    loss = self._loss(*args, **kwargs)

                    sync_loss = self.detach_and_sync_loss(loss)

                    self._last_step_loss = sync_loss

                    self._agg_loss += sync_loss

                    self._handle_ema_loss(loss=sync_loss)

                    # Handle grad accumulation by dividing by the accumulation steps

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = loss / self.grad_accum

                return loss



        def _handle_ema_loss(self, loss: Union[float, List[float], Tuple[float]]):

            &quot;&quot;&quot;Handles calculating the ema loss



            Parameters

            ----------

            loss: Union[float, List[float], Tuple[float]]

                current calculated loss list, tuple or float



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._rolling_loss_steps += 1

            if isinstance(loss, (list, tuple)):

                self._rolling_mean_loss = type(self._rolling_mean_loss)(

                    self._ema_loss(value=val, current_mean=self._rolling_mean_loss[idx])

                    for idx, val in enumerate(loss)

                )

            else:

                self._rolling_mean_loss = self._ema_loss(

                    value=loss, current_mean=self._rolling_mean_loss

                )



        def _ema_loss(self, value: float, current_mean: float):

            &quot;&quot;&quot;Calculate the ema of the loss



            Parameters

            ----------

            value: float

                current loss value

            current_mean: float

                current mean value



            Returns

            -------

            current ema value: float



            &quot;&quot;&quot;

            if self._rolling_loss_steps == 1:

                return value

            else:

                return (self._ema_weight * value) + (

                    (1.0 - self._ema_weight) * current_mean

                )



        def backward(

            self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

        ):

            &quot;&quot;&quot;Wrapped backwards call



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                Callable loss function(s)



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Increment the grad counter

            self._grad_accum_counter += 1

            # Set the context based on the counter

            dist_cm = (

                nullcontext()

                if self._check_accum()

                else self._runner.grad_accum_context(self._model)

            )

            with dist_cm:

                self._runner.backward_call(

                    loss=loss, model=self.model_access, optimizer=self._optimizer

                )

            # Increment the number of total calls to backward (each backward to a loss is only considered 1)

            self._backward_steps += 1



        def step(self):

            &quot;&quot;&quot;Wrapped step call



            Handles grad clipping internally



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer only if the modulo is zero

            if self._check_accum():

                if self._verbose and self.grad_accum &gt; 0:

                    self.print(f&quot;Gradient Accumulation Steps: {self.grad_accum}&quot;)

                # Clip if needed

                if self.grad_clip is not None:

                    self._runner.clip_grad(

                        self.grad_clip,

                        self._model if self.fully_sharded else self.model_access,

                        self._optimizer,

                        oss=self.oss,

                        horovod=self.is_horovod,

                        deepspeed=self.is_deepspeed,

                        fsdp=self.fully_sharded,

                    )

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )

                # Reset for the accumulated step

                self._reset()

                # Increment the number of step calls to the optimizer

                self._optimizer_steps += 1

            # if deepspeed we need to step everytime as it handles the grad accumulation internally

            elif self.is_deepspeed:

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )



        def _reset(self):

            &quot;&quot;&quot;Resets the state post optimizer step call



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self.print(&quot;Resetting all grad/variables for next optimizer step&quot;)

            # Zero the grads if not deepspeed

            if not self.is_deepspeed:

                self.zero_grads()

            # Reset counter

            self._grad_accum_counter = 0

            # Reset agg loss -- single or mutiple losses

            self._agg_loss = self._set_loss_to_zero()



        def save(

            self,

            path: str,

            name: str = uuid4(),

            extension: str = &quot;pt&quot;,

            create_directory: bool = True,

            extras: Optional[dict] = None,

        ):

            &quot;&quot;&quot;Saves a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory to save the model checkpoint (prefer absolute paths over relative paths)

            name: str, default: uuid4()

                name used to save checkpoint file

            extension: str, default: &#39;.pt&#39;

                extension used to save PyTorch model checkpoint

            create_directory: bool, default: True

                flag to create the directory path if it doesn&#39;t exist

            extras: dict, default: None

                a dictionary of any extra things to save



            Returns

            -------

            path: str

                path to directory that the model checkpoint was saved

            tag: str

                full tag name the model checkpoint was saved as



            &quot;&quot;&quot;

            out_path, tag = self._runner.save(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                path=path,

                backward_step=self._backward_steps,

                grad_accum_step=self._grad_accum_counter,

                optimizer_step=self._optimizer_steps,

                name=name,

                scaler_dict=self.fp16_state_dict,

                extension=extension,

                create_directory=create_directory,

                extras=extras,

                status=self.status.status,

            )

            self.print(f&quot;Successfully saved model checkpoint to {out_path}/{tag}&quot;)

            return out_path, tag



        def load(self, path: str, tag: str, strict: bool = True):

            &quot;&quot;&quot;Loads a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory that the model checkpoint was saved (prefer absolute paths over relative paths)

            tag: str

                full tag name the model checkpoint was saved as

            strict: bool

                ignore non-matching keys



            Returns

            -------

            extras: dict, default: None

                a dictionary of any custom fields the user passed to the save function



            &quot;&quot;&quot;

            # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU?

            backward_step, grad_accum_step, optimizer_step, extras = self._runner.load(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                gpu=self.gpu,

                path=path,

                tag=tag,

                scaler_dict_fn=self._load_fp16_state_dict_fn(),

                strict=strict,

            )

            # Reset values based on what was in the load dict

            self._backward_steps = backward_step

            self._grad_accum_counter = grad_accum_step

            self._optimizer_steps = optimizer_step

            self.print(f&quot;Successfully loaded model checkpoint from {path}/{tag}&quot;)

            # Return the extras dict

            return extras



        def print_num_model_parameters(

            self, normalize: ParamNormalize = ParamNormalize.MILLION

        ):

            &quot;&quot;&quot;



            Parameters

            ----------

            normalize: ParamNormalize, default: ParamNormalize.MILLION

                ParamNormalize choice for pretty print normalizing



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(

                f&quot;Total Trainable Model Parameters: &quot;

                f&quot;{(self.num_model_parameters / normalize.value):.3f} {normalize.name}&quot;

            )



        def detach_and_sync_loss(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Shorthand method to detach and sync loss



            Maps to the runner function of the same name



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es)

            device: default: None

                device to sync across



            Returns

            -------

            loss that is synced across devices and all_reduced w/ SUM



            &quot;&quot;&quot;

            return self._runner.detach_and_sync_loss(loss=loss, device=device)



        def zero_grads(self):

            &quot;&quot;&quot;Zeros the optimizer grads depending on the optimizer type



            Returns

            -------

            None



            &quot;&quot;&quot;

            zero_optimizer_grads(

                optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod

            )



        def reset(self):

            &quot;&quot;&quot;Public method for resetting the underlying stoke state



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._reset()



        def reset_tracking(self):

            &quot;&quot;&quot;Public method for resetting all underlying stoke tracked variables



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Create some tracking vars

            self._grad_accum_counter = 0

            self._optimizer_steps = 0

            self._backward_steps = 0

            self._last_step_loss = self._set_loss_to_zero()

            self._agg_loss = self._set_loss_to_zero()

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0



        def dump_model_parameter_info(self):

            &quot;&quot;&quot;Dumps all parameter information for named parameters (shape, device, dtype)



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(&quot;Dumping all model parameter information to stdout....&quot;)

            for name, param in self.model_access.named_parameters():

                if param.requires_grad:

                    self.print(

                        f&quot;Name: {name}, Shape: {param.shape}, &quot;

                        f&quot;Device: {param.device}, dtype: {param.dtype}&quot;

                    )



        def _load_fp16_state_dict_fn(self):

            &quot;&quot;&quot;Returns the function to load the sacler state dict



            Returns

            -------

            mp_state_dict_fn: Callable, default: None

                callable function to load the scaler state dict



            &quot;&quot;&quot;

            mp_state_dict_fn = None

            if self.scaler is not None:

                if self.is_apex:

                    try:

                        from apex import amp



                        mp_state_dict_fn = amp.load_state_dict

                    except ImportError as e:

                        print(

                            e,

                            &quot;: Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                        )

                else:

                    mp_state_dict_fn = self.scaler.load_state_dict

            return mp_state_dict_fn



        def barrier(self):

            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            self._runner.barrier()



        @property

        def step_loss(self):

            &quot;&quot;&quot;Gets the last step loss synced across device(s) (unscaled)&quot;&quot;&quot;

            return self._last_step_loss



        @property

        def model_access(self):

            &quot;&quot;&quot;Interface for model access due to the different types between the DP, DDP, and SDDP implementations&quot;&quot;&quot;

            if isinstance(self._model, (DDP, DP, SDDP, FSDP)):

                return self._model.module

            else:

                return self._model



        @property

        def loss_access(self):

            &quot;&quot;&quot;Gets loss tensor(s)&quot;&quot;&quot;

            return self._loss



        @property

        def optimizer(self):

            &quot;&quot;&quot;Gets the optimizer&quot;&quot;&quot;

            return self._optimizer



        @property

        def scaler(self):

            &quot;&quot;&quot;Gets the current scaler object&quot;&quot;&quot;

            return self._runner.scaler



        @property

        def fp16_state_dict(self):

            &quot;&quot;&quot;Gets the fp16 state dict from various methods&quot;&quot;&quot;

            mp_state_dict = None

            if self.scaler is not None:

                if self.is_apex:

                    try:

                        from apex import amp



                        mp_state_dict = amp.state_dict()

                    except ImportError as e:

                        print(

                            e,

                            &quot;: Stoke -- apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                        )

                elif self.is_amp:

                    mp_state_dict = self.scaler.state_dict()

            return mp_state_dict



        @property

        def status(self):

            &quot;&quot;&quot;Gets the StokeStatus object&quot;&quot;&quot;

            return self._status



        @property

        def batch_size(self):

            &quot;&quot;&quot;Shortcut to batch size&quot;&quot;&quot;

            return self._status.batch_size



        @property

        def effective_batch_size(self):

            &quot;&quot;&quot;Shortcut to effective batch size&quot;&quot;&quot;

            return self._status.effective_batch_size



        @property

        def grad_clip(self):

            &quot;&quot;&quot;Shortcut to get grad clip&quot;&quot;&quot;

            return self._status.grad_clip



        @property

        def grad_accum(self):

            &quot;&quot;&quot;Shortcut to get grad accumulation&quot;&quot;&quot;

            return self._status.grad_accum



        @property

        def gpu(self):

            &quot;&quot;&quot;Shortcut to get GPU status&quot;&quot;&quot;

            return self._status.gpu



        @property

        def cuda(self):

            &quot;&quot;&quot;Shortcut to get cuda status&quot;&quot;&quot;

            return self._status.cuda



        @property

        def nccl(self):

            &quot;&quot;&quot;Shortcut to get nccl status&quot;&quot;&quot;

            return self._status.nccl



        @property

        def fp16(self):

            &quot;&quot;&quot;Shortcut to get FP16 status&quot;&quot;&quot;

            return self._status.fp16



        @property

        def is_apex(self):

            &quot;&quot;&quot;Returns if APEX is activated&quot;&quot;&quot;

            return self._status.is_fp16_apex



        @property

        def is_amp(self):

            &quot;&quot;&quot;Returns if AMP is activated&quot;&quot;&quot;

            return self._status.is_fp16_amp



        @property

        def distributed(self):

            &quot;&quot;&quot;Shortcut to distributed status&quot;&quot;&quot;

            return self._status.distributed



        @property

        def is_ddp(self):

            &quot;&quot;&quot;Returns if DDP is activated&quot;&quot;&quot;

            return self._status.is_distributed_ddp



        @property

        def is_horovod(self):

            &quot;&quot;&quot;Returns if Horovod is activated&quot;&quot;&quot;

            return self._status.is_distributed_horovod



        @property

        def is_deepspeed(self):

            &quot;&quot;&quot;Returns if Deepspeed is acticated&quot;&quot;&quot;

            return self._status.is_distributed_deepspeed



        @property

        def oss(self):

            &quot;&quot;&quot;Returns if Fairscale optimizer state sharding status&quot;&quot;&quot;

            return self._status.oss



        @property

        def sharded(self):

            &quot;&quot;&quot;Returns if Fairscale sharded DDP status&quot;&quot;&quot;

            return self._status.sharded



        @property

        def fully_sharded(self):

            &quot;&quot;&quot;Returns if Fairscale fully sharded DDP status&quot;&quot;&quot;

            return self._status.fully_sharded



        @property

        def world_size(self):

            &quot;&quot;&quot;Shortcut to get world size&quot;&quot;&quot;

            return self._runner.world_size



        @property

        def rank(self):

            &quot;&quot;&quot;Shortcut to get rank&quot;&quot;&quot;

            return self._runner.rank



        @property

        def amp_config(self):

            &quot;&quot;&quot;Returns amp config or None based on amp state&quot;&quot;&quot;

            return self._status.amp_config if self.is_amp else None



        @property

        def apex_config(self):

            &quot;&quot;&quot;Returns apex config or None based on apex state&quot;&quot;&quot;

            return self._status.apex_config if self.is_apex else None



        @property

        def ddp_config(self):

            &quot;&quot;&quot;Returns ddp config or None based on ddp state&quot;&quot;&quot;

            return self._status.ddp_config if self.is_ddp else None



        @property

        def deepspeed_config(self):

            &quot;&quot;&quot;Returns deepspeed config or None based on deepspeed state&quot;&quot;&quot;

            return self._status.deepspeed_config if self.is_deepspeed else None



        @property

        def oss_config(self):

            &quot;&quot;&quot;Returns oss config or None based on ossstate&quot;&quot;&quot;

            return self._status.oss_config if self.oss else None



        @property

        def sddp_config(self):

            &quot;&quot;&quot;Returns sddp config or None based on sddp state&quot;&quot;&quot;

            return self._status.sddp_config if self.sharded else None



        @property

        def fsdp_config(self):

            &quot;&quot;&quot;Returns fsdp config or None based on fsdp state&quot;&quot;&quot;

            return self._status.fsdp_config if self.fully_sharded else None



        @property

        def horovod_config(self):

            &quot;&quot;&quot;Returns horovod config or None based on horovod state&quot;&quot;&quot;

            return self._status.horovod_config if self.is_horovod else None



        @property

        def num_model_parameters(self):

            &quot;&quot;&quot;Returns number of parameters that require gradients&quot;&quot;&quot;

            return sum(p.numel() for p in self.model_access.parameters() if p.requires_grad)



        @property

        def ema_loss(self):

            &quot;&quot;&quot;Returns the current rolling mean loss&quot;&quot;&quot;

            return self._rolling_mean_loss
</code></pre></div>
<hr />
<h4 id="instance-variables">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">amp_config</span>
</code></pre></div>
<p>Returns amp config or None based on amp state</p>
<div class="highlight"><pre><span></span><code><span class="n">apex_config</span>
</code></pre></div>
<p>Returns apex config or None based on apex state</p>
<div class="highlight"><pre><span></span><code><span class="n">batch_size</span>
</code></pre></div>
<p>Shortcut to batch size</p>
<div class="highlight"><pre><span></span><code><span class="n">cuda</span>
</code></pre></div>
<p>Shortcut to get cuda status</p>
<div class="highlight"><pre><span></span><code><span class="n">ddp_config</span>
</code></pre></div>
<p>Returns ddp config or None based on ddp state</p>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed_config</span>
</code></pre></div>
<p>Returns deepspeed config or None based on deepspeed state</p>
<div class="highlight"><pre><span></span><code><span class="n">distributed</span>
</code></pre></div>
<p>Shortcut to distributed status</p>
<div class="highlight"><pre><span></span><code><span class="n">effective_batch_size</span>
</code></pre></div>
<p>Shortcut to effective batch size</p>
<div class="highlight"><pre><span></span><code><span class="n">ema_loss</span>
</code></pre></div>
<p>Returns the current rolling mean loss</p>
<div class="highlight"><pre><span></span><code><span class="n">fp16</span>
</code></pre></div>
<p>Shortcut to get FP16 status</p>
<div class="highlight"><pre><span></span><code><span class="n">fp16_state_dict</span>
</code></pre></div>
<p>Gets the fp16 state dict from various methods</p>
<div class="highlight"><pre><span></span><code><span class="n">fsdp_config</span>
</code></pre></div>
<p>Returns fsdp config or None based on fsdp state</p>
<div class="highlight"><pre><span></span><code><span class="n">fully_sharded</span>
</code></pre></div>
<p>Returns if Fairscale fully sharded DDP status</p>
<div class="highlight"><pre><span></span><code><span class="n">gpu</span>
</code></pre></div>
<p>Shortcut to get GPU status</p>
<div class="highlight"><pre><span></span><code><span class="n">grad_accum</span>
</code></pre></div>
<p>Shortcut to get grad accumulation</p>
<div class="highlight"><pre><span></span><code><span class="n">grad_clip</span>
</code></pre></div>
<p>Shortcut to get grad clip</p>
<div class="highlight"><pre><span></span><code><span class="n">horovod_config</span>
</code></pre></div>
<p>Returns horovod config or None based on horovod state</p>
<div class="highlight"><pre><span></span><code><span class="n">is_amp</span>
</code></pre></div>
<p>Returns if AMP is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_apex</span>
</code></pre></div>
<p>Returns if APEX is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_ddp</span>
</code></pre></div>
<p>Returns if DDP is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_deepspeed</span>
</code></pre></div>
<p>Returns if Deepspeed is acticated</p>
<div class="highlight"><pre><span></span><code><span class="n">is_horovod</span>
</code></pre></div>
<p>Returns if Horovod is activated</p>
<div class="highlight"><pre><span></span><code><span class="n">loss_access</span>
</code></pre></div>
<p>Gets loss tensor(s)</p>
<div class="highlight"><pre><span></span><code><span class="n">model_access</span>
</code></pre></div>
<p>Interface for model access due to the different types between the DP, DDP, and SDDP implementations</p>
<div class="highlight"><pre><span></span><code><span class="n">nccl</span>
</code></pre></div>
<p>Shortcut to get nccl status</p>
<div class="highlight"><pre><span></span><code><span class="n">num_model_parameters</span>
</code></pre></div>
<p>Returns number of parameters that require gradients</p>
<div class="highlight"><pre><span></span><code><span class="n">optimizer</span>
</code></pre></div>
<p>Gets the optimizer</p>
<div class="highlight"><pre><span></span><code><span class="n">oss</span>
</code></pre></div>
<p>Returns if Fairscale optimizer state sharding status</p>
<div class="highlight"><pre><span></span><code><span class="n">oss_config</span>
</code></pre></div>
<p>Returns oss config or None based on ossstate</p>
<div class="highlight"><pre><span></span><code><span class="n">rank</span>
</code></pre></div>
<p>Shortcut to get rank</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Gets the current scaler object</p>
<div class="highlight"><pre><span></span><code><span class="n">sddp_config</span>
</code></pre></div>
<p>Returns sddp config or None based on sddp state</p>
<div class="highlight"><pre><span></span><code><span class="n">sharded</span>
</code></pre></div>
<p>Returns if Fairscale sharded DDP status</p>
<div class="highlight"><pre><span></span><code><span class="n">status</span>
</code></pre></div>
<p>Gets the StokeStatus object</p>
<div class="highlight"><pre><span></span><code><span class="n">step_loss</span>
</code></pre></div>
<p>Gets the last step loss synced across device(s) (unscaled)</p>
<div class="highlight"><pre><span></span><code><span class="n">world_size</span>
</code></pre></div>
<p>Shortcut to get world size</p>
<h4 id="methods">Methods</h4>
<h4 id="dataloader">DataLoader</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">DataLoader</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dataset</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">Dataset</span><span class="p">[</span><span class="o">+</span><span class="n">T_co</span><span class="p">],</span>
    <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Sampler</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_sampler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sampler</span><span class="o">.</span><span class="n">Sampler</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">List</span><span class="p">[</span><span class="o">~</span><span class="n">T</span><span class="p">]],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">worker_init_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">multiprocessing_context</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">generator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">prefetch_factor</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">persistent_workers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs.</p>
<p>Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called)
and to automatically handle device placement since the gpu/fp16 flags can't be determined until the StokeStatus
object is available which is post init. This could be disconnected from this class but it would require the
user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and
never handled</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>dataset</td>
<td>Dataset</td>
<td>dataset from which to load the data.</td>
<td>None</td>
</tr>
<tr>
<td>shuffle</td>
<td>bool, default: False</td>
<td>set to <code>True</code> to have the data reshuffled at every epoch.</td>
<td>None</td>
</tr>
<tr>
<td>sampler</td>
<td>Sampler or Iterable, default: None</td>
<td>defines the strategy to draw samples from the dataset. Can be any <code>Iterable</code> with <code>__len__</code></td>
<td></td>
</tr>
<tr>
<td>implemented. If specified, :attr:<code>shuffle</code> must not be specified.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>batch_sampler</td>
<td>Sampler or Iterable, default: None:</td>
<td>like :attr:<code>sampler</code>, but returns a batch of indices at a time. Mutually exclusive with</td>
<td></td>
</tr>
<tr>
<td>:attr:<code>batch_size</code>, :attr:<code>shuffle</code>, :attr:<code>sampler</code>, and :attr:<code>drop_last</code>.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>num_workers</td>
<td>int, default: 0</td>
<td>how many subprocesses to use for data loading. <code>0</code> means that the data will be loaded in the main process.</td>
<td>None</td>
</tr>
<tr>
<td>collate_fn</td>
<td>callable, optional:</td>
<td>merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a</td>
<td></td>
</tr>
<tr>
<td>map-style dataset.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>pin_memory</td>
<td>bool, default: False:</td>
<td>If <code>True</code>, the data loader will copy Tensors into CUDA pinned memory before returning them. If your</td>
<td></td>
</tr>
<tr>
<td>data elements are a custom type, or your :attr:<code>collate_fn</code> returns a batch that is a custom type,</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>see the example below.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>drop_last</td>
<td>bool, default: False</td>
<td>set to <code>True</code> to drop the last incomplete batch, if the dataset size is not divisible by the batch size.</td>
<td></td>
</tr>
<tr>
<td>If <code>False</code> and the size of dataset is not divisible by the batch size, then the last batch</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>will be smaller.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>timeout</td>
<td>numeric, default: 0</td>
<td>if positive, the timeout value for collecting a batch from workers. Should always be non-negative.</td>
<td>None</td>
</tr>
<tr>
<td>worker_init_fn</td>
<td>callable, default: None</td>
<td>If not <code>None</code>, this will be called on each worker subprocess with the worker id</td>
<td></td>
</tr>
<tr>
<td>(an int in <code>[0, num_workers - 1]</code>) as input, after seeding and before data loading.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>prefetch_factor</td>
<td>int, default: 2</td>
<td>Number of samples loaded in advance by each worker. <code>2</code> means there will be a total of 2 * num_workers</td>
<td></td>
</tr>
<tr>
<td>samples prefetched across all workers.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>persistent_workers</td>
<td>bool, default: False</td>
<td>If <code>True</code>, the data loader will not shutdown the worker processes after a dataset has been</td>
<td></td>
</tr>
<tr>
<td>consumed once. This allows to maintain the workers <code>Dataset</code> instances alive.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>StokeDataLoader</td>
<td>wrapped torch.utils.data.DataLoader object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def DataLoader(</p>
<div class="highlight"><pre><span></span><code>            self,

            dataset: Dataset[T_co],

            shuffle: bool = False,

            sampler: Optional[Sampler[int]] = None,

            batch_sampler: Optional[Sampler[Sequence[int]]] = None,

            num_workers: int = 0,

            collate_fn: _collate_fn_t = None,

            pin_memory: bool = False,

            drop_last: bool = False,

            timeout: float = 0,

            worker_init_fn: _worker_init_fn_t = None,

            multiprocessing_context=None,

            generator=None,

            *,

            prefetch_factor: int = 2,

            persistent_workers: bool = False,

        ):

            &quot;&quot;&quot;Provides a shim interface to torch.utils.data.DataLoader with mapped kwargs.



            Shim is necessary for two reasons... to inject some horovod runtime configs (make sure forkserver is called)

            and to automatically handle device placement since the gpu/fp16 flags can&#39;t be determined until the StokeStatus

            object is available which is post init. This could be disconnected from this class but it would require the

            user to forward on device or fp16 configs which breaks the paradigm that the flags only need to be set and

            never handled



            Parameters

            ----------

            dataset: Dataset

                dataset from which to load the data.

            shuffle: bool, default: False

                set to ``True`` to have the data reshuffled at every epoch.

            sampler: Sampler or Iterable, default: None

                defines the strategy to draw samples from the dataset. Can be any ``Iterable`` with ``__len__``

                implemented. If specified, :attr:`shuffle` must not be specified.

            batch_sampler: Sampler or Iterable, default: None:

                like :attr:`sampler`, but returns a batch of indices at a time. Mutually exclusive with

                :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.

            num_workers: int, default: 0

                how many subprocesses to use for data loading. ``0`` means that the data will be loaded in the main process.

            collate_fn: callable, optional:

                merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a

                map-style dataset.

            pin_memory: bool, default: False:

                If ``True``, the data loader will copy Tensors into CUDA pinned memory before returning them. If your

                data elements are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,

                see the example below.

            drop_last: bool, default: False

                set to ``True`` to drop the last incomplete batch, if the dataset size is not divisible by the batch size.

                If ``False`` and the size of dataset is not divisible by the batch size, then the last batch

                will be smaller.

            timeout: numeric, default: 0

                if positive, the timeout value for collecting a batch from workers. Should always be non-negative.

            worker_init_fn: callable, default: None

                If not ``None``, this will be called on each worker subprocess with the worker id

                (an int in ``[0, num_workers - 1]``) as input, after seeding and before data loading.

            prefetch_factor: int, default: 2

                Number of samples loaded in advance by each worker. ``2`` means there will be a total of 2 * num_workers

                samples prefetched across all workers.

            persistent_workers: bool, default: False

                If ``True``, the data loader will not shutdown the worker processes after a dataset has been

                consumed once. This allows to maintain the workers `Dataset` instances alive.



            Returns

            -------

            StokeDataLoader

                wrapped torch.utils.data.DataLoader object



            &quot;&quot;&quot;

            # Check if forkserver is available for horovod and use

            if (

                num_workers &gt; 0

                and hasattr(torch.multiprocessing, &quot;_supports_context&quot;)

                and torch.multiprocessing._supports_context

                and &quot;forkserver&quot; in torch.multiprocessing.get_all_start_methods()

                and self.is_horovod

            ):

                multiprocessing_context = &quot;forkserver&quot;



            if self._verbose and self.gpu:

                print(f&quot;Automatically handling moving model input data to GPU(s)...&quot;)

            # Forward the already known options from the Stoke status

            return StokeDataLoader(

                gpu=self.gpu,

                fp16=self.fp16,

                batch_size=self.batch_size,

                dataset=dataset,

                shuffle=shuffle,

                sampler=sampler,

                batch_sampler=batch_sampler,

                num_workers=num_workers,

                collate_fn=collate_fn,

                pin_memory=pin_memory,

                drop_last=drop_last,

                timeout=timeout,

                worker_init_fn=worker_init_fn,

                multiprocessing_context=multiprocessing_context,

                generator=generator,

                prefetch_factor=prefetch_factor,

                persistent_workers=persistent_workers,

            )
</code></pre></div>
<h4 id="backward">backward</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span>
</code></pre></div>
<p>Wrapped backwards call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>Callable loss function(s)</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward(</p>
<div class="highlight"><pre><span></span><code>            self, loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

        ):

            &quot;&quot;&quot;Wrapped backwards call



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                Callable loss function(s)



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Increment the grad counter

            self._grad_accum_counter += 1

            # Set the context based on the counter

            dist_cm = (

                nullcontext()

                if self._check_accum()

                else self._runner.grad_accum_context(self._model)

            )

            with dist_cm:

                self._runner.backward_call(

                    loss=loss, model=self.model_access, optimizer=self._optimizer

                )

            # Increment the number of total calls to backward (each backward to a loss is only considered 1)

            self._backward_steps += 1
</code></pre></div>
<h4 id="barrier">barrier</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Calls the underlying distributed barrier if available</p>
<p>??? example "View Source"
            def barrier(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Calls the underlying distributed barrier if available&quot;&quot;&quot;

            self._runner.barrier()
</code></pre></div>
<h4 id="detach_and_sync_loss">detach_and_sync_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detach_and_sync_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Shorthand method to detach and sync loss</p>
<p>Maps to the runner function of the same name</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es)</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>device to sync across</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss that is synced across devices and all_reduced w/ SUM</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def detach_and_sync_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            device=None,

        ):

            &quot;&quot;&quot;Shorthand method to detach and sync loss



            Maps to the runner function of the same name



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es)

            device: default: None

                device to sync across



            Returns

            -------

            loss that is synced across devices and all_reduced w/ SUM



            &quot;&quot;&quot;

            return self._runner.detach_and_sync_loss(loss=loss, device=device)
</code></pre></div>
<h4 id="dump_model_parameter_info">dump_model_parameter_info</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">dump_model_parameter_info</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Dumps all parameter information for named parameters (shape, device, dtype)</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def dump_model_parameter_info(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Dumps all parameter information for named parameters (shape, device, dtype)



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(&quot;Dumping all model parameter information to stdout....&quot;)

            for name, param in self.model_access.named_parameters():

                if param.requires_grad:

                    self.print(

                        f&quot;Name: {name}, Shape: {param.shape}, &quot;

                        f&quot;Device: {param.device}, dtype: {param.dtype}&quot;

                    )
</code></pre></div>
<h4 id="load">load</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">load</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">tag</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<p>Loads a model checkpoint using the correct backend interface</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>str</td>
<td>path to directory that the model checkpoint was saved (prefer absolute paths over relative paths)</td>
<td>None</td>
</tr>
<tr>
<td>tag</td>
<td>str</td>
<td>full tag name the model checkpoint was saved as</td>
<td>None</td>
</tr>
<tr>
<td>strict</td>
<td>bool</td>
<td>ignore non-matching keys</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>dict, default: None</td>
<td>a dictionary of any custom fields the user passed to the save function</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def load(self, path: str, tag: str, strict: bool = True):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Loads a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory that the model checkpoint was saved (prefer absolute paths over relative paths)

            tag: str

                full tag name the model checkpoint was saved as

            strict: bool

                ignore non-matching keys



            Returns

            -------

            extras: dict, default: None

                a dictionary of any custom fields the user passed to the save function



            &quot;&quot;&quot;

            # TODO: How to deal with mapping between backends? e.g. FP16 model back to FP32? Or multi-gpu to CPU?

            backward_step, grad_accum_step, optimizer_step, extras = self._runner.load(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                gpu=self.gpu,

                path=path,

                tag=tag,

                scaler_dict_fn=self._load_fp16_state_dict_fn(),

                strict=strict,

            )

            # Reset values based on what was in the load dict

            self._backward_steps = backward_step

            self._grad_accum_counter = grad_accum_step

            self._optimizer_steps = optimizer_step

            self.print(f&quot;Successfully loaded model checkpoint from {path}/{tag}&quot;)

            # Return the extras dict

            return extras
</code></pre></div>
<h4 id="loss">loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>Wrapped callable loss function call</p>
<p>Handles internal logic of aggregating up the losses for single and multiple losses</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>*args</td>
<td>list or tuple</td>
<td>Additional arguments should be passed as keyword arguments</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>dict</td>
<td>Extra arguments passed to the loss function call(s)</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>outputs of callable loss function(s)</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def loss(self, <em>args, </em>*kwargs):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Wrapped callable loss function call



            Handles internal logic of aggregating up the losses for single and multiple losses



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the loss function call(s)



            Returns

            -------

            outputs of callable loss function(s)



            &quot;&quot;&quot;

            # TODO: WIP Handle multiple losses. Should support list/tuple of losses. Check non base PyTorch

            with self._runner.loss_context:

                if isinstance(self._loss, (list, tuple)):

                    loss = type(self._loss)(val(*args, **kwargs) for val in self._loss)

                    sync_loss = [self.detach_and_sync_loss(val) for val in loss]

                    self._last_step_loss = type(self._loss)(

                        val for idx, val in enumerate(sync_loss)

                    )

                    self._agg_loss = type(self._loss)(

                        self._agg_loss[idx] + val for idx, val in enumerate(sync_loss)

                    )

                    self._handle_ema_loss(loss=sync_loss)

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = type(loss)(val / self.grad_accum for val in loss)

                else:

                    loss = self._loss(*args, **kwargs)

                    sync_loss = self.detach_and_sync_loss(loss)

                    self._last_step_loss = sync_loss

                    self._agg_loss += sync_loss

                    self._handle_ema_loss(loss=sync_loss)

                    # Handle grad accumulation by dividing by the accumulation steps

                    if self.grad_accum &gt; 1 and self.model_access.training:

                        loss = loss / self.grad_accum

                return loss
</code></pre></div>
<h4 id="model">model</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">model</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>Wrapped model forward call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>*args</td>
<td>list or tuple</td>
<td>Additional arguments should be passed as keyword arguments</td>
<td>None</td>
</tr>
<tr>
<td>**kwargs</td>
<td>dict</td>
<td>Extra arguments passed to the model forward call</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>model forward output</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def model(self, <em>args, </em>*kwargs):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Wrapped model forward call



            Parameters

            ----------

            *args: list or tuple

                Additional arguments should be passed as keyword arguments

            **kwargs: dict, optional

                Extra arguments passed to the model forward call



            Returns

            -------

            model forward output



            &quot;&quot;&quot;

            with self._runner.model_context:

                return self._model(*args, **kwargs)

                # return self.model_access(*args, **kwargs)
</code></pre></div>
<h4 id="print">print</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Wraps the runners print device and forces print on the _info_rank attribute(s)</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>str</td>
<td>message to print</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print(self, msg: Union[str, List[str]], single_line: bool = False):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Wraps the runners print device and forces print on the _info_rank attribute(s)



            Parameters

            ----------

            msg: str

                message to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(

                msg=msg, rank=self._info_rank, single_line=single_line

            )
</code></pre></div>
<h4 id="print_ema_loss">print_ema_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_ema_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prepend_msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Current EMA Loss&#39;</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Prints the current ema loss synced across all devices</p>
<p>Handles single or multiple losses. Prints only on devices specified by self._info_rank</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>prepend_msg</td>
<td>str, default: "Current EMA Loss"</td>
<td>message prepend to print</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_ema_loss(</p>
<div class="highlight"><pre><span></span><code>            self, prepend_msg: str = &quot;Current EMA Loss&quot;, single_line: bool = False

        ):

            &quot;&quot;&quot;Prints the current ema loss synced across all devices



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Current EMA Loss&quot;

                message prepend to print

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(self._rolling_mean_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val:.3f}&quot;

                    for idx, val in enumerate(self._rolling_mean_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(f&quot;{prepend_msg}: {self._rolling_mean_loss:.3f}&quot;)
</code></pre></div>
<h4 id="print_mean_accumulated_synced_loss">print_mean_accumulated_synced_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_mean_accumulated_synced_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">prepend_msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Mean Accumulated &amp; Synced Loss&#39;</span><span class="p">,</span>
    <span class="n">pre_backwards</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Prints the mean accumulated and device synced loss only after the grad accumulation step</p>
<p>Handles single or multiple losses. Prints only on devices specified by self._info_rank</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>prepend_msg</td>
<td>str, default: "Mean Accumulated &amp; Synced Loss"</td>
<td>message prepend to print</td>
<td>None</td>
</tr>
<tr>
<td>pre_backwards</td>
<td>bool, default: True</td>
<td>if being called pre backward step</td>
<td>None</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_mean_accumulated_synced_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            prepend_msg: str = &quot;Mean Accumulated &amp; Synced Loss&quot;,

            pre_backwards: bool = True,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints the mean accumulated and device synced loss only after the grad accumulation step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            prepend_msg: str, default: &quot;Mean Accumulated &amp; Synced Loss&quot;

                message prepend to print

            pre_backwards: bool, default: True

                if being called pre backward step

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            check_fn = self._check_pre_accum if pre_backwards else self._check_accum

            if check_fn():

                if isinstance(self._agg_loss, (list, tuple)):

                    print_vals = self._scale_agg_loss()

                    self.print(print_vals, single_line=single_line)

                else:

                    self.print(f&quot;{prepend_msg}: {self._scale_agg_loss():.3f}&quot;)
</code></pre></div>
<h4 id="print_num_model_parameters">print_num_model_parameters</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_num_model_parameters</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">normalize</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">ParamNormalize</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">ParamNormalize</span><span class="o">.</span><span class="n">MILLION</span><span class="p">:</span> <span class="mf">1000000.0</span><span class="o">&gt;</span>
<span class="p">)</span>
</code></pre></div>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>normalize</td>
<td>ParamNormalize, default: ParamNormalize.MILLION</td>
<td>ParamNormalize choice for pretty print normalizing</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_num_model_parameters(</p>
<div class="highlight"><pre><span></span><code>            self, normalize: ParamNormalize = ParamNormalize.MILLION

        ):

            &quot;&quot;&quot;



            Parameters

            ----------

            normalize: ParamNormalize, default: ParamNormalize.MILLION

                ParamNormalize choice for pretty print normalizing



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.print(

                f&quot;Total Trainable Model Parameters: &quot;

                f&quot;{(self.num_model_parameters / normalize.value):.3f} {normalize.name}&quot;

            )
</code></pre></div>
<h4 id="print_on_devices">print_on_devices</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_on_devices</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">msg</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div>
<p>Wraps runner print interface for shorter semantics</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>msg</td>
<td>str</td>
<td>message to print</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>Union[int, List[int]], default: 0</td>
<td>which ranks to print on</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_on_devices(</p>
<div class="highlight"><pre><span></span><code>            self, msg: Union[str, List[str]], rank: Optional[Union[int, List[int]]] = 0

        ):

            &quot;&quot;&quot;Wraps runner print interface for shorter semantics



            Parameters

            ----------

            msg: str

                message to print

            rank: Union[int, List[int]], default: 0

                which ranks to print on



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._runner.print_device(msg=msg, rank=rank)
</code></pre></div>
<h4 id="print_synced_loss">print_synced_loss</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_synced_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">prepend_msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;Step Synced Loss&#39;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">single_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Prints a device synced loss at a single step</p>
<p>Handles single or multiple losses. Prints only on devices specified by self._info_rank</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>current loss(es) on the device</td>
<td>None</td>
</tr>
<tr>
<td>prepend_msg</td>
<td>str, default: "Step Synced Loss"</td>
<td>message prepend to print</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>default: None</td>
<td>specify the device to place the synced loss on (defaults to same device)</td>
<td>same</td>
</tr>
<tr>
<td>single_line</td>
<td>bool, default: False</td>
<td>if iterable print all on one line space and comma separated</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def print_synced_loss(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            prepend_msg: str = &quot;Step Synced Loss&quot;,

            device=None,

            single_line: bool = False,

        ):

            &quot;&quot;&quot;Prints a device synced loss at a single step



            Handles single or multiple losses. Prints only on devices specified by self._info_rank



            Parameters

            ----------

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                current loss(es) on the device

            prepend_msg: str, default: &quot;Step Synced Loss&quot;

                message prepend to print

            device: default: None

                specify the device to place the synced loss on (defaults to same device)

            single_line: bool, default: False

                if iterable print all on one line space and comma separated



            Returns

            -------

            None



            &quot;&quot;&quot;

            printable_loss = self.detach_and_sync_loss(loss, device)

            if isinstance(printable_loss, (list, tuple)):

                print_vals = [

                    f&quot;{prepend_msg} {idx}: {val * self.grad_accum:.3f}&quot;

                    for idx, val in enumerate(printable_loss)

                ]

                self.print(print_vals, single_line=single_line)

            else:

                self.print(msg=f&quot;{prepend_msg}: {printable_loss * self.grad_accum:.3f}&quot;)
</code></pre></div>
<h4 id="reset">reset</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Public method for resetting the underlying stoke state</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def reset(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Public method for resetting the underlying stoke state



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._reset()
</code></pre></div>
<h4 id="reset_ema">reset_ema</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset_ema</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Used to reset the current state of the rolling mean loss</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def reset_ema(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Used to reset the current state of the rolling mean loss



            Returns

            -------

            None



            &quot;&quot;&quot;

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0
</code></pre></div>
<h4 id="reset_tracking">reset_tracking</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset_tracking</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Public method for resetting all underlying stoke tracked variables</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def reset_tracking(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Public method for resetting all underlying stoke tracked variables



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Create some tracking vars

            self._grad_accum_counter = 0

            self._optimizer_steps = 0

            self._backward_steps = 0

            self._last_step_loss = self._set_loss_to_zero()

            self._agg_loss = self._set_loss_to_zero()

            self._rolling_mean_loss = self._set_loss_to_zero()

            self._rolling_loss_steps = 0
</code></pre></div>
<h4 id="save">save</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">save</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">UUID</span><span class="p">(</span><span class="s1">&#39;1bec68f4-7df7-48d2-a526-14685e92f54f&#39;</span><span class="p">),</span>
    <span class="n">extension</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;pt&#39;</span><span class="p">,</span>
    <span class="n">create_directory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">extras</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>
<p>Saves a model checkpoint using the correct backend interface</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>path</td>
<td>str</td>
<td>path to directory to save the model checkpoint (prefer absolute paths over relative paths)</td>
<td>None</td>
</tr>
<tr>
<td>name</td>
<td>str, default: uuid4()</td>
<td>name used to save checkpoint file</td>
<td>None</td>
</tr>
<tr>
<td>extension</td>
<td>str, default: '.pt'</td>
<td>extension used to save PyTorch model checkpoint</td>
<td>None</td>
</tr>
<tr>
<td>create_directory</td>
<td>bool, default: True</td>
<td>flag to create the directory path if it doesn't exist</td>
<td>None</td>
</tr>
<tr>
<td>extras</td>
<td>dict, default: None</td>
<td>a dictionary of any extra things to save</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>str</td>
<td>path to directory that the model checkpoint was saved</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def save(</p>
<div class="highlight"><pre><span></span><code>            self,

            path: str,

            name: str = uuid4(),

            extension: str = &quot;pt&quot;,

            create_directory: bool = True,

            extras: Optional[dict] = None,

        ):

            &quot;&quot;&quot;Saves a model checkpoint using the correct backend interface



            Parameters

            ----------

            path: str

                path to directory to save the model checkpoint (prefer absolute paths over relative paths)

            name: str, default: uuid4()

                name used to save checkpoint file

            extension: str, default: &#39;.pt&#39;

                extension used to save PyTorch model checkpoint

            create_directory: bool, default: True

                flag to create the directory path if it doesn&#39;t exist

            extras: dict, default: None

                a dictionary of any extra things to save



            Returns

            -------

            path: str

                path to directory that the model checkpoint was saved

            tag: str

                full tag name the model checkpoint was saved as



            &quot;&quot;&quot;

            out_path, tag = self._runner.save(

                model=self._model if self.fully_sharded else self.model_access,

                optimizer=self.optimizer,

                path=path,

                backward_step=self._backward_steps,

                grad_accum_step=self._grad_accum_counter,

                optimizer_step=self._optimizer_steps,

                name=name,

                scaler_dict=self.fp16_state_dict,

                extension=extension,

                create_directory=create_directory,

                extras=extras,

                status=self.status.status,

            )

            self.print(f&quot;Successfully saved model checkpoint to {out_path}/{tag}&quot;)

            return out_path, tag
</code></pre></div>
<h4 id="step">step</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Wrapped step call</p>
<p>Handles grad clipping internally</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Wrapped step call



            Handles grad clipping internally



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer only if the modulo is zero

            if self._check_accum():

                if self._verbose and self.grad_accum &gt; 0:

                    self.print(f&quot;Gradient Accumulation Steps: {self.grad_accum}&quot;)

                # Clip if needed

                if self.grad_clip is not None:

                    self._runner.clip_grad(

                        self.grad_clip,

                        self._model if self.fully_sharded else self.model_access,

                        self._optimizer,

                        oss=self.oss,

                        horovod=self.is_horovod,

                        deepspeed=self.is_deepspeed,

                        fsdp=self.fully_sharded,

                    )

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )

                # Reset for the accumulated step

                self._reset()

                # Increment the number of step calls to the optimizer

                self._optimizer_steps += 1

            # if deepspeed we need to step everytime as it handles the grad accumulation internally

            elif self.is_deepspeed:

                # Handle the optimizer step

                step_cm = (

                    self._runner.step_context(self._optimizer)

                    if self.grad_clip is not None

                    else nullcontext()

                )

                with step_cm:

                    self._runner.step_call(

                        model=self.model_access, optimizer=self._optimizer

                    )
</code></pre></div>
<h4 id="zero_grads">zero_grads</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">zero_grads</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>
<p>Zeros the optimizer grads depending on the optimizer type</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def zero_grads(self):</p>
<div class="highlight"><pre><span></span><code>            &quot;&quot;&quot;Zeros the optimizer grads depending on the optimizer type



            Returns

            -------

            None



            &quot;&quot;&quot;

            zero_optimizer_grads(

                optimizer=self._optimizer, apex=self.is_apex, horovod=self.is_horovod

            )
</code></pre></div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../status/" title="Status" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Status
              </span>
            </div>
          </a>
        
        
          <a href="../utils/" title="Utils" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Utils
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ncilfone" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/ncilfone/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
    
  </body>
</html>