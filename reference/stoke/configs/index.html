
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../../resources/images/stoke_small.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Configs - stoke</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-stokeconfigs" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="stoke" class="md-header__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            stoke
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Configs
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Home/" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Installation/" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Quick-Start/" class="md-tabs__link">
      Quick Start
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Launchers/" class="md-tabs__link">
      Launchers
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Examples/" class="md-tabs__link">
      Examples
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../CONTRIBUTING/" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="./" class="md-tabs__link md-tabs__link--active">
        Reference
      </a>
    </li>
  

  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="stoke" class="md-nav__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    stoke
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Home/" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Quick-Start/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Launchers/" class="md-nav__link">
        Launchers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Examples/" class="md-nav__link">
        Examples
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_1" type="checkbox" id="__nav_7_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_1">
          Stoke
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stoke" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          Stoke
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Configs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Configs
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ampconfig" class="md-nav__link">
    AMPConfig
  </a>
  
    <nav class="md-nav" aria-label="AMPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apexconfig" class="md-nav__link">
    ApexConfig
  </a>
  
    <nav class="md-nav" aria-label="ApexConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backendoptions" class="md-nav__link">
    BackendOptions
  </a>
  
    <nav class="md-nav" aria-label="BackendOptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clipgradconfig" class="md-nav__link">
    ClipGradConfig
  </a>
  
    <nav class="md-nav" aria-label="ClipGradConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clipgradnormconfig" class="md-nav__link">
    ClipGradNormConfig
  </a>
  
    <nav class="md-nav" aria-label="ClipGradNormConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddpconfig" class="md-nav__link">
    DDPConfig
  </a>
  
    <nav class="md-nav" aria-label="DDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedaioconfig" class="md-nav__link">
    DeepspeedAIOConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedAIOConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_5" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedactivationcheckpointingconfig" class="md-nav__link">
    DeepspeedActivationCheckpointingConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedActivationCheckpointingConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_6" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedconfig" class="md-nav__link">
    DeepspeedConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_7" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedfp16config" class="md-nav__link">
    DeepspeedFP16Config
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFP16Config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_8" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedflopsconfig" class="md-nav__link">
    DeepspeedFlopsConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFlopsConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_9" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedoffloadoptimizerconfig" class="md-nav__link">
    DeepspeedOffloadOptimizerConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedOffloadOptimizerConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_10" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedoffloadparamconfig" class="md-nav__link">
    DeepspeedOffloadParamConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedOffloadParamConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_11" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedpldconfig" class="md-nav__link">
    DeepspeedPLDConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedPLDConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_12" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedtensorboardconfig" class="md-nav__link">
    DeepspeedTensorboardConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedTensorboardConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_13" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedzeroconfig" class="md-nav__link">
    DeepspeedZeROConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedZeROConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_14" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalefsdpconfig" class="md-nav__link">
    FairscaleFSDPConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleFSDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_15" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscaleossconfig" class="md-nav__link">
    FairscaleOSSConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleOSSConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_16" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalesddpconfig" class="md-nav__link">
    FairscaleSDDPConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleSDDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_17" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#horovodconfig" class="md-nav__link">
    HorovodConfig
  </a>
  
    <nav class="md-nav" aria-label="HorovodConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_18" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#horovodops" class="md-nav__link">
    HorovodOps
  </a>
  
    <nav class="md-nav" aria-label="HorovodOps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_1" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offloaddevice" class="md-nav__link">
    OffloadDevice
  </a>
  
    <nav class="md-nav" aria-label="OffloadDevice">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_2" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stokeoptimizer" class="md-nav__link">
    StokeOptimizer
  </a>
  
    <nav class="md-nav" aria-label="StokeOptimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_19" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clear" class="md-nav__link">
    clear
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#copy" class="md-nav__link">
    copy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fromkeys" class="md-nav__link">
    fromkeys
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get" class="md-nav__link">
    get
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#items" class="md-nav__link">
    items
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#keys" class="md-nav__link">
    keys
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pop" class="md-nav__link">
    pop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#popitem" class="md-nav__link">
    popitem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setdefault" class="md-nav__link">
    setdefault
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#update" class="md-nav__link">
    update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#values" class="md-nav__link">
    values
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../distributed/" class="md-nav__link">
        Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../extensions/" class="md-nav__link">
        Extensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../fp16/" class="md-nav__link">
        Fp16
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../io/" class="md-nav__link">
        Io
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../status/" class="md-nav__link">
        Status
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../stoke/" class="md-nav__link">
        Stoke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ampconfig" class="md-nav__link">
    AMPConfig
  </a>
  
    <nav class="md-nav" aria-label="AMPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apexconfig" class="md-nav__link">
    ApexConfig
  </a>
  
    <nav class="md-nav" aria-label="ApexConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backendoptions" class="md-nav__link">
    BackendOptions
  </a>
  
    <nav class="md-nav" aria-label="BackendOptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clipgradconfig" class="md-nav__link">
    ClipGradConfig
  </a>
  
    <nav class="md-nav" aria-label="ClipGradConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clipgradnormconfig" class="md-nav__link">
    ClipGradNormConfig
  </a>
  
    <nav class="md-nav" aria-label="ClipGradNormConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ddpconfig" class="md-nav__link">
    DDPConfig
  </a>
  
    <nav class="md-nav" aria-label="DDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedaioconfig" class="md-nav__link">
    DeepspeedAIOConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedAIOConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_5" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedactivationcheckpointingconfig" class="md-nav__link">
    DeepspeedActivationCheckpointingConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedActivationCheckpointingConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_6" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedconfig" class="md-nav__link">
    DeepspeedConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_7" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedfp16config" class="md-nav__link">
    DeepspeedFP16Config
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFP16Config">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_8" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedflopsconfig" class="md-nav__link">
    DeepspeedFlopsConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFlopsConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_9" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedoffloadoptimizerconfig" class="md-nav__link">
    DeepspeedOffloadOptimizerConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedOffloadOptimizerConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_10" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedoffloadparamconfig" class="md-nav__link">
    DeepspeedOffloadParamConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedOffloadParamConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_11" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedpldconfig" class="md-nav__link">
    DeepspeedPLDConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedPLDConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_12" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedtensorboardconfig" class="md-nav__link">
    DeepspeedTensorboardConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedTensorboardConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_13" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedzeroconfig" class="md-nav__link">
    DeepspeedZeROConfig
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedZeROConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_14" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalefsdpconfig" class="md-nav__link">
    FairscaleFSDPConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleFSDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_15" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscaleossconfig" class="md-nav__link">
    FairscaleOSSConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleOSSConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_16" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalesddpconfig" class="md-nav__link">
    FairscaleSDDPConfig
  </a>
  
    <nav class="md-nav" aria-label="FairscaleSDDPConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_17" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#horovodconfig" class="md-nav__link">
    HorovodConfig
  </a>
  
    <nav class="md-nav" aria-label="HorovodConfig">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_18" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#horovodops" class="md-nav__link">
    HorovodOps
  </a>
  
    <nav class="md-nav" aria-label="HorovodOps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_1" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offloaddevice" class="md-nav__link">
    OffloadDevice
  </a>
  
    <nav class="md-nav" aria-label="OffloadDevice">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_2" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stokeoptimizer" class="md-nav__link">
    StokeOptimizer
  </a>
  
    <nav class="md-nav" aria-label="StokeOptimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_19" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clear" class="md-nav__link">
    clear
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#copy" class="md-nav__link">
    copy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fromkeys" class="md-nav__link">
    fromkeys
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get" class="md-nav__link">
    get
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#items" class="md-nav__link">
    items
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#keys" class="md-nav__link">
    keys
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pop" class="md-nav__link">
    pop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#popitem" class="md-nav__link">
    popitem
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setdefault" class="md-nav__link">
    setdefault
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#update" class="md-nav__link">
    update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#values" class="md-nav__link">
    values
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/fidelity/stoke/edit/main/reference/stoke/configs.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="module-stokeconfigs">Module stoke.configs</h1>
<p>Handles all config objects</p>
<p>None</p>
<p>??? example "View Source"
        # -<em>- coding: utf-8 -</em>-</p>
<div class="highlight"><pre><span></span><code>    # Copyright FMR LLC &lt;opensource@fidelity.com&gt;

    # SPDX-License-Identifier: Apache-2.0



    &quot;&quot;&quot;Handles all config objects&quot;&quot;&quot;



    from enum import Enum

    from typing import Dict, Optional, Type



    import attr

    import torch



    try:

        from typing import TypedDict

    except ImportError:

        from mypy_extensions import TypedDict





    class HorovodOps(Enum):

        &quot;&quot;&quot;Horovod ops options&quot;&quot;&quot;



        Average = &quot;Average&quot;

        Sum = &quot;Sum&quot;

        Adasum = &quot;Adasum&quot;





    class OffloadDevice(Enum):

        &quot;&quot;&quot;Offload device options&quot;&quot;&quot;



        none = &quot;none&quot;

        cpu = &quot;cpu&quot;

        nvme = &quot;nvme&quot;





    class BackendOptions(Enum):

        &quot;&quot;&quot;Communication backend options&quot;&quot;&quot;



        nccl = &quot;nccl&quot;

        mpi = &quot; mpi&quot;

        gloo = &quot;gloo&quot;





    @attr.s(auto_attribs=True)

    class AMPConfig:

        &quot;&quot;&quot;PyTorch AMP configuration class



        Attributes

        ----------

        backoff_factor : float, default: 0.5

            Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration

        growth_factor : float, default: 2.0

            Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations.

        growth_interval : int, default: 2000

            Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by

            growth_factor

        init_scale : float, default: 2.**16

            Initial scale factor



        &quot;&quot;&quot;



        backoff_factor: float = 0.5

        growth_factor: float = 2.0

        growth_interval: int = 2000

        init_scale: float = 2.0 ** 16





    @attr.s(auto_attribs=True)

    class ApexConfig:

        &quot;&quot;&quot;Nvidia APEX configuration class



        Attributes

        ----------

        cast_model_outputs: Optional[torch.dtype], default: None

            Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls

            https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm

        max_loss_scale: float, default: 2.**24

            Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling

        min_loss_scale: Optional[float], default: None

            Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None

            means that no floor is imposed

        scaler_per_loss: bool, default: False

            Option to impose a scaler for each loss instead of a global scaler

        verbosity: int, default: 0

            Set to 0 to suppress Amp-related output



        &quot;&quot;&quot;



        cast_model_outputs: Optional[torch.dtype] = None

        convert_to_sync_batch_norm: bool = False

        max_loss_scale: float = 2.0 ** 24

        min_loss_scale: Optional[float] = None

        scaler_per_loss: bool = False

        verbosity: int = 0





    @attr.s(auto_attribs=True)

    class ClipGradConfig:

        &quot;&quot;&quot;Gradient clipping by value configuration class



        Attributes

        ----------

        clip_value: float

            maximum allowed absolute value of the gradients [-clip_value, clip_value]



        &quot;&quot;&quot;



        clip_value: float





    @attr.s(auto_attribs=True)

    class ClipGradNormConfig:

        &quot;&quot;&quot;Gradient clipping by p-norm configuration class



        Attributes

        ----------

        max_norm: float

            max norm of the gradients

        norm_type: float

            type of the used p-norm



        &quot;&quot;&quot;



        max_norm: float

        norm_type: float





    @attr.s(auto_attribs=True)

    class DDPConfig:

        &quot;&quot;&quot;PyTorch DistributedDataParallel configuration class



        Attributes

        ----------

        local_rank: Optional[int]

            Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg)

        auto_mpi_discovery: bool, default: False

            if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed

            function call)

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls

            https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html

        backend: BackendOptions, default: &#39;nccl&#39;

            Which communication backend to use

        broadcast_buffers: bool, default: True

            Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function

        bucket_cap_mb: int, default: 25

            DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket

            can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB)

        find_unused_parameters: bool, default: False

            Traverse the autograd graph from all tensors contained in the return value of the wrapped module’s forward

            function. Parameters that don’t receive gradients as part of this graph are preemptively marked as being ready

            to be reduced. Note that all forward outputs that are derived from module parameters must participate in

            calculating loss and later the gradient computation. If they don’t, this wrapper will hang waiting for autograd

            to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused

            can be detached from the autograd graph using torch.Tensor.detach

        gradient_as_bucket_view: bool, default: False

            When set to True, gradients will be views pointing to different offsets of allreduce communication

            buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients

            size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When

            gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by

            referring to the zero_grad() function in torch/optim/optimizer.py as a solution.

        init_method: str, default: &#39;env://&#39;

            URL specifying how to initialize the process group

        no_sync: bool, default: True

            for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on

            module variables, which will later be synchronized in the first forward-backward pass after exiting the

            context. no sync might lead to higher memory usage but lower communication overhead



        &quot;&quot;&quot;



        local_rank: Optional[int]

        auto_mpi_discovery: bool = False

        convert_to_sync_batch_norm: bool = False

        backend: BackendOptions = &quot;nccl&quot;

        broadcast_buffers: bool = True

        bucket_cap_mb: int = 25

        find_unused_parameters: bool = False

        gradient_as_bucket_view: bool = False

        init_method: str = &quot;env://&quot;

        no_sync: bool = True





    @attr.s(auto_attribs=True)

    class DeepspeedAIOConfig:

        &quot;&quot;&quot;Deepspeed asynchronous I/O configuration class



        Attributes

        ----------

        block_size: int, default: 1048576

            I/O block size in bytes

        ignore_unused_parameters: bool, default: True

            Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks.

            This controls whether or not training should terminate with an error message when unused parameters are

            detected.

        overlap_events: bool, default: True

            Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests.

        queue_depth: int, default: 8

            I/O queue depth

        single_submit: bool, default: False

            Submit requests to storage device as multiple individual requests as opposed to one block of requests.

        thread_count: int, default: 1

            Intra-request parallelism for each read/write submitted by a user thread.



        &quot;&quot;&quot;



        block_size: int = 1048576

        ignore_unused_parameters: bool = True

        overlap_events: bool = True

        queue_depth: int = 8

        single_submit: bool = False

        thread_count: int = 1





    @attr.s(auto_attribs=True)

    class DeepspeedActivationCheckpointingConfig:

        &quot;&quot;&quot;Deepspeed activation checkpointing configuration class



        Attributes

        ----------

        contiguous_memory_optimization: bool, default: False

            Copies partitioned activations so that they are contiguous in memory

        cpu_checkpointing: bool, default: False

            Offloads partitioned activations to CPU if partition_activations is enabled

        number_checkpoints: Optional[int], default: None

            Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization

        partition_activations: bool, default: False

            Enables partition activation when used with model parallelism

        profile: bool, default: False

            Logs the forward and backward time for each checkpoint function

        synchronize_checkpoint_boundary: bool, default: False

            Inserts torch.cuda.synchronize() at each checkpoint boundary



        &quot;&quot;&quot;



        contiguous_memory_optimization: bool = False

        cpu_checkpointing: bool = False

        number_checkpoints: Optional[int] = None

        partition_activations: bool = False

        profile: bool = False

        synchronize_checkpoint_boundary: bool = False





    @attr.s(auto_attribs=True)

    class DeepspeedFlopsConfig:

        &quot;&quot;&quot;Deepspeed flops profiler configuration class



        Attributes

        ----------

        detailed: bool, default: True

            Whether to print the detailed model profile

        module_depth: int, default: -1

            The depth of the model at which to print the aggregated module information. When set to -1, it prints

            information from the top module to the innermost modules (the maximum depth).

        output_file: Optional[str], default: None

            Path to the output file. If None, the profiler prints to stdout

        profile_step: int, default: 1

            The global training step at which to profile.

        top_modules: int, default: 1

            Limits the aggregated profile output to the number of top modules specified.



        Notes

        -----

        Warm up steps are needed for accurate time measurement



        &quot;&quot;&quot;



        detailed: bool = True

        module_depth: int = -1

        output_file: Optional[str] = None

        profile_step: int = 1

        top_modules: int = 1





    @attr.s(auto_attribs=True)

    class DeepspeedFP16Config:

        &quot;&quot;&quot;Deepspeed FP16 configuration class



        Attributes

        ----------

        hysteresis: int, default: 2

            represents the delay shift in dynamic loss scaling

        initial_scale_power: int, default: 32

            power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power

        loss_scale: float, default: 0.0

            loss scaling value for FP16 training (0.0 --&gt; dynamic scaling)

        loss_scale_window: int, default: 1000

            the window over which to raise/lower the dynamic loss scale value

        min_loss_scale: int, default: 1000

            minimum dynamic loss scale value



        &quot;&quot;&quot;



        hysteresis: int = 2

        initial_scale_power: int = 32

        loss_scale: float = 0.0

        loss_scale_window: int = 1000

        min_loss_scale: int = 1000





    @attr.s(auto_attribs=True)

    class DeepspeedOffloadOptimizerConfig:

        &quot;&quot;&quot;Deepspeed optimizer offloading configuration class



        Attributes

        ----------

        buffer_count: int, default: 4

            Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number

            of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter,

            gradient, momentum, and variance).

        device: OffloadDevice, default: &#39;cpu&#39;

            Device memory to offload optimizer state

        fast_init: bool, default: False

            Enable fast optimizer initialization when offloading to NVMe

        nvme_path: str, default: &#39;/local_nvme&#39;

            Filesystem path for NVMe device for optimizer state offloading

        pin_memory: bool, default: False

            Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.

        pipeline: bool, default: False

            pipeline activated (will default to True if either pipeline_read or pipeline_write is set

        pipeline_read: bool, default: False

            activate pipeline read (deepspeed has limited docs for what this does)

        pipeline_write: bool, default: False

            activate pipeline write(deepspeed has limited docs for what this does)



        &quot;&quot;&quot;



        buffer_count: int = 4

        device: OffloadDevice = &quot;cpu&quot;

        fast_init: bool = False

        nvme_path: str = &quot;/local_nvme&quot;

        pin_memory: bool = False

        pipeline: bool = False

        pipeline_read: bool = False

        pipeline_write: bool = False





    @attr.s(auto_attribs=True)

    class DeepspeedOffloadParamConfig:

        &quot;&quot;&quot;Deepspeed parameter offloading configuration class



        Attributes

        ----------

        buffer_count: int, default: 5

            Number of buffers in buffer pool for parameter offloading to NVMe

        buffer_size: int, default: int(1E8)

            Size of buffers in buffer pool for parameter offloading to NVMe

        device: OffloadDevice, default: &#39;cpu&#39;

            Device memory to offload model parameters

        max_in_cpu: int, default: int(1E9)

            Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.

        nvme_path: str, default: &#39;/local_nvme&#39;

            Filesystem path for NVMe device for parameter offloading

        pin_memory: bool, default: False

            Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.



        &quot;&quot;&quot;



        buffer_count: int = 5

        buffer_size: int = int(1e8)

        device: OffloadDevice = &quot;cpu&quot;

        max_in_cpu: int = int(1e9)

        nvme_path: str = &quot;/local_nvme&quot;

        pin_memory: bool = False





    @attr.s(auto_attribs=True)

    class DeepspeedPLDConfig:

        &quot;&quot;&quot;

        Attributes

        ----------

        theta: float, default: 1.0

            Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value,

            the faster the training speed

        gamma: float, default: 0.001

            Hyper-parameter that controls how fast the drop ratio increases



        &quot;&quot;&quot;



        theta: float = 1.0

        gamma: float = 0.001





    @attr.s(auto_attribs=True)

    class DeepspeedTensorboardConfig:

        &quot;&quot;&quot;Deepspeed Tensorboard configuration class



        Attributes

        ----------

        output_path: str, default: &#39;&#39;

            Tensorboard output path

        job_name: str, default: &#39;DeepSpeedJobName&#39;

            Tensorboard job name



        &quot;&quot;&quot;



        output_path: str = &quot;&quot;

        job_name: str = &quot;DeepSpeedJobName&quot;





    @attr.s(auto_attribs=True)

    class DeepspeedZeROConfig:

        &quot;&quot;&quot;Deepspeed ZeRO configuration class



        Attributes

        ----------

        allgather_bucket_size: int, default: int(5E8)

            Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes

        allgather_partitions: bool, default: True

            Chooses between allgather collective or a series of broadcast collectives to gather updated parameters

            from all the GPUs at the end of each step

        contiguous_gradients: bool, default: False

            Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward

            pass. Only useful when running very large models.

        ignore_unused_parameters: bool, default: True

            Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload

            Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707

        legacy_stage1: bool, default: False

            Use deepspeed &lt; v0.3.17 zero stage 1, kept for backwards compatability reasons

        offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None

            Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU

            memory for larger models or batch sizes. Valid only with stage 3

        offload_param: Optional[DeepspeedOffloadParamConfig], default: None

            Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch

            sizes. Valid only with stage 3.

        overlap_comm: bool, default: False

            Attempts to overlap the reduction of the gradients with backward computation

        reduce_bucket_size: int, default: int(5E8)

            Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large

            model sizes

        reduce_scatter: bool, default: True

            Uses reduce or reduce scatter instead of allreduce to average gradients

        stage: int, default: 0

            Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state

            partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning,

            respectively

        stage3_max_live_parameters: int, default: int(1E9)

            The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but

            perform more communication.

        stage3_max_reuse_distance: int, default: int(1E9)

            Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less

            memory, but perform more communication.

        stage3_prefetch_bucket_size: int, default: int(5E8)

            The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase

            stalls due to communication.

        stage3_param_persistence_threshold: int, default: int(1E6)

            Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly

            increase communication (especially latency-bound messages).

        stage3_gather_fp16_weights_on_model_save: bool, default: False

            Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned

            across GPUs, they aren’t part of state_dict, so this function automatically gather the weights when this

            option is enabled and then saves the fp16 model weights.

        sub_group_size: int, default: int(1E12)

            sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are

            grouped into buckets of sub_group_size and each buckets is updated one at a time.



        &quot;&quot;&quot;



        allgather_bucket_size: int = int(5e8)

        allgather_partitions: bool = True

        contiguous_gradients: bool = False

        ignore_unused_parameters: bool = True

        legacy_stage1: bool = False

        offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None

        offload_param: Optional[DeepspeedOffloadParamConfig] = None

        overlap_comm: bool = False

        reduce_bucket_size: int = int(5e8)

        reduce_scatter: bool = True

        stage: int = 0

        stage3_max_live_parameters: int = int(1e9)

        stage3_max_reuse_distance: int = int(1e9)

        stage3_prefetch_bucket_size: int = int(5e8)

        stage3_param_persistence_threshold: int = int(1e6)

        stage3_gather_fp16_weights_on_model_save: bool = False

        sub_group_size: int = int(1e12)





    @attr.s(auto_attribs=True)

    class DeepspeedConfig:

        &quot;&quot;&quot;Deepspeed configuration class



        Composed of other configuration classes related to specific functionality



        Attributes

        ----------

        activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig()

            Enables and configures activation checkpointing

        aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig()

            Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent

            (NVMe) storage

        auto_mpi_discovery: bool, default: True

            if distributed environment variables are not set, attempt to discover them from MPI

        disable_allgather: bool, default: False

            Disables allgather

        dist_backend: BackendOptions, default: &#39;nccl&#39;

            Which communication backend to use

        distributed_port: int, default: 29500

            torch distributed backend port

        dump_state: bool, default: False

            Print out state information of DeepSpeed object after initialization

        flops_profiler: Optional[DeepspeedFlopsConfig], default: None

            Enables and configures the flops profiler. This would also enable wall_clock_breakdown

        fp16: Optional[DeepspeedFP16Config], default: None

            Enables and configures mixed precision/FP16 training that leverages NVIDIA’s Apex package

        fp32_allreduce: bool, default: False

            During gradient averaging perform allreduce with 32 bit values

        gradient_predivide_factor: float, default: 1.0

            Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability

            when scaling to large numbers of GPUs

        init_method: str, default: &#39;env://&#39;

            URL specifying how to initialize the process group

        prescale_gradients: float, default: 1.0

            Scale gradients before doing allreduce

        progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None

            Enables and configures progressive layer dropping

        sparse_gradients: bool, default: False

            Enable sparse compression of torch.nn.Embedding gradients

        steps_per_print: int, default: 10

            Print train loss every N steps

        tensorboard: Optional[DeepspeedTensorboardConfig], default: None

            Enables and configures tensorboard support

        verbose: bool, default: True

            flag to make deepspeed engine verbose with information

        wall_clock_breakdown: bool, default: False

            Enable timing of the latency of forward/backward/update training phases

        zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig()

            Enables and configures ZeRO memory optimizations



        Notes

        -----

        Deepspeed does not use Apex’s AMP mode whihc allows for more flexibility in mixed precision training modes. FP16

        here is similar to AMP’s O2 mode



        &quot;&quot;&quot;



        activation_checkpointing: Optional[

            DeepspeedActivationCheckpointingConfig

        ] = DeepspeedActivationCheckpointingConfig()

        aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig()

        auto_mpi_discovery: bool = True

        disable_allgather: bool = False

        dist_backend: BackendOptions = &quot;nccl&quot;

        distributed_port: int = 29500

        dump_state: bool = False

        flops_profiler: Optional[DeepspeedFlopsConfig] = None

        fp16: Optional[DeepspeedFP16Config] = None

        fp32_allreduce: bool = False

        gradient_predivide_factor: float = 1.0

        init_method: str = &quot;env://&quot;

        prescale_gradients: bool = False

        progressive_layer_drop: Optional[DeepspeedPLDConfig] = None

        sparse_gradients: bool = False

        steps_per_print: int = 10

        tensorboard: Optional[DeepspeedTensorboardConfig] = None

        verbose: bool = True

        wall_clock_breakdown: bool = False

        zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig()





    @attr.s(auto_attribs=True)

    class FairscaleOSSConfig:

        &quot;&quot;&quot;Fairscale optimizer state sharding configuration class



        Attributes

        ----------

        broadcast_fp16: bool, default: False

            Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP

            is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy.



        &quot;&quot;&quot;



        broadcast_fp16: bool = False





    @attr.s(auto_attribs=True)

    class FairscaleSDDPConfig:

        &quot;&quot;&quot;Fairscale sharded data parallel (SDDP) configuration class



        Attributes

        ----------

        auto_refresh_trainable: bool, default: True

            Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS

            automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a

            parameter is frozen or unfrozen

        broadcast_buffers: bool, default: True

            Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same

            setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters.

        reduce_buffer_size: int, default: 2 ** 23

            he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact

            the long term memory consumption, because these buckets correspond to parameters which will not be sharded.

            Set to 0 to remove all bucketing, 1M to 8M is usually reasonable.

        reduce_fp16: bool, default: False

            cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve

            performance for multi node jobs using PyTorch AMP. The effect is similar to DDP’s fp16_compress_hook and

            will also save some memory.

        sync_models_at_startup: bool, default: True

            Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or

            the training restarts from a saved state



        &quot;&quot;&quot;



        auto_refresh_trainable: bool = True

        broadcast_buffers: bool = True

        reduce_buffer_size: int = 2 ** 23

        reduce_fp16: bool = False

        sync_models_at_startup: bool = True





    @attr.s(auto_attribs=True)

    class FairscaleFSDPConfig:

        &quot;&quot;&quot;Fairscale Fully Sharded Data Parallel configuration class



        Attributes

        ----------

        bucket_cap_mb: int, default: 25

            FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters.

            bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the

            max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple

            bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without

            using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with

            computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory

            overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during

            the backward pass and freed at the end of the backward pass to save more memory for other phases of the

            training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP

            engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The

            order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the

            backward. In FSDP, the buffer size does not change with model size (it changes based on number of

            &lt;dtype, device, process_group&gt; tuples) and gradient ready order matters little since FSDP has a final flush

            call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with

            compute is done differently too. Values &lt;= 0 disable bucketing

        buffer_dtype: Optional[torch.dtype], default: None

            dtype for buffers for computation. defaults to value of compute_dtype

        clear_autocast_cache: bool, default: False

            When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast

            maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this

            flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save

            GPU memory

        compute_dtype: Optional[torch.dtype], default: None

            dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set,

            in which case it defaults to torch.float16.

        flatten_parameters: bool, default: True

            flatten parameters into a single contiguous tensor, which improves training speed

        force_input_to_fp32: bool, default: False:

            force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision

            mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper.

        fp32_reduce_scatter: bool, default: False

            reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used

        gradient_predivide_factor: Optional[float], default: None

            divide factor before the reduction

        gradient_postdivide_factor: Optional[float], default: None

            divide factor after the reduction

        move_grads_to_cpu: Optional[bool], default: None

            move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used

        move_params_to_cpu: bool, default: False

            offload FP32 params to CPU. This is only relevant when FP16 AMP is used

        no_broadcast_optim_state: Optional[bool], default: False

            do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this

            true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the

            proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few

            parameters can fit on one node

        reshard_after_forward: bool, default: True

            reshard parameters after the forward pass. This saves memory but slows training. This is only relevant

            when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html)

        verbose: bool, default: True

            turn on verbose output for model’s string representation



        Notes

        -----

        mixed_precision: bool

            This value will automatically be set from the Stoke FP16 selected option (AMP only)

        state_dict_device: torch.device

            this is not exposed as it should be managed internally from the DDP backend setup

        compute_device: torch.device

            this is not exposed as it should be managed internally from the DDP backend setup



        &quot;&quot;&quot;



        bucket_cap_mb: int = 25

        buffer_dtype: Optional[torch.dtype] = None

        clear_autocast_cache: bool = False

        compute_dtype: Optional[torch.dtype] = None

        flatten_parameters: bool = True

        force_input_to_fp32: bool = False

        fp32_reduce_scatter: bool = False

        gradient_predivide_factor: Optional[float] = None

        gradient_postdivide_factor: Optional[float] = None

        move_grads_to_cpu: Optional[bool] = None

        move_params_to_cpu: bool = False

        no_broadcast_optim_state: Optional[bool] = False

        reshard_after_forward: bool = True

        verbose: bool = False





    @attr.s(auto_attribs=True)

    class HorovodConfig:

        &quot;&quot;&quot;Horovod configuration class



        Attributes

        ----------

        compression: bool, default: False

            Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter

            update step.

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls

            https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm

        gradient_predivide_factor: float, default: 1.0

            If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled

            by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum.

        op: HorovodOps, default: &#39;Average&#39;

            The reduction operation to use when combining gradients across different ranks.



        &quot;&quot;&quot;



        compression: bool = False

        convert_to_sync_batch_norm: bool = False

        gradient_predivide_factor: float = 1.0

        op: HorovodOps = &quot;Average&quot;





    class StokeOptimizer(TypedDict):

        &quot;&quot;&quot;Stoke optimizer wrapper class



        Given all the different backends and extensions the optimizer might need to be instantiated in a different way

        thus this typed dict holds the configuration without instantiation



        Attributes

        ----------

        optimizer: Type[torch.optim.Optimizer]

            un-instantiated torch.optim.Optimizer class

        optimizer_kwargs: Dict

            any keyword args to be unrolled into the optimizer at instantiation time



        &quot;&quot;&quot;



        optimizer: Type[torch.optim.Optimizer]

        optimizer_kwargs: Dict
</code></pre></div>
<h2 id="classes">Classes</h2>
<h3 id="ampconfig">AMPConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">AMPConfig</span><span class="p">(</span>
    <span class="n">backoff_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">growth_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
    <span class="n">growth_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">,</span>
    <span class="n">init_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">65536.0</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>backoff_factor</td>
<td>float, default: 0.5</td>
<td>Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration</td>
<td>None</td>
</tr>
<tr>
<td>growth_factor</td>
<td>float, default: 2.0</td>
<td>Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations.</td>
<td>None</td>
</tr>
<tr>
<td>growth_interval</td>
<td>int, default: 2000</td>
<td>Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by</td>
<td></td>
</tr>
<tr>
<td>growth_factor</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>init_scale</td>
<td>float, default: 2.**16</td>
<td>Initial scale factor</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class AMPConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;PyTorch AMP configuration class



        Attributes

        ----------

        backoff_factor : float, default: 0.5

            Factor by which the scale is multiplied during update if inf/NaN gradients occur in an iteration

        growth_factor : float, default: 2.0

            Factor by which the scale is multiplied during update if no inf/NaN gradients occur for growth_interval consecutive iterations.

        growth_interval : int, default: 2000

            Number of consecutive iterations without inf/NaN gradients that must occur for the scale to be multiplied by

            growth_factor

        init_scale : float, default: 2.**16

            Initial scale factor



        &quot;&quot;&quot;



        backoff_factor: float = 0.5

        growth_factor: float = 2.0

        growth_interval: int = 2000

        init_scale: float = 2.0 ** 16
</code></pre></div>
<hr />
<h3 id="apexconfig">ApexConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ApexConfig</span><span class="p">(</span>
    <span class="n">cast_model_outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">convert_to_sync_batch_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">max_loss_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">16777216.0</span><span class="p">,</span>
    <span class="n">min_loss_scale</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scaler_per_loss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">verbosity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_1">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>cast_model_outputs</td>
<td>Optional[torch.dtype], default: None</td>
<td>Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level</td>
<td>None</td>
</tr>
<tr>
<td>convert_to_sync_batch_norm</td>
<td>bool, default: False</td>
<td>Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls</td>
<td></td>
</tr>
<tr>
<td>https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>max_loss_scale</td>
<td>float, default: 2.**24</td>
<td>Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling</td>
<td>None</td>
</tr>
<tr>
<td>min_loss_scale</td>
<td>Optional[float], default: None</td>
<td>Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None</td>
<td></td>
</tr>
<tr>
<td>means that no floor is imposed</td>
<td>value</td>
<td></td>
<td></td>
</tr>
<tr>
<td>scaler_per_loss</td>
<td>bool, default: False</td>
<td>Option to impose a scaler for each loss instead of a global scaler</td>
<td>None</td>
</tr>
<tr>
<td>verbosity</td>
<td>int, default: 0</td>
<td>Set to 0 to suppress Amp-related output</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ApexConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Nvidia APEX configuration class



        Attributes

        ----------

        cast_model_outputs: Optional[torch.dtype], default: None

            Option to ensure that the outputs of your model(s) are always cast to a particular type regardless of opt_level

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to apex.parallel.SyncBatchNorm calls

            https://nvidia.github.io/apex/parallel.html#apex.parallel.SyncBatchNorm

        max_loss_scale: float, default: 2.**24

            Sets a ceiling for the loss scale values that can be chosen by dynamic loss scaling

        min_loss_scale: Optional[float], default: None

            Sets a floor for the loss scale values that can be chosen by dynamic loss scaling. The default value of None

            means that no floor is imposed

        scaler_per_loss: bool, default: False

            Option to impose a scaler for each loss instead of a global scaler

        verbosity: int, default: 0

            Set to 0 to suppress Amp-related output



        &quot;&quot;&quot;



        cast_model_outputs: Optional[torch.dtype] = None

        convert_to_sync_batch_norm: bool = False

        max_loss_scale: float = 2.0 ** 24

        min_loss_scale: Optional[float] = None

        scaler_per_loss: bool = False

        verbosity: int = 0
</code></pre></div>
<hr />
<h3 id="backendoptions">BackendOptions</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BackendOptions</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class BackendOptions(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Communication backend options&quot;&quot;&quot;



        nccl = &quot;nccl&quot;

        mpi = &quot; mpi&quot;

        gloo = &quot;gloo&quot;
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">gloo</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mpi</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">nccl</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="clipgradconfig">ClipGradConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ClipGradConfig</span><span class="p">(</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_2">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>clip_value</td>
<td>float</td>
<td>maximum allowed absolute value of the gradients [-clip_value, clip_value]</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ClipGradConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Gradient clipping by value configuration class



        Attributes

        ----------

        clip_value: float

            maximum allowed absolute value of the gradients [-clip_value, clip_value]



        &quot;&quot;&quot;



        clip_value: float
</code></pre></div>
<hr />
<h3 id="clipgradnormconfig">ClipGradNormConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ClipGradNormConfig</span><span class="p">(</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_3">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>max_norm</td>
<td>float</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>float</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ClipGradNormConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Gradient clipping by p-norm configuration class



        Attributes

        ----------

        max_norm: float

            max norm of the gradients

        norm_type: float

            type of the used p-norm



        &quot;&quot;&quot;



        max_norm: float

        norm_type: float
</code></pre></div>
<hr />
<h3 id="ddpconfig">DDPConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DDPConfig</span><span class="p">(</span>
    <span class="n">local_rank</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">auto_mpi_discovery</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">convert_to_sync_batch_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">backend</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">BackendOptions</span> <span class="o">=</span> <span class="s1">&#39;nccl&#39;</span><span class="p">,</span>
    <span class="n">broadcast_buffers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">bucket_cap_mb</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span>
    <span class="n">find_unused_parameters</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gradient_as_bucket_view</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;env://&#39;</span><span class="p">,</span>
    <span class="n">no_sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_4">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>local_rank</td>
<td>Optional[int]</td>
<td>Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg)</td>
<td>None</td>
</tr>
<tr>
<td>auto_mpi_discovery</td>
<td>bool, default: False</td>
<td>if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed</td>
<td></td>
</tr>
<tr>
<td>function call)</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>convert_to_sync_batch_norm</td>
<td>bool, default: False</td>
<td>Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls</td>
<td></td>
</tr>
<tr>
<td>https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>backend</td>
<td>BackendOptions, default: 'nccl'</td>
<td>Which communication backend to use</td>
<td>None</td>
</tr>
<tr>
<td>broadcast_buffers</td>
<td>bool, default: True</td>
<td>Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function</td>
<td>None</td>
</tr>
<tr>
<td>bucket_cap_mb</td>
<td>int, default: 25</td>
<td>DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket</td>
<td></td>
</tr>
<tr>
<td>can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB)</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>find_unused_parameters</td>
<td>bool, default: False</td>
<td>Traverse the autograd graph from all tensors contained in the return value of the wrapped module’s forward</td>
<td></td>
</tr>
<tr>
<td>function. Parameters that don’t receive gradients as part of this graph are preemptively marked as being ready</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>to be reduced. Note that all forward outputs that are derived from module parameters must participate in</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>calculating loss and later the gradient computation. If they don’t, this wrapper will hang waiting for autograd</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>can be detached from the autograd graph using torch.Tensor.detach</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradient_as_bucket_view</td>
<td>bool, default: False</td>
<td>When set to True, gradients will be views pointing to different offsets of allreduce communication</td>
<td></td>
</tr>
<tr>
<td>buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>referring to the zero_grad() function in torch/optim/optimizer.py as a solution.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>init_method</td>
<td>str, default: 'env://'</td>
<td>URL specifying how to initialize the process group</td>
<td>None</td>
</tr>
<tr>
<td>no_sync</td>
<td>bool, default: True</td>
<td>for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on</td>
<td></td>
</tr>
<tr>
<td>module variables, which will later be synchronized in the first forward-backward pass after exiting the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>context. no sync might lead to higher memory usage but lower communication overhead</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DDPConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;PyTorch DistributedDataParallel configuration class



        Attributes

        ----------

        local_rank: Optional[int]

            Current local rank of the device (provided here, as LOCAL_RANK env var, or parsed from --local_arg)

        auto_mpi_discovery: bool, default: False

            if distributed environment variables are not set, attempt to discover them from MPI (using underlying deepspeed

            function call)

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to torch.nn.SyncBatchNorm calls

            https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html

        backend: BackendOptions, default: &#39;nccl&#39;

            Which communication backend to use

        broadcast_buffers: bool, default: True

            Flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function

        bucket_cap_mb: int, default: 25

            DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket

            can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB)

        find_unused_parameters: bool, default: False

            Traverse the autograd graph from all tensors contained in the return value of the wrapped module’s forward

            function. Parameters that don’t receive gradients as part of this graph are preemptively marked as being ready

            to be reduced. Note that all forward outputs that are derived from module parameters must participate in

            calculating loss and later the gradient computation. If they don’t, this wrapper will hang waiting for autograd

            to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused

            can be detached from the autograd graph using torch.Tensor.detach

        gradient_as_bucket_view: bool, default: False

            When set to True, gradients will be views pointing to different offsets of allreduce communication

            buckets. This can reduce peak memory usage, where the saved memory size will be equal to the total gradients

            size. Moreover, it avoids the overhead of copying between gradients and allreduce communication buckets. When

            gradients are views, detach_() cannot be called on the gradients. If hitting such errors, please fix it by

            referring to the zero_grad() function in torch/optim/optimizer.py as a solution.

        init_method: str, default: &#39;env://&#39;

            URL specifying how to initialize the process group

        no_sync: bool, default: True

            for any DDP based method (including SDDP and FSDP wrappers -- if activated gradients will be accumulated on

            module variables, which will later be synchronized in the first forward-backward pass after exiting the

            context. no sync might lead to higher memory usage but lower communication overhead



        &quot;&quot;&quot;



        local_rank: Optional[int]

        auto_mpi_discovery: bool = False

        convert_to_sync_batch_norm: bool = False

        backend: BackendOptions = &quot;nccl&quot;

        broadcast_buffers: bool = True

        bucket_cap_mb: int = 25

        find_unused_parameters: bool = False

        gradient_as_bucket_view: bool = False

        init_method: str = &quot;env://&quot;

        no_sync: bool = True
</code></pre></div>
<hr />
<h3 id="deepspeedaioconfig">DeepspeedAIOConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedAIOConfig</span><span class="p">(</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1048576</span><span class="p">,</span>
    <span class="n">ignore_unused_parameters</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">overlap_events</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">queue_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">single_submit</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">thread_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_5">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>block_size</td>
<td>int, default: 1048576</td>
<td>I/O block size in bytes</td>
<td>None</td>
</tr>
<tr>
<td>ignore_unused_parameters</td>
<td>bool, default: True</td>
<td>Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks.</td>
<td></td>
</tr>
<tr>
<td>This controls whether or not training should terminate with an error message when unused parameters are</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>detected.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>overlap_events</td>
<td>bool, default: True</td>
<td>Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests.</td>
<td>None</td>
</tr>
<tr>
<td>queue_depth</td>
<td>int, default: 8</td>
<td>I/O queue depth</td>
<td>None</td>
</tr>
<tr>
<td>single_submit</td>
<td>bool, default: False</td>
<td>Submit requests to storage device as multiple individual requests as opposed to one block of requests.</td>
<td>None</td>
</tr>
<tr>
<td>thread_count</td>
<td>int, default: 1</td>
<td>Intra-request parallelism for each read/write submitted by a user thread.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedAIOConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed asynchronous I/O configuration class



        Attributes

        ----------

        block_size: int, default: 1048576

            I/O block size in bytes

        ignore_unused_parameters: bool, default: True

            Unused parameters in modules may be unexpected in static networks, but could be normal in dynamic networks.

            This controls whether or not training should terminate with an error message when unused parameters are

            detected.

        overlap_events: bool, default: True

            Submit requests to storage device in an overlapped fashion without waiting for completion of earlier requests.

        queue_depth: int, default: 8

            I/O queue depth

        single_submit: bool, default: False

            Submit requests to storage device as multiple individual requests as opposed to one block of requests.

        thread_count: int, default: 1

            Intra-request parallelism for each read/write submitted by a user thread.



        &quot;&quot;&quot;



        block_size: int = 1048576

        ignore_unused_parameters: bool = True

        overlap_events: bool = True

        queue_depth: int = 8

        single_submit: bool = False

        thread_count: int = 1
</code></pre></div>
<hr />
<h3 id="deepspeedactivationcheckpointingconfig">DeepspeedActivationCheckpointingConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedActivationCheckpointingConfig</span><span class="p">(</span>
    <span class="n">contiguous_memory_optimization</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">cpu_checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">number_checkpoints</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">partition_activations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">profile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">synchronize_checkpoint_boundary</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_6">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>contiguous_memory_optimization</td>
<td>bool, default: False</td>
<td>Copies partitioned activations so that they are contiguous in memory</td>
<td>None</td>
</tr>
<tr>
<td>cpu_checkpointing</td>
<td>bool, default: False</td>
<td>Offloads partitioned activations to CPU if partition_activations is enabled</td>
<td>None</td>
</tr>
<tr>
<td>number_checkpoints</td>
<td>Optional[int], default: None</td>
<td>Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization</td>
<td>None</td>
</tr>
<tr>
<td>partition_activations</td>
<td>bool, default: False</td>
<td>Enables partition activation when used with model parallelism</td>
<td>None</td>
</tr>
<tr>
<td>profile</td>
<td>bool, default: False</td>
<td>Logs the forward and backward time for each checkpoint function</td>
<td>None</td>
</tr>
<tr>
<td>synchronize_checkpoint_boundary</td>
<td>bool, default: False</td>
<td>Inserts torch.cuda.synchronize() at each checkpoint boundary</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedActivationCheckpointingConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed activation checkpointing configuration class



        Attributes

        ----------

        contiguous_memory_optimization: bool, default: False

            Copies partitioned activations so that they are contiguous in memory

        cpu_checkpointing: bool, default: False

            Offloads partitioned activations to CPU if partition_activations is enabled

        number_checkpoints: Optional[int], default: None

            Total number of activation checkpoints used to allocate memory buffer for contiguous_memoty_optimization

        partition_activations: bool, default: False

            Enables partition activation when used with model parallelism

        profile: bool, default: False

            Logs the forward and backward time for each checkpoint function

        synchronize_checkpoint_boundary: bool, default: False

            Inserts torch.cuda.synchronize() at each checkpoint boundary



        &quot;&quot;&quot;



        contiguous_memory_optimization: bool = False

        cpu_checkpointing: bool = False

        number_checkpoints: Optional[int] = None

        partition_activations: bool = False

        profile: bool = False

        synchronize_checkpoint_boundary: bool = False
</code></pre></div>
<hr />
<h3 id="deepspeedconfig">DeepspeedConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedConfig</span><span class="p">(</span>
    <span class="n">activation_checkpointing</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedActivationCheckpointingConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="n">DeepspeedActivationCheckpointingConfig</span><span class="p">(</span><span class="n">contiguous_memory_optimization</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cpu_checkpointing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">number_checkpoints</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">partition_activations</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">profile</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">synchronize_checkpoint_boundary</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="n">aio</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedAIOConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="n">DeepspeedAIOConfig</span><span class="p">(</span><span class="n">block_size</span><span class="o">=</span><span class="mi">1048576</span><span class="p">,</span> <span class="n">ignore_unused_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">overlap_events</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">queue_depth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">single_submit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">thread_count</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">auto_mpi_discovery</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">disable_allgather</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dist_backend</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">BackendOptions</span> <span class="o">=</span> <span class="s1">&#39;nccl&#39;</span><span class="p">,</span>
    <span class="n">distributed_port</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">29500</span><span class="p">,</span>
    <span class="n">dump_state</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">flops_profiler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedFlopsConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fp16</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedFP16Config</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fp32_allreduce</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gradient_predivide_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">init_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;env://&#39;</span><span class="p">,</span>
    <span class="n">prescale_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">progressive_layer_drop</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedPLDConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sparse_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">steps_per_print</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">tensorboard</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedTensorboardConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">wall_clock_breakdown</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">zero_optimization</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedZeROConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="n">DeepspeedZeROConfig</span><span class="p">(</span><span class="n">allgather_bucket_size</span><span class="o">=</span><span class="mi">500000000</span><span class="p">,</span> <span class="n">allgather_partitions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">contiguous_gradients</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_unused_parameters</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">legacy_stage1</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">offload_optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offload_param</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">overlap_comm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduce_bucket_size</span><span class="o">=</span><span class="mi">500000000</span><span class="p">,</span> <span class="n">reduce_scatter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stage3_max_live_parameters</span><span class="o">=</span><span class="mi">1000000000</span><span class="p">,</span> <span class="n">stage3_max_reuse_distance</span><span class="o">=</span><span class="mi">1000000000</span><span class="p">,</span> <span class="n">stage3_prefetch_bucket_size</span><span class="o">=</span><span class="mi">500000000</span><span class="p">,</span> <span class="n">stage3_param_persistence_threshold</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">stage3_gather_fp16_weights_on_model_save</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sub_group_size</span><span class="o">=</span><span class="mi">1000000000000</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_7">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>activation_checkpointing</td>
<td>Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig()</td>
<td>Enables and configures activation checkpointing</td>
<td>None</td>
</tr>
<tr>
<td>aio</td>
<td>Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig()</td>
<td>Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent</td>
<td></td>
</tr>
<tr>
<td>(NVMe) storage</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>auto_mpi_discovery</td>
<td>bool, default: True</td>
<td>if distributed environment variables are not set, attempt to discover them from MPI</td>
<td>None</td>
</tr>
<tr>
<td>disable_allgather</td>
<td>bool, default: False</td>
<td>Disables allgather</td>
<td>None</td>
</tr>
<tr>
<td>dist_backend</td>
<td>BackendOptions, default: 'nccl'</td>
<td>Which communication backend to use</td>
<td>None</td>
</tr>
<tr>
<td>distributed_port</td>
<td>int, default: 29500</td>
<td>torch distributed backend port</td>
<td>None</td>
</tr>
<tr>
<td>dump_state</td>
<td>bool, default: False</td>
<td>Print out state information of DeepSpeed object after initialization</td>
<td>None</td>
</tr>
<tr>
<td>flops_profiler</td>
<td>Optional[DeepspeedFlopsConfig], default: None</td>
<td>Enables and configures the flops profiler. This would also enable wall_clock_breakdown</td>
<td>None</td>
</tr>
<tr>
<td>fp16</td>
<td>Optional[DeepspeedFP16Config], default: None</td>
<td>Enables and configures mixed precision/FP16 training that leverages NVIDIA’s Apex package</td>
<td>None</td>
</tr>
<tr>
<td>fp32_allreduce</td>
<td>bool, default: False</td>
<td>During gradient averaging perform allreduce with 32 bit values</td>
<td>None</td>
</tr>
<tr>
<td>gradient_predivide_factor</td>
<td>float, default: 1.0</td>
<td>Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability</td>
<td></td>
</tr>
<tr>
<td>when scaling to large numbers of GPUs</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>init_method</td>
<td>str, default: 'env://'</td>
<td>URL specifying how to initialize the process group</td>
<td>None</td>
</tr>
<tr>
<td>prescale_gradients</td>
<td>float, default: 1.0</td>
<td>Scale gradients before doing allreduce</td>
<td>None</td>
</tr>
<tr>
<td>progressive_layer_drop</td>
<td>Optional[DeepspeedPLDConfig], default: None</td>
<td>Enables and configures progressive layer dropping</td>
<td>None</td>
</tr>
<tr>
<td>sparse_gradients</td>
<td>bool, default: False</td>
<td>Enable sparse compression of torch.nn.Embedding gradients</td>
<td>None</td>
</tr>
<tr>
<td>steps_per_print</td>
<td>int, default: 10</td>
<td>Print train loss every N steps</td>
<td>None</td>
</tr>
<tr>
<td>tensorboard</td>
<td>Optional[DeepspeedTensorboardConfig], default: None</td>
<td>Enables and configures tensorboard support</td>
<td>None</td>
</tr>
<tr>
<td>verbose</td>
<td>bool, default: True</td>
<td>flag to make deepspeed engine verbose with information</td>
<td>None</td>
</tr>
<tr>
<td>wall_clock_breakdown</td>
<td>bool, default: False</td>
<td>Enable timing of the latency of forward/backward/update training phases</td>
<td>None</td>
</tr>
<tr>
<td>zero_optimization</td>
<td>Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig()</td>
<td>Enables and configures ZeRO memory optimizations</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed configuration class



        Composed of other configuration classes related to specific functionality



        Attributes

        ----------

        activation_checkpointing: Optional[DeepspeedActivationCheckpointingConfig], default: DeepspeedActivationCheckpointingConfig()

            Enables and configures activation checkpointing

        aio: Optional[DeepspeedAIOConfig], default: DeepspeedAIOConfig()

            Configuring the asynchronous I/O module for offloading parameter and optimizer states to persistent

            (NVMe) storage

        auto_mpi_discovery: bool, default: True

            if distributed environment variables are not set, attempt to discover them from MPI

        disable_allgather: bool, default: False

            Disables allgather

        dist_backend: BackendOptions, default: &#39;nccl&#39;

            Which communication backend to use

        distributed_port: int, default: 29500

            torch distributed backend port

        dump_state: bool, default: False

            Print out state information of DeepSpeed object after initialization

        flops_profiler: Optional[DeepspeedFlopsConfig], default: None

            Enables and configures the flops profiler. This would also enable wall_clock_breakdown

        fp16: Optional[DeepspeedFP16Config], default: None

            Enables and configures mixed precision/FP16 training that leverages NVIDIA’s Apex package

        fp32_allreduce: bool, default: False

            During gradient averaging perform allreduce with 32 bit values

        gradient_predivide_factor: float, default: 1.0

            Before gradient averaging predivide gradients by a specified factor, can sometimes help with fp16 stability

            when scaling to large numbers of GPUs

        init_method: str, default: &#39;env://&#39;

            URL specifying how to initialize the process group

        prescale_gradients: float, default: 1.0

            Scale gradients before doing allreduce

        progressive_layer_drop: Optional[DeepspeedPLDConfig], default: None

            Enables and configures progressive layer dropping

        sparse_gradients: bool, default: False

            Enable sparse compression of torch.nn.Embedding gradients

        steps_per_print: int, default: 10

            Print train loss every N steps

        tensorboard: Optional[DeepspeedTensorboardConfig], default: None

            Enables and configures tensorboard support

        verbose: bool, default: True

            flag to make deepspeed engine verbose with information

        wall_clock_breakdown: bool, default: False

            Enable timing of the latency of forward/backward/update training phases

        zero_optimization: Optional[DeepspeedZeROConfig], default: DeepspeedZeROConfig()

            Enables and configures ZeRO memory optimizations



        Notes

        -----

        Deepspeed does not use Apex’s AMP mode whihc allows for more flexibility in mixed precision training modes. FP16

        here is similar to AMP’s O2 mode



        &quot;&quot;&quot;



        activation_checkpointing: Optional[

            DeepspeedActivationCheckpointingConfig

        ] = DeepspeedActivationCheckpointingConfig()

        aio: Optional[DeepspeedAIOConfig] = DeepspeedAIOConfig()

        auto_mpi_discovery: bool = True

        disable_allgather: bool = False

        dist_backend: BackendOptions = &quot;nccl&quot;

        distributed_port: int = 29500

        dump_state: bool = False

        flops_profiler: Optional[DeepspeedFlopsConfig] = None

        fp16: Optional[DeepspeedFP16Config] = None

        fp32_allreduce: bool = False

        gradient_predivide_factor: float = 1.0

        init_method: str = &quot;env://&quot;

        prescale_gradients: bool = False

        progressive_layer_drop: Optional[DeepspeedPLDConfig] = None

        sparse_gradients: bool = False

        steps_per_print: int = 10

        tensorboard: Optional[DeepspeedTensorboardConfig] = None

        verbose: bool = True

        wall_clock_breakdown: bool = False

        zero_optimization: Optional[DeepspeedZeROConfig] = DeepspeedZeROConfig()
</code></pre></div>
<hr />
<h3 id="deepspeedfp16config">DeepspeedFP16Config</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedFP16Config</span><span class="p">(</span>
    <span class="n">hysteresis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">initial_scale_power</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">loss_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">loss_scale_window</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="n">min_loss_scale</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_8">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>hysteresis</td>
<td>int, default: 2</td>
<td>represents the delay shift in dynamic loss scaling</td>
<td>None</td>
</tr>
<tr>
<td>initial_scale_power</td>
<td>int, default: 32</td>
<td>power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power</td>
<td>None</td>
</tr>
<tr>
<td>loss_scale</td>
<td>float, default: 0.0</td>
<td>loss scaling value for FP16 training (0.0 --&gt; dynamic scaling)</td>
<td>None</td>
</tr>
<tr>
<td>loss_scale_window</td>
<td>int, default: 1000</td>
<td>the window over which to raise/lower the dynamic loss scale value</td>
<td>None</td>
</tr>
<tr>
<td>min_loss_scale</td>
<td>int, default: 1000</td>
<td>minimum dynamic loss scale value</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedFP16Config:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed FP16 configuration class



        Attributes

        ----------

        hysteresis: int, default: 2

            represents the delay shift in dynamic loss scaling

        initial_scale_power: int, default: 32

            power of the initial dynamic loss scale value. The actual loss scale is computed as 2 ** initial_scale_power

        loss_scale: float, default: 0.0

            loss scaling value for FP16 training (0.0 --&gt; dynamic scaling)

        loss_scale_window: int, default: 1000

            the window over which to raise/lower the dynamic loss scale value

        min_loss_scale: int, default: 1000

            minimum dynamic loss scale value



        &quot;&quot;&quot;



        hysteresis: int = 2

        initial_scale_power: int = 32

        loss_scale: float = 0.0

        loss_scale_window: int = 1000

        min_loss_scale: int = 1000
</code></pre></div>
<hr />
<h3 id="deepspeedflopsconfig">DeepspeedFlopsConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedFlopsConfig</span><span class="p">(</span>
    <span class="n">detailed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">module_depth</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">output_file</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">profile_step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">top_modules</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_9">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>detailed</td>
<td>bool, default: True</td>
<td>Whether to print the detailed model profile</td>
<td>None</td>
</tr>
<tr>
<td>module_depth</td>
<td>int, default: -1</td>
<td>The depth of the model at which to print the aggregated module information. When set to -1, it prints</td>
<td></td>
</tr>
<tr>
<td>information from the top module to the innermost modules (the maximum depth).</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>output_file</td>
<td>Optional[str], default: None</td>
<td>Path to the output file. If None, the profiler prints to stdout</td>
<td>None</td>
</tr>
<tr>
<td>profile_step</td>
<td>int, default: 1</td>
<td>The global training step at which to profile.</td>
<td>None</td>
</tr>
<tr>
<td>top_modules</td>
<td>int, default: 1</td>
<td>Limits the aggregated profile output to the number of top modules specified.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedFlopsConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed flops profiler configuration class



        Attributes

        ----------

        detailed: bool, default: True

            Whether to print the detailed model profile

        module_depth: int, default: -1

            The depth of the model at which to print the aggregated module information. When set to -1, it prints

            information from the top module to the innermost modules (the maximum depth).

        output_file: Optional[str], default: None

            Path to the output file. If None, the profiler prints to stdout

        profile_step: int, default: 1

            The global training step at which to profile.

        top_modules: int, default: 1

            Limits the aggregated profile output to the number of top modules specified.



        Notes

        -----

        Warm up steps are needed for accurate time measurement



        &quot;&quot;&quot;



        detailed: bool = True

        module_depth: int = -1

        output_file: Optional[str] = None

        profile_step: int = 1

        top_modules: int = 1
</code></pre></div>
<hr />
<h3 id="deepspeedoffloadoptimizerconfig">DeepspeedOffloadOptimizerConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedOffloadOptimizerConfig</span><span class="p">(</span>
    <span class="n">buffer_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">OffloadDevice</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span>
    <span class="n">fast_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">nvme_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;/local_nvme&#39;</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">pipeline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">pipeline_read</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">pipeline_write</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_10">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>buffer_count</td>
<td>int, default: 4</td>
<td>Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number</td>
<td></td>
</tr>
<tr>
<td>of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter,</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradient, momentum, and variance).</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>device</td>
<td>OffloadDevice, default: 'cpu'</td>
<td>Device memory to offload optimizer state</td>
<td>None</td>
</tr>
<tr>
<td>fast_init</td>
<td>bool, default: False</td>
<td>Enable fast optimizer initialization when offloading to NVMe</td>
<td>None</td>
</tr>
<tr>
<td>nvme_path</td>
<td>str, default: '/local_nvme'</td>
<td>Filesystem path for NVMe device for optimizer state offloading</td>
<td>None</td>
</tr>
<tr>
<td>pin_memory</td>
<td>bool, default: False</td>
<td>Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.</td>
<td>None</td>
</tr>
<tr>
<td>pipeline</td>
<td>bool, default: False</td>
<td>pipeline activated (will default to True if either pipeline_read or pipeline_write is set</td>
<td>to</td>
</tr>
<tr>
<td>pipeline_read</td>
<td>bool, default: False</td>
<td>activate pipeline read (deepspeed has limited docs for what this does)</td>
<td>None</td>
</tr>
<tr>
<td>pipeline_write</td>
<td>bool, default: False</td>
<td>activate pipeline write(deepspeed has limited docs for what this does)</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedOffloadOptimizerConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed optimizer offloading configuration class



        Attributes

        ----------

        buffer_count: int, default: 4

            Number of buffers in buffer pool for optimizer state offloading to NVMe. This should be at least the number

            of states maintained per parameter by the optimizer. For example, Adam optimizer has 4 states (parameter,

            gradient, momentum, and variance).

        device: OffloadDevice, default: &#39;cpu&#39;

            Device memory to offload optimizer state

        fast_init: bool, default: False

            Enable fast optimizer initialization when offloading to NVMe

        nvme_path: str, default: &#39;/local_nvme&#39;

            Filesystem path for NVMe device for optimizer state offloading

        pin_memory: bool, default: False

            Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.

        pipeline: bool, default: False

            pipeline activated (will default to True if either pipeline_read or pipeline_write is set

        pipeline_read: bool, default: False

            activate pipeline read (deepspeed has limited docs for what this does)

        pipeline_write: bool, default: False

            activate pipeline write(deepspeed has limited docs for what this does)



        &quot;&quot;&quot;



        buffer_count: int = 4

        device: OffloadDevice = &quot;cpu&quot;

        fast_init: bool = False

        nvme_path: str = &quot;/local_nvme&quot;

        pin_memory: bool = False

        pipeline: bool = False

        pipeline_read: bool = False

        pipeline_write: bool = False
</code></pre></div>
<hr />
<h3 id="deepspeedoffloadparamconfig">DeepspeedOffloadParamConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedOffloadParamConfig</span><span class="p">(</span>
    <span class="n">buffer_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100000000</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">OffloadDevice</span> <span class="o">=</span> <span class="s1">&#39;cpu&#39;</span><span class="p">,</span>
    <span class="n">max_in_cpu</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000000</span><span class="p">,</span>
    <span class="n">nvme_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;/local_nvme&#39;</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_11">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>buffer_count</td>
<td>int, default: 5</td>
<td>Number of buffers in buffer pool for parameter offloading to NVMe</td>
<td>None</td>
</tr>
<tr>
<td>buffer_size</td>
<td>int, default: int(1E8)</td>
<td>Size of buffers in buffer pool for parameter offloading to NVMe</td>
<td>None</td>
</tr>
<tr>
<td>device</td>
<td>OffloadDevice, default: 'cpu'</td>
<td>Device memory to offload model parameters</td>
<td>None</td>
</tr>
<tr>
<td>max_in_cpu</td>
<td>int, default: int(1E9)</td>
<td>Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.</td>
<td>None</td>
</tr>
<tr>
<td>nvme_path</td>
<td>str, default: '/local_nvme'</td>
<td>Filesystem path for NVMe device for parameter offloading</td>
<td>None</td>
</tr>
<tr>
<td>pin_memory</td>
<td>bool, default: False</td>
<td>Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedOffloadParamConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed parameter offloading configuration class



        Attributes

        ----------

        buffer_count: int, default: 5

            Number of buffers in buffer pool for parameter offloading to NVMe

        buffer_size: int, default: int(1E8)

            Size of buffers in buffer pool for parameter offloading to NVMe

        device: OffloadDevice, default: &#39;cpu&#39;

            Device memory to offload model parameters

        max_in_cpu: int, default: int(1E9)

            Number of parameter elements to maintain in CPU memory when offloading to NVMe is enabled.

        nvme_path: str, default: &#39;/local_nvme&#39;

            Filesystem path for NVMe device for parameter offloading

        pin_memory: bool, default: False

            Offload to page-locked CPU memory. This could boost throughput at the cost of extra memory overhead.



        &quot;&quot;&quot;



        buffer_count: int = 5

        buffer_size: int = int(1e8)

        device: OffloadDevice = &quot;cpu&quot;

        max_in_cpu: int = int(1e9)

        nvme_path: str = &quot;/local_nvme&quot;

        pin_memory: bool = False
</code></pre></div>
<hr />
<h3 id="deepspeedpldconfig">DeepspeedPLDConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedPLDConfig</span><span class="p">(</span>
    <span class="n">theta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_12">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>theta</td>
<td>float, default: 1.0</td>
<td>Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value,</td>
<td></td>
</tr>
<tr>
<td>the faster the training speed</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>gamma</td>
<td>float, default: 0.001</td>
<td>Hyper-parameter that controls how fast the drop ratio increases</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedPLDConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;

        Attributes

        ----------

        theta: float, default: 1.0

            Hyper-parameter that controls the trade-off between training time and robustness. The lower the theta value,

            the faster the training speed

        gamma: float, default: 0.001

            Hyper-parameter that controls how fast the drop ratio increases



        &quot;&quot;&quot;



        theta: float = 1.0

        gamma: float = 0.001
</code></pre></div>
<hr />
<h3 id="deepspeedtensorboardconfig">DeepspeedTensorboardConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedTensorboardConfig</span><span class="p">(</span>
    <span class="n">output_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
    <span class="n">job_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;DeepSpeedJobName&#39;</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_13">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>output_path</td>
<td>str, default: ''</td>
<td>Tensorboard output path</td>
<td>None</td>
</tr>
<tr>
<td>job_name</td>
<td>str, default: 'DeepSpeedJobName'</td>
<td>Tensorboard job name</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedTensorboardConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed Tensorboard configuration class



        Attributes

        ----------

        output_path: str, default: &#39;&#39;

            Tensorboard output path

        job_name: str, default: &#39;DeepSpeedJobName&#39;

            Tensorboard job name



        &quot;&quot;&quot;



        output_path: str = &quot;&quot;

        job_name: str = &quot;DeepSpeedJobName&quot;
</code></pre></div>
<hr />
<h3 id="deepspeedzeroconfig">DeepspeedZeROConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedZeROConfig</span><span class="p">(</span>
    <span class="n">allgather_bucket_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500000000</span><span class="p">,</span>
    <span class="n">allgather_partitions</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">contiguous_gradients</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">ignore_unused_parameters</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">legacy_stage1</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">offload_optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedOffloadOptimizerConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">offload_param</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DeepspeedOffloadParamConfig</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">overlap_comm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">reduce_bucket_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500000000</span><span class="p">,</span>
    <span class="n">reduce_scatter</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">stage</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">stage3_max_live_parameters</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000000</span><span class="p">,</span>
    <span class="n">stage3_max_reuse_distance</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000000</span><span class="p">,</span>
    <span class="n">stage3_prefetch_bucket_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500000000</span><span class="p">,</span>
    <span class="n">stage3_param_persistence_threshold</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000</span><span class="p">,</span>
    <span class="n">stage3_gather_fp16_weights_on_model_save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sub_group_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000000000000</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_14">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>allgather_bucket_size</td>
<td>int, default: int(5E8)</td>
<td>Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes</td>
<td>None</td>
</tr>
<tr>
<td>allgather_partitions</td>
<td>bool, default: True</td>
<td>Chooses between allgather collective or a series of broadcast collectives to gather updated parameters</td>
<td></td>
</tr>
<tr>
<td>from all the GPUs at the end of each step</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>contiguous_gradients</td>
<td>bool, default: False</td>
<td>Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward</td>
<td></td>
</tr>
<tr>
<td>pass. Only useful when running very large models.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ignore_unused_parameters</td>
<td>bool, default: True</td>
<td>Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload</td>
<td></td>
</tr>
<tr>
<td>Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>legacy_stage1</td>
<td>bool, default: False</td>
<td>Use deepspeed &lt; v0.3.17 zero stage 1, kept for backwards compatability reasons</td>
<td>None</td>
</tr>
<tr>
<td>offload_optimizer</td>
<td>Optional[DeepspeedOffloadOptimizerConfig], default: None</td>
<td>Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU</td>
<td></td>
</tr>
<tr>
<td>memory for larger models or batch sizes. Valid only with stage 3</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>offload_param</td>
<td>Optional[DeepspeedOffloadParamConfig], default: None</td>
<td>Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch</td>
<td></td>
</tr>
<tr>
<td>sizes. Valid only with stage 3.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>overlap_comm</td>
<td>bool, default: False</td>
<td>Attempts to overlap the reduction of the gradients with backward computation</td>
<td>None</td>
</tr>
<tr>
<td>reduce_bucket_size</td>
<td>int, default: int(5E8)</td>
<td>Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large</td>
<td></td>
</tr>
<tr>
<td>model sizes</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>reduce_scatter</td>
<td>bool, default: True</td>
<td>Uses reduce or reduce scatter instead of allreduce to average gradients</td>
<td>None</td>
</tr>
<tr>
<td>stage</td>
<td>int, default: 0</td>
<td>Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state</td>
<td></td>
</tr>
<tr>
<td>partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning,</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>respectively</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_max_live_parameters</td>
<td>int, default: int(1E9)</td>
<td>The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but</td>
<td></td>
</tr>
<tr>
<td>perform more communication.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_max_reuse_distance</td>
<td>int, default: int(1E9)</td>
<td>Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less</td>
<td></td>
</tr>
<tr>
<td>memory, but perform more communication.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_prefetch_bucket_size</td>
<td>int, default: int(5E8)</td>
<td>The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase</td>
<td></td>
</tr>
<tr>
<td>stalls due to communication.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_param_persistence_threshold</td>
<td>int, default: int(1E6)</td>
<td>Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly</td>
<td></td>
</tr>
<tr>
<td>increase communication (especially latency-bound messages).</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>stage3_gather_fp16_weights_on_model_save</td>
<td>bool, default: False</td>
<td>Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned</td>
<td></td>
</tr>
<tr>
<td>across GPUs, they aren’t part of state_dict, so this function automatically gather the weights when this</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>option is enabled and then saves the fp16 model weights.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>sub_group_size</td>
<td>int, default: int(1E12)</td>
<td>sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are</td>
<td></td>
</tr>
<tr>
<td>grouped into buckets of sub_group_size and each buckets is updated one at a time.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedZeROConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Deepspeed ZeRO configuration class



        Attributes

        ----------

        allgather_bucket_size: int, default: int(5E8)

            Number of elements allgathered at a time. Limits the memory required for the allgather for large model sizes

        allgather_partitions: bool, default: True

            Chooses between allgather collective or a series of broadcast collectives to gather updated parameters

            from all the GPUs at the end of each step

        contiguous_gradients: bool, default: False

            Copies the gradients to a contiguous buffer as they are produced. Avoids memory fragmentation during backward

            pass. Only useful when running very large models.

        ignore_unused_parameters: bool, default: True

            Now just used in stage2 complete_grad_norm_calculation_for_cpu_offload

            Enable this option to avoid -- https://github.com/microsoft/DeepSpeed/issues/707

        legacy_stage1: bool, default: False

            Use deepspeed &lt; v0.3.17 zero stage 1, kept for backwards compatability reasons

        offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig], default: None

            Enable offloading of optimizer state to CPU or NVMe, and optimizer computation to CPU. This frees up GPU

            memory for larger models or batch sizes. Valid only with stage 3

        offload_param: Optional[DeepspeedOffloadParamConfig], default: None

            Enable offloading of model parameters to CPU or NVMe. This frees up GPU memory for larger models or batch

            sizes. Valid only with stage 3.

        overlap_comm: bool, default: False

            Attempts to overlap the reduction of the gradients with backward computation

        reduce_bucket_size: int, default: int(5E8)

            Number of elements reduced/allreduced at a time. Limits the memory required for the allgather for large

            model sizes

        reduce_scatter: bool, default: True

            Uses reduce or reduce scatter instead of allreduce to average gradients

        stage: int, default: 0

            Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer to disabled, optimizer state

            partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning,

            respectively

        stage3_max_live_parameters: int, default: int(1E9)

            The maximum number of parameters resident per GPU before releasing. Smaller values use less memory, but

            perform more communication.

        stage3_max_reuse_distance: int, default: int(1E9)

            Do not release a parameter if it will be reused within this threshold of parameters. Smaller values use less

            memory, but perform more communication.

        stage3_prefetch_bucket_size: int, default: int(5E8)

            The size of the fixed buffer for prefetching parameters. Smaller values use less memory, but can increase

            stalls due to communication.

        stage3_param_persistence_threshold: int, default: int(1E6)

            Do not partition parameters smaller than this threshold. Smaller values use less memory, but can greatly

            increase communication (especially latency-bound messages).

        stage3_gather_fp16_weights_on_model_save: bool, default: False

            Consolidate the weights before saving the model by save_fp16_model(). Since the weights are partitioned

            across GPUs, they aren’t part of state_dict, so this function automatically gather the weights when this

            option is enabled and then saves the fp16 model weights.

        sub_group_size: int, default: int(1E12)

            sub_group_size controls the granularity in which parameters are updated during optimizer steps. Parameters are

            grouped into buckets of sub_group_size and each buckets is updated one at a time.



        &quot;&quot;&quot;



        allgather_bucket_size: int = int(5e8)

        allgather_partitions: bool = True

        contiguous_gradients: bool = False

        ignore_unused_parameters: bool = True

        legacy_stage1: bool = False

        offload_optimizer: Optional[DeepspeedOffloadOptimizerConfig] = None

        offload_param: Optional[DeepspeedOffloadParamConfig] = None

        overlap_comm: bool = False

        reduce_bucket_size: int = int(5e8)

        reduce_scatter: bool = True

        stage: int = 0

        stage3_max_live_parameters: int = int(1e9)

        stage3_max_reuse_distance: int = int(1e9)

        stage3_prefetch_bucket_size: int = int(5e8)

        stage3_param_persistence_threshold: int = int(1e6)

        stage3_gather_fp16_weights_on_model_save: bool = False

        sub_group_size: int = int(1e12)
</code></pre></div>
<hr />
<h3 id="fairscalefsdpconfig">FairscaleFSDPConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleFSDPConfig</span><span class="p">(</span>
    <span class="n">bucket_cap_mb</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span>
    <span class="n">buffer_dtype</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">clear_autocast_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">compute_dtype</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">flatten_parameters</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">force_input_to_fp32</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fp32_reduce_scatter</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gradient_predivide_factor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gradient_postdivide_factor</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">move_grads_to_cpu</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">move_params_to_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">no_broadcast_optim_state</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">reshard_after_forward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_15">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>bucket_cap_mb</td>
<td>int, default: 25</td>
<td>FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters.</td>
<td></td>
</tr>
<tr>
<td>bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>the backward pass and freed at the end of the backward pass to save more memory for other phases of the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>backward. In FSDP, the buffer size does not change with model size (it changes based on number of</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><dtype, device, process_group> tuples) and gradient ready order matters little since FSDP has a final flush</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>compute is done differently too. Values &lt;= 0 disable bucketing</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>buffer_dtype</td>
<td>Optional[torch.dtype], default: None</td>
<td>dtype for buffers for computation. defaults to value of compute_dtype</td>
<td>value</td>
</tr>
<tr>
<td>clear_autocast_cache</td>
<td>bool, default: False</td>
<td>When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast</td>
<td></td>
</tr>
<tr>
<td>maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GPU memory</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>compute_dtype</td>
<td>Optional[torch.dtype], default: None</td>
<td>dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set,</td>
<td></td>
</tr>
<tr>
<td>in which case it defaults to torch.float16.</td>
<td>torch.float32</td>
<td></td>
<td></td>
</tr>
<tr>
<td>flatten_parameters</td>
<td>bool, default: True</td>
<td>flatten parameters into a single contiguous tensor, which improves training speed</td>
<td>None</td>
</tr>
<tr>
<td>force_input_to_fp32</td>
<td>bool, default: False:</td>
<td>force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision</td>
<td></td>
</tr>
<tr>
<td>mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>fp32_reduce_scatter</td>
<td>bool, default: False</td>
<td>reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used</td>
<td>None</td>
</tr>
<tr>
<td>gradient_predivide_factor</td>
<td>Optional[float], default: None</td>
<td>divide factor before the reduction</td>
<td>None</td>
</tr>
<tr>
<td>gradient_postdivide_factor</td>
<td>Optional[float], default: None</td>
<td>divide factor after the reduction</td>
<td>None</td>
</tr>
<tr>
<td>move_grads_to_cpu</td>
<td>Optional[bool], default: None</td>
<td>move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used</td>
<td>None</td>
</tr>
<tr>
<td>move_params_to_cpu</td>
<td>bool, default: False</td>
<td>offload FP32 params to CPU. This is only relevant when FP16 AMP is used</td>
<td>None</td>
</tr>
<tr>
<td>no_broadcast_optim_state</td>
<td>Optional[bool], default: False</td>
<td>do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this</td>
<td></td>
</tr>
<tr>
<td>true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>parameters can fit on one node</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>reshard_after_forward</td>
<td>bool, default: True</td>
<td>reshard parameters after the forward pass. This saves memory but slows training. This is only relevant</td>
<td></td>
</tr>
<tr>
<td>when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html)</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>verbose</td>
<td>bool, default: True</td>
<td>turn on verbose output for model’s string representation</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleFSDPConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Fairscale Fully Sharded Data Parallel configuration class



        Attributes

        ----------

        bucket_cap_mb: int, default: 25

            FSDP will bucket parameters so that gradient reduction can be more efficient for small parameters.

            bucket_cap_mb controls the bucket size in MegaBytes (MB). Buckets are sub-divided based on world_size, so the

            max shard size is roughly bucket_cap_mb / world_size. There is one bucketer (with potentially multiple

            bucket_cap_mb sized buffers shared by all FSDP instances. Large gradient tensors are directly reduced without

            using the buffers. The buffers are there to reduce communication overhead for small tensors. Overlapping with

            computation happens due to use of a different CUDA stream than the computation CUDA stream. The total memory

            overhead per buffer is around bucket_cap_mb / world_size * (world_size + 1). The buffers are allocated during

            the backward pass and freed at the end of the backward pass to save more memory for other phases of the

            training process. Note, the memory vs. speed tradeoff of bucket size is very different from that of the DDP

            engine. In DDP, the buffer size 1MB + n*cap_mb, until n is big enough to cover the entire model size. The

            order of which buffer is ready there is more rigid and DDP requires all gradients to be computed in the

            backward. In FSDP, the buffer size does not change with model size (it changes based on number of

            &lt;dtype, device, process_group&gt; tuples) and gradient ready order matters little since FSDP has a final flush

            call that ensures everything is reduced and not all gradients need to be upfront known. Overlapping with

            compute is done differently too. Values &lt;= 0 disable bucketing

        buffer_dtype: Optional[torch.dtype], default: None

            dtype for buffers for computation. defaults to value of compute_dtype

        clear_autocast_cache: bool, default: False

            When using mixed precision training with FP16 AMP, if the model weights are in FP32, autocast

            maintains a cache for downcasted weights. The cache can cause GPU OOM during the forward pass. Setting this

            flag to true will help clearing this cache as inner FSDP instances finish part of the forward pass to save

            GPU memory

        compute_dtype: Optional[torch.dtype], default: None

            dtype for full parameters for computation. This defaults to torch.float32 unless FP 16 AMP is set,

            in which case it defaults to torch.float16.

        flatten_parameters: bool, default: True

            flatten parameters into a single contiguous tensor, which improves training speed

        force_input_to_fp32: bool, default: False:

            force input floating point tensors to be FP32 (if they are FP16) when the FSDP instance is in full precision

            mode. This helps avoid issues of running SyncBatchNorm with AMP and checkpoint_wrapper.

        fp32_reduce_scatter: bool, default: False

            reduce-scatter gradients in FP32. This is only relevant when FP16 AMP is used

        gradient_predivide_factor: Optional[float], default: None

            divide factor before the reduction

        gradient_postdivide_factor: Optional[float], default: None

            divide factor after the reduction

        move_grads_to_cpu: Optional[bool], default: None

            move gradient shard to CPU after reduction. This is only relevant when FP16 AMP is used

        move_params_to_cpu: bool, default: False

            offload FP32 params to CPU. This is only relevant when FP16 AMP is used

        no_broadcast_optim_state: Optional[bool], default: False

            do not broadcast this modules optimizer state when gather_full_optim_state_dict is called. If you set this

            true, you are expected to overwrite the relevant state entries of the returned optimizer state dict with the

            proper state at each rank. This is useful for situations, like Mixture Of Experts, where all but a few

            parameters can fit on one node

        reshard_after_forward: bool, default: True

            reshard parameters after the forward pass. This saves memory but slows training. This is only relevant

            when resharding individual layers (see https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html)

        verbose: bool, default: True

            turn on verbose output for model’s string representation



        Notes

        -----

        mixed_precision: bool

            This value will automatically be set from the Stoke FP16 selected option (AMP only)

        state_dict_device: torch.device

            this is not exposed as it should be managed internally from the DDP backend setup

        compute_device: torch.device

            this is not exposed as it should be managed internally from the DDP backend setup



        &quot;&quot;&quot;



        bucket_cap_mb: int = 25

        buffer_dtype: Optional[torch.dtype] = None

        clear_autocast_cache: bool = False

        compute_dtype: Optional[torch.dtype] = None

        flatten_parameters: bool = True

        force_input_to_fp32: bool = False

        fp32_reduce_scatter: bool = False

        gradient_predivide_factor: Optional[float] = None

        gradient_postdivide_factor: Optional[float] = None

        move_grads_to_cpu: Optional[bool] = None

        move_params_to_cpu: bool = False

        no_broadcast_optim_state: Optional[bool] = False

        reshard_after_forward: bool = True

        verbose: bool = False
</code></pre></div>
<hr />
<h4 id="descendants">Descendants</h4>
<ul>
<li>stoke.extensions._FairscaleFSDPConfig</li>
</ul>
<h3 id="fairscaleossconfig">FairscaleOSSConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleOSSConfig</span><span class="p">(</span>
    <span class="n">broadcast_fp16</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_16">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>broadcast_fp16</td>
<td>bool, default: False</td>
<td>Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP</td>
<td></td>
</tr>
<tr>
<td>is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleOSSConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Fairscale optimizer state sharding configuration class



        Attributes

        ----------

        broadcast_fp16: bool, default: False

            Compress the model shards in fp16 before sharing them in between ranks. This is safe to use when PyTorch AMP

            is activated. Without torch AMP this will lead to a slight degradation in terms of accuracy.



        &quot;&quot;&quot;



        broadcast_fp16: bool = False
</code></pre></div>
<hr />
<h3 id="fairscalesddpconfig">FairscaleSDDPConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleSDDPConfig</span><span class="p">(</span>
    <span class="n">auto_refresh_trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">broadcast_buffers</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">reduce_buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8388608</span><span class="p">,</span>
    <span class="n">reduce_fp16</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sync_models_at_startup</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_17">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>auto_refresh_trainable</td>
<td>bool, default: True</td>
<td>Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS</td>
<td></td>
</tr>
<tr>
<td>automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>parameter is frozen or unfrozen</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>broadcast_buffers</td>
<td>bool, default: True</td>
<td>Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same</td>
<td></td>
</tr>
<tr>
<td>setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>reduce_buffer_size</td>
<td>int, default: 2 ** 23</td>
<td>he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact</td>
<td></td>
</tr>
<tr>
<td>the long term memory consumption, because these buckets correspond to parameters which will not be sharded.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Set to 0 to remove all bucketing, 1M to 8M is usually reasonable.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>reduce_fp16</td>
<td>bool, default: False</td>
<td>cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve</td>
<td></td>
</tr>
<tr>
<td>performance for multi node jobs using PyTorch AMP. The effect is similar to DDP’s fp16_compress_hook and</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>will also save some memory.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>sync_models_at_startup</td>
<td>bool, default: True</td>
<td>Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or</td>
<td></td>
</tr>
<tr>
<td>the training restarts from a saved state</td>
<td>None</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleSDDPConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Fairscale sharded data parallel (SDDP) configuration class



        Attributes

        ----------

        auto_refresh_trainable: bool, default: True

            Check whether the parameters trainability (requires_grad) has changed and update both ShardedDDP and OSS

            automatically if this is the case. If set to False, refresh_trainable() needs to be called anytime a

            parameter is frozen or unfrozen

        broadcast_buffers: bool, default: True

            Whether to additionally broadcast model buffers in between ranks at the beginning of each forward pass. Same

            setting as in Pytorch DDP, this is in addition to the broadcast and reduction of the model parameters.

        reduce_buffer_size: int, default: 2 ** 23

            he max size of the buffer used to batch the small parameter tensors, in number of elements. This will impact

            the long term memory consumption, because these buckets correspond to parameters which will not be sharded.

            Set to 0 to remove all bucketing, 1M to 8M is usually reasonable.

        reduce_fp16: bool, default: False

            cast the grads to fp16 before reducing. Not needed if the model is already fp16, but will probably improve

            performance for multi node jobs using PyTorch AMP. The effect is similar to DDP’s fp16_compress_hook and

            will also save some memory.

        sync_models_at_startup: bool, default: True

            Synchronize the models in between the ranks when starting up. Not needed if each rank has the same seed, or

            the training restarts from a saved state



        &quot;&quot;&quot;



        auto_refresh_trainable: bool = True

        broadcast_buffers: bool = True

        reduce_buffer_size: int = 2 ** 23

        reduce_fp16: bool = False

        sync_models_at_startup: bool = True
</code></pre></div>
<hr />
<h3 id="horovodconfig">HorovodConfig</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">HorovodConfig</span><span class="p">(</span>
    <span class="n">compression</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">convert_to_sync_batch_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gradient_predivide_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">op</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">HorovodOps</span> <span class="o">=</span> <span class="s1">&#39;Average&#39;</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_18">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>compression</td>
<td>bool, default: False</td>
<td>Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter</td>
<td></td>
</tr>
<tr>
<td>update step.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>convert_to_sync_batch_norm</td>
<td>bool, default: False</td>
<td>Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls</td>
<td></td>
</tr>
<tr>
<td>https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>gradient_predivide_factor</td>
<td>float, default: 1.0</td>
<td>If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled</td>
<td></td>
</tr>
<tr>
<td>by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum.</td>
<td>None</td>
<td></td>
<td></td>
</tr>
<tr>
<td>op</td>
<td>HorovodOps, default: 'Average'</td>
<td>The reduction operation to use when combining gradients across different ranks.</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class HorovodConfig:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Horovod configuration class



        Attributes

        ----------

        compression: bool, default: False

            Compression algorithm used during allreduce to reduce the amount of data sent during the each parameter

            update step.

        convert_to_sync_batch_norm: bool, default: False

            Automatically convert all batch norm calls to horovod.torch.SyncBatchNorm calls

            https://horovod.readthedocs.io/en/stable/api.html#horovod.torch.SyncBatchNorm

        gradient_predivide_factor: float, default: 1.0

            If op == Average, gradient_predivide_factor splits the averaging before and after the sum. Gradients are scaled

            by 1.0 / gradient_predivide_factor before the sum and gradient_predivide_factor / size after the sum.

        op: HorovodOps, default: &#39;Average&#39;

            The reduction operation to use when combining gradients across different ranks.



        &quot;&quot;&quot;



        compression: bool = False

        convert_to_sync_batch_norm: bool = False

        gradient_predivide_factor: float = 1.0

        op: HorovodOps = &quot;Average&quot;
</code></pre></div>
<hr />
<h3 id="horovodops">HorovodOps</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">HorovodOps</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class HorovodOps(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Horovod ops options&quot;&quot;&quot;



        Average = &quot;Average&quot;

        Sum = &quot;Sum&quot;

        Adasum = &quot;Adasum&quot;
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables_1">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">Adasum</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">Average</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">Sum</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="offloaddevice">OffloadDevice</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">OffloadDevice</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class OffloadDevice(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Offload device options&quot;&quot;&quot;



        none = &quot;none&quot;

        cpu = &quot;cpu&quot;

        nvme = &quot;nvme&quot;
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_2">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables_2">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">cpu</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">none</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">nvme</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="stokeoptimizer">StokeOptimizer</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">StokeOptimizer</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_19">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Type[torch.optim.Optimizer]</td>
<td>un-instantiated torch.optim.Optimizer class</td>
<td>None</td>
</tr>
<tr>
<td>optimizer_kwargs</td>
<td>Dict</td>
<td>any keyword args to be unrolled into the optimizer at instantiation time</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class StokeOptimizer(TypedDict):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Stoke optimizer wrapper class



        Given all the different backends and extensions the optimizer might need to be instantiated in a different way

        thus this typed dict holds the configuration without instantiation



        Attributes

        ----------

        optimizer: Type[torch.optim.Optimizer]

            un-instantiated torch.optim.Optimizer class

        optimizer_kwargs: Dict

            any keyword args to be unrolled into the optimizer at instantiation time



        &quot;&quot;&quot;



        optimizer: Type[torch.optim.Optimizer]

        optimizer_kwargs: Dict
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_3">Ancestors (in MRO)</h4>
<ul>
<li>builtins.dict</li>
</ul>
<h4 id="methods">Methods</h4>
<h4 id="clear">clear</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clear</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.clear() -&gt; None.  Remove all items from D.</p>
<h4 id="copy">copy</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">copy</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.copy() -&gt; a shallow copy of D</p>
<h4 id="fromkeys">fromkeys</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fromkeys</span><span class="p">(</span>
    <span class="n">iterable</span><span class="p">,</span>
    <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">/</span>
<span class="p">)</span>
</code></pre></div>
<p>Create a new dictionary with keys from iterable and values set to value.</p>
<h4 id="get">get</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">key</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">/</span>
<span class="p">)</span>
</code></pre></div>
<p>Return the value for key if key is in the dictionary, else default.</p>
<h4 id="items">items</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">items</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.items() -&gt; a set-like object providing a view on D's items</p>
<h4 id="keys">keys</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">keys</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.keys() -&gt; a set-like object providing a view on D's keys</p>
<h4 id="pop">pop</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">pop</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.</p>
<p>If key is not found, d is returned if given, otherwise KeyError is raised</p>
<h4 id="popitem">popitem</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">popitem</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">/</span>
<span class="p">)</span>
</code></pre></div>
<p>Remove and return a (key, value) pair as a 2-tuple.</p>
<p>Pairs are returned in LIFO (last-in, first-out) order.
Raises KeyError if the dict is empty.</p>
<h4 id="setdefault">setdefault</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">setdefault</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">key</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">/</span>
<span class="p">)</span>
</code></pre></div>
<p>Insert key with a value of default if key is not in the dictionary.</p>
<p>Return the value for key if key is in the dictionary, else default.</p>
<h4 id="update">update</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.update([E, ]**F) -&gt; None.  Update D from dict/iterable E and F.</p>
<p>If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]
If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v
In either case, this is followed by: for k in F:  D[k] = F[k]</p>
<h4 id="values">values</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">values</span><span class="p">(</span>
    <span class="o">...</span>
<span class="p">)</span>
</code></pre></div>
<p>D.values() -&gt; an object providing a view on D's values</p>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../../../CONTRIBUTING/" title="Contributing" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Contributing
              </span>
            </div>
          </a>
        
        
          <a href="../data/" title="Data" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Data
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ncilfone" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/ncilfone/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
    
  </body>
</html>