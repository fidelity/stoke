
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../../resources/images/stoke_small.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Fp16 - stoke</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-stokefp16" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="stoke" class="md-header__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            stoke
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Fp16
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Home/" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Installation/" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Quick-Start/" class="md-tabs__link">
      Quick Start
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Launchers/" class="md-tabs__link">
      Launchers
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Examples/" class="md-tabs__link">
      Examples
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../CONTRIBUTING/" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../configs/" class="md-tabs__link md-tabs__link--active">
        Reference
      </a>
    </li>
  

  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="stoke" class="md-nav__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    stoke
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Home/" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Quick-Start/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Launchers/" class="md-nav__link">
        Launchers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Examples/" class="md-nav__link">
        Examples
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_1" type="checkbox" id="__nav_7_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_1">
          Stoke
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stoke" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          Stoke
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../configs/" class="md-nav__link">
        Configs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../distributed/" class="md-nav__link">
        Distributed
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../extensions/" class="md-nav__link">
        Extensions
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Fp16
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Fp16
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#apexbasefp16" class="md-nav__link">
    ApexBaseFP16
  </a>
  
    <nav class="md-nav" aria-label="ApexBaseFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apexo1ampfp16" class="md-nav__link">
    ApexO1AmpFP16
  </a>
  
    <nav class="md-nav" aria-label="ApexO1AmpFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_1" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_1" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_1" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_1" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_1" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_1" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_1" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apexo2ampfp16" class="md-nav__link">
    ApexO2AmpFP16
  </a>
  
    <nav class="md-nav" aria-label="ApexO2AmpFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_2" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_2" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_2" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_2" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_2" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_2" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_2" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basefp16" class="md-nav__link">
    BaseFP16
  </a>
  
    <nav class="md-nav" aria-label="BaseFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants_1" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_3" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_3" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_3" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_3" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_3" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_3" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_3" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedfp16" class="md-nav__link">
    DeepspeedFP16
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_4" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_4" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_4" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_4" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_4" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_4" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_4" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nativeampfp16" class="md-nav__link">
    NativeAmpFP16
  </a>
  
    <nav class="md-nav" aria-label="NativeAmpFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_5" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_5" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_5" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_5" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_5" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_5" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_5" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_5" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_5" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_5" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nullfp16" class="md-nav__link">
    NullFP16
  </a>
  
    <nav class="md-nav" aria-label="NullFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_6" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_6" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants_2" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_6" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_6" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_6" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_6" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_6" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_6" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_6" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_6" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runnerfp16enum" class="md-nav__link">
    RunnerFP16Enum
  </a>
  
    <nav class="md-nav" aria-label="RunnerFP16Enum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_7" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../io/" class="md-nav__link">
        Io
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../status/" class="md-nav__link">
        Status
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../stoke/" class="md-nav__link">
        Stoke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#apexbasefp16" class="md-nav__link">
    ApexBaseFP16
  </a>
  
    <nav class="md-nav" aria-label="ApexBaseFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apexo1ampfp16" class="md-nav__link">
    ApexO1AmpFP16
  </a>
  
    <nav class="md-nav" aria-label="ApexO1AmpFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_1" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_1" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_1" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_1" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_1" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_1" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_1" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apexo2ampfp16" class="md-nav__link">
    ApexO2AmpFP16
  </a>
  
    <nav class="md-nav" aria-label="ApexO2AmpFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_2" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_2" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_2" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_2" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_2" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_2" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_2" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#basefp16" class="md-nav__link">
    BaseFP16
  </a>
  
    <nav class="md-nav" aria-label="BaseFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants_1" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_3" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_3" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_3" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_3" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_3" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_3" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_3" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeedfp16" class="md-nav__link">
    DeepspeedFP16
  </a>
  
    <nav class="md-nav" aria-label="DeepspeedFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_4" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_4" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_4" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_4" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_4" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_4" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_4" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_4" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nativeampfp16" class="md-nav__link">
    NativeAmpFP16
  </a>
  
    <nav class="md-nav" aria-label="NativeAmpFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_5" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_5" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_5" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_5" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_5" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_5" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_5" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_5" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_5" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_5" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nullfp16" class="md-nav__link">
    NullFP16
  </a>
  
    <nav class="md-nav" aria-label="NullFP16">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_6" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_6" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants_2" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#instance-variables_6" class="md-nav__link">
    Instance variables
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_6" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backward_call_6" class="md-nav__link">
    backward_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_6" class="md-nav__link">
    clip_grad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_norm_6" class="md-nav__link">
    clip_grad_norm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip_grad_value_6" class="md-nav__link">
    clip_grad_value
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step_call_6" class="md-nav__link">
    step_call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrap_fp16_6" class="md-nav__link">
    wrap_fp16
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runnerfp16enum" class="md-nav__link">
    RunnerFP16Enum
  </a>
  
    <nav class="md-nav" aria-label="RunnerFP16Enum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_7" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/fidelity/stoke/edit/main/reference/stoke/fp16.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="module-stokefp16">Module stoke.fp16</h1>
<p>Handles FP16/mixed-precision related classes -- mixin style</p>
<p>None</p>
<p>??? example "View Source"
        # -<em>- coding: utf-8 -</em>-</p>
<div class="highlight"><pre><span></span><code>    # Copyright FMR LLC &lt;opensource@fidelity.com&gt;

    # SPDX-License-Identifier: Apache-2.0



    &quot;&quot;&quot;Handles FP16/mixed-precision related classes -- mixin style&quot;&quot;&quot;



    from abc import ABC

    from contextlib import nullcontext

    from enum import Enum

    from typing import List, Optional, Tuple, Union



    import torch

    from fairscale.optim.grad_scaler import ShardedGradScaler

    from fairscale.optim.oss import OSS



    from stoke.configs import ClipGradConfig, ClipGradNormConfig





    class BaseFP16(ABC):

        &quot;&quot;&quot;Base class for mixed precision and FP16 functionality



        This class handles base and common functionality for all of the different mixed-precision backends. Contains

        functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward

        and loss calls



        Attributes

        ----------

        loss_context

        model_context

        scaler

        _scaler: default: None

            scaler object for backends that require one

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, scaler=None, verbose: bool = True):

            &quot;&quot;&quot;Init for BaseFP16 class



            Parameters

            ----------

            scaler: default: None

                scaler object for backends that require one

            verbose: bool, default: True

                flag for verbosity



            &quot;&quot;&quot;

            self._scaler = scaler

            self._verbose = verbose



        def _scaler_info(self):

            if self._verbose and self._scaler is not None:

                self._print_device(

                    f&quot;FP16 Mixin: Initialized scaler of type {type(self._scaler).__name__}&quot;

                )



        def wrap_fp16(

            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with specific mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            return model, optimizer



        def clip_grad(

            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )



        def clip_grad_value(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Base handle clip gradients by value



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)



        def clip_grad_norm(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Base handle clip gradients by the norm



            Depending on some extension flags switch between the correct clip_grad_norm calls



            OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm

            if oss:

                optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type)

            # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm

            elif fsdp:

                model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)

            else:

                torch.nn.utils.clip_grad_norm_(

                    model.parameters(), max_norm=max_norm, norm_type=norm_type

                )



        @property

        def scaler(self):

            &quot;&quot;&quot;Returns grad scaler&quot;&quot;&quot;

            return self._scaler



        @property

        def loss_context(self):

            &quot;&quot;&quot;Returns the base context wrapper for the loss call&quot;&quot;&quot;

            return nullcontext()



        @property

        def model_context(self):

            &quot;&quot;&quot;Returns the base context wrapper for the model call&quot;&quot;&quot;

            return nullcontext()



        def backward_call(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Base wrapped backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    val.backward(retain_graph=(idx == 0))

            else:

                loss.backward()



        def step_call(

            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Base wrapped step of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer

            optimizer.step()





    class NullFP16(BaseFP16):

        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for NullFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            Notes

            -----

            Scaler set to None as it is not needed



            &quot;&quot;&quot;

            super(NullFP16, self).__init__(scaler=None, verbose=verbose)





    class DeepspeedFP16(NullFP16):

        def __init__(self, verbose: bool = True, **kwargs):

            super(DeepspeedFP16, self).__init__(verbose=verbose)



        def backward_call(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Override of deepspeed wrapped backward call



            Deepspeed calls backward via the model engine instead of the loss



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    model.backward(val, retain_graph=(idx == 0))

            else:

                model.backward(loss)



        def step_call(

            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Override of deepspeed wrapped backward call



            Deepspeed calls step via the model engine instead of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            model.step()





    class ApexBaseFP16(BaseFP16):

        &quot;&quot;&quot;Base class for Apex FP16 methods



        This class handles base and common functionality for O1 and O2 Apex mixed-precision backends. Contains

        functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward

        and loss calls



        Attributes

        ----------

        loss_context

        model_context

        scaler

        _apex_config: ApexConfig

            Configuration object for Apex

        _multi_loss: int, default: 1

            Holds the number of losses to use (apex can use multiple scalers per loss)

        _scaler: default: None

            scaler object for backends that require one

        _verbose bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for ApexBaseFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in



            Notes

            -----

            Scaler set to None as it is not needed



            &quot;&quot;&quot;

            super(ApexBaseFP16, self).__init__(scaler=None, verbose=verbose)

            self._conditional_import()

            self._apex_config = kwargs[&quot;apex_config&quot;]

            self._multi_loss = (

                len(kwargs[&quot;loss&quot;]) if isinstance(kwargs[&quot;loss&quot;], (list, tuple)) else 1

            )



        @staticmethod

        def _conditional_import():

            &quot;&quot;&quot;Attempts to conditionally import apex if the functionality is required



            Raises

            ------

            ImportError

                If apex cannot be imported



            Returns

            -------

            None



            &quot;&quot;&quot;

            try:

                global amp

                from apex import amp

            except ImportError as e:

                print(

                    e,

                    &quot;: apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                )



        def _apex_convert_to_sync_batch_norm(self, model: torch.nn.Module):

            &quot;&quot;&quot;Replaces all BatchNorm*D layers with apex.parallel.SyncBatchNorm layers



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            model: torch.nn.Module

                modified version of model with all BatchNorm*D layers replaced with apex.parallel.SyncBatchNorm layers



            &quot;&quot;&quot;

            self.print_device(

                f&quot;Converting all BatchNorm*D layers to apex.parallel.SyncBatchNorm layers...&quot;

            )

            try:

                from apex.parallel import convert_syncbn_model



                model = convert_syncbn_model(module=model)

            except ImportError as e:

                print(

                    e,

                    &quot;: apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                )

            return model



        def clip_grad_value(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Override handle clip gradients by value for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_value_(

                amp.master_params(optimizer), clip_value=clip_value

            )



        def clip_grad_norm(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Override handle clip gradients by the norm for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_norm_(

                amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type

            )



        def backward_call(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Override wrapped backward call for APEX



            Need to use APEX scale_loss context with backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    with amp.scale_loss(

                        val,

                        optimizer,

                        loss_id=idx if self._apex_config.scaler_per_loss else 0,

                    ) as scaled_loss:

                        scaled_loss.backward(retain_graph=(idx == 0))

            else:

                with amp.scale_loss(loss, optimizer) as scaled_loss:

                    scaled_loss.backward()





    class ApexO2AmpFP16(ApexBaseFP16):

        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for ApexO2AmpFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in



            Notes

            -----

            Scaler set to None as it is not needed



            &quot;&quot;&quot;

            super(ApexO2AmpFP16, self).__init__(verbose=verbose, **kwargs)



        def wrap_fp16(

            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            if self._apex_config.convert_to_sync_batch_norm:

                model = self._apex_convert_to_sync_batch_norm(model=model)

            model, optimizer = amp.initialize(

                model,

                optimizer,

                opt_level=&quot;O2&quot;,

                cast_model_outputs=self._apex_config.cast_model_outputs,

                max_loss_scale=self._apex_config.max_loss_scale,

                min_loss_scale=self._apex_config.min_loss_scale,

                verbosity=self._apex_config.verbosity,

                num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1,

            )

            return model, optimizer





    class ApexO1AmpFP16(ApexBaseFP16):

        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for ApexO1AmpFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in



            Notes

            -----

            Scaler set to None as it is not needed



            &quot;&quot;&quot;

            super(ApexO1AmpFP16, self).__init__(verbose=verbose, **kwargs)



        def wrap_fp16(

            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            if self._apex_config.convert_to_sync_batch_norm:

                model = self._apex_convert_to_sync_batch_norm(model=model)

            model, optimizer = amp.initialize(

                model,

                optimizer,

                opt_level=&quot;O1&quot;,

                cast_model_outputs=self._apex_config.cast_model_outputs,

                max_loss_scale=self._apex_config.max_loss_scale,

                min_loss_scale=self._apex_config.min_loss_scale,

                verbosity=self._apex_config.verbosity,

                num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1,

            )

            return model, optimizer





    class NativeAmpFP16(BaseFP16):

        &quot;&quot;&quot;Base class for PyTorch Native AMP FP16 methods



        This class handles base and common functionality for native PyTorch AMP mixed-precision backends. Contains

        functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward

        and loss calls



        Attributes

        ----------

        loss_context

        model_context

        scaler

        _amp_config: AMPConfig

            Configuration object for Apex

        _scaler: default: torch.cuda.amp.GradScaler

            scaler object for loss

        _verbose bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for NativeAmpFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here amp_config or sharded_config might be passed in



            Notes

            -----

            Scaler set between torch.cuda.amp.GradScaler and ShardedGradScaler depending on if a sharded config is passed

            via kwargs



            &quot;&quot;&quot;

            self._amp_config = kwargs[&quot;amp_config&quot;]

            # Switch the scaler obj ref depending on fairscale sharding

            scaler = (

                ShardedGradScaler

                if (kwargs[&quot;sharded_config&quot;] is not None)

                or (kwargs[&quot;fully_sharded_config&quot;] is not None)

                else torch.cuda.amp.GradScaler

            )

            super(NativeAmpFP16, self).__init__(

                scaler=scaler(

                    backoff_factor=self._amp_config.backoff_factor,

                    enabled=True,

                    growth_factor=self._amp_config.growth_factor,

                    growth_interval=self._amp_config.growth_interval,

                    init_scale=self._amp_config.init_scale,

                ),

                verbose=verbose,

            )



        @property

        def loss_context(self):

            &quot;&quot;&quot;Overrides base and returns the native AMP autocast context&quot;&quot;&quot;

            return torch.cuda.amp.autocast(enabled=True)



        @property

        def model_context(self):

            &quot;&quot;&quot;Overrides base and returns the native AMP autocast context&quot;&quot;&quot;

            return torch.cuda.amp.autocast(enabled=True)



        def backward_call(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Overrides base wrapped backward call for AMP scaled backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    self._scaler.scale(val).backward(retain_graph=(idx == 0))

            else:

                self._scaler.scale(loss).backward()



        def step_call(

            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Overrides base wrapped step of the optimizer with the AMP scaler version



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.scaler.step(optimizer)

            self.scaler.update()





    class RunnerFP16Enum(Enum):

        &quot;&quot;&quot;Enum for building the runtime object with mixed-precision functionality&quot;&quot;&quot;



        full = NullFP16

        apex_O1 = ApexO1AmpFP16

        apex_O2 = ApexO2AmpFP16

        amp = NativeAmpFP16

        deepspeed = DeepspeedFP16
</code></pre></div>
<h2 id="classes">Classes</h2>
<h3 id="apexbasefp16">ApexBaseFP16</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ApexBaseFP16</span><span class="p">(</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_apex_config</td>
<td>ApexConfig</td>
<td>Configuration object for Apex</td>
<td>None</td>
</tr>
<tr>
<td>_multi_loss</td>
<td>int, default: 1</td>
<td>Holds the number of losses to use (apex can use multiple scalers per loss)</td>
<td>None</td>
</tr>
<tr>
<td>_scaler</td>
<td>default: None</td>
<td>scaler object for backends that require one</td>
<td>None</td>
</tr>
<tr>
<td>_verbose bool, default</td>
<td>True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ApexBaseFP16(BaseFP16):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Base class for Apex FP16 methods



        This class handles base and common functionality for O1 and O2 Apex mixed-precision backends. Contains

        functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward

        and loss calls



        Attributes

        ----------

        loss_context

        model_context

        scaler

        _apex_config: ApexConfig

            Configuration object for Apex

        _multi_loss: int, default: 1

            Holds the number of losses to use (apex can use multiple scalers per loss)

        _scaler: default: None

            scaler object for backends that require one

        _verbose bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for ApexBaseFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in



            Notes

            -----

            Scaler set to None as it is not needed



            &quot;&quot;&quot;

            super(ApexBaseFP16, self).__init__(scaler=None, verbose=verbose)

            self._conditional_import()

            self._apex_config = kwargs[&quot;apex_config&quot;]

            self._multi_loss = (

                len(kwargs[&quot;loss&quot;]) if isinstance(kwargs[&quot;loss&quot;], (list, tuple)) else 1

            )



        @staticmethod

        def _conditional_import():

            &quot;&quot;&quot;Attempts to conditionally import apex if the functionality is required



            Raises

            ------

            ImportError

                If apex cannot be imported



            Returns

            -------

            None



            &quot;&quot;&quot;

            try:

                global amp

                from apex import amp

            except ImportError as e:

                print(

                    e,

                    &quot;: apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                )



        def _apex_convert_to_sync_batch_norm(self, model: torch.nn.Module):

            &quot;&quot;&quot;Replaces all BatchNorm*D layers with apex.parallel.SyncBatchNorm layers



            Parameters

            ----------

            model: torch.nn.Module

                current model object



            Returns

            -------

            model: torch.nn.Module

                modified version of model with all BatchNorm*D layers replaced with apex.parallel.SyncBatchNorm layers



            &quot;&quot;&quot;

            self.print_device(

                f&quot;Converting all BatchNorm*D layers to apex.parallel.SyncBatchNorm layers...&quot;

            )

            try:

                from apex.parallel import convert_syncbn_model



                model = convert_syncbn_model(module=model)

            except ImportError as e:

                print(

                    e,

                    &quot;: apex cannot be imported -- please install (https://github.com/NVIDIA/apex)&quot;,

                )

            return model



        def clip_grad_value(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Override handle clip gradients by value for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_value_(

                amp.master_params(optimizer), clip_value=clip_value

            )



        def clip_grad_norm(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Override handle clip gradients by the norm for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_norm_(

                amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type

            )



        def backward_call(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Override wrapped backward call for APEX



            Need to use APEX scale_loss context with backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    with amp.scale_loss(

                        val,

                        optimizer,

                        loss_id=idx if self._apex_config.scaler_per_loss else 0,

                    ) as scaled_loss:

                        scaled_loss.backward(retain_graph=(idx == 0))

            else:

                with amp.scale_loss(loss, optimizer) as scaled_loss:

                    scaled_loss.backward()
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>stoke.fp16.BaseFP16</li>
<li>abc.ABC</li>
</ul>
<h4 id="descendants">Descendants</h4>
<ul>
<li>stoke.fp16.ApexO2AmpFP16</li>
<li>stoke.fp16.ApexO1AmpFP16</li>
</ul>
<h4 id="instance-variables">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">loss_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the loss call</p>
<div class="highlight"><pre><span></span><code><span class="n">model_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the model call</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Returns grad scaler</p>
<h4 id="methods">Methods</h4>
<h4 id="backward_call">backward_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Override wrapped backward call for APEX</p>
<p>Need to use APEX scale_loss context with backward call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>loss tensor(s)</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward_call(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Override wrapped backward call for APEX



            Need to use APEX scale_loss context with backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    with amp.scale_loss(

                        val,

                        optimizer,

                        loss_id=idx if self._apex_config.scaler_per_loss else 0,

                    ) as scaled_loss:

                        scaled_loss.backward(retain_graph=(idx == 0))

            else:

                with amp.scale_loss(loss, optimizer) as scaled_loss:

                    scaled_loss.backward()
</code></pre></div>
<h4 id="clip_grad">clip_grad</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">horovod</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">deepspeed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clipping the current gradients</p>
<p>Determines which method to use based on the gradient clipping config and the current runtime state</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>grad_clip</td>
<td>Union[ClipGradConfig, ClipGradNormConfig]</td>
<td>gradient clipping config that will determine which method to use</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>horovod</td>
<td>bool</td>
<td>horovod flag</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed</td>
<td>bool</td>
<td>deepspeed flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad(</p>
<div class="highlight"><pre><span></span><code>            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )
</code></pre></div>
<h4 id="clip_grad_norm">clip_grad_norm</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_norm</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Override handle clip gradients by the norm for APEX</p>
<p>Need to call master_params within APEX to clip correctly</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>max_norm</td>
<td>Union[float, int]</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>Union[float, int]</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool, default: False</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool, default: False</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_norm(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Override handle clip gradients by the norm for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_norm_(

                amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type

            )
</code></pre></div>
<h4 id="clip_grad_value">clip_grad_value</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_value</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<p>Override handle clip gradients by value for APEX</p>
<p>Need to call master_params within APEX to clip correctly</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>clip_value</td>
<td>float</td>
<td>absolute value to clip grads</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_value(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Override handle clip gradients by value for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_value_(

                amp.master_params(optimizer), clip_value=clip_value

            )
</code></pre></div>
<h4 id="step_call">step_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Base wrapped step of the optimizer</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_call(</p>
<div class="highlight"><pre><span></span><code>            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Base wrapped step of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer

            optimizer.step()
</code></pre></div>
<h4 id="wrap_fp16">wrap_fp16</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_fp16</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps model and optimizer with specific mixed-precision related backend wrappers</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>modified version of model object for mixed-precision backends</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_fp16(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with specific mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            return model, optimizer
</code></pre></div>
<h3 id="apexo1ampfp16">ApexO1AmpFP16</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ApexO1AmpFP16</span><span class="p">(</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_1">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_apex_config</td>
<td>ApexConfig</td>
<td>Configuration object for Apex</td>
<td>None</td>
</tr>
<tr>
<td>_multi_loss</td>
<td>int, default: 1</td>
<td>Holds the number of losses to use (apex can use multiple scalers per loss)</td>
<td>None</td>
</tr>
<tr>
<td>_scaler</td>
<td>default: None</td>
<td>scaler object for backends that require one</td>
<td>None</td>
</tr>
<tr>
<td>_verbose bool, default</td>
<td>True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ApexO1AmpFP16(ApexBaseFP16):</p>
<div class="highlight"><pre><span></span><code>        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for ApexO1AmpFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in



            Notes

            -----

            Scaler set to None as it is not needed



            &quot;&quot;&quot;

            super(ApexO1AmpFP16, self).__init__(verbose=verbose, **kwargs)



        def wrap_fp16(

            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            if self._apex_config.convert_to_sync_batch_norm:

                model = self._apex_convert_to_sync_batch_norm(model=model)

            model, optimizer = amp.initialize(

                model,

                optimizer,

                opt_level=&quot;O1&quot;,

                cast_model_outputs=self._apex_config.cast_model_outputs,

                max_loss_scale=self._apex_config.max_loss_scale,

                min_loss_scale=self._apex_config.min_loss_scale,

                verbosity=self._apex_config.verbosity,

                num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1,

            )

            return model, optimizer
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>stoke.fp16.ApexBaseFP16</li>
<li>stoke.fp16.BaseFP16</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_1">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">loss_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the loss call</p>
<div class="highlight"><pre><span></span><code><span class="n">model_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the model call</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Returns grad scaler</p>
<h4 id="methods_1">Methods</h4>
<h4 id="backward_call_1">backward_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Override wrapped backward call for APEX</p>
<p>Need to use APEX scale_loss context with backward call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>loss tensor(s)</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward_call(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Override wrapped backward call for APEX



            Need to use APEX scale_loss context with backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    with amp.scale_loss(

                        val,

                        optimizer,

                        loss_id=idx if self._apex_config.scaler_per_loss else 0,

                    ) as scaled_loss:

                        scaled_loss.backward(retain_graph=(idx == 0))

            else:

                with amp.scale_loss(loss, optimizer) as scaled_loss:

                    scaled_loss.backward()
</code></pre></div>
<h4 id="clip_grad_1">clip_grad</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">horovod</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">deepspeed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clipping the current gradients</p>
<p>Determines which method to use based on the gradient clipping config and the current runtime state</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>grad_clip</td>
<td>Union[ClipGradConfig, ClipGradNormConfig]</td>
<td>gradient clipping config that will determine which method to use</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>horovod</td>
<td>bool</td>
<td>horovod flag</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed</td>
<td>bool</td>
<td>deepspeed flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad(</p>
<div class="highlight"><pre><span></span><code>            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )
</code></pre></div>
<h4 id="clip_grad_norm_1">clip_grad_norm</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_norm</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Override handle clip gradients by the norm for APEX</p>
<p>Need to call master_params within APEX to clip correctly</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>max_norm</td>
<td>Union[float, int]</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>Union[float, int]</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool, default: False</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool, default: False</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_norm(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Override handle clip gradients by the norm for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_norm_(

                amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type

            )
</code></pre></div>
<h4 id="clip_grad_value_1">clip_grad_value</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_value</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<p>Override handle clip gradients by value for APEX</p>
<p>Need to call master_params within APEX to clip correctly</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>clip_value</td>
<td>float</td>
<td>absolute value to clip grads</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_value(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Override handle clip gradients by value for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_value_(

                amp.master_params(optimizer), clip_value=clip_value

            )
</code></pre></div>
<h4 id="step_call_1">step_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Base wrapped step of the optimizer</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_call(</p>
<div class="highlight"><pre><span></span><code>            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Base wrapped step of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer

            optimizer.step()
</code></pre></div>
<h4 id="wrap_fp16_1">wrap_fp16</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_fp16</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>modified version of model object for mixed-precision backends</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_fp16(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with Apex O1 mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            if self._apex_config.convert_to_sync_batch_norm:

                model = self._apex_convert_to_sync_batch_norm(model=model)

            model, optimizer = amp.initialize(

                model,

                optimizer,

                opt_level=&quot;O1&quot;,

                cast_model_outputs=self._apex_config.cast_model_outputs,

                max_loss_scale=self._apex_config.max_loss_scale,

                min_loss_scale=self._apex_config.min_loss_scale,

                verbosity=self._apex_config.verbosity,

                num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1,

            )

            return model, optimizer
</code></pre></div>
<h3 id="apexo2ampfp16">ApexO2AmpFP16</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">ApexO2AmpFP16</span><span class="p">(</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_2">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_apex_config</td>
<td>ApexConfig</td>
<td>Configuration object for Apex</td>
<td>None</td>
</tr>
<tr>
<td>_multi_loss</td>
<td>int, default: 1</td>
<td>Holds the number of losses to use (apex can use multiple scalers per loss)</td>
<td>None</td>
</tr>
<tr>
<td>_scaler</td>
<td>default: None</td>
<td>scaler object for backends that require one</td>
<td>None</td>
</tr>
<tr>
<td>_verbose bool, default</td>
<td>True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class ApexO2AmpFP16(ApexBaseFP16):</p>
<div class="highlight"><pre><span></span><code>        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for ApexO2AmpFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here loss or apex_config might be passed in



            Notes

            -----

            Scaler set to None as it is not needed



            &quot;&quot;&quot;

            super(ApexO2AmpFP16, self).__init__(verbose=verbose, **kwargs)



        def wrap_fp16(

            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            if self._apex_config.convert_to_sync_batch_norm:

                model = self._apex_convert_to_sync_batch_norm(model=model)

            model, optimizer = amp.initialize(

                model,

                optimizer,

                opt_level=&quot;O2&quot;,

                cast_model_outputs=self._apex_config.cast_model_outputs,

                max_loss_scale=self._apex_config.max_loss_scale,

                min_loss_scale=self._apex_config.min_loss_scale,

                verbosity=self._apex_config.verbosity,

                num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1,

            )

            return model, optimizer
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_2">Ancestors (in MRO)</h4>
<ul>
<li>stoke.fp16.ApexBaseFP16</li>
<li>stoke.fp16.BaseFP16</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_2">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">loss_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the loss call</p>
<div class="highlight"><pre><span></span><code><span class="n">model_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the model call</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Returns grad scaler</p>
<h4 id="methods_2">Methods</h4>
<h4 id="backward_call_2">backward_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Override wrapped backward call for APEX</p>
<p>Need to use APEX scale_loss context with backward call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>loss tensor(s)</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward_call(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Override wrapped backward call for APEX



            Need to use APEX scale_loss context with backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    with amp.scale_loss(

                        val,

                        optimizer,

                        loss_id=idx if self._apex_config.scaler_per_loss else 0,

                    ) as scaled_loss:

                        scaled_loss.backward(retain_graph=(idx == 0))

            else:

                with amp.scale_loss(loss, optimizer) as scaled_loss:

                    scaled_loss.backward()
</code></pre></div>
<h4 id="clip_grad_2">clip_grad</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">horovod</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">deepspeed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clipping the current gradients</p>
<p>Determines which method to use based on the gradient clipping config and the current runtime state</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>grad_clip</td>
<td>Union[ClipGradConfig, ClipGradNormConfig]</td>
<td>gradient clipping config that will determine which method to use</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>horovod</td>
<td>bool</td>
<td>horovod flag</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed</td>
<td>bool</td>
<td>deepspeed flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad(</p>
<div class="highlight"><pre><span></span><code>            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )
</code></pre></div>
<h4 id="clip_grad_norm_2">clip_grad_norm</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_norm</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Override handle clip gradients by the norm for APEX</p>
<p>Need to call master_params within APEX to clip correctly</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>max_norm</td>
<td>Union[float, int]</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>Union[float, int]</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool, default: False</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool, default: False</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_norm(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Override handle clip gradients by the norm for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_norm_(

                amp.master_params(optimizer), max_norm=max_norm, norm_type=norm_type

            )
</code></pre></div>
<h4 id="clip_grad_value_2">clip_grad_value</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_value</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<p>Override handle clip gradients by value for APEX</p>
<p>Need to call master_params within APEX to clip correctly</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>clip_value</td>
<td>float</td>
<td>absolute value to clip grads</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_value(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Override handle clip gradients by value for APEX



            Need to call master_params within APEX to clip correctly



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Automatically clipping calculated/accumulated gradients...&quot;

                )

            torch.nn.utils.clip_grad_value_(

                amp.master_params(optimizer), clip_value=clip_value

            )
</code></pre></div>
<h4 id="step_call_2">step_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Base wrapped step of the optimizer</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_call(</p>
<div class="highlight"><pre><span></span><code>            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Base wrapped step of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer

            optimizer.step()
</code></pre></div>
<h4 id="wrap_fp16_2">wrap_fp16</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_fp16</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>modified version of model object for mixed-precision backends</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_fp16(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with Apex O2 mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            if self._apex_config.convert_to_sync_batch_norm:

                model = self._apex_convert_to_sync_batch_norm(model=model)

            model, optimizer = amp.initialize(

                model,

                optimizer,

                opt_level=&quot;O2&quot;,

                cast_model_outputs=self._apex_config.cast_model_outputs,

                max_loss_scale=self._apex_config.max_loss_scale,

                min_loss_scale=self._apex_config.min_loss_scale,

                verbosity=self._apex_config.verbosity,

                num_losses=self._multi_loss if self._apex_config.scaler_per_loss else 1,

            )

            return model, optimizer
</code></pre></div>
<h3 id="basefp16">BaseFP16</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BaseFP16</span><span class="p">(</span>
    <span class="n">scaler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_3">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_scaler</td>
<td>default: None</td>
<td>scaler object for backends that require one</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class BaseFP16(ABC):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Base class for mixed precision and FP16 functionality



        This class handles base and common functionality for all of the different mixed-precision backends. Contains

        functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward

        and loss calls



        Attributes

        ----------

        loss_context

        model_context

        scaler

        _scaler: default: None

            scaler object for backends that require one

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, scaler=None, verbose: bool = True):

            &quot;&quot;&quot;Init for BaseFP16 class



            Parameters

            ----------

            scaler: default: None

                scaler object for backends that require one

            verbose: bool, default: True

                flag for verbosity



            &quot;&quot;&quot;

            self._scaler = scaler

            self._verbose = verbose



        def _scaler_info(self):

            if self._verbose and self._scaler is not None:

                self._print_device(

                    f&quot;FP16 Mixin: Initialized scaler of type {type(self._scaler).__name__}&quot;

                )



        def wrap_fp16(

            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with specific mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            return model, optimizer



        def clip_grad(

            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )



        def clip_grad_value(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Base handle clip gradients by value



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)



        def clip_grad_norm(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Base handle clip gradients by the norm



            Depending on some extension flags switch between the correct clip_grad_norm calls



            OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm

            if oss:

                optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type)

            # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm

            elif fsdp:

                model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)

            else:

                torch.nn.utils.clip_grad_norm_(

                    model.parameters(), max_norm=max_norm, norm_type=norm_type

                )



        @property

        def scaler(self):

            &quot;&quot;&quot;Returns grad scaler&quot;&quot;&quot;

            return self._scaler



        @property

        def loss_context(self):

            &quot;&quot;&quot;Returns the base context wrapper for the loss call&quot;&quot;&quot;

            return nullcontext()



        @property

        def model_context(self):

            &quot;&quot;&quot;Returns the base context wrapper for the model call&quot;&quot;&quot;

            return nullcontext()



        def backward_call(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Base wrapped backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    val.backward(retain_graph=(idx == 0))

            else:

                loss.backward()



        def step_call(

            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Base wrapped step of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer

            optimizer.step()
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_3">Ancestors (in MRO)</h4>
<ul>
<li>abc.ABC</li>
</ul>
<h4 id="descendants_1">Descendants</h4>
<ul>
<li>stoke.fp16.NullFP16</li>
<li>stoke.fp16.ApexBaseFP16</li>
<li>stoke.fp16.NativeAmpFP16</li>
</ul>
<h4 id="instance-variables_3">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">loss_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the loss call</p>
<div class="highlight"><pre><span></span><code><span class="n">model_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the model call</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Returns grad scaler</p>
<h4 id="methods_3">Methods</h4>
<h4 id="backward_call_3">backward_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Base wrapped backward call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>loss tensor(s)</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward_call(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Base wrapped backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    val.backward(retain_graph=(idx == 0))

            else:

                loss.backward()
</code></pre></div>
<h4 id="clip_grad_3">clip_grad</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">horovod</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">deepspeed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clipping the current gradients</p>
<p>Determines which method to use based on the gradient clipping config and the current runtime state</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>grad_clip</td>
<td>Union[ClipGradConfig, ClipGradNormConfig]</td>
<td>gradient clipping config that will determine which method to use</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>horovod</td>
<td>bool</td>
<td>horovod flag</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed</td>
<td>bool</td>
<td>deepspeed flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad(</p>
<div class="highlight"><pre><span></span><code>            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )
</code></pre></div>
<h4 id="clip_grad_norm_3">clip_grad_norm</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_norm</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clip gradients by the norm</p>
<p>Depending on some extension flags switch between the correct clip_grad_norm calls</p>
<p>OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html
FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>max_norm</td>
<td>Union[float, int]</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>Union[float, int]</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool, default: False</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool, default: False</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_norm(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Base handle clip gradients by the norm



            Depending on some extension flags switch between the correct clip_grad_norm calls



            OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm

            if oss:

                optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type)

            # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm

            elif fsdp:

                model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)

            else:

                torch.nn.utils.clip_grad_norm_(

                    model.parameters(), max_norm=max_norm, norm_type=norm_type

                )
</code></pre></div>
<h4 id="clip_grad_value_3">clip_grad_value</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_value</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clip gradients by value</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>clip_value</td>
<td>float</td>
<td>absolute value to clip grads</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_value(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Base handle clip gradients by value



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)
</code></pre></div>
<h4 id="step_call_3">step_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Base wrapped step of the optimizer</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_call(</p>
<div class="highlight"><pre><span></span><code>            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Base wrapped step of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer

            optimizer.step()
</code></pre></div>
<h4 id="wrap_fp16_3">wrap_fp16</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_fp16</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps model and optimizer with specific mixed-precision related backend wrappers</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>modified version of model object for mixed-precision backends</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_fp16(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with specific mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            return model, optimizer
</code></pre></div>
<h3 id="deepspeedfp16">DeepspeedFP16</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DeepspeedFP16</span><span class="p">(</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_4">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_scaler</td>
<td>default: None</td>
<td>scaler object for backends that require one</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class DeepspeedFP16(NullFP16):</p>
<div class="highlight"><pre><span></span><code>        def __init__(self, verbose: bool = True, **kwargs):

            super(DeepspeedFP16, self).__init__(verbose=verbose)



        def backward_call(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Override of deepspeed wrapped backward call



            Deepspeed calls backward via the model engine instead of the loss



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    model.backward(val, retain_graph=(idx == 0))

            else:

                model.backward(loss)



        def step_call(

            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Override of deepspeed wrapped backward call



            Deepspeed calls step via the model engine instead of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            model.step()
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_4">Ancestors (in MRO)</h4>
<ul>
<li>stoke.fp16.NullFP16</li>
<li>stoke.fp16.BaseFP16</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_4">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">loss_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the loss call</p>
<div class="highlight"><pre><span></span><code><span class="n">model_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the model call</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Returns grad scaler</p>
<h4 id="methods_4">Methods</h4>
<h4 id="backward_call_4">backward_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Override of deepspeed wrapped backward call</p>
<p>Deepspeed calls backward via the model engine instead of the loss</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>loss tensor(s)</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward_call(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Override of deepspeed wrapped backward call



            Deepspeed calls backward via the model engine instead of the loss



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    model.backward(val, retain_graph=(idx == 0))

            else:

                model.backward(loss)
</code></pre></div>
<h4 id="clip_grad_4">clip_grad</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">horovod</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">deepspeed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clipping the current gradients</p>
<p>Determines which method to use based on the gradient clipping config and the current runtime state</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>grad_clip</td>
<td>Union[ClipGradConfig, ClipGradNormConfig]</td>
<td>gradient clipping config that will determine which method to use</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>horovod</td>
<td>bool</td>
<td>horovod flag</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed</td>
<td>bool</td>
<td>deepspeed flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad(</p>
<div class="highlight"><pre><span></span><code>            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )
</code></pre></div>
<h4 id="clip_grad_norm_4">clip_grad_norm</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_norm</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clip gradients by the norm</p>
<p>Depending on some extension flags switch between the correct clip_grad_norm calls</p>
<p>OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html
FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>max_norm</td>
<td>Union[float, int]</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>Union[float, int]</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool, default: False</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool, default: False</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_norm(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Base handle clip gradients by the norm



            Depending on some extension flags switch between the correct clip_grad_norm calls



            OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm

            if oss:

                optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type)

            # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm

            elif fsdp:

                model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)

            else:

                torch.nn.utils.clip_grad_norm_(

                    model.parameters(), max_norm=max_norm, norm_type=norm_type

                )
</code></pre></div>
<h4 id="clip_grad_value_4">clip_grad_value</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_value</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clip gradients by value</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>clip_value</td>
<td>float</td>
<td>absolute value to clip grads</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_value(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Base handle clip gradients by value



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)
</code></pre></div>
<h4 id="step_call_4">step_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Override of deepspeed wrapped backward call</p>
<p>Deepspeed calls step via the model engine instead of the optimizer</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_call(</p>
<div class="highlight"><pre><span></span><code>            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Override of deepspeed wrapped backward call



            Deepspeed calls step via the model engine instead of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            model.step()
</code></pre></div>
<h4 id="wrap_fp16_4">wrap_fp16</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_fp16</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps model and optimizer with specific mixed-precision related backend wrappers</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>modified version of model object for mixed-precision backends</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_fp16(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with specific mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            return model, optimizer
</code></pre></div>
<h3 id="nativeampfp16">NativeAmpFP16</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">NativeAmpFP16</span><span class="p">(</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_5">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_amp_config</td>
<td>AMPConfig</td>
<td>Configuration object for Apex</td>
<td>None</td>
</tr>
<tr>
<td>_scaler</td>
<td>default: torch.cuda.amp.GradScaler</td>
<td>scaler object for loss</td>
<td>None</td>
</tr>
<tr>
<td>_verbose bool, default</td>
<td>True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class NativeAmpFP16(BaseFP16):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Base class for PyTorch Native AMP FP16 methods



        This class handles base and common functionality for native PyTorch AMP mixed-precision backends. Contains

        functionality related to gradient clipping, backward call, step call, and context wrappers for the model forward

        and loss calls



        Attributes

        ----------

        loss_context

        model_context

        scaler

        _amp_config: AMPConfig

            Configuration object for Apex

        _scaler: default: torch.cuda.amp.GradScaler

            scaler object for loss

        _verbose bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for NativeAmpFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call -- here amp_config or sharded_config might be passed in



            Notes

            -----

            Scaler set between torch.cuda.amp.GradScaler and ShardedGradScaler depending on if a sharded config is passed

            via kwargs



            &quot;&quot;&quot;

            self._amp_config = kwargs[&quot;amp_config&quot;]

            # Switch the scaler obj ref depending on fairscale sharding

            scaler = (

                ShardedGradScaler

                if (kwargs[&quot;sharded_config&quot;] is not None)

                or (kwargs[&quot;fully_sharded_config&quot;] is not None)

                else torch.cuda.amp.GradScaler

            )

            super(NativeAmpFP16, self).__init__(

                scaler=scaler(

                    backoff_factor=self._amp_config.backoff_factor,

                    enabled=True,

                    growth_factor=self._amp_config.growth_factor,

                    growth_interval=self._amp_config.growth_interval,

                    init_scale=self._amp_config.init_scale,

                ),

                verbose=verbose,

            )



        @property

        def loss_context(self):

            &quot;&quot;&quot;Overrides base and returns the native AMP autocast context&quot;&quot;&quot;

            return torch.cuda.amp.autocast(enabled=True)



        @property

        def model_context(self):

            &quot;&quot;&quot;Overrides base and returns the native AMP autocast context&quot;&quot;&quot;

            return torch.cuda.amp.autocast(enabled=True)



        def backward_call(

            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Overrides base wrapped backward call for AMP scaled backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    self._scaler.scale(val).backward(retain_graph=(idx == 0))

            else:

                self._scaler.scale(loss).backward()



        def step_call(

            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Overrides base wrapped step of the optimizer with the AMP scaler version



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.scaler.step(optimizer)

            self.scaler.update()
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_5">Ancestors (in MRO)</h4>
<ul>
<li>stoke.fp16.BaseFP16</li>
<li>abc.ABC</li>
</ul>
<h4 id="instance-variables_5">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">loss_context</span>
</code></pre></div>
<p>Overrides base and returns the native AMP autocast context</p>
<div class="highlight"><pre><span></span><code><span class="n">model_context</span>
</code></pre></div>
<p>Overrides base and returns the native AMP autocast context</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Returns grad scaler</p>
<h4 id="methods_5">Methods</h4>
<h4 id="backward_call_5">backward_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Overrides base wrapped backward call for AMP scaled backward call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>loss tensor(s)</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward_call(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Overrides base wrapped backward call for AMP scaled backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    self._scaler.scale(val).backward(retain_graph=(idx == 0))

            else:

                self._scaler.scale(loss).backward()
</code></pre></div>
<h4 id="clip_grad_5">clip_grad</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">horovod</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">deepspeed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clipping the current gradients</p>
<p>Determines which method to use based on the gradient clipping config and the current runtime state</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>grad_clip</td>
<td>Union[ClipGradConfig, ClipGradNormConfig]</td>
<td>gradient clipping config that will determine which method to use</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>horovod</td>
<td>bool</td>
<td>horovod flag</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed</td>
<td>bool</td>
<td>deepspeed flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad(</p>
<div class="highlight"><pre><span></span><code>            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )
</code></pre></div>
<h4 id="clip_grad_norm_5">clip_grad_norm</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_norm</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clip gradients by the norm</p>
<p>Depending on some extension flags switch between the correct clip_grad_norm calls</p>
<p>OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html
FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>max_norm</td>
<td>Union[float, int]</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>Union[float, int]</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool, default: False</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool, default: False</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_norm(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Base handle clip gradients by the norm



            Depending on some extension flags switch between the correct clip_grad_norm calls



            OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm

            if oss:

                optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type)

            # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm

            elif fsdp:

                model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)

            else:

                torch.nn.utils.clip_grad_norm_(

                    model.parameters(), max_norm=max_norm, norm_type=norm_type

                )
</code></pre></div>
<h4 id="clip_grad_value_5">clip_grad_value</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_value</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clip gradients by value</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>clip_value</td>
<td>float</td>
<td>absolute value to clip grads</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_value(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Base handle clip gradients by value



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)
</code></pre></div>
<h4 id="step_call_5">step_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Overrides base wrapped step of the optimizer with the AMP scaler version</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_call(</p>
<div class="highlight"><pre><span></span><code>            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Overrides base wrapped step of the optimizer with the AMP scaler version



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            self.scaler.step(optimizer)

            self.scaler.update()
</code></pre></div>
<h4 id="wrap_fp16_5">wrap_fp16</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_fp16</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps model and optimizer with specific mixed-precision related backend wrappers</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>modified version of model object for mixed-precision backends</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_fp16(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with specific mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            return model, optimizer
</code></pre></div>
<h3 id="nullfp16">NullFP16</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">NullFP16</span><span class="p">(</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_6">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>model_context</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>scaler</td>
<td>None</td>
<td>None</td>
<td>None</td>
</tr>
<tr>
<td>_scaler</td>
<td>default: None</td>
<td>scaler object for backends that require one</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class NullFP16(BaseFP16):</p>
<div class="highlight"><pre><span></span><code>        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for NullFP16 class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            Notes

            -----

            Scaler set to None as it is not needed



            &quot;&quot;&quot;

            super(NullFP16, self).__init__(scaler=None, verbose=verbose)
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_6">Ancestors (in MRO)</h4>
<ul>
<li>stoke.fp16.BaseFP16</li>
<li>abc.ABC</li>
</ul>
<h4 id="descendants_2">Descendants</h4>
<ul>
<li>stoke.fp16.DeepspeedFP16</li>
</ul>
<h4 id="instance-variables_6">Instance variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">loss_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the loss call</p>
<div class="highlight"><pre><span></span><code><span class="n">model_context</span>
</code></pre></div>
<p>Returns the base context wrapper for the model call</p>
<div class="highlight"><pre><span></span><code><span class="n">scaler</span>
</code></pre></div>
<p>Returns grad scaler</p>
<h4 id="methods_6">Methods</h4>
<h4 id="backward_call_6">backward_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">backward_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Base wrapped backward call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>loss</td>
<td>loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]</td>
<td>loss tensor(s)</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def backward_call(</p>
<div class="highlight"><pre><span></span><code>            self,

            loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

        ):

            &quot;&quot;&quot;Base wrapped backward call



            Parameters

            ----------

            loss: loss: Union[torch.Tensor, List[torch.Tensor], Tuple[torch.Tensor]]

                loss tensor(s)

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            if isinstance(loss, (list, tuple)):

                for idx, val in enumerate(loss):

                    val.backward(retain_graph=(idx == 0))

            else:

                loss.backward()
</code></pre></div>
<h4 id="clip_grad_6">clip_grad</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">grad_clip</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradConfig</span><span class="p">,</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">ClipGradNormConfig</span><span class="p">],</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">horovod</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">deepspeed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clipping the current gradients</p>
<p>Determines which method to use based on the gradient clipping config and the current runtime state</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>grad_clip</td>
<td>Union[ClipGradConfig, ClipGradNormConfig]</td>
<td>gradient clipping config that will determine which method to use</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>horovod</td>
<td>bool</td>
<td>horovod flag</td>
<td>None</td>
</tr>
<tr>
<td>deepspeed</td>
<td>bool</td>
<td>deepspeed flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad(</p>
<div class="highlight"><pre><span></span><code>            self,

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig],

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            oss: bool,

            horovod: bool,

            deepspeed: bool,

            fsdp: bool,

        ):

            &quot;&quot;&quot;Base handle clipping the current gradients



            Determines which method to use based on the gradient clipping config and the current runtime state



            Parameters

            ----------

            grad_clip: Union[ClipGradConfig, ClipGradNormConfig]

                gradient clipping config that will determine which method to use

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            oss: bool

                optimizer state sharding flag

            horovod: bool

                horovod flag

            deepspeed: bool

                deepspeed flag

            fsdp: bool

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if deepspeed:

                if self._verbose:

                    self._print_device(

                        &quot;Letting deepspeed internally handle clipping calculated/accumulated &quot;

                        &quot;gradients...&quot;

                    )

            else:

                if self._verbose:

                    self._print_device(

                        f&#39;{type(grad_clip).__name__.replace(&quot;Config&quot;, &quot;&quot;)} &#39;

                        f&quot;is automatically clipping calculated/accumulated gradients...&quot;

                    )

                if horovod:

                    # Hidden here -- Horovod docs are terrible

                    # https://horovod.readthedocs.io/en/latest/api.html#horovod.torch.DistributedOptimizer

                    if self._verbose:

                        self._print_device(

                            f&quot;Calling Horovod optimizer.synchronize() pre grad-clip&quot;

                        )

                    optimizer.synchronize()

                if isinstance(grad_clip, ClipGradConfig):

                    self.clip_grad_value(

                        model=model, optimizer=optimizer, clip_value=grad_clip.clip_value

                    )

                elif isinstance(grad_clip, ClipGradNormConfig):

                    self.clip_grad_norm(

                        model=model,

                        optimizer=optimizer,

                        max_norm=grad_clip.max_norm,

                        norm_type=grad_clip.norm_type,

                        oss=oss,

                        fsdp=fsdp,

                    )

                else:

                    raise ValueError(

                        f&quot;Stoke -- clip_grad received an incorrect instance type of {type(grad_clip)}&quot;

                    )
</code></pre></div>
<h4 id="clip_grad_norm_6">clip_grad_norm</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_norm</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">max_norm</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">oss</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fsdp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clip gradients by the norm</p>
<p>Depending on some extension flags switch between the correct clip_grad_norm calls</p>
<p>OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html
FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>max_norm</td>
<td>Union[float, int]</td>
<td>max norm of the gradients</td>
<td>None</td>
</tr>
<tr>
<td>norm_type</td>
<td>Union[float, int]</td>
<td>type of the used p-norm</td>
<td>None</td>
</tr>
<tr>
<td>oss</td>
<td>bool, default: False</td>
<td>optimizer state sharding flag</td>
<td>None</td>
</tr>
<tr>
<td>fsdp</td>
<td>bool, default: False</td>
<td>fully sharded data parallel flag for Fairscale</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_norm(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            max_norm: Union[float, int],

            norm_type: Union[float, int],

            oss: bool = False,

            fsdp: bool = False,

        ):

            &quot;&quot;&quot;Base handle clip gradients by the norm



            Depending on some extension flags switch between the correct clip_grad_norm calls



            OSS: https://fairscale.readthedocs.io/en/latest/api/optim/oss.html

            FSDP: https://fairscale.readthedocs.io/en/latest/api/nn/fsdp.html



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            max_norm: Union[float, int]

                max norm of the gradients

            norm_type: Union[float, int]

                type of the used p-norm

            oss: bool, default: False

                optimizer state sharding flag

            fsdp: bool, default: False

                fully sharded data parallel flag for Fairscale



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            # need to fallback to the OSS Fairscale implementation for norm as the shards need to sync for the norm

            if oss:

                optimizer.clip_grad_norm(max_norm=max_norm, norm_type=norm_type)

            # need to fallback to the Fairscale FSDP implementation for norm as the shards need to sync for the norm

            elif fsdp:

                model.clip_grad_norm_(max_norm=max_norm, norm_type=norm_type)

            else:

                torch.nn.utils.clip_grad_norm_(

                    model.parameters(), max_norm=max_norm, norm_type=norm_type

                )
</code></pre></div>
<h4 id="clip_grad_value_6">clip_grad_value</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">clip_grad_value</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">clip_value</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">)</span>
</code></pre></div>
<p>Base handle clip gradients by value</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>clip_value</td>
<td>float</td>
<td>absolute value to clip grads</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def clip_grad_value(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            clip_value: float,

        ):

            &quot;&quot;&quot;Base handle clip gradients by value



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object

            clip_value: float

                absolute value to clip grads



            Returns

            -------

            None



            &quot;&quot;&quot;

            if self.scaler is not None:

                if self._verbose:

                    self._print_device(f&quot;Automatically unscaling gradients...&quot;)

                self._scaler.unscale_(optimizer)

            torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=clip_value)
</code></pre></div>
<h4 id="step_call_6">step_call</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">step_call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div>
<p>Base wrapped step of the optimizer</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def step_call(</p>
<div class="highlight"><pre><span></span><code>            self, model: torch.nn.Module, optimizer: Union[torch.optim.Optimizer, OSS]

        ):

            &quot;&quot;&quot;Base wrapped step of the optimizer



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            None



            &quot;&quot;&quot;

            # Step the optimizer

            optimizer.step()
</code></pre></div>
<h4 id="wrap_fp16_6">wrap_fp16</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">wrap_fp16</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps model and optimizer with specific mixed-precision related backend wrappers</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>current optimizer object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>modified version of model object for mixed-precision backends</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def wrap_fp16(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Optional[Union[torch.optim.Optimizer, OSS]] = None,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps model and optimizer with specific mixed-precision related backend wrappers



            Parameters

            ----------

            model: torch.nn.Module

                current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            Returns

            -------

            model: torch.nn.Module

                modified version of model object for mixed-precision backends

            optimizer: Union[torch.optim.Optimizer, OSS]]

                modified version of optimizer object for mixed-precision backends



            &quot;&quot;&quot;

            self._scaler_info()

            return model, optimizer
</code></pre></div>
<h3 id="runnerfp16enum">RunnerFP16Enum</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">RunnerFP16Enum</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class RunnerFP16Enum(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Enum for building the runtime object with mixed-precision functionality&quot;&quot;&quot;



        full = NullFP16

        apex_O1 = ApexO1AmpFP16

        apex_O2 = ApexO2AmpFP16

        amp = NativeAmpFP16

        deepspeed = DeepspeedFP16
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_7">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">amp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">apex_O1</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">apex_O2</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">deepspeed</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">full</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../extensions/" title="Extensions" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Extensions
              </span>
            </div>
          </a>
        
        
          <a href="../" title="Index" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Index
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ncilfone" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/ncilfone/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
    
  </body>
</html>