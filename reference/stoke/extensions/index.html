
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../../resources/images/stoke_small.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Extensions - stoke</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="red">
  
    
    <script>function __prefix(e){return new URL("../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-stokeextensions" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="stoke" class="md-header__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            stoke
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Extensions
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Home/" class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Installation/" class="md-tabs__link">
      Installation
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Quick-Start/" class="md-tabs__link">
      Quick Start
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Launchers/" class="md-tabs__link">
      Launchers
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../docs/Examples/" class="md-tabs__link">
      Examples
    </a>
  </li>

      
        
  
  


  <li class="md-tabs__item">
    <a href="../../../CONTRIBUTING/" class="md-tabs__link">
      Contributing
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../configs/" class="md-tabs__link md-tabs__link--active">
        Reference
      </a>
    </li>
  

  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="stoke" class="md-nav__button md-logo" aria-label="stoke" data-md-component="logo">
      
  <img src="../../../resources/images/stoke_small.png" alt="logo">

    </a>
    stoke
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/fidelity/stoke/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    stoke
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Home/" class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Installation/" class="md-nav__link">
        Installation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Quick-Start/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Launchers/" class="md-nav__link">
        Launchers
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Examples/" class="md-nav__link">
        Examples
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7_1" type="checkbox" id="__nav_7_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_7_1">
          Stoke
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stoke" data-md-level="2">
        <label class="md-nav__title" for="__nav_7_1">
          <span class="md-nav__icon md-icon"></span>
          Stoke
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../configs/" class="md-nav__link">
        Configs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../distributed/" class="md-nav__link">
        Distributed
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Extensions
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Extensions
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#baseddp" class="md-nav__link">
    BaseDDP
  </a>
  
    <nav class="md-nav" aria-label="BaseDDP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handle_ddp" class="md-nav__link">
    handle_ddp
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#baseoptimizer" class="md-nav__link">
    BaseOptimizer
  </a>
  
    <nav class="md-nav" aria-label="BaseOptimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build_optimizer" class="md-nav__link">
    build_optimizer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedhandlerenum" class="md-nav__link">
    DistributedHandlerEnum
  </a>
  
    <nav class="md-nav" aria-label="DistributedHandlerEnum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalefsdpextension" class="md-nav__link">
    FairscaleFSDPExtension
  </a>
  
    <nav class="md-nav" aria-label="FairscaleFSDPExtension">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handle_ddp_1" class="md-nav__link">
    handle_ddp
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscaleossextension" class="md-nav__link">
    FairscaleOSSExtension
  </a>
  
    <nav class="md-nav" aria-label="FairscaleOSSExtension">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build_optimizer_1" class="md-nav__link">
    build_optimizer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalesddpextension" class="md-nav__link">
    FairscaleSDDPExtension
  </a>
  
    <nav class="md-nav" aria-label="FairscaleSDDPExtension">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handle_ddp_2" class="md-nav__link">
    handle_ddp
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runneroptimizerenum" class="md-nav__link">
    RunnerOptimizerEnum
  </a>
  
    <nav class="md-nav" aria-label="RunnerOptimizerEnum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_1" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../fp16/" class="md-nav__link">
        Fp16
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../io/" class="md-nav__link">
        Io
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../status/" class="md-nav__link">
        Status
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../stoke/" class="md-nav__link">
        Stoke
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#baseddp" class="md-nav__link">
    BaseDDP
  </a>
  
    <nav class="md-nav" aria-label="BaseDDP">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handle_ddp" class="md-nav__link">
    handle_ddp
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#baseoptimizer" class="md-nav__link">
    BaseOptimizer
  </a>
  
    <nav class="md-nav" aria-label="BaseOptimizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_1" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build_optimizer" class="md-nav__link">
    build_optimizer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributedhandlerenum" class="md-nav__link">
    DistributedHandlerEnum
  </a>
  
    <nav class="md-nav" aria-label="DistributedHandlerEnum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalefsdpextension" class="md-nav__link">
    FairscaleFSDPExtension
  </a>
  
    <nav class="md-nav" aria-label="FairscaleFSDPExtension">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_2" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handle_ddp_1" class="md-nav__link">
    handle_ddp
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscaleossextension" class="md-nav__link">
    FairscaleOSSExtension
  </a>
  
    <nav class="md-nav" aria-label="FairscaleOSSExtension">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_3" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_3" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#build_optimizer_1" class="md-nav__link">
    build_optimizer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fairscalesddpextension" class="md-nav__link">
    FairscaleSDDPExtension
  </a>
  
    <nav class="md-nav" aria-label="FairscaleSDDPExtension">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attributes_4" class="md-nav__link">
    Attributes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_4" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handle_ddp_2" class="md-nav__link">
    handle_ddp
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#runneroptimizerenum" class="md-nav__link">
    RunnerOptimizerEnum
  </a>
  
    <nav class="md-nav" aria-label="RunnerOptimizerEnum">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_3" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#class-variables_1" class="md-nav__link">
    Class variables
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/fidelity/stoke/edit/main/reference/stoke/extensions.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="module-stokeextensions">Module stoke.extensions</h1>
<p>Handles extension wrapper related classes -- mixin style</p>
<p>None</p>
<p>??? example "View Source"
        # -<em>- coding: utf-8 -</em>-</p>
<div class="highlight"><pre><span></span><code>    # Copyright FMR LLC &lt;opensource@fidelity.com&gt;

    # SPDX-License-Identifier: Apache-2.0



    &quot;&quot;&quot;Handles extension wrapper related classes -- mixin style&quot;&quot;&quot;



    from abc import ABC

    from enum import Enum

    from typing import Dict, Optional, Tuple, Type, Union



    import attr

    import torch

    from fairscale.nn.data_parallel import FullyShardedDataParallel, ShardedDataParallel

    from fairscale.optim.oss import OSS



    from stoke.configs import (

        DDPConfig,

        FairscaleFSDPConfig,

        FairscaleOSSConfig,

        FairscaleSDDPConfig,

    )





    @attr.s(auto_attribs=True)

    class _FairscaleFSDPConfig(FairscaleFSDPConfig):

        mixed_precision: bool = False





    class BaseOptimizer(ABC):

        &quot;&quot;&quot;Base class for creating an optimizer



        Attributes

        ----------

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for BaseOptimizer class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            self._verbose = verbose



        def build_optimizer(

            self,

            optimizer: Type[torch.optim.Optimizer],

            optimizer_kwargs: Dict,

            model: torch.nn.Module,

        ) -&gt; torch.optim.Optimizer:

            &quot;&quot;&quot;Instantiates a torch optimizer object from the type and optimizer kwargs



            Parameters

            ----------

            optimizer: Type[torch.optim.Optimizer]

                type of torch optimizer

            optimizer_kwargs: Dict

                dictionary of all kwargs to pass to the optimizer

            model: torch.nn.Module

                model object



            Returns

            -------

            torch.optim.Optimizer

                instantiated torch optimizer object



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(f&quot;Creating basic torch optimizer: {optimizer.__name__}&quot;)

            return optimizer(params=model.parameters(), **optimizer_kwargs)





    class FairscaleOSSExtension(BaseOptimizer):

        &quot;&quot;&quot;Inherits from BaseOptimizer for OSS class creation



        Attributes

        ----------

        _oss_config: FairscaleOSSConfig,

            Configuration object for Fairscale OSS

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, oss_config: FairscaleOSSConfig, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for FairscaleOSSExtension class



            Parameters

            ----------

            oss_config: FairscaleOSSConfig

                Configuration object for Fairscale OSS

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            super(FairscaleOSSExtension, self).__init__(verbose=verbose)

            self._oss_config = oss_config



        def build_optimizer(

            self,

            optimizer: Type[torch.optim.Optimizer],

            optimizer_kwargs: Dict,

            model: torch.nn.Module,

        ) -&gt; OSS:

            &quot;&quot;&quot;Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs



            Parameters

            ----------

            optimizer: Type[torch.optim.Optimizer]

                type of torch optimizer

            optimizer_kwargs: Dict

                dictionary of all kwargs to pass to the optimizer

            model: torch.nn.Module

                model object



            Returns

            -------

            OSS

                instantiated Fairscale OSS optimizer object



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}&quot;

                )

            return OSS(

                params=model.parameters(),

                optim=optimizer,

                broadcast_fp16=self._oss_config.broadcast_fp16,

                **optimizer_kwargs,

            )





    class RunnerOptimizerEnum(Enum):

        &quot;&quot;&quot;Enum for optimizer creation&quot;&quot;&quot;



        oss = FairscaleOSSExtension

        base = BaseOptimizer





    class BaseDDP:

        &quot;&quot;&quot;Base class for using the DDP backend



        Attributes

        ----------

        _ddp_config: DDPConfig

            Base DDP configuration object

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, ddp_config: DDPConfig, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for BaseDDP



            Parameters

            ----------

            ddp_config: DDPConfig

                Base DDP configuration object

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            self._verbose = verbose

            self._ddp_config = ddp_config



        def handle_ddp(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the base DDP call



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = torch.nn.parallel.DistributedDataParallel(

                module=model,

                device_ids=[rank],

                output_device=rank,

                bucket_cap_mb=self._ddp_config.bucket_cap_mb,

                broadcast_buffers=self._ddp_config.broadcast_buffers,

                find_unused_parameters=self._ddp_config.find_unused_parameters,

                gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view,

            )

            return model, optimizer





    class FairscaleSDDPExtension:

        &quot;&quot;&quot;Class for using the Fairscale SDDP backend



        Attributes

        ----------

        _sddp_config: FairscaleSDDPConfig

            Base Fairscale ShardedDataParallel configuration object

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self, sddp_config: FairscaleSDDPConfig, verbose: bool = True, **kwargs

        ):

            &quot;&quot;&quot;Init for FairscaleSDDPExtension



            Parameters

            ----------

            sddp_config: FairscaleSDDPConfig

                Base Fairscale ShardedDataParallel configuration objet

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            self._verbose = verbose

            self._sddp_config = sddp_config



        def handle_ddp(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the ShardedDataParallel call



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = ShardedDataParallel(

                module=model,

                sharded_optimizer=optimizer,

                broadcast_buffers=self._sddp_config.broadcast_buffers,

                sync_models_at_startup=self._sddp_config.sync_models_at_startup,

                reduce_buffer_size=self._sddp_config.reduce_buffer_size,

                auto_refresh_trainable=self._sddp_config.auto_refresh_trainable,

                reduce_fp16=self._sddp_config.reduce_fp16,

            )

            return model, optimizer





    class FairscaleFSDPExtension:

        &quot;&quot;&quot;Class for using the Fairscale FSDP backend



        Attributes

        ----------

        _fsdp_config: _FairscaleFSDPConfig

            Base Fairscale Fully Sharded Data Parallel configuration object

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self, fsdp_config: _FairscaleFSDPConfig, verbose: bool = True, **kwargs

        ):

            &quot;&quot;&quot;Init for FairscaleSDDPExtension



            Parameters

            ----------

            _fsdp_config: _FairscaleFSDPConfig

                Base Fairscale Fully Sharded Data Parallel configuration object

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            self._verbose = verbose

            self._fsdpp_config = fsdp_config



        def handle_ddp(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the FullyShardedDataParallel call



            Also sets grad divide factors

            https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = FullyShardedDataParallel(

                module=model,

                reshard_after_forward=self._fsdpp_config.reshard_after_forward,

                mixed_precision=self._fsdpp_config.mixed_precision,

                fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter,

                flatten_parameters=self._fsdpp_config.flatten_parameters,

                move_params_to_cpu=self._fsdpp_config.move_params_to_cpu,

                compute_dtype=self._fsdpp_config.compute_dtype,

                buffer_dtype=self._fsdpp_config.buffer_dtype,

                move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu,

                bucket_cap_mb=self._fsdpp_config.bucket_cap_mb,

                no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state,

                clear_autocast_cache=self._fsdpp_config.clear_autocast_cache,

                force_input_to_fp32=self._fsdpp_config.force_input_to_fp32,

                verbose=self._fsdpp_config.verbose,

            )

            # Trigger the set of pre-divide or post-divide factors if set in the config

            model.set_gradient_divide_factors(

                pre=self._fsdpp_config.gradient_predivide_factor

                if self._fsdpp_config.gradient_predivide_factor is not None

                else model.gradient_predivide_factor,

                post=self._fsdpp_config.gradient_postdivide_factor

                if self._fsdpp_config.gradient_postdivide_factor is not None

                else model.gradient_postdivide_factor,

                recursive=True,

            )

            return model, optimizer





    class DistributedHandlerEnum(Enum):

        &quot;&quot;&quot;Enum for DDP use&quot;&quot;&quot;



        sddp = FairscaleSDDPExtension

        fsdp = FairscaleFSDPExtension

        base = BaseDDP
</code></pre></div>
<h2 id="classes">Classes</h2>
<h3 id="baseddp">BaseDDP</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BaseDDP</span><span class="p">(</span>
    <span class="n">ddp_config</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">DDPConfig</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>_ddp_config</td>
<td>DDPConfig</td>
<td>Base DDP configuration object</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class BaseDDP:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Base class for using the DDP backend



        Attributes

        ----------

        _ddp_config: DDPConfig

            Base DDP configuration object

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, ddp_config: DDPConfig, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for BaseDDP



            Parameters

            ----------

            ddp_config: DDPConfig

                Base DDP configuration object

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            self._verbose = verbose

            self._ddp_config = ddp_config



        def handle_ddp(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the base DDP call



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = torch.nn.parallel.DistributedDataParallel(

                module=model,

                device_ids=[rank],

                output_device=rank,

                bucket_cap_mb=self._ddp_config.bucket_cap_mb,

                broadcast_buffers=self._ddp_config.broadcast_buffers,

                find_unused_parameters=self._ddp_config.find_unused_parameters,

                gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view,

            )

            return model, optimizer
</code></pre></div>
<hr />
<h4 id="methods">Methods</h4>
<h4 id="handle_ddp">handle_ddp</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">handle_ddp</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps the model in the base DDP call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>Current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>Current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>int</td>
<td>Current CUDA device rank in the distributed setup</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>Wrapped model object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def handle_ddp(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the base DDP call



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = torch.nn.parallel.DistributedDataParallel(

                module=model,

                device_ids=[rank],

                output_device=rank,

                bucket_cap_mb=self._ddp_config.bucket_cap_mb,

                broadcast_buffers=self._ddp_config.broadcast_buffers,

                find_unused_parameters=self._ddp_config.find_unused_parameters,

                gradient_as_bucket_view=self._ddp_config.gradient_as_bucket_view,

            )

            return model, optimizer
</code></pre></div>
<h3 id="baseoptimizer">BaseOptimizer</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">BaseOptimizer</span><span class="p">(</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_1">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class BaseOptimizer(ABC):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Base class for creating an optimizer



        Attributes

        ----------

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for BaseOptimizer class



            Parameters

            ----------

            verbose: bool, default: True

                flag for verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            self._verbose = verbose



        def build_optimizer(

            self,

            optimizer: Type[torch.optim.Optimizer],

            optimizer_kwargs: Dict,

            model: torch.nn.Module,

        ) -&gt; torch.optim.Optimizer:

            &quot;&quot;&quot;Instantiates a torch optimizer object from the type and optimizer kwargs



            Parameters

            ----------

            optimizer: Type[torch.optim.Optimizer]

                type of torch optimizer

            optimizer_kwargs: Dict

                dictionary of all kwargs to pass to the optimizer

            model: torch.nn.Module

                model object



            Returns

            -------

            torch.optim.Optimizer

                instantiated torch optimizer object



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(f&quot;Creating basic torch optimizer: {optimizer.__name__}&quot;)

            return optimizer(params=model.parameters(), **optimizer_kwargs)
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>abc.ABC</li>
</ul>
<h4 id="descendants">Descendants</h4>
<ul>
<li>stoke.extensions.FairscaleOSSExtension</li>
</ul>
<h4 id="methods_1">Methods</h4>
<h4 id="build_optimizer">build_optimizer</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">build_optimizer</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">],</span>
    <span class="n">optimizer_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span>
</code></pre></div>
<p>Instantiates a torch optimizer object from the type and optimizer kwargs</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Type[torch.optim.Optimizer]</td>
<td>type of torch optimizer</td>
<td>None</td>
</tr>
<tr>
<td>optimizer_kwargs</td>
<td>Dict</td>
<td>dictionary of all kwargs to pass to the optimizer</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>model object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.optim.Optimizer</td>
<td>instantiated torch optimizer object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def build_optimizer(</p>
<div class="highlight"><pre><span></span><code>            self,

            optimizer: Type[torch.optim.Optimizer],

            optimizer_kwargs: Dict,

            model: torch.nn.Module,

        ) -&gt; torch.optim.Optimizer:

            &quot;&quot;&quot;Instantiates a torch optimizer object from the type and optimizer kwargs



            Parameters

            ----------

            optimizer: Type[torch.optim.Optimizer]

                type of torch optimizer

            optimizer_kwargs: Dict

                dictionary of all kwargs to pass to the optimizer

            model: torch.nn.Module

                model object



            Returns

            -------

            torch.optim.Optimizer

                instantiated torch optimizer object



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(f&quot;Creating basic torch optimizer: {optimizer.__name__}&quot;)

            return optimizer(params=model.parameters(), **optimizer_kwargs)
</code></pre></div>
<h3 id="distributedhandlerenum">DistributedHandlerEnum</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">DistributedHandlerEnum</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class DistributedHandlerEnum(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Enum for DDP use&quot;&quot;&quot;



        sddp = FairscaleSDDPExtension

        fsdp = FairscaleFSDPExtension

        base = BaseDDP
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">base</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">fsdp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">sddp</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
<h3 id="fairscalefsdpextension">FairscaleFSDPExtension</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleFSDPExtension</span><span class="p">(</span>
    <span class="n">fsdp_config</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">_FairscaleFSDPConfig</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_2">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>_fsdp_config</td>
<td>_FairscaleFSDPConfig</td>
<td>Base Fairscale Fully Sharded Data Parallel configuration object</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleFSDPExtension:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Class for using the Fairscale FSDP backend



        Attributes

        ----------

        _fsdp_config: _FairscaleFSDPConfig

            Base Fairscale Fully Sharded Data Parallel configuration object

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self, fsdp_config: _FairscaleFSDPConfig, verbose: bool = True, **kwargs

        ):

            &quot;&quot;&quot;Init for FairscaleSDDPExtension



            Parameters

            ----------

            _fsdp_config: _FairscaleFSDPConfig

                Base Fairscale Fully Sharded Data Parallel configuration object

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            self._verbose = verbose

            self._fsdpp_config = fsdp_config



        def handle_ddp(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the FullyShardedDataParallel call



            Also sets grad divide factors

            https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = FullyShardedDataParallel(

                module=model,

                reshard_after_forward=self._fsdpp_config.reshard_after_forward,

                mixed_precision=self._fsdpp_config.mixed_precision,

                fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter,

                flatten_parameters=self._fsdpp_config.flatten_parameters,

                move_params_to_cpu=self._fsdpp_config.move_params_to_cpu,

                compute_dtype=self._fsdpp_config.compute_dtype,

                buffer_dtype=self._fsdpp_config.buffer_dtype,

                move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu,

                bucket_cap_mb=self._fsdpp_config.bucket_cap_mb,

                no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state,

                clear_autocast_cache=self._fsdpp_config.clear_autocast_cache,

                force_input_to_fp32=self._fsdpp_config.force_input_to_fp32,

                verbose=self._fsdpp_config.verbose,

            )

            # Trigger the set of pre-divide or post-divide factors if set in the config

            model.set_gradient_divide_factors(

                pre=self._fsdpp_config.gradient_predivide_factor

                if self._fsdpp_config.gradient_predivide_factor is not None

                else model.gradient_predivide_factor,

                post=self._fsdpp_config.gradient_postdivide_factor

                if self._fsdpp_config.gradient_postdivide_factor is not None

                else model.gradient_postdivide_factor,

                recursive=True,

            )

            return model, optimizer
</code></pre></div>
<hr />
<h4 id="methods_2">Methods</h4>
<h4 id="handle_ddp_1">handle_ddp</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">handle_ddp</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps the model in the FullyShardedDataParallel call</p>
<p>Also sets grad divide factors
https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>Current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>Current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>int</td>
<td>Current CUDA device rank in the distributed setup</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>Wrapped model object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def handle_ddp(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the FullyShardedDataParallel call



            Also sets grad divide factors

            https://fairscale.readthedocs.io/en/latest/_modules/fairscale/nn/data_parallel/fully_sharded_data_parallel.html#FullyShardedDataParallel.set_gradient_divide_factors



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = FullyShardedDataParallel(

                module=model,

                reshard_after_forward=self._fsdpp_config.reshard_after_forward,

                mixed_precision=self._fsdpp_config.mixed_precision,

                fp32_reduce_scatter=self._fsdpp_config.fp32_reduce_scatter,

                flatten_parameters=self._fsdpp_config.flatten_parameters,

                move_params_to_cpu=self._fsdpp_config.move_params_to_cpu,

                compute_dtype=self._fsdpp_config.compute_dtype,

                buffer_dtype=self._fsdpp_config.buffer_dtype,

                move_grads_to_cpu=self._fsdpp_config.move_grads_to_cpu,

                bucket_cap_mb=self._fsdpp_config.bucket_cap_mb,

                no_broadcast_optim_state=self._fsdpp_config.no_broadcast_optim_state,

                clear_autocast_cache=self._fsdpp_config.clear_autocast_cache,

                force_input_to_fp32=self._fsdpp_config.force_input_to_fp32,

                verbose=self._fsdpp_config.verbose,

            )

            # Trigger the set of pre-divide or post-divide factors if set in the config

            model.set_gradient_divide_factors(

                pre=self._fsdpp_config.gradient_predivide_factor

                if self._fsdpp_config.gradient_predivide_factor is not None

                else model.gradient_predivide_factor,

                post=self._fsdpp_config.gradient_postdivide_factor

                if self._fsdpp_config.gradient_postdivide_factor is not None

                else model.gradient_postdivide_factor,

                recursive=True,

            )

            return model, optimizer
</code></pre></div>
<h3 id="fairscaleossextension">FairscaleOSSExtension</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleOSSExtension</span><span class="p">(</span>
    <span class="n">oss_config</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleOSSConfig</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_3">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>_oss_config</td>
<td>FairscaleOSSConfig,</td>
<td>Configuration object for Fairscale OSS</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleOSSExtension(BaseOptimizer):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Inherits from BaseOptimizer for OSS class creation



        Attributes

        ----------

        _oss_config: FairscaleOSSConfig,

            Configuration object for Fairscale OSS

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(self, oss_config: FairscaleOSSConfig, verbose: bool = True, **kwargs):

            &quot;&quot;&quot;Init for FairscaleOSSExtension class



            Parameters

            ----------

            oss_config: FairscaleOSSConfig

                Configuration object for Fairscale OSS

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            super(FairscaleOSSExtension, self).__init__(verbose=verbose)

            self._oss_config = oss_config



        def build_optimizer(

            self,

            optimizer: Type[torch.optim.Optimizer],

            optimizer_kwargs: Dict,

            model: torch.nn.Module,

        ) -&gt; OSS:

            &quot;&quot;&quot;Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs



            Parameters

            ----------

            optimizer: Type[torch.optim.Optimizer]

                type of torch optimizer

            optimizer_kwargs: Dict

                dictionary of all kwargs to pass to the optimizer

            model: torch.nn.Module

                model object



            Returns

            -------

            OSS

                instantiated Fairscale OSS optimizer object



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}&quot;

                )

            return OSS(

                params=model.parameters(),

                optim=optimizer,

                broadcast_fp16=self._oss_config.broadcast_fp16,

                **optimizer_kwargs,

            )
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_2">Ancestors (in MRO)</h4>
<ul>
<li>stoke.extensions.BaseOptimizer</li>
<li>abc.ABC</li>
</ul>
<h4 id="methods_3">Methods</h4>
<h4 id="build_optimizer_1">build_optimizer</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">build_optimizer</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">],</span>
    <span class="n">optimizer_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span>
</code></pre></div>
<p>Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>optimizer</td>
<td>Type[torch.optim.Optimizer]</td>
<td>type of torch optimizer</td>
<td>None</td>
</tr>
<tr>
<td>optimizer_kwargs</td>
<td>Dict</td>
<td>dictionary of all kwargs to pass to the optimizer</td>
<td>None</td>
</tr>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>model object</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>OSS</td>
<td>instantiated Fairscale OSS optimizer object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def build_optimizer(</p>
<div class="highlight"><pre><span></span><code>            self,

            optimizer: Type[torch.optim.Optimizer],

            optimizer_kwargs: Dict,

            model: torch.nn.Module,

        ) -&gt; OSS:

            &quot;&quot;&quot;Instantiates a Fairscale OSS optimizer object from the type and optimizer kwargs



            Parameters

            ----------

            optimizer: Type[torch.optim.Optimizer]

                type of torch optimizer

            optimizer_kwargs: Dict

                dictionary of all kwargs to pass to the optimizer

            model: torch.nn.Module

                model object



            Returns

            -------

            OSS

                instantiated Fairscale OSS optimizer object



            &quot;&quot;&quot;

            if self._verbose:

                self._print_device(

                    f&quot;Creating Fairscale OSS wrapped PyTorch optimizer: {optimizer.__name__}&quot;

                )

            return OSS(

                params=model.parameters(),

                optim=optimizer,

                broadcast_fp16=self._oss_config.broadcast_fp16,

                **optimizer_kwargs,

            )
</code></pre></div>
<h3 id="fairscalesddpextension">FairscaleSDDPExtension</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">FairscaleSDDPExtension</span><span class="p">(</span>
    <span class="n">sddp_config</span><span class="p">:</span> <span class="n">stoke</span><span class="o">.</span><span class="n">configs</span><span class="o">.</span><span class="n">FairscaleSDDPConfig</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<h4 id="attributes_4">Attributes</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>_sddp_config</td>
<td>FairscaleSDDPConfig</td>
<td>Base Fairscale ShardedDataParallel configuration object</td>
<td>None</td>
</tr>
<tr>
<td>_verbose</td>
<td>bool, default: True</td>
<td>flag for Stoke print verbosity</td>
<td>None</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
        class FairscaleSDDPExtension:</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Class for using the Fairscale SDDP backend



        Attributes

        ----------

        _sddp_config: FairscaleSDDPConfig

            Base Fairscale ShardedDataParallel configuration object

        _verbose: bool, default: True

            flag for Stoke print verbosity



        &quot;&quot;&quot;



        def __init__(

            self, sddp_config: FairscaleSDDPConfig, verbose: bool = True, **kwargs

        ):

            &quot;&quot;&quot;Init for FairscaleSDDPExtension



            Parameters

            ----------

            sddp_config: FairscaleSDDPConfig

                Base Fairscale ShardedDataParallel configuration objet

            verbose: bool, default: True

                flag for Stoke print verbosity

            **kwargs: dict, optional

                Extra arguments passed to the __init__ call



            &quot;&quot;&quot;

            self._verbose = verbose

            self._sddp_config = sddp_config



        def handle_ddp(

            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the ShardedDataParallel call



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = ShardedDataParallel(

                module=model,

                sharded_optimizer=optimizer,

                broadcast_buffers=self._sddp_config.broadcast_buffers,

                sync_models_at_startup=self._sddp_config.sync_models_at_startup,

                reduce_buffer_size=self._sddp_config.reduce_buffer_size,

                auto_refresh_trainable=self._sddp_config.auto_refresh_trainable,

                reduce_fp16=self._sddp_config.reduce_fp16,

            )

            return model, optimizer
</code></pre></div>
<hr />
<h4 id="methods_4">Methods</h4>
<h4 id="handle_ddp_2">handle_ddp</h4>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">handle_ddp</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">],</span>
    <span class="n">grad_accum</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">NoneType</span><span class="p">],</span>
    <span class="n">rank</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">fairscale</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">oss</span><span class="o">.</span><span class="n">OSS</span><span class="p">]]</span>
</code></pre></div>
<p>Wraps the model in the ShardedDataParallel call</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Description</th>
<th>Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>model</td>
<td>torch.nn.Module</td>
<td>Current model object</td>
<td>None</td>
</tr>
<tr>
<td>optimizer</td>
<td>Union[torch.optim.Optimizer, OSS]</td>
<td>Current optimizer object</td>
<td>None</td>
</tr>
<tr>
<td>grad_accum</td>
<td>int, default: None</td>
<td>Number of gradient accumulation steps</td>
<td>None</td>
</tr>
<tr>
<td>rank</td>
<td>int</td>
<td>Current CUDA device rank in the distributed setup</td>
<td>None</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>torch.nn.Module</td>
<td>Wrapped model object</td>
</tr>
</tbody>
</table>
<p>??? example "View Source"
            def handle_ddp(</p>
<div class="highlight"><pre><span></span><code>            self,

            model: torch.nn.Module,

            optimizer: Union[torch.optim.Optimizer, OSS],

            grad_accum: Optional[int],

            rank: int,

        ) -&gt; Tuple[torch.nn.Module, Union[torch.optim.Optimizer, OSS]]:

            &quot;&quot;&quot;Wraps the model in the ShardedDataParallel call



            Parameters

            ----------

            model: torch.nn.Module

                Current model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                Current optimizer object

            grad_accum: int, default: None

                Number of gradient accumulation steps

            rank: int

                Current CUDA device rank in the distributed setup



            Returns

            -------

            model: torch.nn.Module

                Wrapped model object

            optimizer: Union[torch.optim.Optimizer, OSS]

                current optimizer object



            &quot;&quot;&quot;

            model = ShardedDataParallel(

                module=model,

                sharded_optimizer=optimizer,

                broadcast_buffers=self._sddp_config.broadcast_buffers,

                sync_models_at_startup=self._sddp_config.sync_models_at_startup,

                reduce_buffer_size=self._sddp_config.reduce_buffer_size,

                auto_refresh_trainable=self._sddp_config.auto_refresh_trainable,

                reduce_fp16=self._sddp_config.reduce_fp16,

            )

            return model, optimizer
</code></pre></div>
<h3 id="runneroptimizerenum">RunnerOptimizerEnum</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span> <span class="nc">RunnerOptimizerEnum</span><span class="p">(</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>
<p>??? example "View Source"
        class RunnerOptimizerEnum(Enum):</p>
<div class="highlight"><pre><span></span><code>        &quot;&quot;&quot;Enum for optimizer creation&quot;&quot;&quot;



        oss = FairscaleOSSExtension

        base = BaseOptimizer
</code></pre></div>
<hr />
<h4 id="ancestors-in-mro_3">Ancestors (in MRO)</h4>
<ul>
<li>enum.Enum</li>
</ul>
<h4 id="class-variables_1">Class variables</h4>
<div class="highlight"><pre><span></span><code><span class="n">base</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">name</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">oss</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">value</span>
</code></pre></div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../distributed/" title="Distributed" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Distributed
              </span>
            </div>
          </a>
        
        
          <a href="../fp16/" title="Fp16" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Fp16
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/ncilfone" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/ncilfone/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tabs", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
    
  </body>
</html>